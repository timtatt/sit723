ParentPostId,ParentPostTitle,ParentTags,Id,Title,Tags,ParentBody,Body
"16296753","Can you run GUI applications in a Docker container?","<x11><sandbox><docker><vnc>","66666539","How do I show pygame in cocalc docker? I am trying to run pygame on an x11 file, but it keeps saying ""No Available Video Device.""","<python><docker><pygame><remote-access><remote-debugging>","<p>How can you run GUI applications in a <a href=""http://www.docker.io"" rel=""noreferrer"">Docker</a> container?</p>

<p>Are there any images that set up <code>vncserver</code> or something so that you can - for example - add an extra speedbump sandbox around say Firefox?</p>
","<p>I opened an x11 file in cocalc docker and ran a pygame python file in the x11 terminal. However, it keeps saying &quot;pygame.error: No available video device.&quot; I know that the reason this error might have occurred is because the pygame file does not know where to display the graphics, and I also know that the $DISPLAY variable is related to where to display. However, I don't know what to set $DISPLAY to, or if I am even supposed to do this. How can I show pygame graphics on cocalc and make sure the pygame file finds the x11 display?</p>
"
"23735149","What is the difference between a Docker image and a container?","<docker><docker-container><docker-image>","66733387","Difference between docker image and docker container","<docker>","<p>When using Docker, we start with a base image. We boot it up, create changes and those changes are saved in layers forming another image.</p>

<p>So eventually I have an image for my PostgreSQL instance and an image for my web application, changes to which keep on being persisted.</p>

<p>What is a container?</p>
","<p>Why do we need a docker container?. If a container is same as image why can't we use image directly. And help me understand the basic difference between them</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","67004579","What is the purpose of the EXPOSE command in a Dockerfile?","<docker><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I am just getting started with Docker and I've been trying to read through some other people's Dockerfiles and I occasionally see the command <code>EXPOSE $PORT</code>. Now I noticed that some people add it and some people don't, some tutorials tell you to add that command while others don't even mention it.</p>
<p>I looked up the command in the <a href=""https://docs.docker.com/engine/reference/builder/#expose"" rel=""nofollow noreferrer"">Dockerfile reference</a> for the command and it almost sounds like it's there to just tell whoever is reading the file that this is the intended port that the container will be listening on?</p>
<p>In that case, does the value of the port you expose have any effect on what happens when the container runs?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66550844","MySQL connection error with native database","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have containerized a simple PHP app. It needs to connect to the host's database (localhost) and do some operations. However, while running the app, I have the following error:
<code>Fatal error: Uncaught PDOException: SQLSTATE[HY000] [2002] No such file or directory ...</code>
My docker-compose file looks like this:</p>
<pre><code>version: &quot;3.9&quot;
services:
  app:
    build: .
    stdin_open: true
    tty: true
    network_mode: &quot;host&quot;
    restart: on-failure
</code></pre>
<p>Looks like dockerized app can't connect with localhost database. I added <code>network_mode: &quot;host&quot;</code>, but it didn't help.</p>
<p>I use Docker Desktop and Mac OS.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66639744","translate localhost to host's localhost inside a docker container","<docker><docker-compose><host>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have an application(A) running inside docker(it starts using a docker compose). It needs to communicate with another application(B) which runs on the host's localhost.
But when the application(A) tries to connect to 'localhost' it means that application(A) tries to connect to the localhost inside the docker container.</p>
<p>I don't want to run application B on docker or another loop-back interface.
I don't want to use host network mode either.</p>
<p>How do I translate localhost inside the docker container to localhost on host?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66672038","Docker container can't connect to the host machine database","<mysql><docker><drupal-6>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I tried to deploy a drupal 6 docker container that works with the host machine database (for testing purpose before deploying it to a remote database) but it just doesn't want to work despite following the different advices I found. ( particularly in this question <a href=""https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach"">From inside of a Docker container, how do I connect to the localhost of the machine?</a> when it talks about host mode, so even if the question looks alike it doesn't feel like it is a duplicate )</p>
<p>Dockerfile</p>
<pre><code>FROM php:7.3-apache

COPY . /var/www/html

EXPOSE 80

RUN chmod o+w /var/www/html/sites/default/settings.php &amp;&amp;\
    chmod o+w /var/www/html/sites/default/files &amp;&amp;\
    apt-get update &amp;&amp;\
    apt-get install -y libfreetype6-dev libjpeg62-turbo-dev libpng-dev &amp;&amp;\
    docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ &amp;&amp;\
    docker-php-ext-install mysqli gd mbstring pdo pdo_mysql &amp;&amp;\
    docker-php-ext-enable mysqli
</code></pre>
<p>docker-compose.yml</p>
<pre><code>version: '3.8'

services:
    website:
        build: 
            context: ./drupal-6-lts
            network: host
        volumes:
            - ./drupal-6-lts:/var/www/html
        ports:
            - 8080:80
</code></pre>
<p>For the database connection I specified these settings :</p>
<p>Driver : mysqli
Host : 127.0.0.1:8889 (the mysql port is 8889, I also tried to replace 127.0.0.1 by localhost)
And the right username, password and database name.</p>
<p>The drupal installation does work when I launch it with MAMP only, but it seems that it can't do the link to the db when in the container despite the <code>network: host</code> statement in docker-compose.yml</p>
<p>When I try Do Trung Duc solution I get this error when executing docker compose build :</p>
<blockquote>
<p>services.network_mode must be a mapping</p>
</blockquote>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66976384","Accessing a service host from inside a container","<docker><docker-compose><host><spring-cloud-config>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a Spring Boot config server running in localhost on port 8888. I'd like to access this service from inside a docker container. I cannot put this service inside the same docker-compose and in the same network.</p>
<p>Service: http://localhost:8888/cpo-executor/dev <br/>
Using my ip: <a href=""http://192.168.0.6:8888/cpo-executor/dev"" rel=""nofollow noreferrer"">http://192.168.0.6:8888/cpo-executor/dev</a></p>
<p>I've checked this address on browser and it's working. When I try to access from inside a docker container I got an error:</p>
<pre><code>docker exec -it 7febe846f2ea /bin/bash
curl http://192.168.0.6:8888/cpo-executor/dev
</code></pre>
<p>Error:</p>
<pre><code>curl: (7) Failed to connect to 192.168.0.6 port 8888: Connection timed out
</code></pre>
<p>I've tried to start my containers putting &quot;network_mode: host&quot; in docker-compose but I ended up falling into another error, from one container not communicating with another.</p>
<pre><code>Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.

Caused by: java.net.UnknownHostException: mysql
</code></pre>
<p>How can I access a service from host from inside a docker container?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","66595778","Run docker container on localhost","<docker><flask><kubernetes><deployment>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Below is my DockerFile:</p>
<pre><code>FROM python:3.8.5
COPY . /usr/app/
EXPOSE 5500
WORKDIR /usr/app/
RUN pip install -r requirements.txt
CMD python app.py
</code></pre>
<p>I am running this container using the command &quot;<code>docker run -p 5500:5500 api_name</code>&quot;, after which I get the below message in cmd:
C:\Users\win10\Documents\ML files&gt;docker run -p 5500:5500 api_name</p>
<ul>
<li>Serving Flask app &quot;app&quot; (lazy loading)</li>
<li>Environment: production
WARNING: This is a development server. Do not use it in a production deployment.
Use a production WSGI server instead.</li>
<li>Debug mode: on</li>
<li>Running on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</li>
<li>Restarting with stat</li>
<li>Debugger is active!</li>
<li>Debugger PIN: xxx-xxx-xxx</li>
</ul>
<p>I tried accessing <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> or <a href=""http://127.0.0.1:5500/"" rel=""nofollow noreferrer"">http://127.0.0.1:5500/</a> but &quot;This site can’t be reached&quot;.
PS: I am using docker desktop for windows</p>
"
"25305788","How to avoid reinstalling packages when building Docker image for Python projects?","<python><docker>","66651022","Docker - Python: Posibiility to use locally installed pip packages","<python><docker><dockerfile>","<p>My Dockerfile is something like</p>
<pre><code>FROM my/base

ADD . /srv
RUN pip install -r requirements.txt
RUN python setup.py install

ENTRYPOINT [&quot;run_server&quot;]
</code></pre>
<p>Every time I build a new image, dependencies have to be reinstalled, which could be very slow in my region.</p>
<p>One way I think of to <code>cache</code> packages that have been installed is to override the <code>my/base</code> image with newer images like this:</p>
<pre><code>docker build -t new_image_1 .
docker tag new_image_1 my/base
</code></pre>
<p>So next time I build with this Dockerfile, my/base already has some packages installed.</p>
<p>But this solution has two problems:</p>
<ol>
<li>It is not always possible to override a base image</li>
<li>The base image grow bigger and bigger as newer images are layered on it</li>
</ol>
<p>So what better solution could I use to solve this problem?</p>
<h2>EDIT:</h2>
<p>Some information about the docker on my machine:</p>
<pre><code>☁  test  docker version
Client version: 1.1.2
Client API version: 1.13
Go version (client): go1.2.1
Git commit (client): d84a070
Server version: 1.1.2
Server API version: 1.13
Go version (server): go1.2.1
Git commit (server): d84a070
☁  test  docker info
Containers: 0
Images: 56
Storage Driver: aufs
 Root Dir: /var/lib/docker/aufs
 Dirs: 56
Execution Driver: native-0.2
Kernel Version: 3.13.0-29-generic
WARNING: No swap limit support
</code></pre>
","<p>Hey all im currently building an web application based on flask/mariadb/nginx and so on.
Im currently wondering if you guys know of a way to use the python packages i have installed on my local dev machine.</p>
<p>Currently all is done via the regular requirements.txt in my Dockerfile.</p>
<p>Here is my Compose file:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>version: '3'
services:
  backendserver:
    build:
      context: ./backendServer
      #dockerfile: ./backendServer/Dockerfile
    ports:
     - ""5000:5000""
    networks:
      - db_network

  db:
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: DB_PW
      MYSQL_USER: DB_USER
      MYSQL_DATABASE: DB_NAME
    volumes:
      - ./shared-files:/var/lib/mysql
    networks:
      - db_network
    ports:
      - ""3306:3306""
  pabs_ss:
    restart: always
    env_file: .env
    build: .

    volumes:
      - ./shared-files:/shared-files
    ports:
      - ""5005:5005""
    networks:
      - db_network
      - web_network
  nginx:
    restart: always
    image: ""nginx:latest""
    ports:
      - ""85:85""
    volumes:
      - ./nginx:/etc/nginx/conf.d
    networks:
      - web_network
    depends_on: 
      - pabs_ss
networks:
  db_network:
    driver: bridge
  web_network:
    driver: bridge</code></pre>
</div>
</div>
</p>
<p>Here is my Dockerfile:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>FROM python:3.6

ENV FLASK_APP run.py

COPY run.py gunicorn-cfg.py requirements.txt config.py .env ./
COPY app app
#COPY shared-files/db.sqlite3 /

RUN pip install -r requirements.txt


EXPOSE 5005 8080
CMD [""gunicorn"", ""--config"", ""gunicorn-cfg.py"", ""run:app""]</code></pre>
</div>
</div>
</p>
<p>As you can see im currently using the &quot;normal&quot; way of doing, though im growing quite tired of basically downloading everything everytime.</p>
<p>So any idea of how i can use the python packages from my local machine in the container?</p>
<p>Thanks and advance and so long :)</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","66762567","Dockerfile: Copy directory from outside of docker folder","<docker><dockerfile>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I would like to copy a directory from program files folder to a docker container.</p>
<pre><code>ARG xsrc=&quot;C:\\Program Files (x86)\\x&quot;
ARG xdstdst=&quot;/home/workdir/.wine/Program Files (x86)/x&quot;
RUN mkdir -p ${xdst}

COPY ${xsrc} ${xdst}
</code></pre>
<pre class=""lang-sh prettyprint-override""><code>docker build -t x .
</code></pre>
<p>Results in this error.</p>
<pre><code>failed to compute cache key: &quot;/C:\\Program Files (x86)\\x&quot; not found: not found
</code></pre>
<p>I am kind of new to docker and guess thatt there is some design choice that you should not be able to access files outside the projects folder. But if I want to include files in my docker build like this, how should I do?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66894494","how do I connect a docker container to local redis server running on the host","<docker><docker-compose><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a redis-server running on the host computer, and a dockerized python app (single container).</p>
<p>How do I make the dockerized app talk to my local redis-server on the host?</p>
<p>I know I can create a docker-compose.yaml and connect both the containers but I want to connect to this local redis server running on the host.</p>
<p>Thanks</p>
"
"28583665","How to use docker logs","<docker>","67001652","see logs in docker container with .sh script in detach mode","<docker><docker-compose>","<p>The question may be a bit of newbie.
I run <code>docker exec -it mycontainer bash</code> to enter into a daemon container(postgresSQL ),
and <code>echo</code> something.
now I exit it , and use <code>docker logs mycontainer</code> so as to see my echos.</p>

<p>According to</p>

<blockquote>
  <p>The docker logs command batch-retrieves logs present at the time of execution.
  The docker logs --follow command will continue streaming the new output from the container's STDOUT and STDERR.</p>
</blockquote>

<p>The <code>docker logs</code> listen <code>STDOUT</code> of the container, why I don't see my string just <em>echoed</em> inside it?</p>
","<p>I started my container with <code>.sh</code> script due this command:</p>
<pre><code>docker-compose exec -d eos-indexer-node ./etc/runnodeos.sh
</code></pre>
<p>but when i want to see logs with:</p>
<pre><code>docker logs &lt;container&gt;

docker-compose logs &lt;service&gt;
</code></pre>
<p>everything is empty.</p>
<p>How to fix it ?</p>
"
"18460016","Connect from one Docker container to another","<rabbitmq><celery><docker>","62167356","Communicate between two Docker-Container in Docker for Windows","<docker><networking><port><docker-for-windows>","<p>I want to run rabbitmq-server in one docker container and connect to it from another container using celery (<a href=""http://celeryproject.org/"" rel=""noreferrer"">http://celeryproject.org/</a>)</p>

<p>I have rabbitmq running using the below command...</p>

<pre><code>sudo docker run -d -p :5672 markellul/rabbitmq /usr/sbin/rabbitmq-server
</code></pre>

<p>and running the celery via</p>

<pre><code>sudo docker run -i -t markellul/celery /bin/bash
</code></pre>

<p>When I am trying to do the very basic tutorial to validate the connection on <a href=""http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html"" rel=""noreferrer"">http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html</a></p>

<p>I am getting a connection refused error:</p>

<blockquote>
  <p>consumer: Cannot connect to amqp://guest@127.0.0.1:5672//: [Errno 111]
  Connection refused.</p>
</blockquote>

<p>When I install rabbitmq on the same container as celery it works fine.</p>

<p>What do I need to do to have container interacting with each other?</p>
","<p>this is my first question and I hope I summarize my problem as good as needed.</p>

<p>I am using docker for windows, which is, well known, not running on a VM. Windows uses Hyper-V.</p>

<p>I want to allow communication betweetn 2 running containers.</p>

<p>Container 1 (Jboss EAP, with AMQ): <br>
<code>docker run -p 9990:9990 -d jboss1</code></p>

<p>Container 2 (Jboss EAP, with AMQ): <br>
<code>docker run -p 9555:9555 -d jboss2</code></p>

<p>Booth are accessible with my local browser. I am using the base Image ""ubi7/ubi7-init"" version 7.6.</p>

<p>What should I do, to allow communication? curl to each other does not work. Do I need to expose ports? If I need to, how can I do that?</p>

<p>Thanks</p>
"
"16296753","Can you run GUI applications in a Docker container?","<x11><sandbox><docker><vnc>","64169954","How to run xterm in docker mininet","<docker><mininet><xterm>","<p>How can you run GUI applications in a <a href=""http://www.docker.io"" rel=""noreferrer"">Docker</a> container?</p>

<p>Are there any images that set up <code>vncserver</code> or something so that you can - for example - add an extra speedbump sandbox around say Firefox?</p>
","<p>I am new to docker. Inside of the docker, I opened mininet CLI and ran <code>xterm h1</code>, but got the error message <code>Error: Cannot connect to display</code>
<a href=""https://i.stack.imgur.com/OvNOZ.png"" rel=""nofollow noreferrer"">(msg picture)</a> Is there anyone know how to fix this problem?</p>
<p>p.s. I found a page to solving this problem (<a href=""https://hub.docker.com/r/iwaseyusuke/mininet/"" rel=""nofollow noreferrer"">link</a>), but could not understand what is the &quot;docker host&quot; and where should I run <code>xhost +si:localuser:root</code>.</p>
"
"19104847","How to generate a Dockerfile from an image?","<image><repository><docker>","59730797","How to generate the Dockerfile from an official image?","<docker><dockerfile>","<p>Is it possible to generate a Dockerfile from an image?  I want to know for two reasons:</p>

<ol>
<li><p>I can download images from the repository but would like to see the recipe that generated them.</p></li>
<li><p>I like the idea of saving snapshots, but once I am done it would be nice to have a structured format to review what was done.</p></li>
</ol>
","<p>I want to generate the <code>Dockerfile</code> from a official Dockerhub image namely the <a href=""https://hub.docker.com/_/influxdb"" rel=""nofollow noreferrer"">influxDB</a>  </p>

<p>After trying the suggested method on </p>

<pre><code>$ alias dfimage=""docker run -v /var/run/docker.sock:/var/run/docker.sock --rm centurylink/dockerfile-from-image""
$ dfimage --help
$ docker pull influxdb
$ dfimage influxdb  
</code></pre>

<p>I get the error message </p>

<pre><code>/usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:42:in `rescue in request': 400 Bad Request: malformed Host header (Docker::Error::ClientError)
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:38:in `request'
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:65:in `block (2 levels) in &lt;class:Connection&gt;'
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/image.rb:172:in `all'
    from /usr/src/app/dockerfile-from-image.rb:32:in `&lt;main&gt;'
</code></pre>

<p>Therefore how can I generate the Dockerfile from the aforementioned image.</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","60466005","Docker port networking","<docker><docker-networking>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>How is possible to open ports on existing docker container, without creating a new configured one?</p>

<p>I have the container with the OpenVAS configuration, just need to open the ports for the Greenbone assistant being opened in the local browser from the container. No IPTables, no additional configuration were made to the container. Just Openvas and Greenbone assistant configuration. By default using host network, also have tried to bridge the connection between localhost and the container.</p>
"
"36996046","How to prevent Dockerfile caching git clone","<git><docker><dockerfile>","66566713","How to rebuild docker image when remote repository changes","<git><docker><dockerfile>","<p>I have a Dockerfile trying to package and deploy a web app to a container. The code of app fetches from git repository during Docker image building.
Here's the Dockerfile snapshot:</p>

<pre><code>........
RUN git clone --depth=1 git-repository-url $GIT_HOME/
RUN mvn package -Dmaven.test.skip
........
</code></pre>

<p>I want the docker do not cache the step of <code>RUN git clone --depth=1 git-repository-url $GIT_HOME/</code> so that the on-going updated on the the repository can be reflected on the Docker image building. Is it possible to a achieve that?</p>
","<p>I build a docker image containing the Foo package, which is built from its master version pulled from its git repo:</p>
<pre><code>RUN git clone git://my.git.server/Foo.git
RUN (configure, build and install Foo)
</code></pre>
<p>Every time the image is built I'd like the Docker build cache to be invalidated if a new commit has been pushed into the Foo repository, in order to always build the latest version. On the contrary, if the master version is the same as the last time the image was built then the build cache should be used to avoid an unnecessary rebuild.</p>
<p>In its current form, my Dockerfile never needs to be changed so the build cache is never invalidated. Given my poor skills with Docker I can't figure out a possible modification to achieve my desired behavior, so I'd need help with this. Thanks in advance.</p>
<p>Edit: as suggested in the linked question the desired behavior can be achieved with a trick: modifying the Dockerfile as</p>
<pre><code>ARG FOO_MASTER_COMMIT
RUN git clone git://my.git.server/Foo.git
RUN (configure, build and install Foo)
</code></pre>
<p>and then launching the build as:</p>
<pre><code>git clone git://my.git.server/Foo.git
DOCKER_BUILDKIT=1 docker build --build-arg FOO_MASTER_COMMIT=$(git -C Foo rev-parse HEAD) -t myimage:ver .
</code></pre>
<p>does the job.</p>
"
"31843792","Can not install package within docker debian:jessie","<docker><dockerfile>","66696674","Installing libsndfile1 on docker container","<docker><libsndfile><soundfile>","<p>I am tryint to install git within debian based container</p>

<p>postgres image is based on debian:jessie</p>

<p>dockerfile</p>

<pre><code>FROM postgres:9.4


RUN apt-get -qq update

RUN apt-get install git-core
RUN apt-get install osm2pgsql
</code></pre>

<p>Both git and osm2pgsql can not be located </p>

<p>error</p>

<pre><code>E: Unable to locate package git-core
</code></pre>

<p>What have I missed ?</p>
","<p>I'm trying to install soundfile over pip install on my docker container. Sadly i need to install <code>libsndfile1</code> manually over <code>apt get</code> by myself. This fails somehow and i don't really get why and does anyone know how to install it.</p>
<p>I'm running docker desktop on Win10 - but container will finally run on a Linux machine.</p>
<pre><code> &gt; [ 7/11] RUN apt-get install libsndfile1:
#11 0.618 Reading package lists...
#11 1.814 Building dependency tree...
#11 2.219 Reading state information...
#11 2.829 The following additional packages will be installed:
#11 2.830   libflac8 libogg0 libvorbis0a libvorbisenc2
#11 2.942 The following NEW packages will be installed:
#11 2.944   libflac8 libogg0 libsndfile1 libvorbis0a libvorbisenc2
#11 2.956 0 upgraded, 5 newly installed, 0 to remove and 3 not upgraded.
#11 2.956 Need to get 669 kB of archives.
#11 2.956 After this operation, 2136 kB of additional disk space will be used.
#11 2.956 Do you want to continue? [Y/n] Abort.
------
executor failed running [/bin/sh -c apt-get install libsndfile1]: exit code: 1
</code></pre>
<p>Anyone know something?</p>
"
"30740828","Commit data in a mysql container","<mysql><linux><database><docker><virtualization>","66734991","Cannot save database changes to MySQL docker image","<mysql><docker>","<p>I created a mysql container using the officially supported <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql image</a>. I ran the image mounting a folder that contains a sql dump, then I created a new database in the container and imported the .sql dump in it:</p>

<pre><code>sudo docker run --name mysql-psat1 -v /opt/Projets/P1/sqldumps:/mnt -e MYSQL_ROOT_PASSWORD=secret -d mysql:latest
sudo docker exec -it mysql-psat1 bash
&gt; mysql -uroot -psecret -e 'create database liferay_psat1;'
&gt; mysql -uroot -psecret liferay_psat1 &lt; /mnt/liferay_sql_dump.sql
</code></pre>

<p>Then I listed the running containers to get that container's id:</p>

<pre><code>sudo docker ps -a
</code></pre>

<p>Then, I commited the container (with the imported sql) as a new container image</p>

<pre><code>sudo docker commit -m ""Imported liferay sql dump"" &lt;id-of-the-container&gt; jihedamine/mysql-psat1:v1
</code></pre>

<p>However, when if I start a container using that new image, the mysql database doesn't contain the newly created database liferay_psat1.</p>

<pre><code>sudo docker run -ti jihedamine/mysql-psat1:v1 bash
&gt; mysql -uroot -psecret
# show databases;
</code></pre>

<p>What am I doing wrong?</p>

<p>Thanks for your help!</p>
","<p>I am attempting to make a change to a MySQL docker instance of a database and save this as an image to use at later point. I can spawn the MySQL database, make changes to it, but when I <em><strong>COMMIT</strong></em> the container instance, then the database changes are not saved. If I spawn a container from the image, it still has the old changes.</p>
<p>These are the steps that I executed :</p>
<ol>
<li>I first spawn a MySQL instance like so:</li>
</ol>
<blockquote>
<pre><code>docker run -d -p 6033:3306 --name=test1 --env=&quot;MYSQL_ROOT_PASSWORD=root&quot; --env=&quot;MYSQL_PASSWORD=root&quot; --env=&quot;MYSQL_DATABASE=myDB&quot; mysql
</code></pre>
</blockquote>
<ol start=""2"">
<li>Next I pull in my SCHEMA.sql which creates some new tables like so :</li>
</ol>
<blockquote>
<pre><code>docker exec -i test1 mysql -uroot -proot myDB &lt; SCHEMA.sql
</code></pre>
</blockquote>
<ol start=""3"">
<li>Next I inspect my instance to see if the new tables are there.</li>
</ol>
<blockquote>
<pre><code>docker exec -it test1 bash
mysql -uroot -proot
show databases;
use myDB;
show tables;
</code></pre>
</blockquote>
<p>My new tables are there!!</p>
<p>My plan now is to take an image of this and recreate another container instance.</p>
<ol start=""4"">
<li>I commit</li>
</ol>
<blockquote>
<pre><code>docker commit test1
</code></pre>
</blockquote>
<ol start=""5"">
<li>I tag the image</li>
</ol>
<blockquote>
<pre><code>docker tag e2d6f1b70bbe new_image:1.1
</code></pre>
</blockquote>
<ol start=""6"">
<li>Now I run :</li>
</ol>
<blockquote>
<pre><code>docker run -d -p 3401:3306 --name test2 new_image:1.1
</code></pre>
</blockquote>
<ol start=""7"">
<li>Next I Inspect to see if tables are there</li>
</ol>
<blockquote>
<pre><code>docker exec -it test2 bash
mysql -uroot -proot
show databases;
use myDB;
show tables;
</code></pre>
</blockquote>
<p>My tables are NOT there!! (and that makes me sad :´( )</p>
"
"35931579","How can I install lxml in docker","<python><docker><lxml><dockerfile>","66779632","Docker | Problem with installing lxml on Python 3.8","<docker><dockerfile><lxml><alpine>","<p>I want to deploy my python project in docker, I wrote <code>lxml&gt;=3.5.0</code> in the requirments.txt as the project needs lxml. Here is my dockfile:</p>

<pre><code>FROM gliderlabs/alpine:3.3
RUN set -x \
    &amp;&amp; buildDeps='\
        python-dev \
        py-pip \
        build-base \
    ' \
    &amp;&amp; apk --update add python py-lxml $buildDeps \
    &amp;&amp; rm -rf /var/cache/apk/* \
    &amp;&amp; mkdir -p /app
ENV INSTALL_PATH /app
WORKDIR $INSTALL_PATH
COPY requirements-docker.txt ./
RUN pip install -r requirements.txt
COPY . .
RUN apk del --purge $buildDeps
ENTRYPOINT [""celery"", ""-A"", ""tasks"", ""worker"", ""-l"", ""info"", ""-B""]
</code></pre>

<p>I got this when I deploy it to docker:</p>

<pre><code>*********************************************************************************
Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
*********************************************************************************
error: command 'gcc' failed with exit status 1
----------------------------------------
Rolling back uninstall of lxml
</code></pre>

<p>I though it was because 'python-dev' and 'python-lxml', then I edited the dockfile like this:</p>

<pre><code>WORKDIR $INSTALL_PATH
COPY requirements-docker.txt ./
RUN apt-get build-dev python-lxml
RUN pip install -r requirements.txt
</code></pre>

<p>It did not work, and I got another error:</p>

<pre><code>---&gt; Running in 73201a0dcd59
/bin/sh: apt-get: not found
</code></pre>

<p>How can I install lxml correctly in docker?</p>
","<p>I'm building a docker container that uses <a href=""https://hub.docker.com/r/tiangolo/uwsgi-nginx-flask"" rel=""nofollow noreferrer"">this</a> image. (uwsgi-nginx-flask) This is based on an Alpine Linux image.</p>
<p>The latest python version available in this container is python 3.8.
I developed my flask application on Python 3.9. One of the python package dependencies is <code>lxml</code>. When I tried installing this, it throws the error given below.</p>
<pre><code>  cc -I/usr/include/libxml2 -c /tmp/xmlXPathInitnq3gdzvk.c -o tmp/xmlXPathInitnq3gdzvk.o
  unable to execute 'cc': No such file or directory
  *********************************************************************************
  Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
  *********************************************************************************
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for lxml

unable to execute 'cc': No such file or directory
*********************************************************************************
Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
*********************************************************************************
error: command 'gcc' failed with exit status 1
</code></pre>
<p>This is my Dockerfile</p>
<pre><code>FROM tiangolo/uwsgi-nginx-flask:python3.8-alpine
RUN apk add libxml2-dev 
RUN apk --update add bash nano
RUN apk --update add --no-cache python3
RUN python --version
RUN mkdir -p /var/www/app/
RUN pip3 install --no-cache --upgrade pip setuptools
WORKDIR /var/www/app/
COPY . /var/www/app/
ENV STATIC_URL /static
ENV STATIC_PATH /var/www/app/static
RUN pip3 install -r requirements.txt
</code></pre>
<p>You can see that I even tried to install <code>libxml2</code> on the alpine image as it is considered a dependency. But that doesn't work.</p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","62630456","Where on my drive is my docker image? (It's not under /)","<linux><docker><filesystems>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I recently installed a docker image that is 26.3GB uncompressed (as reported by its listing under <code>docker images</code>)</p>
<p>I'm on Ubuntu 18.04, and Dolphin reports my free hard drive space as 3GB. However, when I run QDirStat (a disk usage analyzer) it tells me that the total space the <code>/</code> subtree is taking up is only 25.6GB. The partition Linux is on is 60GB, so I can only conclude that the docker image is not anywhere under <code>/</code>. So where is it?</p>
"
"19104847","How to generate a Dockerfile from an image?","<image><repository><docker>","64741013","Is it possible to create Dockerfile from the container/image?","<docker><dockerfile>","<p>Is it possible to generate a Dockerfile from an image?  I want to know for two reasons:</p>

<ol>
<li><p>I can download images from the repository but would like to see the recipe that generated them.</p></li>
<li><p>I like the idea of saving snapshots, but once I am done it would be nice to have a structured format to review what was done.</p></li>
</ol>
","<p>I have a container up and running. Is it a way to generate Dockerfile from the running container or from its image? Please advice.</p>
"
"20845056","How can I expose more than 1 port with Docker?","<docker><docker-networking>","60131882","Publish several ports","<docker>","<p>So I have 3 ports that should be exposed to the machine's interface. Is it possible to do this with a Docker container?</p>
","<p>Docker command <code>docker container run --publish 8000:8080 --detach --name bb bulletinboard:1.0</code> publishes dockers <code>8080</code> port to <code>8000</code>. But how to deal when I need to publish two ports <code>8000:8080</code> and <code>1001:1010</code>?</p>
"
"19537645","Get environment variable value in Dockerfile","<docker>","60154995","Why can't I load environment variables in dockerfile?","<ruby-on-rails><ruby><docker>","<p>I'm building a container for a ruby app. My app's configuration is contained within environment variables (loaded inside the app with <a href=""http://github.com/bkeepers/dotenv"">dotenv</a>).</p>

<p>One of those configuration variables is the public ip of the app, which is used internally to make links.
I need to add a dnsmasq entry pointing this ip to 127.0.0.1 inside the container, so it can fetch the app's links as if it were not containerized.</p>

<p>I'm therefore trying to set an <code>ENV</code> in my Dockerfile which would pass an environment variable to the container.</p>

<p>I tried a few things.</p>

<pre><code>ENV REQUEST_DOMAIN $REQUEST_DOMAIN
ENV REQUEST_DOMAIN `REQUEST_DOMAIN`
</code></pre>

<p>Everything passes the ""REQUEST_DOMAIN"" string instead of the value of the environment variable though.
Is there a way to pass environment variables values from the host machine to the container?</p>
","<p>this is my dockerfile. environment variables was not recognized.<br>
However, hardcoded environment variables were successfully recognized. like 'DOMAIN' or 'NOTICE_FILE_PATH' were successfully recognized.</p>

<pre><code>MAINTAINER JeongWooYeong(wjd030811@gmail.com)

ENV SECRET_KEY_BASE $SECRET_KEY_BASE
ENV SCARFS_PASSWORD $SCARFS_PASSWORD
ENV API_KEY $API_KEY
ENV DOMAIN dsm-scarfs.hs.kr
ENV NOTICE_FILE_PATH /home/ubuntu/scarfs/storage/notice_file
ENV EXCEL_FILE_PATH /home/ubuntu/scarfs/storage/excel_file
ENV SINGLE_FILE_PATH /home/ubuntu/scarfs/storage/single_file
ENV MULTI_FILE_PATH /home/ubuntu/scarfs/storage/multi_file
ENV SCARFS_DB $SCARFS_DB

RUN apt-get update &amp;&amp; \
    apt-get install -y \
    default-libmysqlclient-dev \
    nodejs

RUN gem install bundler

COPY . .
WORKDIR .

RUN bundle install

EXPOSE 3000
VOLUME ["".storage"", ""/home/ubuntu/T-bone""]
RUN  [""bundle"", ""exec"", ""unicorn"", ""-E"", ""production"", ""-c"", ""config/unicorn.rb""]
</code></pre>

<p>and it caused the following error:</p>

<pre><code>ArgumentError: `secret_key_base` for production environment must be a type of String`
</code></pre>

<p>what can i do?</p>
"
"21553353","What is the difference between CMD and ENTRYPOINT in a Dockerfile?","<docker>","61503220","Dockerfile: when to use CMD or ENTRYPOINT","<docker><dockerfile><containers>","<p>In Dockerfiles there are two commands that look similar to me: <code>CMD</code> and <code>ENTRYPOINT</code>. But I guess that there is a (subtle?) difference between them - otherwise it would not make any sense to have two commands for the very same thing.</p>

<p>The documentation states for <code>CMD</code></p>

<blockquote>
  <p>The main purpose of a CMD is to provide defaults for an executing container.</p>
</blockquote>

<p>and for <code>ENTRYPOINT</code>:</p>

<blockquote>
  <p>An ENTRYPOINT helps you to configure a container that you can run as an executable.</p>
</blockquote>

<p>So, what's the difference between those two commands?</p>
","<p>I am new to Docker and trying to write some <code>Dockerfile</code>.</p>

<p>file1 is:</p>

<pre><code>FROM ruby-alpine

RUN apk build-base ruby-nokogirl

CMD ['rspec']
</code></pre>

<p>and file 2 is:</p>

<pre><code>FROM ruby-alpine
RUN apk build-base ruby-nokogirl

ENTRYPOINT ['rspec']
</code></pre>

<p>However, both works fine.
But I got a confusion on when should I use <code>CMD</code> command and when should I use <code>ENTRYPOINT</code>?</p>

<p>Any suggestion?</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","61849319","Difference between -p during image run and EXPOSE in dockerfile","<docker><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I've a website running on node and it is hosted at port 8080 inside a docker container. To use it in the host machine, I need to use -p against a relevant port of the host machine. There isn't any Web Server like ngInx here. It's just npm start.</p>

<p>Now, when should I use EXPOSE in docker file and why is it required ? How is it different from -p during run ?</p>

<p>In this case, is EXPOSE required ? I can directly run with -p and i can access the site with out EXPOSE as well. </p>
"
"43535394","Docker-compose not passing environment variable to container","<docker><docker-compose>","66548556","Docker-compose is not reading env variable COMPOSE_FILE","<python><django><linux><docker><devops>","<p>I am using Docker 17.04.0-ce, build 4845c56 with docker-compose 1.12.0, build b31ff33 on Ubuntu 16.04.2 LTS. I simply want to pass an environment variable and display it from my script running in a container. I am doing this according to the documentation <a href=""https://docs.docker.com/compose/compose-file/#environment"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#environment</a> . The problem is that the variable is not passed to the container.</p>

<p>My docker-compose.yml file:</p>

<pre><code>env-file-test:
  build: .
  dockerfile: Dockerfile
  environment:
    - DEMO_VAR
</code></pre>

<p>My Dockerfile:</p>

<pre><code>FROM alpine
COPY docker-start.sh /
CMD [""/docker-start.sh""]
</code></pre>

<p>And the <code>docker-start.sh</code> file:</p>

<pre><code>#!/bin/sh
echo ""DEMO_VAR Var Passed in: $DEMO_VAR""
</code></pre>

<p>I try to set the variable in my current terminal session and pass it to the container:</p>

<pre><code>$ export DEMO_VAR=aabbdd
$ echo $DEMO_VAR
aabbdd
$ sudo docker-compose up
Starting envfiletest_env-file-test_1
Attaching to envfiletest_env-file-test_1
env-file-test_1  | DEMO_VAR Var Passed in: 
envfiletest_env-file-test_1 exited with code 0
</code></pre>

<p>So you can see that the variable DEMO_VAR is empty!</p>

<p>I also tried using variables in docker-compose.yml like this: <code>DEMO_VAR=${DEMO_VAR}</code> but then when I run <code>sudo docker-compose up</code>, I get a warning: ""WARNING: The DEMO_VAR variable is not set. Defaulting to a blank string."".</p>

<p>What am I doing wrong? What should I do to pass the variable to the container?</p>
","<p>I am using django cookiecutter configuration with Docker and I can not make docker-compose read $COMPOSE_FILE env variable.</p>
<p>I did this</p>
<pre><code>export COMPOSE_FILE=local.yml
</code></pre>
<p>And this is the outcome when a I run either docker-compose up or build:</p>
<pre><code>Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml
</code></pre>
"
"62154016","docker on wsl2 very slow","<docker><windows-10><wsl-2>","66549583","Docker - Windows10 WSL2 - Compilation Speed ""Super-BAD""","<docker><windows-subsystem-for-linux>","<p>After having read about the performance improvements when running Docker on wsl2, I have been waiting for the official release of Windows 10 that supports wsl2.
I updated Windows and Docker and switched on the Docker flag to use wsl2 and was hoping for some performance boost for my Oracle Database running in a Docker container but unfortunately the change slowed down the container and my laptop dramatically.
The performance of the container is about 10x slower and my laptop is pretty much stuck when starting the container.
It seems as if the memory consumption would completely use up my 8GB and heavy memory swapping starts to take place.
Is there anything I can do to improve the performance of Docker on wsl2 or at least to better understand what's wrong in my setup?</p>

<p>My environment:</p>

<ul>
<li>Processor Intel(R) Core(TM) i7-2620M CPU @ 2.70GHz, 2 Core(s)</li>
<li>Installed Physical Memory (RAM)   8.00 GB</li>
<li>Microsoft Windows 10 Pro Version 10.0.19041 Build 19041</li>
<li>Docker version 19.03.8, build afacb8b</li>
</ul>
","<p>In our company we use a special docker image to execute the build for our projects.
The docker image has:</p>
<ul>
<li>ARM GCC with ccache</li>
<li>CMAKE</li>
<li>NINJA</li>
</ul>
<p>we have 2 setups that work super great and one setup which is super slow:</p>
<p>great:</p>
<ul>
<li>docker on a linux host</li>
<li>docker on a virtualbox-vm with windows7</li>
</ul>
<p>super-bad:</p>
<ul>
<li>docker on windows10wsl2 host</li>
</ul>
<p>what we see if the compilation is runs on wsl2, is that the cpu/ram usage max out (cpu usage is expected as we use max parallel threads) but actual task takes more than 30 times longer than on either one of the good options listed above.</p>
<p>does someone have a good tipp for us why wsl2 performs so badly?</p>
<p>i currently assume its the filesystem, so windows uses ntfs, and wsl accesses files from the windows file system (our project to build).
what makes matters worse is that building c code means compiling 100erts of small files, in words of a file system this means, fetching, processing and writing 100erts of files super fast.</p>
"
"63678049","How to handle missing dependencies with composer & docker?","<php><docker><composer-php>","66553264","Laravel project not showing in localhost: ""Composer detected issues in your platform....""","<php><laravel><composer-php><laravel-8>","<p>I have an application that I want to install for local development on my laptop with Docker. The application requires libraries from composer.</p>
<p>I would like to avoid installing PHP on my laptop and all necessary extensions just to be able to run composer.</p>
<p>I also don't want to run composer during the build inside of my application container, because I need the vendor folder on my local computer using mount binding.</p>
<p>It looked like the perfect solution to me to install composer though a docker container as explained <a href=""https://hub.docker.com/_/composer"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-sh prettyprint-override""><code>docker run --rm --interactive --tty \
  --volume $PWD:/app \
  composer install 
</code></pre>
<p>However, when doing so, how do you resolve any PHP dependency conflicts?</p>
<p>For example</p>
<pre class=""lang-sh prettyprint-override""><code>docker run --rm --interactive --tty \
       --volume $PWD:/app \
       composer require phpoffice/phpspreadsheet
</code></pre>
<p>will fail with</p>
<pre class=""lang-none prettyprint-override""><code>Using version ^1.14 for phpoffice/phpspreadsheet
./composer.json has been updated
Loading composer repositories with package information
Updating dependencies (including require-dev)
Your requirements could not be resolved to an installable set of packages.

  Problem 1
    - phpoffice/phpspreadsheet 1.14.1 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
    - phpoffice/phpspreadsheet 1.14.0 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
    - Installation request for phpoffice/phpspreadsheet ^1.14 -&gt; satisfiable by phpoffice/phpspreadsheet[1.14.0, 1.14.1].
</code></pre>
<p>How can I solve this kind of problems?</p>
","<p>When I want to see my Laravel project in localhost, I get the following error message.</p>
<blockquote>
<p>Composer detected issues in your platform: Your Composer dependencies
require a PHP version &quot;&gt;= 8.0.2&quot;.</p>
</blockquote>
<p>My terminal PHP version is 8.0.2, and my MAMP PHP version is 8.0 (no higher version is available, it has always worked like this). I don't know what else to do. I have already put my composer.json and .lock files at 8.0.2 PHP.</p>
"
"40366192","Kubernetes how to make Deployment to update image","<docker><kubernetes>","66555295","Forcing Kubernetes to redeploy a deployment YAML if docker image updates?","<docker><kubernetes><gitlab><gitlab-ci>","<p>I do have deployment with single pod, with my custom docker image like:</p>

<pre><code>containers:
  - name: mycontainer
    image: myimage:latest
</code></pre>

<p>During development I want to push new latest version and make Deployment updated.
Can't find how to do that, without explicitly defining tag/version and increment it for each build, and do</p>

<pre><code>kubectl set image deployment/my-deployment mycontainer=myimage:1.9.1
</code></pre>
","<p>I have this workflow where I write some code and a docker image is deployed under <code>latest</code>. Currently, it deploys to my container registry and then I run this <code>kubectl apply file.yaml</code> after the container deploys, but K8s doesn't seem to recognize that it needs to re-pull and rollout a new deployment with the newly pulled image.</p>
<p>How can I basically feed in the YAML spec of my deployments and just rollout restart the deployments?</p>
<p>Alternatively, is there a better approach? I unconditionally am rolling out deployment restarts on all my deployments this way.</p>
"
"51011552","MongoDB on with Docker ""failed to connect to server [localhost:27017] on first connect ""","<node.js><mongodb><docker><docker-compose>","66562704","Node js and mongodb cannot connect docker-compose","<node.js><mongodb><docker>","<p>I am using mongoDB with and NodeJS backend. The Problem is I am getting the following error</p>

<blockquote>
  <p>node:16) UnhandledPromiseRejectionWarning: MongoNetworkError: failed
  to connect to server [localhost:27017] on first connect
  [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]</p>
</blockquote>

<p>This is my docker-compose</p>

<pre><code>version: '3.4'

services:
  db:
    image: mongo:latest
    ports:
      - '27017:27017'

  rest-api-node:
    build: .
    ports:
      - '5000:5000'
    links:
      - db
    restart: on-failure
</code></pre>

<p>I have tried with <code>depends_on</code> as well , was not working.</p>

<p>On backend I am mongoose as a middleware to communicate with DB. this is the part of my <code>index.js</code></p>

<pre><code>mongoose.Promise = global.Promise
mongoose.connect('mongodb://localhost/demo')
app.listen(port, () =&gt; console.log(""live""))
</code></pre>

<p>I have tried using promise as well , no change though. Please Help me out.
Thanks in advance </p>

<p>complete error log</p>

<blockquote>
  <p>at Pool.
  (/app/node_modules/mongodb-core/lib/topologies/server.js:505:11)
  rest-api-node_1  |     at Pool.emit (events.js:180:13) rest-api-node_1
  |     at Connection.
  (/app/node_modules/mongodb-core/lib/connection/pool.js:329:12)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Connection.emit (events.js:180:13)
  rest-api-node_1  |     at Socket.
  (/app/node_modules/mongodb-core/lib/connection/connection.js:245:50)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Socket.emit (events.js:180:13)
  rest-api-node_1  |     at emitErrorNT
  (internal/streams/destroy.js:64:8) rest-api-node_1  |     at
  process._tickCallback (internal/process/next_tick.js:178:19)
  rest-api-node_1  |   name: 'MongoNetworkError', rest-api-node_1  |<br>
  message: 'failed to connect to server [localhost:27017] on first
  connect [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]' }</p>
</blockquote>
","<p>Both these services work individually, Yet when I go to the docker endpoint through the container the app crashes as it fails to connect to my database. Can anyone see why?</p>
<p>After running docker-compose i have inspected each container individually and they work fine but just not together. Unsure as to why as for others it seems to work</p>
<p>Docker compose</p>
<pre><code>version: '3.3'
services:
  db:
    container_name: db
    image: mongo
    # volumes:
    #   - ./db:/data/db
    ports:
      - '27017:27017'
  # frontend:
  #   build:
  #     context: './frontend/'
  #     dockerfile: Dockerfile
  #   container_name: reactfront
  #   # depends_on: [server]
  #   ports:
  #     - '3000:3000'
  #   restart: always
  server:
    build:
      context: ./backend/
      dockerfile: Dockerfile
    container_name: nodeserver
    ports:
      - '4000:4000'
    restart: always
    depends_on: [db]
    links:
      - db
volumes:
    db_data: {}
</code></pre>
<p>Node js app</p>
<pre><code>const express = require('express')
const app = express()
const port = 4000
var cors = require('cors')
const { MongoClient } = require(&quot;mongodb&quot;);
const uri = 'mongodb://localhost:27017'
const client = new MongoClient(uri);
app.use(cors())
app.get('/', (req, res) =&gt; {
  res.send('Hello Worldd!')
})
app.get('/docker', async (req, res) =&gt; {
  let myres;
  client.connect().then(async (db, b) =&gt; {
    myres = await db.db().admin().listDatabases()
    console.log(await db.db().admin().listDatabases());
    res.json(myres)
  })
})
app.listen(port, () =&gt; {
  console.log(`Example app listening at http://localhost:${port}`)
})

</code></pre>
"
"62441307","How can I change the location of docker images when using Docker Desktop on WSL2 with Windows 10 Home?","<docker><windows-10><docker-desktop><wsl-2>","66570306","I need to use my D disc to run my Docker. How can I do?","<docker>","<p>I've just upgraded to Windows 10 Home May 2020, <a href=""https://docs.microsoft.com/en-us/windows/wsl/install-win10"" rel=""noreferrer"">activated WSL2</a>, and installed <a href=""https://hub.docker.com/editions/community/docker-ce-desktop-windows"" rel=""noreferrer"">Docker Desktop</a>.</p>
<p>WSL2 must be installed in my system disk, which is a small SSD. I don't want to fill it with docker images. How do I change the docker images path? I'd like to use a path in my big Windows filesystem.</p>
<p>The <a href=""https://stackoverflow.com/questions/42250222/what-is-docker-image-location-on-windows-10/"">image location</a> is somewhat confusing. I believe it is in <code>/mnt/wsl/docker-desktop-data/</code>.</p>
<p>How do I change the directory of docker images inside WSL2? May I change docker configuration to select a path inside <code>/mnt/d</code>, or mount a path from /mnt/d over docker data dirs?</p>
","<p>I checked that the Docker container has only about 2 GB disk space in my C disc so I need to use my D disc to run my Docker. How can I do? My C disk is not enough, what should I do for docker for everything?</p>
<p><img src=""https://i.stack.imgur.com/mlLHJ.jpg"" alt=""my disc spaces"" /></p>
"
"59299133","How to silent install Postgresql in Ubuntu via. Dockerfile?","<postgresql><docker><dockerfile>","66574785","Why does docker build keep asking me for the time zone even after I set it?","<docker><timezone><dockerfile>","<p>I have the following docker file, and I am using the command <code>docker build -t demo:v1 .</code> to build the image.</p>

<pre><code>FROM ubuntu:18.04
WORKDIR /app
RUN apt update \
    &amp;&amp; apt -y upgrade \
    &amp;&amp; apt install -y python3 \
    &amp;&amp; apt install -y python3-pip \
    &amp;&amp; apt install -y poppler-utils \
    &amp;&amp; apt install -y libsm6 libxext6 libxrender-dev

RUN apt install -y postgresql

COPY requirements.txt /app/requirements.txt

RUN pip3 install -r requirements.txt

COPY . /app

CMD gunicorn -t 300 --workers 5 --bind  0.0.0.0:8080 wsgi
</code></pre>

<p>When I build an image using this, while installing postgresql, it expects input and stops the building process like this </p>

<pre><code>.
.
.
.
Setting up libpopt0:amd64 (1.16-11) ...
Setting up tzdata (2019c-0ubuntu0.18.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area:
</code></pre>

<p>So, how can I setup postgresql inside my image, so that it builds without expecting this input? Also, surprisingly, even after I input my option, nothing happens further, and the process gets stuck. </p>
","<p>Why does creating a new docker image using the dockerfile below keep asking me to select the timezone? I have tried multiple variations of the <code>ENV TZ</code> command to no avail.</p>
<pre><code> =&gt; =&gt; # Please select the geographic area in which you live. Subsequent configuration
 =&gt; =&gt; # questions will narrow this down by presenting a list of cities, representing
 =&gt; =&gt; # the time zones in which they are located.
 =&gt; =&gt; #   1. Africa   3. Antarctica  5. Arctic  7. Atlantic  9. Indian    11. US
 =&gt; =&gt; #   2. America  4. Australia   6. Asia    8. Europe    10. Pacific  12. Etc
 =&gt; =&gt; # Geographic area:
</code></pre>
<p>I asks this after the <code>apt-get install</code> is finished.</p>
<p>Dockerfile:</p>
<pre><code>FROM ubuntu:20.10

ENV TZ Europe/London

RUN apt-get update \
 &amp;&amp; apt-get upgrade -y \
 &amp;&amp; apt-get -y install autoconf automake bison bzip2 cmake doxygen diffutils flex g++ gcc git gzip libarchive13 libcurl4 libelf1 libgpgme11 libssl1.1 libtool libusb-dev m4 make libncurses-dev patch pkg-config python3 readline-common subversion tar tcl texinfo unzip wget xz-utils
</code></pre>
<p>Thanks!</p>
"
"44678725","Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?","<linux><ubuntu><docker><ubuntu-14.04>","66604589","How can i fix the docker daemon error in termux?","<android><docker><error-handling><daemon><termux>","<p>I have applied every solution available on internet but still I cannot run Docker.</p>

<p>I want to use Scrapy Splash on my server.</p>

<p>Here is <code>history</code> of commands I ran.</p>

<pre><code>docker run -p 8050:8050 scrapinghub/splash
sudo docker run -p 8050:8050 scrapinghub/splash
sudo usermod -aG docker $(whoami)
sudo docker run -p 8050:8050 scrapinghub/splash
newgrp docker
sudo docker run -p 8050:8050 scrapinghub/splash
reboot
sudo docker run -p 8050:8050 scrapinghub/splash
docker run -p 8050:8050 scrapinghub/splash
</code></pre>

<p>You can see I tried to restart my server as well, but it didnt help.</p>

<p>see output of <code>ps -aux | grep docker</code></p>

<pre><code>root@mani:/var/www/html# ps aux | grep docker
root      8524  0.0  0.8 127904 13964 ?        Ssl  17:21   0:00 /usr/bin/dockerd --raw-logs
root      8534  0.0  0.3  90588  5012 ?        Ssl  17:21   0:00 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc
root      8543  0.0  0.0   8812   764 pts/1    S+   17:21   0:00 grep --color=auto docker
root     16356  0.0  0.0  17200   964 pts/1    S    17:14   0:00 newgrp docker
root     20080  0.0  0.0  17200   964 pts/1    S    17:06   0:00 newgrp docker
root     30221  0.0  0.0  17200   964 pts/1    S    17:09   0:00 newgrp docker
</code></pre>
","<p>I installed docker on termux and when i tried to pull centos it gives me this error</p>
<p>Using default tag: latest
Cannot connect to the Docker daemon at unix:///data/docker/run/docker.sock. Is the docker daemon running?</p>
<p>How can i fix this ?</p>
"
"55115080","How to specify different port for a Docker postgres instance?","<docker><docker-compose>","66611842","keycloak cannot connect to postgres when containerized","<postgresql><docker><keycloak>","<p>I am trying to get a postgres container running on a different port (5433 instead of the default 5432) per several online threads like this one:
<a href=""https://github.com/docker-library/postgres/issues/196#issuecomment-310209118"" rel=""noreferrer"">https://github.com/docker-library/postgres/issues/196#issuecomment-310209118</a></p>

<p>(The reason for the port change is because an unrelated project that I leave running is already using port 5432 so I'd like to be able to run rspec simultaneously on multiple projects.)</p>

<p>In my test ENV I have 
DATABASE_URL=postgresql://postgres:@db.local:5433/test_agile_self</p>

<p>Everything worked when I was using port 5432 (in DATABASE_URL and docker-compose.yml)</p>

<p>After changing the port to 5433, when I run rspec I get:</p>

<pre><code>PG::ConnectionBad:
  could not connect to server: Connection refused
    Is the server running on host ""db.local"" (172.22.0.2) and accepting
    TCP/IP connections on port 5433?
# ./spec/spec_helper.rb:62:in `block (2 levels) in &lt;top (required)&gt;'
</code></pre>

<p>The container does indeed seem to be running on port 5433 and IP address 172.22.0.2:</p>

<pre><code>$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
a8f5dee227e8        postgres:10.5       ""docker-entrypoint.s…""   27 minutes ago      Up 14 seconds       0.0.0.0:5433-&gt;5432/tcp   zarc_db.local_1
</code></pre>

<pre><code>$ docker inspect a8f5dee227e8 | grep ""IPAddress""
            ""SecondaryIPAddresses"": null,
            ""IPAddress"": """",
                    ""IPAddress"": ""172.22.0.2"",
</code></pre>

<p>My docker-compose.yml uses the <code>ports: ""5433:5432""</code> as per the discussion linked above.</p>

<pre><code>#docker-compose.yml
version: '3'
services:
  web:
    build: .
    ports:
      - ""3010:3010""
    volumes:
      - .:/my_app
    links:
      - db.local
    depends_on:
      - db.local

  db.local:
    image: postgres:10.5
    ports:
      - ""5433:5432""
</code></pre>

<p>If I change 5433 back to 5432 in those two spots (ENV and docker-compose.yml) it works again.</p>

<p>This is on a Mac running Mohave 10.14.3, and Docker 18.09.2</p>
","<p>I have the following docker-compose</p>
<pre><code>  postgres_db:
    image: postgres:13
    ports:
      - 5433:5432
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: root
      POSTGRES_DB: keycloak
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
  keycloak:
    image: jboss/keycloak
    ports:
      - 9800:8080
    environment:
      DB_VENDOR: postgres
      DB_ADDR: postgres_db
      DB_PORT: 5433
      DB_USER: root
      DB_PASSWORD: root
      DB_DATABASE: keycloak
      DB_SCHEMA: public
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
    depends_on:
      - postgres_db
</code></pre>
<p>When run, I am able to connect with a database client to the postgres_db container with the provided credentials but Keycloak container doesn't want to start because it cannot reach postgres.</p>
<pre><code>        ... 50 more
Caused by: org.postgresql.util.PSQLException: Connection to postgres_db:5433 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
        at org.postgresql.jdbc@42.2.5//org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195)
        at org.postgresql.jdbc@42.2.5//org.postgresql.Driver.makeConnection(Driver.java:454)
        at org.postgresql.jdbc@42.2.5//org.postgresql.Driver.connect(Driver.java:256)
        at org.jboss.ironjacamar.jdbcadapters@1.4.23.Final//org.jboss.jca.adapters.jdbc.local.LocalManagedConnectionFactory.createLocalManagedConnection(LocalManagedConnectionFactory.java:321)
        ... 57 more
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)
        at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)
        at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.base/java.net.Socket.connect(Socket.java:609)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:70)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)
        ... 62 more
</code></pre>
<p>Keycloak has also a sample docker-compose file for configuring it with Postgres: <a href=""https://github.com/keycloak/keycloak-containers/blob/master/docker-compose-examples/keycloak-postgres.yml"" rel=""nofollow noreferrer"">https://github.com/keycloak/keycloak-containers/blob/master/docker-compose-examples/keycloak-postgres.yml</a></p>
<p>I also tried replacing <code>DB_ADDR: postgres_db</code> with <code>DB_ADDR: localhost</code> or <code>DB_ADDR: postgres_db:5433</code> or <code>DB_ADDR: localhost:5433</code> and it didn't work.</p>
<p>What am I missing?</p>
"
"43383276","How does Docker run a Linux kernel under macOS host?","<linux><macos><docker><linux-kernel>","66618003","Why is it possible to run Linux containers on docker in MacOS","<linux><macos><docker><operating-system><containers>","<p>I installed Docker on my macOS Sierra as follows. Note I don't have VirtualBox installed.</p>

<pre><code>brew cask uninstall virtualbox
brew cask install docker
</code></pre>

<p>My macOS details.</p>

<pre><code>$ uname -a
Darwin m-C02QG7TRG8WN.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

$ docker version
Client:
 Version:      17.03.1-ce
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   c6d412e
 Built:        Tue Mar 28 00:40:02 2017
 OS/Arch:      darwin/amd64

Server:
 Version:      17.03.1-ce
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   c6d412e
 Built:        Fri Mar 24 00:00:50 2017
 OS/Arch:      linux/amd64
 Experimental: true
</code></pre>

<p>Once I run Docker from launchpad, I am able to run Docker containers.</p>

<pre><code>$ docker run -it ubuntu
root@2351d4222a4e:/# uname -a
Linux 2351d4222a4e 4.9.13-moby #1 SMP Sat Mar 25 02:48:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<p>My question is how does Docker manage to run a Linux kernel within macOS? I thought Docker would at least require boot2docker or some other such Linux kernel running so that it can create the Ubuntu's filesystem with the help of it. But the above output seems to indicate that it is not so. Where does the Linux kernel come from then?</p>
","<p>Please correct me if there are any issues in my understanding of Docker and operating systems.</p>
<p>From what I understand, Docker uses the underlying kernel of the OS of the machine running the Docker engine. As a result you are not able to run Windows containers on a Linux machine as it requires Windows kernel. My question is how am I able to run CentOS, Ubuntu containers on my MacOS machine, given that MacOS does not use a Linux kernel even though they are both Unix based.</p>
"
"50229798","How to define docker commit message in Dockerfile","<docker><dockerfile>","66642983","How do I add comments to layers in docker images?","<docker><dockerfile>","<p>Using <code>Dockerfile</code> (<code>docker build</code>) is an alternative for doing <code>docker commit</code> by hand. By using <code>docker commit</code>, there is an option named <code>--message</code>, which can be used to define commit messages. Commit messages are displayed in <code>docker history</code> in a dedicated column called <code>COMMENT</code>. My question is: how to define docker commit message in a <code>Dockerfile</code>?</p>
","<p>The docker history command shows the various layers and has this nice column named &quot;COMMENT&quot; but I can not seem to find a way to make a comment for the layers.</p>
<p>It would be great to have some way to get a comment into the layers such that they can be annotated for users to read the history.  (Nothing big, but a few words would be nice)</p>
<p>What I do now for the &quot;RUN&quot; layers is to have the first thing be an <code>echo &quot;MyThing&quot; &amp;&amp; \</code> such that &quot;MyThing&quot; effectively is the comment but in the &quot;CREATED BY&quot; field.</p>
"
"44678725","Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?","<linux><ubuntu><docker><ubuntu-14.04>","66650896","Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See 'docker run --help'","<linux><docker>","<p>I have applied every solution available on internet but still I cannot run Docker.</p>

<p>I want to use Scrapy Splash on my server.</p>

<p>Here is <code>history</code> of commands I ran.</p>

<pre><code>docker run -p 8050:8050 scrapinghub/splash
sudo docker run -p 8050:8050 scrapinghub/splash
sudo usermod -aG docker $(whoami)
sudo docker run -p 8050:8050 scrapinghub/splash
newgrp docker
sudo docker run -p 8050:8050 scrapinghub/splash
reboot
sudo docker run -p 8050:8050 scrapinghub/splash
docker run -p 8050:8050 scrapinghub/splash
</code></pre>

<p>You can see I tried to restart my server as well, but it didnt help.</p>

<p>see output of <code>ps -aux | grep docker</code></p>

<pre><code>root@mani:/var/www/html# ps aux | grep docker
root      8524  0.0  0.8 127904 13964 ?        Ssl  17:21   0:00 /usr/bin/dockerd --raw-logs
root      8534  0.0  0.3  90588  5012 ?        Ssl  17:21   0:00 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc
root      8543  0.0  0.0   8812   764 pts/1    S+   17:21   0:00 grep --color=auto docker
root     16356  0.0  0.0  17200   964 pts/1    S    17:14   0:00 newgrp docker
root     20080  0.0  0.0  17200   964 pts/1    S    17:06   0:00 newgrp docker
root     30221  0.0  0.0  17200   964 pts/1    S    17:09   0:00 newgrp docker
</code></pre>
","<p>I started to learn docker recently. After I install the docker,and I input <code>sudo docker run hello-world</code> to check does it install successfully, it shows as the title. By the way,when I input <code>docker version</code>, it shows the version that I installed.</p>
"
"50310402","Cannot connect to exposed port of container started with docker-compose on Windows","<docker><docker-compose>","66670606","Unable to access Dockerized Angular application in container","<node.js><angular><docker><dockerfile>","<p>I'm having trouble accessing apps that should expose ports to the host via docker-compose. Here is a reproducible example:</p>

<p>I create a new angular app using the angular CLI:</p>

<pre><code>ng new angular-docker
</code></pre>

<p>Then I create a Dockerfile with the following contents in that directory:</p>

<pre><code>FROM node:8

RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app
COPY . /usr/src/app

RUN npm install

EXPOSE 4200

CMD [""npm"", ""start""]
</code></pre>

<p>Next I create a docker-compose.yaml file in the same directory:</p>

<pre><code>version: '3'

services:
  angular:
    build: .
    container_name: angular-docker
    volumes:
      - ./src:/usr/src/app/src
    ports:
      - ""4200:4200""
</code></pre>

<p>Then I run:</p>

<pre><code>docker-compose up
</code></pre>

<p>I wait until I get the following line in the docker-compose output</p>

<pre><code>angular-docker | ** Angular Live Development Server is listening on localhost: 4200, open your browser on http://localhost:4200/ **
</code></pre>

<p>From <code>docker inspect angular-docker</code> I see that the port-forwarding rule is in place:</p>

<pre><code>...
""Ports"": {
            ""4200/tcp"": [
                {
                    ""HostIp"": ""0.0.0.0"",
                    ""HostPort"": ""4200""
                }
            ]
        },
...
</code></pre>

<p>Now, when I try to go to <a href=""http://localhost:4200"" rel=""nofollow noreferrer"">http://localhost:4200</a> in Chrome, I get ERR_EMPTY_RESPONSE.</p>

<p>However, when I use <code>docker exec -it angular-docker bash</code> to bash into the container, I get a response, when I <code>curl localhost:4200</code>.</p>

<p><strong>My environment:</strong></p>

<ul>
<li>OS: Windows 10 Pro Version 10.0.16299 Build 16299</li>
<li>Docker: 18.03.1-ce-win65 (17513)</li>
</ul>

<p>I figured that this might be a firewall issue. So for debugging, I closed the firewall application (I'm using Kaspersky Internet Security), with no luck.</p>

<p>Why can I not access the container from the exposed port?</p>

<p>EDIT:</p>

<p><code>netstat -an | findstr "":4200""</code> returns the following:</p>

<pre><code>Proto  Local Address          Foreign Address        State
TCP    0.0.0.0:4200           0.0.0.0:0              LISTENING
TCP    [::1]:4200             [::]:0                 LISTENING
</code></pre>
","<p><strong>Dockerfile</strong>:<br></p>
<pre><code>FROM node:current-alpine3.10

RUN mkdir -p /dist/angular

WORKDIR /dist/angular

COPY package.json .

RUN npm install --legacy-peer-deps

COPY . .

EXPOSE 8500

CMD [&quot;npm&quot;,&quot;run&quot;,&quot;start:stage-ena-sso&quot;]
</code></pre>
<p><strong>package.json</strong>:</p>
<pre><code>...
  &quot;scripts&quot;: {
    &quot;start:stage-ena-sso&quot;: &quot;ng serve -o -c=stage-ena-sso --port=8500 --baseHref=/&quot;
  }...
</code></pre>
<p><strong>Folder structure</strong>:<br>
<a href=""https://i.stack.imgur.com/rLLr6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rLLr6.png"" alt=""enter image description here"" /></a></p>
<p><strong>Command used to build the Docker image</strong>:<br></p>
<pre><code>docker build . -t ssoadminuiapp
</code></pre>
<p><strong>Command used to run the Docker image</strong>:<br></p>
<pre><code>docker run --rm -it  -p 8500:8500/tcp ssoadminuiapp:latest
</code></pre>
<p><strong>Check if container is running</strong>:<br></p>
<pre><code>CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                    NAMES
8959e5180eba        ssoadminuiapp:latest   &quot;docker-entrypoint.s…&quot;   6 minutes ago       Up 6 minutes        0.0.0.0:8500-&gt;8500/tcp   recursing_fermat
</code></pre>
<p>But accessing <strong>localhost:8500</strong> doesnt seem to work:<br>
<a href=""https://i.stack.imgur.com/31lN2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31lN2.png"" alt=""enter image description here"" /></a></p>
<p>I'm really new to Docker, so any useful beginner-friendly tips/infos would be very appreciated.</p>
<p>Edit #1, this is the result after running docker run command:<br>
<a href=""https://i.stack.imgur.com/htQNp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/htQNp.png"" alt=""enter image description here"" /></a></p>
"
"54121031","Multiple commands on docker ENTRYPOINT","<docker><dockerfile>","66707054","How to run multiple ENTRYPOINT script in docker","<docker>","<p>I'm trying to build a custom tcserver docker image. But I'm having some problems starting the webserver and the tomcat.<br/>
As far as I understand I should use ENTRYPOINT to run the commands I want.<br/>
The question is, is it possible to run multiple commands with ENTRYPOINT?<br/>
Or should I create a small bash script to start all?<br/><br/></p>

<p>Basically what I would like to do is:<br/></p>

<pre><code>ENTRYPOINT /opt/pivotal/webserver/instance1/bin/httpdctl start &amp;&amp; /opt/pivotal/webserver/instance2/bin/httpdctl start &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance1 -i /opt/pivotal/pivotal-tc-server-standard &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance2 -i /opt/pivotal/pivotal-tc-server-standard
</code></pre>

<p>But I don't know if that is a good practice or if that would even work.</p>
","<p>I have a bunch of script inside my docker image</p>
<pre><code>script01.sh
script02.sh
script03.sh
...
</code></pre>
<p>I would like to run them (in a particular order) in the ENTRYPOINT statemant:</p>
<pre><code>ENTRYPOINT [&quot;script01.sh&quot;, &quot;script02.sh&quot;, &quot;script03.sh&quot;, ...]
</code></pre>
<p>I've tryed different formulations (adding &quot;bash&quot;, &quot;.&quot; in the entrypoint commands), but nothing seems to work.</p>
<p>Any suggestion?</p>
<p>This : <a href=""https://stackoverflow.com/questions/54121031/multiple-commands-on-docker-entrypoint"">Multiple commands on docker ENTRYPOINT</a> did not work in my case so I wrote a new question as the site suggestet me.</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","66710947","configuring docker to put files","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I was just getting started with docker, and I run this:</p>
<pre><code>docker pull redis  
</code></pre>
<p>and I get a permission denied error. It turns out, docker writes to <code>/var/*</code> directories, which requires permission to write. and so many other docker commands also require something like:</p>
<pre><code>sudo docker ***    
</code></pre>
<p>Now, I don't really like the notion of add root privileges to every docker command.(It might be because I just don't know docker much yet, but that's true with every program). Is this a requirement by docker?</p>
<p>If it is not required, then how do I configure it so that it is much like other programs, that only ask me privileges when they need to, all the pulling, running commands would just write to my normal directories or run from them, not from a system directory.</p>
<p>EDIT: my concern was, if docker was allowed access to system files, meaning, it has some embedded scipt that had a potential harm to the computer, and it executed when I ran the docker. Since, I give it root privileges, the script could do anything. Would adding it to the user group instead of sudo fix that?</p>
"
"46440909","How to configure different dockerfile for development and production","<docker><dockerfile>","66748919","Docker for dev and prod","<php><docker><docker-compose><xdebug>","<p>I use docker for development and in production for laravel project. I have slightly different dockerfile for development and production. For example I am mounting local directory to docker container in development environment so that I don't need to do docker build for every change in code. </p>

<p>As mounted directory will only be available when running the docker container I can't put commands like ""composer install"" or ""npm install"" in dockerfile for development.</p>

<p>Currently I am managing two docker files, is there any way that I can do this with single docker file and decide which commands to run when doing docker build by sending parameters.</p>

<p><strong>What I am trying to achieve is</strong> </p>

<p><strong>In docker file</strong></p>

<pre><code>...
IF PROD THEN RUN composer install
...
</code></pre>

<p><strong>During docker build</strong></p>

<pre><code>docker build [PROD] -t mytag .
</code></pre>
","<p>I want to use docker for development and production. I know that in theory the settings should be the same, but I came across the fact that, for example, I only need xbedug during development. In the future, there will probably be other things that are not needed in production.</p>
<p>Right now my docker-compose contains</p>
<pre><code>networks:
  appuser:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${SUBNET_IP}/${SUBNET_MASK}
</code></pre>
<p>for xdebug.</p>
<p>And in the dockerfile for php, I try to use &quot;if&quot;</p>
<pre><code>RUN if [ ${APP_ENV} == &quot;dev&quot; ]; then pecl install xdebug; fi
RUN if [ ${APP_ENV} == &quot;dev&quot; ]; then docker-php-ext-enable xdebug; fi
</code></pre>
<p>This does not generate errors, but it does not install xdebug either.</p>
<p>Hence the question: what is the best way to solve the problem with unnecessary settings and modules in prod? There must be some kind of best practice or something like that. I’m hardly the first person to come across such a question. Use different docker-compose files for prod and dev? I don't really like this solution. You will have to create almost identical docker-compose and dockerfile. IF in docker is also not trustworthy, especially since it does not work and extra instructions remain in docker-compose.</p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","66759002","How to protect docker file from users within a system?","<docker><file><dockerfile><password-protection><password-encryption>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I am working on a project and to show the demo of project the code has to be pushed into sever of the third party, but my code has to be protected so that other users in the system cannot access or modify my code. The code runs on docker, so is there anyways to protect my docker file from other users within the system?</p>
"
"54374724","Docker container save logs on the host directory","<docker><docker-compose><containers><dockerfile><docker-container>","66774731","How to copy docker container logs to a file on host machine in real time?","<docker><logging><docker-compose><consul>","<p>I have a question similar to <a href=""https://stackoverflow.com/questions/41358285/log-doesnt-save-to-file-in-docker-environment"">this one</a>. When run my <code>docker-compose.yml</code> file, I automatically create a docker image and as specified in my <code>dockerfile</code>, I run certain apps. These apps produce some logs, but these logs are written <em>inside</em> the docker container, in <code>/home/logs/</code> folder.</p>
<p>How can I indicate that these logs be writted <em>outside</em> the container, on my <code>/path/on/host</code> address? Simply because if the container was failed, I need to see the logs and not lose them!</p>
<p>This is my <code>docker-compose.yml</code>:</p>
<pre><code>version: '3'
services:
  myapp:
    build: .
    image: myapp
    ports:
      - &quot;9001:9001&quot;
</code></pre>
<p>and here is my <code>dockerfile</code>:</p>
<pre><code>FROM java:latest
COPY myapp-1.0.jar /home
CMD java -jar /home/myapp-1.0.jar
</code></pre>
<p>And I simply run it on production machine with <code>docker-compose up -d</code>.</p>
<p>(BTW, I'm new to dockers. Are all of my steps correct? Am I missing anything?! I see all is fine and myapp is running though!)</p>
","<p>I have a consul server running as a docker image in a docker-compose environment, I want to copy the logs to a file in the host machine in real-time.</p>
<p>I want to automate the process of copying files.</p>
<p><strong>NOTE</strong>: I am aware of other similar answers, but none of them work for me, as I want the host machine logs file to be updated in real-time.</p>
"
"43856554","Connect to a PostgreSQL database on a Docker container","<python><django><postgresql><docker>","66817505","Why do I have connection issues between docker and postgresql?","<python><postgresql><docker><docker-compose>","<p>I want to run an application with a PostgreSQL database and a REST API powered by Django on separate Docker containers. So far the API has been running on Docker connecting to a SQLite database, but I'm having trouble now that I want to connect to a PostgreSQL database instead.</p>

<p>Docker's <code>docker-compose.yml</code> file:</p>

<pre><code>version: '2'
services:
    postgres:
        image: postgres
    api:
        build: .
        command: bash -c ""python manage.py migrate &amp;&amp; python manage.py runserver 0.0.0.0:1337""
        volumes:
            - .:/usr/src/app
        ports:
            - ""1337:1337""
        depends_on:
            - postgres
</code></pre>

<p>Django's <code>settings.py</code> (using the default settings which the base <code>postgres</code> image works with according to the documentation):</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'wgomanager',
        'USER': 'postgres',
        'PASSWORD': 'mysecretpassword',
        'HOST': 'localhost',
        'PORT': '5432',
    }
}
</code></pre>

<p>When I launch the application with <code>docker-compose up</code> eventually this error is thrown:</p>

<pre><code>api_1       |     connection = Database.connect(**conn_params)
api_1       |   File ""/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py"", line 130, in connect
api_1       |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
api_1       | django.db.utils.OperationalError: could not connect to server: Connection refused
api_1       |   Is the server running on host ""localhost"" (::1) and accepting
api_1       |   TCP/IP connections on port 5432?
api_1       | could not connect to server: Connection refused
api_1       |   Is the server running on host ""localhost"" (127.0.0.1) and accepting
api_1       |   TCP/IP connections on port 5432?
api_1       |
orchestrator_api_1 exited with code 1
</code></pre>

<p>What am I doing wrong?</p>
","<p>I am fairly new to docker and I do not know what's causing the issue to not run my python script on docker..
here is how i am creating my docker-compose.yml</p>
<pre><code>version: &quot;3.6&quot;
services:
  app :
    build: ./app/
  db:
    build: ./database/
</code></pre>
<p>Here is the error :</p>
<pre><code>File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
app_1  |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
app_1  | sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
app_1  |        Is the server running on host &quot;127.0.0.1&quot; and accepting
app_1  |        TCP/IP connections on port 5432?

</code></pre>
<p>upon running docker-compose ps:</p>
<pre><code>             Name                            Command               State     Ports  
------------------------------------------------------------------------------------
542132_app_final_db_1   docker-entrypoint.sh postgres    Up       5432/tcp
app_1               python abc ...               Exit 1
</code></pre>
<p>How do I solve it? Please help. I am fairly new to Docker/Docker-compose. Thanks!</p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","66851327","Docker initialize host mounted volume","<docker><docker-volume>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I want to create a docker image for a software that need a persistent data volume which has to be initialized with some specific files and folders and I think you could reach that be creating a volume in the dockerfile after the creation of the files.</p>
<p>But my problem is, that I want to access the files direct from the host system, but it seems that there is no way to initialize the host folder like the named volume would be.</p>
<p>Does anybody know a way to achieve such an initialization?</p>
"
"58423571","Why do I have to delete docker containers?","<docker><docker-container>","66856373","Why does Docker not remove stopped containers?","<docker>","<p>What artifacts do stopped docker containers leave on the host file system that makes it necessary to run the command ""docker container prune"". I thought docker containers where simply running versions of images that are suppose to be wiped out after they exit? In what scenarios are artifacts left behind if it is not always the case?</p>
","<p>I am new to Docker and just getting started. I pulled a basic ubuntu image and started a few containers with it and stopped them. When I run the command to list all the docker containers (even the stopped ones) I get an output like this:</p>
<pre><code>&gt; docker container ls -a
   
CONTAINER ID   IMAGE           COMMAND       CREATED          STATUS                      PORTS     NAMES
099c42011f24   ubuntu:latest   &quot;/bin/bash&quot;   6 seconds ago    Exited (0) 6 seconds ago              sleepy_mccarthy
dde61c10d522   ubuntu:latest   &quot;/bin/bash&quot;   8 seconds ago    Exited (0) 7 seconds ago              determined_rosalind
cd1a6fa35741   ubuntu:latest   &quot;/bin/bash&quot;   9 seconds ago    Exited (0) 8 seconds ago              unruffled_lichterman
ff926b6eba23   ubuntu:latest   &quot;/bin/bash&quot;   10 seconds ago   Exited (0) 10 seconds ago             cool_rosalind
8bd50c2c4729   ubuntu:latest   &quot;/bin/bash&quot;   12 seconds ago   Exited (0) 11 seconds ago             cranky_darwin
</code></pre>
<p>My question is, is there a reason why docker does not delete the stopped containers by default?</p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","66891263","Merging two dockerfiles in one","<docker><ubuntu><dockerfile><conda>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p>I have a custom docker image that i work upon.
The dockerfile i created starts with</p>
<pre><code>FROM ubuntu
</code></pre>
<p>Now, i want to add cadquery to my packages.
Hopefully, there is a dockerfile for doing that.
Here it is:</p>
<pre><code>FROM continuumio/miniconda3
RUN apt install -y libgl1-mesa-glx
RUN conda install conda-build
RUN apt-get install -y git
WORKDIR /
RUN git clone https://github.com/cadquery/cadquery.git
WORKDIR /cadquery
RUN conda env create -n cq -f environment.yml
RUN echo &quot;source activate cq&quot; &gt; ~/.bashrc
ENV PATH /opt/conda/envs/cq/bin:$PATH
WORKDIR /testing
</code></pre>
<p>(source: <a href=""https://github.com/RubenRubens/cq-testing/blob/master/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/RubenRubens/cq-testing/blob/master/Dockerfile</a>)</p>
<p>However, as you can see, it uses a different <code>FROM</code> command than mine.
What i want to do, is keep my dockerfile, and just add conda, and cadquery somehow.</p>
<p>I researched a bit into the <code>continuumio/miniconda3</code> image, but i am to sure what is the best startegy to use after all.</p>
<p>I know that you can have two different <code>FROM</code> commands in your dockerfile, but is this recommended?</p>
<p>If yes, how the structure of the dockerfile should be?</p>
<p>IF no, meaning it is NOT recommended to use two different <code>FROM</code> commands, how should i structure my dockerfile?</p>
<p>Should i use the <code>FROM</code> ubuntu dockerfile, and just add conda manually, and then proceed like this dockerfile i pasted here?</p>
<p>EDIT: I forgot to say that one important thing to note, is that miniconda is installed from a <code>.sh</code> file that you download
(source: <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html"" rel=""nofollow noreferrer"">https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</a>)</p>
<p>So i am not sure if using two <code>FROM</code> commands (and how), or just having the basic ubuntu image and downloading that <code>.sh</code> would work best.</p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","66928999","Docker and Jenkins integration","<docker><jenkins><kubernetes><digital-ocean><jenkins-docker>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I've added the BitBucket server integration plugin (<a href=""https://plugins.jenkins.io/atlassian-bitbucket-server-integration/"" rel=""nofollow noreferrer"">https://plugins.jenkins.io/atlassian-bitbucket-server-integration/</a>) and can connect to the BitBucket cloud repo from Jenkins:</p>
<p><a href=""https://i.stack.imgur.com/0cUEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0cUEI.png"" alt=""enter image description here"" /></a></p>
<p>But I receive an error when I try to build:</p>
<pre><code>/var/jenkins_home/workspace/bb_add-jenkins-file@tmp/durable-c49dbeca/script.sh: 1: /var/jenkins_home/workspace/bb_add-jenkins-file@tmp/durable-c49dbeca/script.sh: docker: not found
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
[Bitbucket] Notifying commit build result
[Bitbucket] Build result notified
ERROR: script returned exit code 127
Finished: FAILURE
</code></pre>
<p>So it seems I need to install Docker on the Jenkins instance ?</p>
<p><a href=""https://plugins.jenkins.io/docker-build-publish/"" rel=""nofollow noreferrer"">https://plugins.jenkins.io/docker-build-publish/</a></p>
<p>I'm following this tutorial to configure Docker with Jenkins: <a href=""https://medium.com/@karthi.net/docker-tutorial-build-docker-images-using-jenkins-d2880e65b74"" rel=""nofollow noreferrer"">https://medium.com/@karthi.net/docker-tutorial-build-docker-images-using-jenkins-d2880e65b74</a></p>
<p>and have reached this step:</p>
<p><a href=""https://i.stack.imgur.com/Cxtsi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cxtsi.png"" alt=""enter image description here"" /></a></p>
<p>On my own Jenkins Docker setup page I have :</p>
<p><a href=""https://i.stack.imgur.com/p5saw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p5saw.png"" alt=""enter image description here"" /></a></p>
<p>I'm unsure what Docker URL should be used? Do I need to provision a new container instance within the Kubernetes cluster and run docker within this new instance? This new Docker instance is then the <code>Docker Host URI</code> field?</p>
"
"52640304","standard_init_linux.go:190: exec user process caused ""no such file or directory"" Docker with go basic web app","<docker><go><dockerfile>","66954240","Dockerfile from scratch with Go Lang return error no such file or directory","<docker><go>","<p>The very basic web app is created in Go</p>
<pre><code>package main

import(
   &quot;fmt&quot;
   &quot;net/http&quot;
   &quot;os&quot;
)

func hostHandler(w http.ResponseWriter, r *http.Request){
    name, err :=os.Hostname()

    if err != nil {
           panic(err)
        }

        fmt.Fprintf(w, &quot;&lt;h1&gt;HOSTNAME: %s&lt;/h1&gt;&lt;br&gt;&quot;,name)
        fmt.Fprintf(w, &quot;&lt;h1&gt;ENVIRONMENT VARS: &lt;/h1&gt;&lt;br&gt;&quot;)
        fmt.Fprintf(w, &quot;&lt;ul&gt;&quot;)

        for _, evar := range os.Environ(){
            fmt.Fprintf(w, &quot;&lt;li&gt;%s&lt;/li&gt;&quot;,evar)
        }
        fmt.Fprintf(w, &quot;&lt;/ul&gt;&quot;)

}

func rootHandler(w http.ResponseWriter, r *http.Request){

    fmt.Fprintf(w, &quot;&lt;h1&gt;Awesome site in Go!&lt;/h1&gt;&lt;br&gt;&quot;)
    fmt.Fprintf(w, &quot;&lt;a href='/host/'&gt;Host info&lt;/a&gt;&lt;br&gt;&quot;)

}

func main() {

        http.HandleFunc(&quot;/&quot;, rootHandler)
        http.HandleFunc(&quot;/host/&quot;, hostHandler)
        http.ListenAndServe(&quot;:8080&quot;, nil)


}
</code></pre>
<p>Docker File for it</p>
<pre><code>FROM scratch
WORKDIR /home/ubuntu/go
COPY webapp /
EXPOSE 8080
CMD [&quot;/webapp&quot;]
</code></pre>
<p>The image is built successfully</p>
<pre><code>ubuntu@ip-172-31-32-125:~/go/src/hello$ docker build -t &quot;webapp&quot; .
Sending build context to Docker daemon  6.152MB
Step 1/5 : FROM scratch
 ---&gt;
Step 2/5 : WORKDIR /home/ubuntu/go
 ---&gt; Using cache
 ---&gt; 8810a06c58c7
Step 3/5 : COPY webapp /
 ---&gt; Using cache
 ---&gt; d75222363d3a
Step 4/5 : EXPOSE 8080
 ---&gt; Using cache
 ---&gt; 45de0853de8e
Step 5/5 : CMD [&quot;/webapp&quot;]
 ---&gt; Using cache
 ---&gt; e9f9031f3632
Successfully built e9f9031f3632
Successfully tagged webapp:latest
</code></pre>
<p>But when i run the docker its show error.</p>
<pre><code>ubuntu@ip:~/go/src/hello$ docker run webapp
standard_init_linux.go:190: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Please guide what is the issue, I am new to docker and go.</p>
<p>Environment-related information</p>
<pre><code>ubuntu@ip:~/go/src/hello$ ls
Dockerfile  webapp
ubuntu@ip:~/go/src/hello$ echo $GOPATH
/home/ubuntu/go
</code></pre>
<p>Code was compiled with <strong>go build webapp.go</strong> command</p>
","<p>here is my dockerfile content:</p>
<pre><code>FROM golang:latest as goCompiler

COPY app/main.go .

RUN go build main.go

FROM scratch

COPY --from=goCompiler /go/main /main

ENTRYPOINT [ &quot;/main&quot; ]
</code></pre>
<p>i got this error <em>standard_init_linux.go:219: exec user process caused: no such file or directory</em></p>
<p>after i change scratch to alpine i get same error but if i provide there ubuntu image everything is working</p>
<p>i watched tutorial in internet with C example and everything was good there</p>
<p>Scratch isnt working with Go Lang?</p>
<p>Solution is add flag to compile  CGO_ENABLED=0</p>
<pre><code>RUN CGO_ENABLED=0 go build main.go
</code></pre>
"
"59693655","Building tensorflow from source with Docker","<python><git><docker><tensorflow>","66967968","how to download the latest tensorflow within docker container","<linux><docker><tensorflow>","<p>Sorry for this basic question, bit of a Docker noob here.</p>

<p>I'm trying to <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">build tensorflow from source</a> following the Docker instructions:</p>

<blockquote>
  <p>docker pull tensorflow/tensorflow:devel</p>
  
  <p>docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
      tensorflow/tensorflow:devel bash</p>
  
  <p>git pull  # within the container, download the latest source code</p>
</blockquote>

<p>Here are the commands I run in the terminal (on Ubuntu), along with their output:</p>

<pre><code>$ docker --version
Docker version 19.03.2, build 6a30dfc

$ docker pull tensorflow/tensorflow:devel
devel: Pulling from tensorflow/tensorflow
Digest: sha256:0ee065743f0001f922561bcba914013929a88263ec2a5af21ba35899c3ac85a7
Status: Image is up to date for tensorflow/tensorflow:devel
docker.io/tensorflow/tensorflow:devel

$ docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
&gt;     tensorflow/tensorflow:devel bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...

root@4746a002f18e:/tensorflow# 

</code></pre>

<p>But now, if I run <code>git pull</code> as instructed, I get</p>

<pre><code>fatal: not a git repository (or any of the parent directories): .git
</code></pre>

<p>How should I instead be running these commands?</p>
","<p>I'm trying to build TensorFlow from source using docker.
I followed instructions in <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/source</a></p>
<p>Now I'm in TensorFlow container and reached to this line:</p>
<pre><code>git pull  # within the container, download the latest source code
</code></pre>
<p>What does the comment mean? If I just run <code>git pull</code> it shows:</p>
<pre><code>root@7cd7f965a7b3:/tensorflow# git pull
fatal: not a git repository (or any of the parent directories): .git
</code></pre>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","66978740","when i run docker command in ubuntu20.04 LTE on virtual box , the following error came","<docker><ubuntu-20.04>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<pre><code>docker run -it -v $(pwd):/openLANE_flow -v $PDK_ROOT:$PDK_ROOT -e PDK_ROOT=$PDK_ROOT -u $(id -u $USER):$(id -g $USER) openlane:rc6 
docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create: dial unix /var/run/docker.sock: connect: permission denied.
See 'docker run –help'.
</code></pre>
"
"48483541","Ubuntu container keep restarting","<ubuntu><docker><containers>","67018329","Why container based on ubuntu didn't run from docker-compose file, when that work for similar nginx container?","<docker><docker-compose>","<p>I'm using docker-compose to add a new ubuntu container but the container keeps restarting and I don't know why... any clue of what I can check ?</p>

<p>here my docker-compose service:</p>

<pre><code>  ubuntu:
    image: ubuntu
    container_name: ubuntu
    network_mode: host
    restart: unless-stopped
    volumes:
      - /mnt:/NAS:rw
    environment:
      - TZ=""Asia/Shanghai""
</code></pre>

<p>and here is the <em>docker ps</em> output:</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                          PORTS                                            NAMES
6c084528838c        ubuntu              ""/bin/bash""              6 minutes ago       Restarting (0) 18 seconds ago                                                    ubuntu
</code></pre>

<p>i'm using Docker 17.09 on Ubuntu server 17.04 and I'm running the container with this alias:</p>

<pre><code>alias dcrun='docker-compose -f /home/docker-compose.yml'

dcrun up -d ubuntu
</code></pre>

<p>Thank you</p>
","<p>I try to run docker container using docker-compose file instead of a long command line.</p>
<p>I want to run docker-compose file based on ubuntu:latest. Container created but can't run.</p>
<pre><code>version: &quot;3.9&quot;

services:

  ubuntu:
    image: ubuntu:latest
    container_name: nginx_from_scratch3
    ports:
     - &quot;80:80&quot;
</code></pre>
<p>But before I've tried add in my docker-compose file line</p>
<pre><code>command: bash
</code></pre>
<p>And noting change. I think what after running container continue to work. But that didn't happend.</p>
<p>But on the other side if I use nginx image all run perfectly.</p>
<pre><code>version: &quot;3.9&quot;

services:

nginx1:
  image: nginx
  container_name: nginx_from_scratch4
  ports:
    - &quot;80:80&quot;
</code></pre>
<p>Why docker-compose file for nginx image work, and doesn’t work for ubuntu image.</p>
"
"54428608","Docker Node Alpine Image Build Fails on node-gyp","<node.js><docker><npm><node-gyp><alpine>","67022931","node-gyp error in docker build, how do i install Python in dockerfile while dockerizing my testcafe project?","<python><node.js><docker><npm><testcafe>","<p>I'm attempting to Dockerize a Vue.js application. I'm using the <code>node:10.15-alpine</code> Docker image as a base. The image build fails with the following error:</p>

<pre><code>gyp ERR! configure error
gyp ERR! stack Error: Can't find Python executable ""python"", you can set the PYTHON env variable.
gyp ERR! stack     at PythonFinder.failNoPython (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:484:19)
gyp ERR! stack     at PythonFinder.&lt;anonymous&gt; (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:406:16)
gyp ERR! stack     at F (/usr/local/lib/node_modules/npm/node_modules/which/which.js:68:16)
gyp ERR! stack     at E (/usr/local/lib/node_modules/npm/node_modules/which/which.js:80:29)
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/which/which.js:89:16
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/isexe/index.js:42:5
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/isexe/mode.js:8:5
gyp ERR! stack     at FSReqWrap.oncomplete (fs.js:154:21)
gyp ERR! System Linux 4.9.125-linuxkit
gyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild""
gyp ERR! cwd /app/node_modules/inotify
gyp ERR! node -v v10.15.0
gyp ERR! node-gyp -v v3.8.0
gyp ERR! not ok
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})

npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! inotify@1.4.2 install: `node-gyp rebuild`
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the inotify@1.4.2 install script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
</code></pre>

<p>The application runs on my Ubuntu machine. And I've searched for a resolution online.</p>

<p>I tried:</p>

<pre><code>FROM node:10.15-alpine
EXPOSE 8080
RUN mkdir -p /app/src
WORKDIR /app
COPY build/ config/ dist/ static/ .babelrc .postcssrc.js index.html /app/
COPY package*.json ./
RUN apk add --no-cache make gcc g++ python &amp;&amp; \
  npm install --production --silent &amp;&amp; \
  apk del make gcc g++ python
ADD src/ /app/src/
CMD [""npm"", ""start""]
</code></pre>

<p>This fails too. The error output is quite verbose and references C/C++ code.</p>

<p>Here's my current Dockerfile:</p>

<pre><code>FROM node:10.15-alpine
EXPOSE 8080
RUN mkdir -p /app/src
WORKDIR /app
COPY build/ config/ dist/ static/ .babelrc .postcssrc.js index.html /app/
COPY package*.json ./
RUN npm install
ADD src/ /app/src/
CMD [""npm"", ""start""]
</code></pre>

<p>Can anyone help me to resolve this issue with node-gyp? I'd like to be able to run the application from with a Docker container, but I need to get the image to build first.</p>

<h3>Update</h3>

<p>Since the build was working on my Ubuntu machine, I checked the <code>node</code> version. It was 8.12, so I switch to using the <code>node:8.12-alpine</code> image and the application now works with the following Dockerfile:</p>

<pre><code>FROM node:8.12-alpine
RUN apk add g++ make python
EXPOSE 8080
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN npm install
CMD [""npm"", ""start""]
</code></pre>
","<p>Hello i am new to docker and i am dockerizing my testcafe project. I get node- gyp error while building the image</p>
<p>gyp ERR! find Python
gyp ERR! find Python Python is not set from command line or npm configuration
gyp ERR! find Python Python is not set from environment variable PYTHON
gyp ERR! find Python checking if &quot;python&quot; can be used
gyp ERR! find Python - &quot;python&quot; is not in PATH or produced an error
gyp ERR! find Python checking if &quot;python2&quot; can be used
gyp ERR! find Python - &quot;python2&quot; is not in PATH or produced an error
gyp ERR! find Python checking if &quot;python3&quot; can be used
gyp ERR! find Python - &quot;python3&quot; is not in PATH or produced an error
gyp ERR! find Python
gyp ERR! find Python **********************************************************
gyp ERR! find Python You need to install the latest version of Python.
gyp ERR! find Python Node-gyp should be able to find and use Python. If not,
gyp ERR! find Python you can try one of the following options:
gyp ERR! find Python - Use the switch --python=&quot;/path/to/pythonexecutable&quot;
gyp ERR! find Python   (accepted by both node-gyp and npm)
gyp ERR! find Python - Set the environment variable PYTHON
gyp ERR! find Python - Set the npm configuration variable python:
gyp ERR! find Python   npm config set python &quot;/path/to/pythonexecutable&quot;
gyp ERR! find Python For more information consult the documentation at:
gyp ERR! find Python <a href=""https://github.com/nodejs/node-gyp#installation"" rel=""nofollow noreferrer"">https://github.com/nodejs/node-gyp#installation</a>
gyp ERR! find Python **********************************************************</p>
<p>Here is my Dockerfile:</p>
<pre><code>FROM testcafe/testcafe
USER root
WORKDIR /app
COPY .npmrc .npmrc
COPY package*.json ./
RUN npm install
COPY . .
USER user
</code></pre>
"
"66508965","Mongo docker setup broken after reboot (unifi controller on raspberry pi)","<mongodb><docker><docker-compose>","66510896","mongod --repair causing backtrace","<mongodb><docker>","<p>i seem to have run into an unclean shutdown after a power failure that I can't seem to recover from. I've tried running mongod --repair within my controller container but it doesn't seem to help. Any suggestions? I don't want to just blow away my unifi_mongo container, since I'm not sure if I'll lose all my configs.</p>
<p>As a somewhat related question, should I be enabling journaling somehow in this config even though I'm on a 32-bit raspbian lite OS? Not sure how I'd do that, but maybe it'd prevent these sorts of issues in the future?</p>
<p><strong>docker logs -f unifi_mongo</strong></p>
<pre><code>2021-03-06T18:35:51.917+0000 I STORAGE  [initandlisten] exception in initAndListen: 12596 old lock file, terminating
2021-03-06T18:35:51.917+0000 I CONTROL  [initandlisten] dbexit:  rc: 100
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 32-bit host=a282e3addaec
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] options: { storage: { journal: { enabled: true } } }
2021-03-06T18:36:44.935+0000 W -        [initandlisten] Detected unclean shutdown - /data/db/mongod.lock is not empty.
2021-03-06T18:36:44.972+0000 I STORAGE  [initandlisten] **************
old lock file: /data/db/mongod.lock.  probably means unclean shutdown,
but there are no journal files to recover.
this is likely human error or filesystem corruption.
please make sure that your journal directory is mounted.
found 3 dbs.
see: http://dochub.mongodb.org/core/repair for more information
</code></pre>
<p><strong>docker-compose.yml:</strong></p>
<pre><code>version: '2.3'
services:
 mongo:
   #   image: mongo
   image: andresvidal/rpi3-mongodb3
   container_name: ${COMPOSE_PROJECT_NAME}_mongo
   networks:
     - unifi
   restart: always
   volumes:
     - db:/data/db
     - dbcfg:/data/configdb
 controller:
   image: &quot;jacobalberty/unifi:${TAG:-latest}&quot;
   container_name: ${COMPOSE_PROJECT_NAME}_controller
   depends_on:
     - mongo
   init: true
   networks:
     - unifi
   restart: always
   privileged: true
   volumes:
     - dir:/unifi
     - data:/unifi/data
     - log:/unifi/log
     - cert:/unifi/cert
     - init:/unifi/init.d
     - run:/var/run/unifi
     # Mount local folder for backups and autobackups
     - ./backup:/unifi/data/backup
   user: unifi
   sysctls:
     net.ipv4.ip_unprivileged_port_start: 0
   environment:
     DB_URI: mongodb://mongo/unifi
     STATDB_URI: mongodb://mongo/unifi_stat
     DB_NAME: unifi
     TZ: America/Toronto
   ports:
     - &quot;3478:3478/udp&quot; # STUN
     - &quot;1900:1900/udp&quot;
     - &quot;6789:6789/tcp&quot; # Speed test
     - &quot;8080:8080/tcp&quot; # Device/ controller comm.
     - &quot;8443:8443/tcp&quot; # Controller GUI/API as seen in a web browser
     - &quot;8880:8880/tcp&quot; # HTTP portal redirection
     - &quot;8843:8843/tcp&quot; # HTTPS portal redirection
     - &quot;10001:10001/udp&quot; # AP discovery
 logs:
   image: bash
   container_name: ${COMPOSE_PROJECT_NAME}_logs
   depends_on:
     - controller
   command: bash -c 'tail -F /unifi/log/*.log'
   restart: always
   volumes:
     - log:/unifi/log


volumes:
 db:
 dbcfg:
 data:
 log:
 cert:
 init:
 dir:
 run:

networks:
 unifi:
</code></pre>
<p>I tried blowing away the lock and re-running &quot;docker-compose up -d&quot; but it didn't solve the problem.</p>
<pre><code>unifi@me:/unifi/data/db$ ls
local  local.0  local.ns  storage.bson  version
</code></pre>
<p>Output of docker ps:</p>
<pre><code>docker ps
CONTAINER ID   IMAGE                       COMMAND                  CREATED       STATUS                            PORTS                                                                                                                                                                                              NAMES
aaaaaaaaaaaa   jacobalberty/unifi:latest   &quot;/usr/local/bin/dock…&quot;   4 weeks ago   Up 3 days (unhealthy)             0.0.0.0:1900-&gt;1900/udp, 0.0.0.0:6789-&gt;6789/tcp, 0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:8443-&gt;8443/tcp, 0.0.0.0:8843-&gt;8843/tcp, 0.0.0.0:3478-&gt;3478/udp, 0.0.0.0:10001-&gt;10001/udp, 0.0.0.0:8880-&gt;8880/tcp   unifi_controller
bbbbbbbbbbbb   andresvidal/rpi3-mongodb3   &quot;/docker-entrypoint.…&quot;   4 weeks ago   Restarting (100) 13 seconds ago                                                                                                                                                                                                      unifi_mongo
</code></pre>
<p>Do I run mongod --repair inside the mongo container? How do I do that if it keeps restarting?</p>
<p>Thanks</p>
<p>Edit:</p>
<p>I tried setting an entrypoint in the docker-compose.yml to run mongod --repair instead of the normal mongo startup, but i got this backtrace:</p>
<pre><code>2021-03-06T18:52:46.599+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T18:52:46.649+0000 I -        [initandlisten] Fatal Assertion 17441
2021-03-06T18:52:46.769+0000 I CONTROL  [initandlisten]
 0x1622348 0x15c50a0 0x15abc08 0xdc63ec 0x13e0730 0x13e026c 0x13f883c 0x13faf84 0x122a440 0xcf1838 0xcf2f1c 0xcf3acc 0xcf4e00 0xcf3e28 0x76bc9678
----- BEGIN BACKTRACE -----
{&quot;backtrace&quot;:[{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;1612348&quot;,&quot;s&quot;:&quot;_ZN5mongo15printStackTraceERSo&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;15B50A0&quot;,&quot;s&quot;:&quot;_ZN5mongo10logContextEPKc&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;159BC08&quot;,&quot;s&quot;:&quot;_ZN5mongo13fassertFailedEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;DB63EC&quot;,&quot;s&quot;:&quot;_ZN5mongo7fassertEib&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D0730&quot;,&quot;s&quot;:&quot;
_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D026C&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13E883C&quot;,&quot;s&quot;:&quot;_ZN5mongo27SimpleRecordStoreV1Iterator7getNe
xtEv&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13EAF84&quot;,&quot;s&quot;:&quot;_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;121A440&quot;,&quot;s&quot;:&quot;_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_str
ingIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE1838&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE2F1C&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3ACC&quot;,&quot;s&quot;:&quot;_ZN5mongo13initAndListenEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE4E00&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3E28&quot;,&quot;s&quot;:&quot;main&quot;},{&quot;b&quot;:&quot;76BB3000&quot;,&quot;o&quot;:&quot;16678&quot;,&quot;s&quot;:&quot;__libc_start_main&quot;}],&quot;processInfo&quot;:{ &quot;m
ongodbVersion&quot; : &quot;3.0.14&quot;, &quot;gitVersion&quot; : &quot;08352afcca24bfc145240a0fac9d28b978ab77f3&quot;, &quot;uname&quot; : { &quot;sysname&quot; : &quot;Linux&quot;, &quot;release&quot; : &quot;5.4.83-v7+&quot;, &quot;version&quot; : &quot;#1379 SMP Mon Dec 14 13:08:57 GMT 2020&quot;, &quot;machine&quot; : &quot;armv7l&quot; }, &quot;somap&quot; : [ { &quot;elfType&quot; : 2, &quot;b&quot; : &quot;10000&quot;, &quot;buildId&quot; : &quot;77BB9B
C6C28CA032211CCD119B903FDEE2C6A7D8&quot; }, { &quot;b&quot; : &quot;7EDCA000&quot;, &quot;path&quot; : &quot;linux-vdso.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;8E8ADD944B36D89CB5A4AE6DAB825D428D5407ED&quot; }, { &quot;b&quot; : &quot;76F22000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/librt.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4C7E415AA306267E5BA73CD0FE8F6F
8ABC5D9370&quot; }, { &quot;b&quot; : &quot;76F0F000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libdl.so.2&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;99B3CD788031A72A37B9C9F10C5A63FEABF1BCDB&quot; }, { &quot;b&quot; : &quot;76DC7000&quot;, &quot;path&quot; : &quot;/usr/lib/arm-linux-gnueabihf/libstdc++.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;5909F48F93D947CDD017977DA4
79EC563E8B426E&quot; }, { &quot;b&quot; : &quot;76D48000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libm.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;1128E26D3F2FA311FE65EDF9E3930D2162AF9BE8&quot; }, { &quot;b&quot; : &quot;76D1B000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libgcc_s.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;030EF284554E9F6259572226A3F2
6F86F86E1B35&quot; }, { &quot;b&quot; : &quot;76CF2000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libpthread.so.0&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4B15D4A8FE60C9A013D924976C36C1281A60E04D&quot; }, { &quot;b&quot; : &quot;76BB3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libc.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;B84C7156F66DE515C6257D0A4A71
D9F31CE6F9CF&quot; }, { &quot;b&quot; : &quot;76F39000&quot;, &quot;path&quot; : &quot;/lib/ld-linux-armhf.so.3&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;21F72FB00897D4F06093D6F0451C9CA7D1F6E14C&quot; } ] }}
 mongod(_ZN5mongo15printStackTraceERSo+0x2C) [0x1622348]
 mongod(_ZN5mongo10logContextEPKc+0x88) [0x15c50a0]
 mongod(_ZN5mongo13fassertFailedEi+0x78) [0x15abc08]
 mongod(_ZN5mongo7fassertEib+0x34) [0xdc63ec]
 mongod(_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE+0x90) [0x13e0730]
 mongod(_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE+0x30) [0x13e026c]
 mongod(_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv+0x8C) [0x13f883c]
 mongod(_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0xD00) [0x13faf84]
 mongod(_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0x1C8) [0x122a440]
 mongod(+0xCE1838) [0xcf1838]
 mongod(+0xCE2F1C) [0xcf2f1c]
 mongod(_ZN5mongo13initAndListenEi+0x20) [0xcf3acc]
 mongod(+0xCE4E00) [0xcf4e00]
 mongod(main+0x28) [0xcf3e28]
 libc.so.6(__libc_start_main+0x114) [0x76bc9678]
-----  END BACKTRACE  -----
2021-03-06T18:52:46.770+0000 I -        [initandlisten]

***aborting after fassert() failure
</code></pre>
<p>Edit2: Trying to run a repair manually doesn't seem to solve the problem</p>
<pre><code>docker run -it -v db:/data/db andresvidal/rpi3-mongodb3:latest mongod --repair
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm/v7) and no specific platform was requested
2021-03-06T19:23:47.049+0000 I CONTROL
2021-03-06T19:23:47.049+0000 W CONTROL  32-bit servers don't have journaling enabled by default. Please use --journal if you want durability.
2021-03-06T19:23:47.049+0000 I CONTROL
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 32-bit host=fa58e6e86cff
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] options: { repair: true }
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] ** NOTE: This is a 32 bit MongoDB binary.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       32 bit builds are limited to less than 2GB of data (or less with --journal).
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       Note that journaling defaults to off for 32 bit and is currently off.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       See http://dochub.mongodb.org/core/32bit
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.166+0000 I STORAGE  [initandlisten] finished checking dbs
2021-03-06T19:23:47.167+0000 I CONTROL  [initandlisten] now exiting
2021-03-06T19:23:47.167+0000 I NETWORK  [initandlisten] shutdown: going to close listening sockets...
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] removing socket file: /tmp/mongodb-27017.sock
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] shutdown: going to flush diaglog...
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] shutdown: going to close sockets...
2021-03-06T19:23:47.168+0000 I STORAGE  [initandlisten] shutdown: waiting for fs preallocator...
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] shutdown: closing all files...
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] closeAllFiles() finished
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] shutdown: removing fs lock...
2021-03-06T19:23:47.169+0000 I CONTROL  [initandlisten] dbexit:  rc: 0
</code></pre>
<p>Edit 3:</p>
<p>mongodump --repair -d /data/db on a stopped instance can't find the database</p>
<p>mongodump --repair -d /data/db on a running mongo gives me the following, after which my container crashes again.</p>
<pre><code> Failed: error getting collections for database `/data/db`: error running `listCollections`. Database: `/data/db` Err: Invalid ns [/data/db.$cmd]
</code></pre>
<p>mongodump --repair on a running mongo instance gives me:</p>
<pre><code>2021-03-07T17:31:44.556+0000    writing repair of unifi.wlanconf to dump/unifi/wlanconf.bson
2021-03-07T17:31:44.560+0000            repair cursor found 4 documents in unifi.wlanconf
2021-03-07T17:31:44.561+0000    writing unifi.wlanconf metadata to dump/unifi/wlanconf.metadata.json
2021-03-07T17:31:44.564+0000    done dumping unifi.wlanconf (0 documents)
2021-03-07T17:31:44.565+0000    writing repair of unifi.site to dump/unifi/site.bson
2021-03-07T17:31:44.569+0000            repair cursor found 4 documents in unifi.site
2021-03-07T17:31:44.569+0000    writing unifi.site metadata to dump/unifi/site.metadata.json
2021-03-07T17:31:44.572+0000    done dumping unifi.site (0 documents)
2021-03-07T17:31:44.572+0000    writing repair of unifi.networkconf to dump/unifi/networkconf.bson
2021-03-07T17:31:44.576+0000            repair cursor found 4 documents in unifi.networkconf
2021-03-07T17:31:44.576+0000    writing unifi.networkconf metadata to dump/unifi/networkconf.metadata.json
2021-03-07T17:31:44.579+0000    done dumping unifi.networkconf (0 documents)
2021-03-07T17:31:44.580+0000    writing repair of unifi.privilege to dump/unifi/privilege.bson
2021-03-07T17:31:44.610+0000            repair cursor found 4 documents in unifi.privilege
2021-03-07T17:31:44.610+0000    writing unifi.privilege metadata to dump/unifi/privilege.metadata.json
2021-03-07T17:31:44.616+0000    done dumping unifi.privilege (0 documents)
2021-03-07T17:31:44.616+0000    writing repair of unifi.apgroup to dump/unifi/apgroup.bson
2021-03-07T17:31:44.640+0000            repair cursor found 2 documents in unifi.apgroup
2021-03-07T17:31:44.640+0000    writing unifi.apgroup metadata to dump/unifi/apgroup.metadata.json
2021-03-07T17:31:44.672+0000    done dumping unifi.apgroup (0 documents)
2021-03-07T17:31:44.675+0000    Failed: repair error: error reading collection: EOF
</code></pre>
","<p>I have a unifi controller running on docker on raspberry pi, but I seem to have some corruption in my mongo database due to an unclean shutdown. I've tried to mongod --repair, but I get the following error. What should I do to recover this? Thanks!</p>
<pre><code>021-03-06T21:34:14.261+0000 I CONTROL
2021-03-06T21:34:14.265+0000 W CONTROL  32-bit servers don't have journaling enabled by default. Please use --journal if you want durability.
2021-03-06T21:34:14.265+0000 I CONTROL
2021-03-06T21:34:14.288+0000 I CONTROL  [initandlisten] MongoDB starting : pid=15 port=27017 dbpath=/data/db 32-bit host=ae6010a02591
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] options: { repair: true }
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten]
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten]
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] ** NOTE: This is a 32 bit MongoDB binary.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       32 bit builds are limited to less than 2GB of data (or less with --journal).
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       Note that journaling defaults to off for 32 bit and is currently off.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       See http://dochub.mongodb.org/core/32bit
2021-03-06T21:34:14.371+0000 I CONTROL  [initandlisten]
</code></pre>
<p>...</p>
<pre><code>2021-03-06T21:34:34.406+0000 I STORAGE  [initandlisten] repairDatabase unifi_stat
2021-03-06T21:34:35.338+0000 I INDEX    [initandlisten] allocating new ns file /data/db/_tmp_repairDatabase_16/unifi_stat.ns, filling with zeroes...
2021-03-06T21:34:36.845+0000 I STORAGE  [FileAllocator] allocating new datafile /data/db/_tmp_repairDatabase_16/unifi_stat.0, filling with zeroes...
2021-03-06T21:34:36.846+0000 I STORAGE  [FileAllocator] creating directory /data/db/_tmp_repairDatabase_16/_tmp
2021-03-06T21:34:36.863+0000 I STORAGE  [FileAllocator] done allocating datafile /data/db/_tmp_repairDatabase_16/unifi_stat.0, size: 64MB,  took 0.011 secs
2021-03-06T21:34:36.885+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.885+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:36.888+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { site_id: 1, datetime: 1 }, name: &quot;site_id_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.888+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:36.893+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { datetime: 1, o: 1, site_id: 1, oid: 1 }, name: &quot;datetime_1_o_1_site_id_1_oid_1&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.893+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.449+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.449+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.452+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { site_id: 1, o: 1, oid: 1, datetime: 1 }, name: &quot;site_id_1_o_1_oid_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.453+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.456+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { datetime: 1 }, name: &quot;datetime_1&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.456+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.075+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.075+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.077+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { site_id: 1, datetime: 1 }, name: &quot;site_id_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.077+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.080+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { datetime: 1, o: 1, site_id: 1, oid: 1 }, name: &quot;datetime_1_o_1_site_id_1_oid_1&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.081+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.454+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_dpi properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_dpi&quot; }
2021-03-06T21:34:39.454+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.457+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_dpi properties: { v: 1, key: { site_id: 1, o: 1, oid: 1, datetime: 1, app: 1, cat: 1 }, name: &quot;site_id_1_o_1_oid_1_datetime_1_app_1_cat_1&quot;, ns: &quot;unifi_stat.stat_dpi&quot; }
2021-03-06T21:34:39.457+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.490+0000 I -        [initandlisten] Fatal Assertion 17441
2021-03-06T21:34:39.629+0000 I CONTROL  [initandlisten]
 0x1622348 0x15c50a0 0x15abc08 0xdc63ec 0x13e0730 0x13e026c 0x13f883c 0x13faf84 0x122a440 0xcf1838 0xcf2f1c 0xcf3acc 0xcf4e00 0xcf3e28 0x76b7a678
----- BEGIN BACKTRACE -----
{&quot;backtrace&quot;:[{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;1612348&quot;,&quot;s&quot;:&quot;_ZN5mongo15printStackTraceERSo&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;15B50A0&quot;,&quot;s&quot;:&quot;_ZN5mongo10logContextEPKc&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;159BC08&quot;,&quot;s&quot;:&quot;_ZN5mongo13fassertFailedEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;DB63EC&quot;,&quot;s&quot;:&quot;_ZN5mongo7fassertEib&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D0730&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D026C&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13E883C&quot;,&quot;s&quot;:&quot;_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13EAF84&quot;,&quot;s&quot;:&quot;_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;121A440&quot;,&quot;s&quot;:&quot;_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE1838&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE2F1C&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3ACC&quot;,&quot;s&quot;:&quot;_ZN5mongo13initAndListenEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE4E00&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3E28&quot;,&quot;s&quot;:&quot;main&quot;},{&quot;b&quot;:&quot;76B64000&quot;,&quot;o&quot;:&quot;16678&quot;,&quot;s&quot;:&quot;__libc_start_main&quot;}],&quot;processInfo&quot;:{ &quot;mongodbVersion&quot; : &quot;3.0.14&quot;, &quot;gitVersion&quot; : &quot;08352afcca24bfc145240a0fac9d28b978ab77f3&quot;, &quot;uname&quot; : { &quot;sysname&quot; : &quot;Linux&quot;, &quot;release&quot; : &quot;5.4.83-v7+&quot;, &quot;version&quot; : &quot;#1379 SMP Mon Dec 14 13:08:57 GMT 2020&quot;, &quot;machine&quot; : &quot;armv7l&quot; }, &quot;somap&quot; : [ { &quot;elfType&quot; : 2, &quot;b&quot; : &quot;10000&quot;, &quot;buildId&quot; : &quot;77BB9BC6C28CA032211CCD119B903FDEE2C6A7D8&quot; }, { &quot;b&quot; : &quot;7ECB5000&quot;, &quot;path&quot; : &quot;linux-vdso.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;8E8ADD944B36D89CB5A4AE6DAB825D428D5407ED&quot; }, { &quot;b&quot; : &quot;76ED3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/librt.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4C7E415AA306267E5BA73CD0FE8F6F8ABC5D9370&quot; }, { &quot;b&quot; : &quot;76EC0000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libdl.so.2&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;99B3CD788031A72A37B9C9F10C5A63FEABF1BCDB&quot; }, { &quot;b&quot; : &quot;76D78000&quot;, &quot;path&quot; : &quot;/usr/lib/arm-linux-gnueabihf/libstdc++.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;5909F48F93D947CDD017977DA479EC563E8B426E&quot; }, { &quot;b&quot; : &quot;76CF9000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libm.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;1128E26D3F2FA311FE65EDF9E3930D2162AF9BE8&quot; }, { &quot;b&quot; : &quot;76CCC000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libgcc_s.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;030EF284554E9F6259572226A3F26F86F86E1B35&quot; }, { &quot;b&quot; : &quot;76CA3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libpthread.so.0&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4B15D4A8FE60C9A013D924976C36C1281A60E04D&quot; }, { &quot;b&quot; : &quot;76B64000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libc.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;B84C7156F66DE515C6257D0A4A71D9F31CE6F9CF&quot; }, { &quot;b&quot; : &quot;76EEA000&quot;, &quot;path&quot; : &quot;/lib/ld-linux-armhf.so.3&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;21F72FB00897D4F06093D6F0451C9CA7D1F6E14C&quot; } ] }}
 mongod(_ZN5mongo15printStackTraceERSo+0x2C) [0x1622348]
 mongod(_ZN5mongo10logContextEPKc+0x88) [0x15c50a0]
 mongod(_ZN5mongo13fassertFailedEi+0x78) [0x15abc08]
 mongod(_ZN5mongo7fassertEib+0x34) [0xdc63ec]
 mongod(_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE+0x90) [0x13e0730]
 mongod(_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE+0x30) [0x13e026c]
 mongod(_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv+0x8C) [0x13f883c]
 mongod(_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0xD00) [0x13faf84]
 mongod(_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0x1C8) [0x122a440]
 mongod(+0xCE1838) [0xcf1838]
 mongod(+0xCE2F1C) [0xcf2f1c]
 mongod(_ZN5mongo13initAndListenEi+0x20) [0xcf3acc]
 mongod(+0xCE4E00) [0xcf4e00]
 mongod(main+0x28) [0xcf3e28]
 libc.so.6(__libc_start_main+0x114) [0x76b7a678]
-----  END BACKTRACE  -----
2021-03-06T21:34:39.630+0000 I -        [initandlisten]

***aborting after fassert() failure
</code></pre>
"
"66472758","Creating a symbolic in shared volume of docker and accessing it in host machine","<python><linux><docker><unix><symlink>","66566429","How to make symbolic links point to files inside a docker container when mounting a directory","<docker><symlink><mount>","<p>I am creating a symbolic link in mounted volume of a host machine inside a docker. But I am unable to access it in host machine. Is it possible to do it. If yes how can I do that.</p>
<p>I used the following command to mount directory</p>
<pre><code>docker run -it --rm --net host -v $(pwd):/workspace --name myproject my-container:dev
</code></pre>
<p>Then I created a symbolic link using</p>
<pre><code>import os
fname = '/workspace/log/project_info_hostinfo_timeinfo_exe_param.log'
symlink_name = '/workspace/log/project_info.log'
os.symlink(fname, symlink_name)
</code></pre>
<p>Now when I am trying to see log info it looks like</p>
<pre><code>$ls
lrwxrwxrwx 1 root root   66 Mar  4 14:54 project_info.log -&gt; /workspace/log/project_info_hostinfo_timeinfo_exe_param.log
-rw-r--r-- 1 root root  206 Mar  4 14:54 project_info_hostinfo_timeinfo_exe_param.log
</code></pre>
<p>But when I try to open file I got message like</p>
<pre><code>$tail -f project_info.log
tail: cannot open 'project_info.log' for reading: No such file or directory
tail: no files remaining
</code></pre>
","<p>I have created a symbolic link to a file on a host machine. Then I have mounted a directory which contains the file and the link to a docker container. I expect the link inside the container to point to the file inside the container. However it points to the file on host, so it is broken inside the container. Is there a way to make all the links inside a container point to files inside a container and links on host point to files on host? Note that I wouldn't like to modify the links itself.</p>
<p>An example:</p>
<pre><code>cd /home/myuser/work/tmp
touch a.txt
ln -s &quot;$(pwd)&quot;/a.txt b.link
docker run -it -v &quot;$(pwd)&quot;:/some ubuntu bash
ls -lah /some/b.link
lrwxrwxrwx 1 1010 1014 30 Mar 10 13:45 /some/b.link -&gt; /home/myuser/work/tmp/a.txt
</code></pre>
<p>Expected output:</p>
<pre><code>lrwxrwxrwx 1 1010 1014 30 Mar 10 13:45 /some/b.link -&gt; /some/a.txt
</code></pre>
"
"66544508","golang Infinite for loop problem with docker run","<docker><go><dockerfile>","66576561","Wait for user input golang cli app on docker","<docker><go>","<p>I tried to do simple infinite for loop task. It is working fine without using docker. But when i used the docker,it only executes the else part of for loop infinitely. What may be problem actually? Is docker having problem with infinite for loop?
My main.go file is shown below.</p>
<pre><code>package main

 import (
&quot;bufio&quot;
&quot;fmt&quot;
&quot;os&quot;
 )

func main() {
 fmt.Println(&quot;Hello, World!.....&quot;)

 for {
    fmt.Print(&quot;-&gt; &quot;)
    var i int
    fmt.Scan(&amp;i)
    if i == 1 {
        fmt.Println(&quot;Hello, World! 1&quot;)
    } else if i == 2 {

        fmt.Println(&quot;Hello, World! 2&quot;)
    } else if i == 3 {
        fmt.Println(&quot;Hello, World! 3&quot;)
    } else if i == 4 {
        fmt.Println(&quot;Hello, World! 4&quot;)
    } else if i == 5 {
        fmt.Println(&quot;Hello, World! 5&quot;)
    } else {
        fmt.Println(&quot;Hello, World! else&quot;)
        
    }

 }
}
</code></pre>
<p>I tried these link as well. <a href=""https://stackoverflow.com/questions/40035635/read-line-in-golang/40061275#40061275"">Read line in golang</a>   <a href=""https://stackoverflow.com/questions/46476170/how-do-i-break-out-of-an-infinite-loop-in-golang"">How do I break out of an infinite loop in Golang</a>  But still of no use. I am trying to solve the problem since yesterday.</p>
<p>The docker file is as given below:</p>
<pre><code>FROM golang:1.12.0-alpine3.9

RUN mkdir /app

ADD . /app

WORKDIR /app

RUN go build -o main .

CMD [&quot;go&quot;,&quot;run&quot;,&quot;/app/main.go&quot;]
</code></pre>
<p>I tried to build the docker using
<strong>docker build -t hello .</strong>
and run using <code>docker run hello</code></p>
<p>Running with</p>
<blockquote>
<p>docker run hello</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/vpK7c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vpK7c.png"" alt=""I tried to run using docker run hello, it only prints else part infinetely"" /></a></p>
<p>Executing with console without docker <code>go run main.go</code>
<a href=""https://i.stack.imgur.com/dWpzr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dWpzr.png"" alt=""I tried to run using go run main.go and it works fine"" /></a></p>
","<p>Hello everyone i am trying to learn how to wait for user input with golang cli app in docker, her is my main.go file</p>
<pre><code>package main

import (
  &quot;bufio&quot;
  &quot;fmt&quot;
  &quot;os&quot;
  &quot;strings&quot;
  &quot;strconv&quot;
)

func main(){
    reader := bufio.NewReader(os.Stdin)
    var errConv error
    for {
        fmt.Print(&quot;-&gt; &quot;)
        inputText, _ := reader.ReadBytes('\n')
        // Receive input
        text := string(inputText)
        var command []string 
        for _, word := range strings.Fields(text) {
            command = append(command,word)
        }

        if command[0] == &quot;create_day_max&quot;{
           //do something
        }
    }
}
</code></pre>
<p>this is my Dockerfile</p>
<pre><code>    # Start from golang base image
    FROM golang:1.15.2
    
    # Copy the source from the current directory to the working Directory inside the container 
    COPY . /usr/src/golang-tunaiku
    
    # Move to working directory
    WORKDIR /usr/src/golang-tunaiku
    
    #install depedencies
    RUN go mod download
    
    # Build the application
    RUN go build -o golang-tunaiku
    
    #Command to run the executable
    CMD [ &quot;./golang-tunaiku&quot; ]

and this is my docker-compose.yml file

version: '3'
services:
  app:
    container_name: golang-linkaja
    build: .
</code></pre>
<p>it run fine when i do go run main.go but when i use <code>docker-compose up --build</code> , it just does not wait for user input, thus it create error like this</p>
<p><code>-&gt; panic: runtime error: index out of range</code></p>
<br>
which is obvious since it is forcing to read index 0 while there is no user input, 
How do i wait for user input on golang clip app with docker compose?
Thanks in advance i am stil learning
"
"66159404","Parameterising Docker Image prefix","<docker><dockerfile><aws-codebuild><amazon-ecr>","66875286","How do I avoid code duplication in Dockerfile?","<docker><dockerfile>","<p>I have a Dockerfile which has the following content</p>
<pre><code># Build
FROM ${ECR_PREFIX}/maven:3.6.3-jdk-11 AS build
COPY src /home/app/src
COPY pom.xml /home/app
RUN mvn -f /home/app/pom.xml clean package

# Package
FROM ${ECR_PREFIX}/openjdk:11-jre-slim
COPY --from=build /home/app/target/application.jar application.jar
EXPOSE 8080
ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;application.jar&quot;]
</code></pre>
<p>I tried to build this using</p>
<pre><code>export PREFIX=${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
docker build -t backend --build-arg ECR_PREFIX=$PREFIX .
</code></pre>
<p>but this would not work. I am really trying not to hard code the prefix especially <strong>${AWS::AccountId}</strong> part of it because of various reasons.</p>
<p>Any pointers here?</p>
","<p>In my Dockerfile I have two almost identical FROM statements:</p>
<pre><code>FROM 9999999999999999.dkr.ecr.us-east.amazonaws.com/sqlserver:myTag AS builder

...

FROM 9999999999999999.dkr.ecr.us-east.amazonaws.com/sqlserver:myTag 
</code></pre>
<p>How do I avoid code duplication here?</p>
"
"66844316","Bundler::GemNotFound: Could not find mimemagic-0.3.5 in any of the sources on Rails project with Docker","<ruby-on-rails><ruby><docker>","66919504","Your bundle is locked to mimemagic (0.3.5), but that version could not be found in any of the sources listed in your Gemfile","<ruby-on-rails><ruby-on-rails-6><mimemagic>","<p>I'm aware of the recent mimemagic issues, which I managed to resolve on one of my Rails projects by bundle updating to 0.3.7 - but for some reason, I can't resolve it on the project below.</p>
<p>I have a Rails 6 project which I'm setting up <strong>for the first time</strong> on a new laptop.  My laptop doesn't have the correct Ruby setup, so I've added a Dockerfile to my project like so:-</p>
<p>Dockerfile</p>
<pre><code>FROM ruby:2.7
RUN apt-get update -qq &amp;&amp; apt-get install -y nodejs postgresql-client
WORKDIR /radius
COPY Gemfile /myapp/Gemfile
COPY Gemfile.lock /myapp/Gemfile.lock
RUN bundle update mimemagic
RUN bundle install
COPY . /myapp


COPY entrypoint.sh /usr/bin/
RUN chmod +x /usr/bin/entrypoint.sh
ENTRYPOINT [&quot;entrypoint.sh&quot;]
EXPOSE 3000


CMD [&quot;rails&quot;, &quot;server&quot;, &quot;-b&quot;, &quot;0.0.0.0&quot;]
</code></pre>
<p>Gemfile</p>
<pre><code># frozen_string_literal: true

source 'https://rubygems.org'
git_source(:github) { |repo| &quot;https://github.com/#{repo}.git&quot; }

ruby '2.7.2'
gem 'airbrake'

# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'
gem 'dotenv-rails'
gem 'rails', '~&gt; 6.0.3.6'
# Use postgresql as the database for Active Record
gem 'pg', '&gt;= 0.18', '&lt; 2.0'
# Use Puma as the app server
gem 'puma', '~&gt; 3.11'
# Use SCSS for stylesheets
gem 'sass-rails', '~&gt; 5'
gem 'shortener'

# Use Honeybadger for error reporting/monitoring
gem 'honeybadger', '~&gt; 4.0'
# Transpile app-like JavaScript. Read more: https://github.com/rails/webpacker
gem 'webpacker', '~&gt; 4.0'
# Turbolinks makes navigating your web application faster. Read more: https://github.com/turbolinks/turbolinks
gem 'turbolinks', '~&gt; 5'
# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder
gem 'jbuilder', '~&gt; 2.5'
# Use Redis adapter to run Action Cable in production
# gem 'redis', '~&gt; 4.0'
# Use Active Model has_secure_password
# gem 'bcrypt', '~&gt; 3.1.7'
gem 'newrelic_rpm'

# Use Active Storage variant
# gem 'image_processing', '~&gt; 1.2'

# Reduces boot times through caching; required in config/boot.rb
gem 'bootsnap', '&gt;= 1.4.2', require: false

group :development, :test do
  gem 'awesome_print'
  gem 'byebug', platforms: %i[mri mingw x64_mingw]
  gem 'pry-byebug'
  # Version specified as workaround for this issue https://github.com/rails/rails/issues/35417
  gem 'rspec-rails', '~&gt; 4.0.0.beta2'
end

group :development do
  gem 'factory_bot_rails', '~&gt; 4.0'
  gem 'fasterer'
  gem 'haml_lint', require: false
  gem 'listen', '&gt;= 3.0.5', '&lt; 3.2'
  gem 'rails_best_practices'
  gem 'reek'
  gem 'rubocop'
  gem 'scss_lint', require: false
  gem 'spring'
  gem 'spring-watcher-listen', '~&gt; 2.0.0'
  gem 'web-console', '&gt;= 3.3.0'
end

group :test do
  # Adds support for Capybara system testing and selenium driver
  gem 'capybara', '&gt;= 2.15'
  gem 'rspec_junit_formatter'
  gem 'selenium-webdriver'
  # Easy installation and use of web drivers to run system tests with browsers
  gem 'webdrivers'
  gem 'webmock'
end

# Windows does not include zoneinfo files, so bundle the tzinfo-data gem
gem 'tzinfo-data', platforms: %i[mingw mswin x64_mingw jruby]

gem 'administrate', '0.12.0'
gem 'bootstrap', '~&gt; 4.3', '&gt;= 4.3.1'
gem 'devise', '~&gt; 4.6', '&gt;= 4.6.1'
gem 'devise-bootstrapped',  github: 'excid3/devise-bootstrapped',
                            branch: 'bootstrap4'
gem 'devise_masquerade', '~&gt; 0.6.2'
gem 'font-awesome-sass', '~&gt; 5.6', '&gt;= 5.6.1'
gem 'friendly_id', '~&gt; 5.2', '&gt;= 5.2.5'
gem 'gibbon'
gem 'gmaps4rails'
gem 'gravatar_image_tag', github: 'mdeering/gravatar_image_tag'
gem 'httparty'
gem 'mini_magick', '~&gt; 4.9', '&gt;= 4.9.2'
gem 'name_of_person', '~&gt; 1.1'
gem 'omniauth-facebook', '~&gt; 5.0'
gem 'omniauth-github', '~&gt; 1.3'
gem 'omniauth-twitter', '~&gt; 1.4'
gem 'pundit', '~&gt; 1.1.0'
gem 'sidekiq', '~&gt; 5.2', '&gt;= 5.2.5'
gem 'sitemap_generator', '~&gt; 6.0', '&gt;= 6.0.1'
gem 'underscore-rails'
gem 'whenever', require: false

</code></pre>
<p>I then try:-</p>
<pre><code>docker-compose build
docker-compose up
</code></pre>
<p>Along with every combination of the following:-</p>
<ul>
<li>Using <code>bundle update mimemagic</code> in my dockerfile</li>
<li>Adding the gem specifically to my Gemfile with <code>gem 'mimemagic', '~&gt; 0.3.7'</code></li>
<li>Manually changing my Gemfile.lock mimemagic version from <code>0.3.5</code> to <code>0.3.7</code></li>
<li>Adding <code>RUN apt-get install shared-mime-info</code> to my Dockefile</li>
</ul>
<p>and I simply get one of the below:-</p>
<pre><code>Could not find mimemagic-0.3.x in any of the sources
</code></pre>
<pre><code>web_1  | Bundler::GemNotFound: You have requested:
web_1  |   mimemagic ~&gt; 0.3.7
web_1  | 
web_1  | The bundle currently has mimemagic locked at 0.3.5.
web_1  | Try running `bundle update mimemagic`

</code></pre>
<p>I've also tried bumping rails to <code>6.0.3.6</code> and running the docker build again, which gives me activesupport issues:-</p>
<pre><code>web_1  | Bundler::VersionConflict: Bundler could not find compatible versions for gem &quot;activesupport&quot;:
web_1  |   In snapshot (Gemfile.lock):
web_1  |     activesupport (= 6.0.3.5)
web_1  | 
web_1  |   In Gemfile:
web_1  |     rails (~&gt; 6.0.3.6) was resolved to 6.0.3.6, which depends on
web_1  |       activesupport (= 6.0.3.6)
web_1  | 
web_1  |     dotenv-rails was resolved to 2.7.6, which depends on
web_1  |       railties (&gt;= 3.2) was resolved to 6.0.3.5, which depends on
web_1  |         activesupport (= 6.0.3.5)
web_1  | 
web_1  | Running `bundle update` will rebuild your snapshot from scratch, using only
web_1  | the gems in your Gemfile, which may resolve the conflict.

</code></pre>
<p>What exactly do I need to do to get this gem to install?</p>
","<p>Today I tried to build a docker for my rails 6.1.0 with active storage, I got the following error:</p>
<pre><code>Your bundle is locked to mimemagic (0.3.5), but that version could not be found in any of the sources listed in your Gemfile. If you haven't changed sources, that means the author of mimemagic (0.3.5) has removed it. You'll need to update your
bundle to a version other than mimemagic (0.3.5) that hasn't been removed in order to install.
</code></pre>
<p>And now I try to install the mimemagic</p>
<pre><code> gem install mimemagic -v 0.3.5
</code></pre>
<p>I've got the following error:</p>
<pre><code>ERROR:  Could not find a valid gem 'mimemagic' (= 0.3.5) in any repository
ERROR:  Possible alternatives: mimemagic
</code></pre>
<p>It seems like the gem is gone. Can someone explain this?</p>
<p>what can be the solution to this issue?</p>
"
"63889102","Unable to run docker image due to libGl error","<python-3.x><docker><opencv><matplotlib><dockerfile>","66921547","Failed importing cv2 into Docker [python]","<python><docker><cv2>","<p>Dockerfile</p>
<pre><code>FROM python:3.6.8
COPY . /app
WORKDIR /app
RUN pip3 install --upgrade pip
RUN pip3 install opencv-python==4.3.0.38
RUN pip3 install -r requirements.txt
EXPOSE 80
CMD [&quot;python3&quot;, &quot;server.py&quot;]
</code></pre>
<p>requirements.txt</p>
<pre><code>Flask==0.12
Werkzeug==0.16.1
boto3==1.14.40
torch
torchvision==0.7.0
numpy==1.15.4
sklearn==0.0
scipy==1.2.1
scikit-image==0.14.2
pandas==0.24.2

</code></pre>
<p>The docker build succeeds but the docker run fails with the error</p>
<pre><code>INFO:matplotlib.font_manager:Generating new fontManager, this may take some time...
PyTorch Version:  1.6.0
Torchvision Version:  0.7.0
Traceback (most recent call last):
  File &quot;server.py&quot;, line 7, in &lt;module&gt;
    from pipeline_prediction.pipeline import ml_pipeline 
  File &quot;/app/pipeline_prediction/pipeline.py&quot;, line 3, in &lt;module&gt;
    from segmentation_color import get_swatch_color_from_segmentation
  File &quot;pipeline_prediction/segmentation_color.py&quot;, line 7, in &lt;module&gt;
    import cv2
  File &quot;/usr/local/lib/python3.6/site-packages/cv2/__init__.py&quot;, line 5, in &lt;module&gt;
    from .cv2 import *
ImportError: libGL.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I looked at answer <a href=""https://stackoverflow.com/questions/47710022/import-matplotlib-pyplot-as-plt-importerror-libgl-so-1-cannot-open-shared-obj"">import matplotlib.pyplot as plt, ImportError: libGL.so.1: cannot open shared object file: No such file or directory</a>  relating to it and replaced</p>
<pre><code>import matplotlib.pyplot as plt
</code></pre>
<p>with</p>
<pre><code>import matplotlib
matplotlib.use(&quot;Agg&quot;)
import matplotlib.pyplot as plt
</code></pre>
<p>but it is not working for me. Also looked at <a href=""https://stackoverflow.com/questions/55313610/importerror-libgl-so-1-cannot-open-shared-object-file-no-such-file-or-directo"">ImportError: libGL.so.1: cannot open shared object file: No such file or directory</a> but I do not have Ubuntu as base image so this installation would not work for me as listed in the answer.</p>
<p>Let me know a way to make this work.</p>
","<p>I am using the Opencv library, in my IDE I run everything fine, but when I run it in a docker container I get the following error</p>
<p><strong>Error code</strong></p>
<pre><code>Traceback (most recent call last):
File &quot;/usr/src/app/./main.py&quot;, line 2, in &lt;module&gt;
from cv2 import cv2
File &quot;/usr/local/lib/python3.9/site-packages/cv2/__init__.py&quot;, line 5, in &lt;module&gt;
from .cv2 import *
ImportError: libGL.so.1: cannot open shared object file: No such file or directory

</code></pre>
<p><strong>Mi dockerfile</strong></p>
<pre><code>FROM python:3
WORKDIR /usr/src/app
COPY requirements.txt ./
RUN pip3 install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;,&quot;./main.py&quot;]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>numpy==1.20.2
opencv-python==4.5.1.48
</code></pre>
<p>Any idea why this error is happening, or what I'm doing wrong</p>
"
"64804749","Docker build not showing any output from commands","<docker><docker-build>","66968821","How to show container logs from remote Docker?","<windows><docker><sh><remote-access>","<p>Snippet from dockerfile:</p>
<pre><code>FROM node:12.18.0
RUN echo &quot;hello world&quot;
RUN psql --version
</code></pre>
<p>When I run <code>docker build .</code> I don't see any output from these two commands even if they are not cached. The documentation says that <code>docker build</code> is verbose by default. Why am I not seeing the output from commands? I used to see them before.</p>
<p>The output while building: <a href=""https://i.stack.imgur.com/xHI6A.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xHI6A.png"" alt=""enter image description here"" /></a>
The output I am seeing after building finishes: <a href=""https://i.stack.imgur.com/mBjhy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mBjhy.png"" alt=""enter image description here"" /></a></p>
<p>Dockerfile is created from node:12.18.0 which is based on Debian 9</p>
<p>Docker version 19.03.13, build 4484c46d9d.</p>
","<p>I've setup Docker at my Windows machine, since my Windows machine is more powerful than my Macbook. However, I do like MacOS more.</p>
<p>So I've setup a Docker context to use remote Windows machine using command <code>docker context create remote ‐‐docker “host=ssh://user@host”</code>.</p>
<p>Now when I try to use following set of commands:</p>
<pre><code>docker context use remote
docker-compose up --build
</code></pre>
<p>Everything works ok, but I don't get live logs from my containers.
When I do <code>docker-compose up --build</code> locally - I get live logs.</p>
<p>What can I do to get live logs from remote containers?</p>
<p>Here is what I have when I execute both remotely and locally:</p>
<pre><code>Creating network &quot;target_default&quot; with the default driver
Creating volume &quot;target_storage&quot; with default driver
Building server
[+] Building 10.1s (10/10) FINISHED
 =&gt; [internal] load build definition from Dockerfile                                                               0.3s
 =&gt; =&gt; transferring dockerfile: 390B                                                                               0.0s
 =&gt; [internal] load .dockerignore                                                                                  0.3s
 =&gt; =&gt; transferring context: 2B                                                                                    0.0s
 =&gt; [internal] load metadata for docker.io/library/openjdk:8-jre-slim                                              0.6s
 =&gt; CACHED [1/5] FROM docker.io/library/openjdk:8-jre-slim@sha256:0330883ffeb5e14c4c15271004cdf6a2df21e827420b71d  0.0s
 =&gt; [internal] load build context                                                                                  8.2s
 =&gt; =&gt; transferring context: 91.16MB                                                                               8.2s
 =&gt; [2/5] RUN groupadd -r user &amp;&amp;     useradd -r -g user -m -s /usr/bin/nologin -c &quot;Docker image user&quot; user        0.3s
 =&gt; [3/5] COPY --chown=user:user [server-[{0-9.}]*-SNAPSHOT.jar, server.sh, /home/user/]                         0.2s
 =&gt; [4/5] WORKDIR /home/user                                                                                       0.0s
 =&gt; [5/5] RUN sed -i 's/localhost/data/g' server.sh &amp;&amp;     mkdir ./storage                                   0.3s
 =&gt; exporting to image                                                                                             0.4s
 =&gt; =&gt; exporting layers                                                                                            0.3s
 =&gt; =&gt; writing image sha256:3daf841e0b9749e1dcb7135e85b16a81104b84caa35930a423ecc6d460d6e8fd                       0.0s
 =&gt; =&gt; naming to docker.io/library/server:1.0-8-jre-slim                                                   0.0s
Successfully built 3daf841e0b9749e1dcb7135e85b16a81104b84caa35930a423ecc6d460d6e8fd
Creating target_server_1    ... done
Attaching to target_server_1    
</code></pre>
<p>But when I do it locally, after the last line above I get all my logs from 'target_server_1' live.
Remotely it just stops there, silently.</p>
"
"66992375","Kafka consumer.poll hangs with bitnami container","<python><docker><apache-kafka><kafka-consumer-api>","67021894","Kafka docker container doesn't respond to requests in network","<python><docker><apache-kafka><kafka-python>","<p>I have the latest bitnami kafka container installed on a remote server.</p>
<pre><code>[2021-04-07 18:05:38,263] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)
[2021-04-07 18:05:40,137] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser)
</code></pre>
<p>My kafka is configured so that I can have external connections.</p>
<pre><code>kafka:
image: 'bitnami/kafka:latest'
container_name: kafka
ports:
  - '9092:9092'
  - '9093:9093'
environment:
  - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  - ALLOW_PLAINTEXT_LISTENER=yes
  - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
  - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
  - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
  - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
</code></pre>
<p>ping and telnet to the ip address both work.</p>
<p>I am able to run a producer and send data in python.</p>
<pre><code>import kafka
from time import sleep
from json import dumps
from kafka import KafkaProducer
from kafka import KafkaConsumer

#Producer---------------------------------------------------------------
producer = KafkaProducer(bootstrap_servers=['192.xxx.xx.xx:9093'],
                     value_serializer=lambda x: 
                     dumps(x).encode('utf-8'))

producer.send('TestTopic1', value='MyTest')
</code></pre>
<p>But, I am unable to consume the data. The script hangs at consumer.poll and never changes lines.</p>
<pre><code>import kafka
from time import sleep
from json import dumps
from kafka import KafkaConsumer

# Consumer---------------------------------------------------------------
consumer = KafkaConsumer(
    'TestTopic1',
    bootstrap_servers=['192.xxx.xx.xx:9093'],
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    group_id='testgroup',
    value_deserializer=lambda x : loads(x.decode('utf-8')))

#I've tried both with group_id to None or with a group_id.

print('BEFORE subscribe: ')
consumer.subscribe(['TestTopic1'])

print('BEFORE poll: ')
# HANGS HERE!! Never gets to the print after
consumer.poll(timeout_ms=500)

print('AFTER POLL: ')
consumer.seek_to_beginning()

print('partitions of the topic: ', consumer.partitions_for_topic('TestTopic1'))

for msg in consumer:
    print(type(msg))
</code></pre>
<p>In the Kafka logs, I see the Topic getting created as well as other lines that I'm not quite sure what they mean.</p>
<pre><code>[2021-04-07 18:05:40,234] INFO [broker-1001-to-controller-send-thread]: Recorded new controller, from now on will use broker 1001 (kafka.server.BrokerToControllerRequestThread)
[2021-04-07 18:06:37,509] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment Map(23 -&gt; ArrayBuffer(1001), 32 -&gt; ArrayBuffer(1001), 41 -&gt; ArrayBuffer(1001), 17 -&gt; ArrayBuffer(1001), 8 -&gt; ArrayBuffer(1001), 35 -&gt; ArrayBuffer(1001), 44 -&gt; ArrayBuffer(1001), 26 -&gt; ArrayBuffer(1001), 11 -&gt; ArrayBuffer(1001), 29 -&gt; ArrayBuffer(1001), 38 -&gt; ArrayBuffer(1001), 47 -&gt; ArrayBuffer(1001), 20 -&gt; ArrayBuffer(1001), 2 -&gt; ArrayBuffer(1001), 5 -&gt; ArrayBuffer(1001), 14 -&gt; ArrayBuffer(1001), 46 -&gt; ArrayBuffer(1001), 49 -&gt; ArrayBuffer(1001), 40 -&gt; ArrayBuffer(1001), 13 -&gt; ArrayBuffer(1001), 4 -&gt; ArrayBuffer(1001), 22 -&gt; ArrayBuffer(1001), 31 -&gt; ArrayBuffer(1001), 16 -&gt; ArrayBuffer(1001), 7 -&gt; ArrayBuffer(1001), 43 -&gt; ArrayBuffer(1001), 25 -&gt; ArrayBuffer(1001), 34 -&gt; ArrayBuffer(1001), 10 -&gt; ArrayBuffer(1001), 37 -&gt; ArrayBuffer(1001), 1 -&gt; ArrayBuffer(1001), 19 -&gt; ArrayBuffer(1001), 28 -&gt; ArrayBuffer(1001), 45 -&gt; ArrayBuffer(1001), 27 -&gt; ArrayBuffer(1001), 36 -&gt; ArrayBuffer(1001), 18 -&gt; ArrayBuffer(1001), 9 -&gt; ArrayBuffer(1001), 21 -&gt; ArrayBuffer(1001), 48 -&gt; ArrayBuffer(1001), 3 -&gt; ArrayBuffer(1001), 12 -&gt; ArrayBuffer(1001), 30 -&gt; ArrayBuffer(1001), 39 -&gt; ArrayBuffer(1001), 15 -&gt; ArrayBuffer(1001), 42 -&gt; ArrayBuffer(1001), 24 -&gt; ArrayBuffer(1001), 6 -&gt; ArrayBuffer(1001), 33 -&gt; ArrayBuffer(1001), 0 -&gt; ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[2021-04-07 18:06:37,534] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2021-04-07 18:06:37,547] INFO Creating topic TestTopic1 with configuration {} and initial partition assignment Map(0 -&gt; ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[2021-04-07 18:06:37,557] INFO [KafkaApi-1001] Auto creation of topic TestTopic1 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2021-04-07 18:06:37,906] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2021-04-07 18:06:37,979] INFO [Log partition=__consumer_offsets-0, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2021-04-07 18:06:37,991] INFO Created log for partition __consumer_offsets-0 in /bitnami/kafka/data/__consumer_offsets-0 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; compact, flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 104857600, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:37,992] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2021-04-07 18:06:37,994] INFO [Partition __consumer_offsets-0 broker=1001] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,011] INFO [Log partition=__consumer_offsets-29, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
</code></pre>
<p>Then I have another series of this type of messages in my log.</p>
<pre><code>[2021-04-07 18:06:38,563] INFO Created log for partition __consumer_offsets-13 in /bitnami/kafka/data/__consumer_offsets-13 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; compact, flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 104857600, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:38,563] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2021-04-07 18:06:38,563] INFO [Partition __consumer_offsets-13 broker=1001] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,577] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,579] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,579] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
....
[2021-04-07 18:06:38,589] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 12 milliseconds, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,596] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 17 milliseconds, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,597] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 18 milliseconds, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
....
[2021-04-07 18:06:38,638] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(TestTopic1-0) (kafka.server.ReplicaFetcherManager)
[2021-04-07 18:06:38,643] INFO [Log partition=TestTopic1-0, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2021-04-07 18:06:38,644] INFO Created log for partition TestTopic1-0 in /bitnami/kafka/data/TestTopic1-0 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; [delete], flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 1073741824, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:38,647] INFO [Partition TestTopic1-0 broker=1001] No checkpointed highwatermark is found for partition TestTopic1-0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,647] INFO [Partition TestTopic1-0 broker=1001] Log loaded for partition TestTopic1-0 with initial high watermark 0 (kafka.cluster.Partition)
</code></pre>
<p>I don't see anything related to the consumer in here.</p>
<p>Note that this is only a dev server. We're supposed to use that as a proof of concept to see if Kafka works for us and see if we'll use it in prod.</p>
<p>Any help would be appreciated as we'd really like to be able to make it work and use it in production.</p>
","<p>I have set up a kafka docker container with ports mapped to the host windows machine.</p>
<p>compose-file:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.0.1
    hostname: zookeeper
    container_name: zookeeper
    restart: always
    ports:
      - &quot;2181:2181&quot;
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-server:6.0.1
    hostname: broker
    container_name: broker
    restart: always
    depends_on:
      - zookeeper
    ports:
      - &quot;9092:9092&quot;
      - &quot;9101:9101&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

  control-center:
    image: confluentinc/cp-enterprise-control-center:6.0.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - broker
    ports:
      - &quot;9021:9021&quot;
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021
</code></pre>
<p>I have also a simple producer and consumer scripts written in python:</p>
<pre><code>from kafka import KafkaProducer

topic = 'kontext-kafka'
bootstrap_servers = '192.168.1.103:9092'
producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

# Generate 100 messages
for _ in range(10):
    msg = f'Kontext kafka msg: {_}'
    future = producer.send(topic, msg.encode('utf-8'))
    print(f'Sending msg: {msg}')
    result = future.get(timeout=60)

metrics = producer.metrics()
print(metrics)
</code></pre>
<p>consumer:</p>
<pre><code>from kafka import KafkaConsumer

topic = 'kontext-kafka'
bootstrap_servers = '192.168.1.103:9092'
consumer = KafkaConsumer(
    topic, bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')
for msg in consumer:
    print(msg)
</code></pre>
<p>These scripts work on the host machine, but when I try to run these scripts on another computer, they can’t send or receive messages even when the broker is connected. In about 10 sec I receive this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;//main.py&quot;, line 12, in &lt;module&gt;
    result = future.get(timeout=60)
  File &quot;//venv/lib/python3.8/site-packages/kafka/producer/future.py&quot;, line 65, in get
    raise self.exception # pylint: disable-msg=raising-bad-type
kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Batch for TopicPartition(topic='kontext-kafka', partition=0) containing 1 record(s) expired: 30 seconds have passed since batch creation plus linger time
</code></pre>
<p>I still can connect the controller-interface container via browser and &quot;netstat -ab&quot; command shows port 9092 as &quot;LISTENING&quot;.</p>
<p>How should I correctly configure my container in the host machine, so I could send and receive messages in my network?</p>
<p>version of myclient:<br />
kafka-python==2.0.2</p>
<p>host os:<br />
win 10</p>
<p>desktop in my network:<br />
Ubuntu 20.04.2</p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","62866360","Where is the location of images that i pulled from docker hub on my windows?","<docker>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I am a newbie in docker. I pulled some images and want to see the location of these images.</p>
<p>I already look at docker disk image location. This is the path where my images is located  --&gt;&gt;</p>
<p>C:\ProgramData\DockerDesktop\vm-data</p>
<p>i went through this specified path. There was DockerDesktop.vhdx file but</p>
<p>nothing more. How can i see the location of my images on windows that i pulled from docker hub ?</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","64347101","How to can I edit Docker port and args on a container? (using Dockers Desktop for Windows)","<python><docker><pip><anaconda>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>I creasted a docker container using:</p>
<pre><code>    docker run -t -i continuumio/anaconda3 /bin/bash
</code></pre>
<p>I've installed all the software, however, I missed out the initial port setup.</p>
<p>When I run the container I can see that I have not setup the port and command line args:</p>
<pre><code>    docker container inspect 135c2d60901e
</code></pre>
<p>I can see what I need to modify to in the JSON it returns but am not able to modify in via Dockers Desktop for windows.  Are there Dockers commands that I should use or do I need to find the location of these files and modify them?</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","64441336","Docker: Expose running container to host","<mysql><docker>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>I have a running MySQL container that I have import a lot of data in it (took me a few hours).</p>
<p>Now I want to query the data from this container using a GUI application (DBeaver) on my host. Is there a way that I can expose the network of the running container to host so I can connect to it?</p>
<p>I've been looking around and the most promising option so far is to run <code>docker run</code> a new container with <code>-p</code> tag <code>3306:3306</code> but when I do that, all the data imported is gone.</p>
<p>Thank you!</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","66180061","Dynamically bind and unbind TCP port in a running docker container","<docker><networking><docker-compose>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>When using docker and docker-compose, ports are binded and reserved at the beginning of the execution process.
Since some applications outside the container can compete (and potentially crash) for the same ports, it would be interesting a solution to dynamically bind / unbind port for running docker containers.</p>
<p>One existing solution is is using variables to define the binding ports in docker-compose, but once started the container can not have any port binded or unbinded.</p>
<pre><code>ports:
  # Service1 
  - 14443:${PORTSERVICE1}
  # Service2
  - 13306:{PORTSERVICE2}
</code></pre>
<p>Is there any way to change ports binding dynamically after container initialization?</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","59672952","How can i make docker to pull image from another host machine's local repository?","<docker><docker-registry><docker-pull>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>Please refer this link:</p>

<p><a href=""https://dev.to/mayeu/saving-time-and-bandwidth-by-caching-docker-images-with-a-local-registry-98b"" rel=""nofollow noreferrer"">Caching Docker Images</a></p>

<p>I have installed a local registry on Machine A, and it caches all the images I pull on Machine A.</p>

<p>Now, I am trying to pull some images on Machine B using Machine B's local repo. I cannot achieve it.</p>

<p>Machine A and B are on same network.</p>

<p>The ways I have tried.</p>

<ol>
<li>Set up proxy configuration on Machine B and tried to route it. </li>
<li>Added the registry-mirrors set to remote machines local registry.</li>
</ol>

<p>Nothing was fruitful. I am looking for help to achieve it.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60148788","How can I retrieve my host machines IP address from inside a docker compose file?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I've looked at many stackoverflow answers but couldn't find a solution to my problem, so I apologise if it seems similar to other questions.<br>
I have a docker compose file which needs to connect to a local database sitting on port 5555. I need to be able to set an env var in the docker compose file that will translate to my host machines IP address.<br>
Something like</p>

<pre><code>environment:
  - db_uri=postgres://(HOST_IP):5555/db_name
</code></pre>

<p>How can I do this?</p>
"
"24309526","How to change the docker image installation directory?","<docker>","60296120","change location of image which is pulled in docker-compose","<docker><docker-compose>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>I have a docker-compose.yml which is pulling an image. When i am running the docker-compose file. It throws no space error. How can i specify the path where the image is downloaded. it getting downloaded it root which just has 15gb. I want it to be downloaded in /home because it has a lot of space left.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60423847","docker container can not connect to local postgres","<postgresql><docker><flask><docker-run>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Can't connect to Postgres from the docker container.
I do not want user docker-compose and create a Postgres container, already got Postgres app running. I think it is a bad idea to container postgres and better use system.</p>

<p>OSX, Postgres10, Flask</p>

<p>I already made</p>

<p><strong>postgresql.conf</strong></p>

<pre><code>listen_addresses = '*'  
port = 5432
</code></pre>

<p><strong>pg_hba.conf</strong></p>

<pre><code>host    all             all             0.0.0.0/0               trust
</code></pre>

<p><em>I Used trust for any result, but no effect.</em></p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM python:3.8-alpine
RUN apk update \
    &amp;&amp; apk add --virtual build-deps gcc musl-dev \
    &amp;&amp; apk add python3-dev \
    &amp;&amp; apk add postgresql-dev \
    &amp;&amp; apk add jpeg-dev zlib-dev libjpeg \
    &amp;&amp; pip install --upgrade pip
ENV PYTHONUNBUFFERED 1 \
    &amp;&amp; FLASK_APP app.py
#EXPOSE 5000 5432
WORKDIR /app
ADD . /app
RUN pip install -r requirements.txt
CMD [""./bin/run.sh""]
</code></pre>

<p><strong>./bin/run.sh</strong></p>

<pre><code>#!/bin/sh
#python run.py
source venv/bin/activate
set -e
flask db upgrade
exec gunicorn -b --workers 4 --access-logfile --error-logfile run:app :5000
</code></pre>

<p><strong>The ""docker run"" command I try to use:</strong></p>

<pre><code>docker run --rm -e SQLALCHEMY_DATABASE_URI=postgresql://postgres:postgres@0.0.0.0:5432/my_db --net=host -p 5000:5000 my_container:v0.1
</code></pre>

<p>the command leads to error</p>

<pre><code>...
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host ""0.0.0.0"" and accepting
        TCP/IP connections on port 5432?
</code></pre>

<p><strong>command connect me to Postgres</strong></p>

<pre><code>psql -U postgres -h 0.0.0.0
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60531220","Expose laptop localhost port to Docker container","<docker><docker-compose><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a RESTful application that takes a callback URL as an argument running on port 8000. It does some work and calls the callback URL to make sure that it should continue work and calls if anything goes wrong. In my tests, I just spin up an HTTP server on localhost port 8009 to wait for responses.</p>

<p>When I run the application directly, it works fine because the localhost is the same. However, when I run the application locally in a container, it obviously doesn't work because of the network isolation of the container. Here's part of my <code>docker-compose.yml</code>:</p>

<pre><code>version: '3'
services:
  app:
    build: .
    ports:
      - ""8000:8000""
</code></pre>

<p>I tried to add <code>- ""8009:8009""</code> under <code>ports</code>, but that only goes one direction. I can call port 8000 to call the API, but I can't figure out how to allow the container to call my laptop's <code>http://localhost:8009</code>.</p>

<p>Any idea how to allow the container to call out to another port on my laptop, or if there's a better way to test the callback functionality? I could run the tests inside the container, but I like being able to run them separately.</p>
"
"23071214","Use environment variables in CMD","<environment-variables><docker>","60828973","docker run python script in CMD with environment variables","<python><python-2.7><docker><environment-variables><dockerfile>","<p>Can I use environment variables in my CMD stanza in a Dockerfile?</p>

<p>I want to do something like this:</p>

<pre><code>CMD [""myserver"", ""--arg=$ARG"", ""--memcache=$MEMCACHE_11211_TCP_ADDR:$MEMCACHE_11211_TCP_PORT""]
</code></pre>

<p>Where $MEMCACHE_11211_TCP_* would be set automatically by the inclusion of the --link parameter of my <code>docker run</code> command.  And $ARG would be configurable by the user at runtime, maybe by the ""-e"" parameter?</p>

<p>This doesn't seem to be working for me, it seems to be literally passing through the string ""$ARG"" for example.</p>
","<p>I'm trying to run a simple Dockerfile that runs a python script and takes some environment variable as an argument (using argparser):</p>

<pre><code>FROM python:2.7
COPY . /app
WORKDIR /app
RUN pip install argparse
ENV POOL ""pool_argument""
CMD [""python"", ""script.py"", ""--pool"", ""${POOL}""]
</code></pre>

<p>and my python script <code>script.py</code>:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--pool', required=True)
known_args, unknown_args = parser.parse_known_args()
print(""Args: {}"".format(known_args))
</code></pre>

<p>after building and running, I'm getting:</p>

<pre><code>Args: Namespace(pool='${POOL}')
</code></pre>

<p>I tried many variations of this, but none seemed to work.</p>
"
"26239116","Run docker inside a docker container?","<centos><docker><fedora>","59710910","is there any way to run a docker image on host from other docker image?","<bash><shell><docker><ubuntu><docker-swarm>","<p>I am using a docker container to build and deploy my software to a collection of ec2's. In the deployment script I build my software and then package it in a docker image. The image is pushed to my private registry, pulled by my production ec2's and then run. So essentially I will need to run docker within a docker container.</p>

<p>The problem is that I can't actually start docker on my container. If I try</p>

<pre><code>service docker start
</code></pre>

<p>I get </p>

<pre><code>bash: service: command not found
</code></pre>

<p>And if I try </p>

<pre><code>docker -d
</code></pre>

<p>I get </p>

<pre><code>2014/10/07 15:54:35 docker daemon: 0.11.1-dev 02d20af/0.11.1; execdriver: native; graphdriver:
[e2feb6f9] +job serveapi(unix:///var/run/docker.sock)
[e2feb6f9] +job initserver()
[e2feb6f9.initserver()] Creating server
2014/10/07 15:54:35 Listening for HTTP on unix (/var/run/docker.sock)
[error] attach_loopback.go:42 There are no more loopback device available.
loopback mounting failed
[e2feb6f9] -job initserver() = ERR (1)
2014/10/07 15:54:35 loopback mounting failed
</code></pre>

<p>The service command doesn't exist on the docker container so I can't start docker. I'm not sure what I should be doing now to start docker so I'm a bit stuck here, any help is appreciated.</p>

<p>A bit more information</p>

<p>Host machine is running fedora 20 (will eventually be running amazon linux on an ec2)</p>

<p>Docker container is running centos 7.0</p>

<p>Host is running Docker version 1.2.0, build fa7b24f/1.2.0</p>

<p>Container is running docker-0.11.1-22.el7.centos.x86_64</p>
","<p>First i'm beginner, bad in explaining thing but i real need help.</p>

<p>i'm trying to create a cluster for MapReduce(my own MapReduce platform)using docker,
 so here is how:</p>

<p><br>1- Run Ubuntu container (Represent the Resource manager: to  controller the number of node to be created).[this container contain listener code every time need to create  an new node,mapperNode,reduceNode,...].
<br>2- Resource manager node or Ubuntu image  should be able to create a new ubuntu container on host.
<br>Note there is no need to understand MapRedce things 
<br> <strong>the question is how can i run other Ubuntu container on host From Ubuntu container?</strong> </p>
"
"26734402","How to upgrade docker container after its image changed","<docker>","60492913","How to restart the ROS docker container with GUI enabled","<docker><user-interface><ros>","<p>Let's say I have pulled the official <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql:5.6.21 image</a>. </p>

<p>I have deployed this image by creating several docker containers.</p>

<p>These containers have been running for some time until MySQL 5.6.22 is released. The official image of mysql:5.6 gets updated with the new release, but my containers still run 5.6.21.</p>

<p>How do I propagate the changes in the image (i.e. upgrade MySQL distro) to all my existing containers? What is the proper Docker way of doing this?</p>
","<ul>
<li>How to <strong>restart</strong> the ROS docker container <strong>with</strong> GUI enabled?  (Points to note: restart &amp; with GUI!)</li>
</ul>

<p>Following from questions like <a href=""https://stackoverflow.com/questions/47099483/run-gui-programs-in-docker-in-ubuntu"">this</a> and <a href=""https://answers.ros.org/question/326807/docker-gazeb-cannot-connect-to-x-server/"" rel=""nofollow noreferrer"">this</a> and the docker tutorial for ROS <a href=""http://wiki.ros.org/docker/Tutorials/GUI"" rel=""nofollow noreferrer"">here</a> I managed to stumble on to how one can ""run"" a docker container with GUI enabled.</p>

<p>It goes something like this:</p>

<pre><code>$ xhost +local:docker
$ docker run -it --env=""DISPLAY"" --env=""QT_X11_NO_MITSHM=1"" --volume=""/tmp/.X11-unix:/tmp/.X11-unix:rw"" --volume=""$HOME/.Xauthority:/root/.Xauthority"" ros
</code></pre>

<p>Afterwards, I can run inside the container in a terminal the following commands:</p>

<pre><code># roscore
</code></pre>

<p>And then in a separate terminal:</p>

<pre><code>$ docker exec -it priceless_engelbart bash
# apt-get update
# apt-get install ros-melodic-rqt-gui
# rosrun rqt_gui rqt_gui
</code></pre>

<p>And a window appears, which shows that the GUI is working from inside the container.</p>

<p><a href=""https://i.stack.imgur.com/QG7S8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QG7S8.png"" alt=""enter image description here""></a></p>

<p><strong>Question:</strong> The problem is, I don't want to create a new container every time, which is what the <code>docker run</code> command does. I have done work in the first container. Why would I want to completely ignore that and spin up a new container and start from scratch?</p>

<p>To remedy the situation, I looked into <a href=""https://docs.docker.com/engine/reference/commandline/start/"" rel=""nofollow noreferrer"">docker start</a>, <a href=""https://docs.docker.com/engine/reference/commandline/restart/"" rel=""nofollow noreferrer"">docker restart</a>, <a href=""https://docs.docker.com/engine/reference/commandline/attach/"" rel=""nofollow noreferrer"">docker attach</a> and  <a href=""https://docs.docker.com/engine/reference/commandline/exec/"" rel=""nofollow noreferrer"">docker exec</a>, to see if there was anyway to pass those extra parameters that I pass to <code>docker run</code>, like <code>--env</code> and <code>--volume</code> to help make the GUI work inside the container here.</p>

<p>Unfortunately, those commands do NOT allow passing any such parameters.</p>

<p>I have not been able to find a solution till now for the situation.</p>
"
"21553353","What is the difference between CMD and ENTRYPOINT in a Dockerfile?","<docker>","63677659","CMD vs ENTRYPOINT?","<linux><docker><dockerfile><devops><linux-development>","<p>In Dockerfiles there are two commands that look similar to me: <code>CMD</code> and <code>ENTRYPOINT</code>. But I guess that there is a (subtle?) difference between them - otherwise it would not make any sense to have two commands for the very same thing.</p>

<p>The documentation states for <code>CMD</code></p>

<blockquote>
  <p>The main purpose of a CMD is to provide defaults for an executing container.</p>
</blockquote>

<p>and for <code>ENTRYPOINT</code>:</p>

<blockquote>
  <p>An ENTRYPOINT helps you to configure a container that you can run as an executable.</p>
</blockquote>

<p>So, what's the difference between those two commands?</p>
","<p>I am learning Docker. However, I can't understand what the difference between CMD, and ENTRYPOINT is, when can I use CMD, and ENTRYPOINT in a dockerfile?  It would be helpful if someone explain that to me.</p>
<p>Thanks,
Hugo</p>
"
"25920029","Setting up MySQL and importing dump within Dockerfile","<mysql><docker>","60808493","How to run a .sql file inside a sql image in Dockerfile?","<mysql><docker><dockerfile>","<p>I'm trying to setup a Dockerfile for my LAMP project, but i'm having a few problems when starting MySQL. I have the folowing lines on my Dockerfile:</p>

<pre><code>VOLUME [""/etc/mysql"", ""/var/lib/mysql""]
ADD dump.sql /tmp/dump.sql
RUN /usr/bin/mysqld_safe &amp; sleep 5s
RUN mysql -u root -e ""CREATE DATABASE mydb""
RUN mysql -u root mydb &lt; /tmp/dump.sql
</code></pre>

<p>But I keep getting this error: </p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)
</code></pre>

<p>Any ideas on how to setup database creation and dump import during a Dockerfile build?</p>
","<p>I need to execute some script in a mysql image and need to create a new image out of it. </p>

<pre><code>FROM mysql:5.7.29
COPY ./multicore.sql /bin
ENV MYSQL_ROOT_PASSWORD=root
RUN /bin/bash -c ""mysql -uroot -proot \
    &amp;&amp; source &gt; /bin/sample.sql""
</code></pre>

<p>So when i execute this Dockerfile i'm getting the following error. The sample.sql run perfectly fine. The error i'm getting is </p>

<pre><code>Sending build context to Docker daemon  116.7kB
Step 1/4 : FROM mysql:5.7.29
 ---&gt; 84164b03fa2e
Step 2/4 : COPY ./multicore.sql /bin
 ---&gt; 54738bfd2ce2
Step 3/4 : ENV MYSQL_ROOT_PASSWORD=root
 ---&gt; Running in 0efc6d84aedb
Removing intermediate container 0efc6d84aedb
 ---&gt; aab5ba5fb5a4
Step 4/4 : RUN /bin/bash -c ""mysql -uroot -proot     &amp;&amp; source &gt; /bin/sample.sql""
 ---&gt; Running in 526d17218da3
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
The command '/bin/sh -c /bin/bash -c ""mysql -uroot -proot     &amp;&amp; source &gt; /bin/sample.sql""' returned a non-zero code: 1
</code></pre>
"
"20635472","Using the RUN instruction in a Dockerfile with 'source' does not work","<bash><shell><docker>","64212302","Source command not working from entrypoint/dockerfile","<linux><docker><openssl>","<p>I have a Dockerfile that I am putting together to install a vanilla python environment (into which I will be installing an app, but at a later date).</p>

<pre><code>FROM ubuntu:12.04

# required to build certain python libraries
RUN apt-get install python-dev -y

# install pip - canonical installation instructions from pip-installer.org
# http://www.pip-installer.org/en/latest/installing.html
ADD https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py /tmp/ez_setup.py
ADD https://raw.github.com/pypa/pip/master/contrib/get-pip.py /tmp/get-pip.py
RUN python /tmp/ez_setup.py
RUN python /tmp/get-pip.py
RUN pip install --upgrade pip 

# install and configure virtualenv
RUN pip install virtualenv 
RUN pip install virtualenvwrapper
ENV WORKON_HOME ~/.virtualenvs
RUN mkdir -p $WORKON_HOME
RUN source /usr/local/bin/virtualenvwrapper.sh
</code></pre>

<p>The build runs ok until the last line, where I get the following exception:</p>

<pre><code>[previous steps 1-9 removed for clarity]
...
Successfully installed virtualenvwrapper virtualenv-clone stevedore
Cleaning up...
 ---&gt; 1fc253a8f860
Step 10 : ENV WORKON_HOME ~/.virtualenvs
 ---&gt; Running in 8b0145d2c80d
 ---&gt; 0f91a5d96013
Step 11 : RUN mkdir -p $WORKON_HOME
 ---&gt; Running in 9d2552712ddf
 ---&gt; 3a87364c7b45
Step 12 : RUN source /usr/local/bin/virtualenvwrapper.sh
 ---&gt; Running in c13a187261ec
/bin/sh: 1: source: not found
</code></pre>

<p>If I <code>ls</code> into that directory (just to test that the previous steps were committed) I can see that the files exist as expected:</p>

<pre><code>$ docker run 3a87 ls /usr/local/bin
easy_install
easy_install-2.7
pip
pip-2.7
virtualenv
virtualenv-2.7
virtualenv-clone
virtualenvwrapper.sh
virtualenvwrapper_lazy.sh
</code></pre>

<p>If I try just running the <code>source</code> command I get the same 'not found' error as above. If I RUN an interactive shell session however, source does work:</p>

<pre><code>$ docker run 3a87 bash
source
bash: line 1: source: filename argument required
source: usage: source filename [arguments]
</code></pre>

<p>I can run the script from here, and then happily access <code>workon</code>, <code>mkvirtualenv</code> etc.</p>

<p>I've done some digging, and initially it looked as if the problem might lie in the difference between <strong>bash</strong> as the Ubuntu <em>login shell</em>, and <strong>dash</strong> as the Ubuntu <em>system shell</em>, <strong>dash</strong> not supporting the <code>source</code> command.</p>

<p>However, the answer to this appears to be to use <strong>'.'</strong> instead of <code>source</code>, but this just causes the Docker runtime to blow up with a go panic exception.</p>

<p>What is the best way to run a shell script from a Dockerfile RUN instruction to get around this (am running off the default base image for Ubuntu 12.04 LTS).</p>
","<p>I'm trying to install <em>openssl</em> from a <em>Dockerfile</em> manually. It's all working fine, until the last command <code>source /etc/environment</code>.
I've tried <code>RUN source /etc/environment</code> inside my Dockerfile, and <code>source /etc/environment</code> in my entrypoint script but neither of them worked.
But if I  run the command manually inside a running container, it works fine.
Any ideas why is this happening?</p>
<p><strong>EDIT</strong></p>
<p>This is the content of my Dockerfile. I don't have any errors building the image but the <em>source</em> doesn't seem to have any effect, unless I access the container and run it manually.</p>
<pre><code>RUN wget https://www.openssl.org/source/openssl-1.1.1c.tar.gz
RUN tar -xf openssl-1.1.1c.tar.gz
RUN cd openssl-1.1.1c; ./config --prefix=/usr/local/ssl --openssldir=/usr/local/ssl shared zlib; make; make test;  make install
RUN echo &quot;/usr/local/ssl/lib&quot; &gt; /etc/ld.so.conf.d/openssl-1.1.1c.conf
RUN mv /usr/bin/c_rehash /usr/bin/c_rehash.backup
RUN mv /usr/bin/openssl /usr/bin/openssl.backup
RUN echo &quot;PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/ssl/bin&quot;&quot; &gt; /etc/environment
RUN source /etc/environment
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60923688","How to access localhost API's inside docker container?","<windows><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>We have two apps running in the system, one is from docker and another is directly running on the system, We need to access the system's app inside container app using <code>localhost:8082</code> API</p>

<p>We are running a <code>node JS</code> container which is port 5000 on <code>Windows 10</code> Home using <code>Docker toolbox</code>, and running an app outside container means directly from the machine which is 8042, We need to access this <code>localhost:8042</code> API inside the node container, how can We archive it?, using <code>Docker version 19.03.1</code></p>

<p>Run cmd:</p>

<pre><code>docker container run -p --net=host -it -v `pwd`:/usr/src/app --rm --name server server/windows:v1
</code></pre>

<p><code>Error: Connection refuesed 127.0.0.1:8042</code> while running docker container.</p>

<p>Complete Error:</p>

<pre><code>(node:80) UnhandledPromiseRejectionWarning: Error: connect ECONNREFUSED 127.0.0.1:8042
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1137:16)
(node:80) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection,
use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)
(node:80) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will
terminate the Node.js process with a non-zero exit code.
</code></pre>

<p>Dockerfile</p>

<pre><code>FROM node:12

WORKDIR /usr/src/app

COPY package*.json ./
RUN npm install
COPY . .

# Specify port app runs on
EXPOSE 5000

# Run the app
CMD [ ""npm"", ""run"", ""dev""]
</code></pre>
"
"20096632","Limit memory on a Docker container doesn't work","<ubuntu><docker><cgroups>","64812804","Problem of limit memory on a Docker container","<docker>","<p>I am running the last version of Docker on top of Ubuntu&nbsp;13.04 (Raring Ringtail):</p>

<pre><code>root@docker:~# docker version
Client version: 0.6.6
Go version (client): go1.2rc3
Git commit (client): 6d42040
Server version: 0.6.6
Git commit (server): 6d42040
Go version (server): go1.2rc3
Last stable version: 0.6.6
</code></pre>

<p>But when I start the container,</p>

<pre><code>root@docker:~# docker run -m=1524288 -i  -t ubuntu /bin/bash
root@7b09f638871a:/# free -m
             total       used       free     shared    buffers     cached
Mem:          1992        608       1383          0         30        341
-/+ buffers/cache:        237       1755
Swap:         2047          0       2047
</code></pre>

<p>I don't see any limiting from any kind, and my kernel has the cgroups memory limit enabled:</p>

<pre><code>kernel /boot/vmlinuz-3.8.0-33-generic ro console=tty0 root=/dev/xvda1 cgroup_enable=memory swapaccount=1
</code></pre>

<p>What obvious thing am I missing here?</p>
","<p>What's the real technical issue preventing the container from displaying a limited amount of RAM instead of host ram?
I mean that if i run container with command</p>
<pre><code>docker run -m=1G -i  -t ubuntu /bin/bash
</code></pre>
<p>and run:</p>
<pre><code>free -g
</code></pre>
<p>i will get the physical amount of ram.
<strong>Why linux kernel don't provide opportunities for that?</strong></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61093838","Can't connect node container to mysql (locally hosted mysql)?","<mysql><docker><docker-compose><dockerfile><port>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p><strong>I have a node container that I'm trying to connect to locally hosted mysql db (outside container).</strong>  I've tried ""localhost"", ""mysql"" and now ip address of the instance container is running on 172.17.x.x:3306  <em>(found using docker inspect  | grep Gateway)</em> in config.js not working. ideas?</p>

<p>Mysql binds to 0.0.0.0</p>

<p>Error: </p>

<pre><code>Unable to connect to the database: { SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306
    at Promise.tap.then.catch.err (/usr/src/app/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:123:19)
    at tryCatcher (/usr/src/app/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/usr/src/app/node_modules/bluebird/js/release/promise.js:547:31)
    at Promise._settlePromise (/usr/src/app/node_modules/bluebird/js/release/promise.js:604:18)
    at Promise._settlePromise0 (/usr/src/app/node_modules/bluebird/js/release/promise.js:649:10)
    at Promise._settlePromises (/usr/src/app/node_modules/bluebird/js/release/promise.js:725:18)
    at _drainQueueStep (/usr/src/app/node_modules/bluebird/js/release/async.js:93:12)
    at _drainQueue (/usr/src/app/node_modules/bluebird/js/release/async.js:86:9)
    at Async._drainQueues (/usr/src/app/node_modules/bluebird/js/release/async.js:102:5)
    at Immediate.Async.drainQueues [as _onImmediate] (/usr/src/app/node_modules/bluebird/js/release/async.js:15:14)
    at runCallback (timers.js:705:18)
    at tryOnImmediate (timers.js:676:5)
    at processImmediate (timers.js:658:5)
  name: 'SequelizeConnectionRefusedError',
  parent:
   { Error: connect ECONNREFUSED 127.0.0.1:3306
       at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
     errno: 'ECONNREFUSED',
     code: 'ECONNREFUSED',
     syscall: 'connect',
     address: '127.0.0.1',
     port: 3306,
     fatal: true },
  original:
   { Error: connect ECONNREFUSED 127.0.0.1:3306
       at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
     errno: 'ECONNREFUSED',
     code: 'ECONNREFUSED',
     syscall: 'connect',
     address: '127.0.0.1',
     port: 3306,
     fatal: true } }
</code></pre>

<p>config.js</p>

<pre><code>console.log(""Running...."");
var sequelize = new Sequelize(""dbname"", ""admin"", ""pass"", {
  host: ""172.17.x.x:3306"",
  dialect: ""mysql"",
  port: 3306,
  pool: {
    max: 5,
    min: 0,
    idle: 10000,
  },
});
</code></pre>

<p><strong>docker file</strong></p>

<pre><code>FROM node:10

# Create app directory
WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
# where available (npm@5+)
COPY package*.json ./

RUN npm install
# If you are building your code for production
# RUN npm ci --only=production

# Bundle app source
COPY . .

EXPOSE 8000
CMD [ ""npm"", ""start"" ]
</code></pre>

<p><strong>is it my docker run command (need to port map to 8000?): docker run -i -t 49920e8c68b2</strong></p>
"
"20813486","Exploring Docker container's file system","<linux><docker><filesystems>","64971053","How to inspect the resulting folder / file structure and contents of a Docker image/container?","<docker><dockerfile>","<p>I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.</p>

<p>What would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.</p>
","<p>Given this <code>Dockerfile</code>:</p>
<pre><code># Use small base image with nodejs 10
FROM node:10.13-alpine

# set current work dir
WORKDIR /

# Copy package.json, packge-lock.json into current dir
COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;./&quot;]

# install dependencies
RUN npm install

# copy sources
COPY . .

# open default port (CAN BE SET ON docker-compose)
EXPOSE 3000

# Run app (CAN BE SET ON docker-compose)
CMD [&quot;node&quot;, &quot;src/index.js&quot;]
</code></pre>
<p><strong>.dockerignore</strong></p>
<pre><code>node_modules
*/**.txt
</code></pre>
<p>The <code>WORKDIR</code> along with the <code>COPY</code> (maybe more than 1) and the <code>.dockerignore</code> file obviously result in a specific folder/file structure and content.</p>
<p>Is there a way to inspect it and see what was the result of that? I mean, I would like to know if my <code>.dockerignore</code> is working properly, for example. And see where my source files were copied to, etc.</p>
<p>Can I do that on the image? Or in a container that was run with that image? Maybe Using Docker Desktop or some interface where I could call <code>dir</code> and see what is going on inside my containerized code?</p>
"
"21866477","nginx: use environment variables","<shell><nginx><proxy><environment-variables><docker>","65178527","How to pass variables from .env or docker-compose.yaml file to nginx server file","<docker><nginx>","<p>I have the following scenario: I have an env variable <code>$SOME_IP</code> defined and want to use it in a nginx block. Referring to the <a href=""http://wiki.nginx.org/CoreModule"" rel=""noreferrer"">nginx documentation</a> I use the <code>env</code> directive in the <code>nginx.conf</code> file like the following:</p>

<pre><code>user www-data;
worker_processes 4;
pid /run/nginx.pid;

env SOME_IP;
</code></pre>

<p>Now I want to use the variable for a <code>proxy_pass</code>. I tried it like the following:</p>

<pre><code>location / {
    proxy_pass http://$SOME_IP:8000;
}
</code></pre>

<p>But I end up with this error message: <code>nginx: [emerg] unknown ""some_ip"" variable</code></p>
","<p>I have a <code>docker-compose.yaml</code> file and a <code>.env</code> file. If I declare a variable in the env file, I can access it in docker-compose. So, this is good.</p>
<p>But, I would like to change nginx's <code>default.conf</code> properties, namely the <code>server_name = xxx</code> to be generated from the value I provide in the env.</p>
<p>So, my env looks like this (only sharing a portion)</p>
<pre><code>#.env
NGINX_SERVER_NAME=dev.example
</code></pre>
<p>In docker-compose I mention this variable here:</p>
<pre><code>nginx:
  container_name: ${APP_NAME}-server
  environment:
    - SERVER_NAME=${NGINX_SERVER_NAME}
</code></pre>
<p>And in <code>default.conf</code> I simple call the variable like this:</p>
<pre><code>server_name ${SERVER_NAME};
</code></pre>
<p>I am expecting the <code>default.conf</code> to contain <code>server_name dev.example</code> but it contains the token i.e. <code>${SERVER_NAME}</code></p>
<p>How can I achieve what I am looking for?
Thanks</p>
"
"20096632","Limit memory on a Docker container doesn't work","<ubuntu><docker><cgroups>","65184514","Docker -memory --memory-swap don't take effect","<windows><docker><ubuntu>","<p>I am running the last version of Docker on top of Ubuntu&nbsp;13.04 (Raring Ringtail):</p>

<pre><code>root@docker:~# docker version
Client version: 0.6.6
Go version (client): go1.2rc3
Git commit (client): 6d42040
Server version: 0.6.6
Git commit (server): 6d42040
Go version (server): go1.2rc3
Last stable version: 0.6.6
</code></pre>

<p>But when I start the container,</p>

<pre><code>root@docker:~# docker run -m=1524288 -i  -t ubuntu /bin/bash
root@7b09f638871a:/# free -m
             total       used       free     shared    buffers     cached
Mem:          1992        608       1383          0         30        341
-/+ buffers/cache:        237       1755
Swap:         2047          0       2047
</code></pre>

<p>I don't see any limiting from any kind, and my kernel has the cgroups memory limit enabled:</p>

<pre><code>kernel /boot/vmlinuz-3.8.0-33-generic ro console=tty0 root=/dev/xvda1 cgroup_enable=memory swapaccount=1
</code></pre>

<p>What obvious thing am I missing here?</p>
","<p>I am running docker v19.03.13 on Windows. Essentially, my problem is that neither --memory=&quot;&quot; or --memory-swap=&quot;&quot; have any effect on any of the containers. I use the commands, they seem to be accepted, but when I examine the memory limits, there is no change. I want to increase my memory swap size so I can construct a large data base.</p>
<p>Default</p>
<p>C:\Users\MyPC&gt; docker ps</p>
<p>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f54e10ee70a8 staphb/kraken2:latest &quot;/bin/bash&quot; 10 days ago Up 14 minutes Kraken2</p>
<p>inside f54e10ee70a8
free - h</p>
<p>total used free shared buff/cache available
Mem: 24G 496M 22G 251M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>so the memory swap is limited to 7G. I have far more space than that available on the local disk for a swap. But when I try to modify it, the commands are accepted there is no change in the container.</p>
<p>C:\Users\MyPC&gt;docker run --memory=&quot;22g&quot; --memory-swap=&quot;600g&quot; ubuntu /bin/bash
free - h</p>
<p>total used free shared buff/cache available
Mem: 24G 496M 22G 251M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>or even when I specify the particular container</p>
<p>C:\Users\MyPC&gt;docker run -it --memory=&quot;21g&quot; --memory-swap=&quot;600g&quot; staphb/kraken2:latest
root@d6fb2f332834:/data# free -h
total used free shared buff/cache available
Mem: 24G 510M 22G 252M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>Any help would be really appreciated. Thanks in advance!</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61391498","Connection between node.js docker container and local redis server(127.0.0.1)","<docker><redis>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>i'm running a node.js application in a docker container. Here is my docker-compose file - </p>

<pre><code>version: '3.7'
services:
  notification-app:
    container_name: ${CONTAINER_NAME}
    build:
      context: ./
      dockerfile: ./Dockerfile
      args:
        - APP_ENV=${APP_ENV}
    ports:
      - ${EXPORTED_PORT}:${APP_PORT}
    environment:
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
</code></pre>

<p>And here is my .env file - </p>

<pre><code>CONTAINER_NAME=sheba_socket

APP_ENV=development
APP_PORT=3000
EXPORTED_PORT=3000

REDIS_HOST=localhost
REDIS_PORT=6379
</code></pre>

<p>I'm running a REDIS SERVER locally (No container, directly on machine).
When i built and run this container, i found this error.</p>

<pre><code>[ioredis] Unhandled error event: Error: getaddrinfo ENOTFOUND redis
    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:64:26)
</code></pre>

<p>I think, the problem is here <strong>REDIS_HOST=localhost</strong>.
I am not sure how to define <strong>redis host</strong>.</p>
"
"20813486","Exploring Docker container's file system","<linux><docker><filesystems>","66141391","How to view files inside docker image without running it?","<docker>","<p>I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.</p>

<p>What would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.</p>
","<p>Sometimes running the docker image fails so ssh’ing into the container is not an option. in that cases how do we see the content inside container?</p>
<p>There is a existing question but mistakenly marked as duplicate. <a href=""https://stackoverflow.com/questions/53243471/how-to-browse-docker-image-without-running-it"">how to browse docker image without running it?</a></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61533423","Communication between Docker Application and localhost application","<ruby-on-rails><ruby><docker><tcp>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have an application running inside Docker(M1). And an another application running on localhost(M2-Not Dockerized). Both are Ruby on Rails applications running on ports 3000 and 3001. I am calling M2 from inside of M1 by using </p>

<p><code>response = http_client.post(""127.0.0.1:3001"", query, {}, options)</code></p>

<p>But I keep on getting</p>

<p><code>Error: Failed to open TCP connection to 127.0.0.1:3001 (Connection refused - connect(2) for ""127.0.0.1"" port 3001), Backtrace: [""/usr/local/lib/ruby/2.5.0/net/http.rb:939:in `rescue in block in connect'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:936:in `block in connect'"", ""/usr/local/lib/ruby/2.5.0/timeout.rb:93:in `block in timeout'"", ""/usr/local/lib/ruby/2.5.0/timeout.rb:103:in `timeout'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:935:in `connect'""</code></p>

<p>If i use - </p>

<p><code>response = http_client.post(""localhost:3001"", query, {}, options)</code></p>

<p>then i get </p>

<p><code>Error: Failed to open TCP connection to localhost:3001 (Cannot assign requested address - connect(2) for ""localhost"" port 3001), Backtrace: [""/usr/local/lib/ruby/2.5.0/net/http.rb:939:in `rescue in block in connect'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:936:in `block in connect'""</code></p>

<p>So it changed from Connection refused to Cannot assign requested address.</p>

<p>I am able to access both the applications by calling localhost:3000 and localhost:3001 from the browser individually. I am assuming it is happening because of a Dockerized application calling a Non-Dockerized one. Can someone suggest how to establish the connection between the two, I have tried many things. I am running Docker-2.1.0.5 on macOS Mojave 10.14.6 and Ruby 2.5.0. Let me know if any other information is required.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61561009","Docker How to allow access to local ports in container?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have two containers in Docker. And I can connect to their ports from local machine. But I can't connect from one Docker to another by local network, Docker container doesn't see ports of other containers.</p>

<p>All work if I use <code>network_mode: host</code>. But it create other problems.</p>

<p>How I can allow access to ports of one Docker container for other containers? WITHOUT USE <code>host.docker.internal</code>. Can I use 127.0.0.1? I have modules, which need only 127.0.0.1.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61661204","How to send http request from docker container to the local machine?","<java><docker><playframework><netty>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have the web application which is running on the docker container and that one must send requests to my local machine (localhost) and when I'm trying to do that I get the following exception:</p>

<pre><code>java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9033
</code></pre>

<p>I'm trying to give you some more details. I looked at this <a href=""https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach"">post</a> but it does not work for me. <br/>
First of all, I want to explain my project's structure:
1. I have a project that consist of two parts: frontend (angular 9) and backend (play 2.8.x framework)
2. I have another one project (web-service)</p>

<p>My first projects (backend, frontend) are placing in the docker container and the second (web-service) is placing at the localhost. I have the following flow between these projects:<br/>
1. The user sends a request (from the web browser) to the backend it may be login request for instance.<br/>
2. The backend process this request (retrieve data from the database) and sends these data to the web-service which is placing on the localhost.
<br/>
and I have a problem with step 2 where I trying to send data to the web-service. I'm getting the exception which was writing above. <br/>
I'm trying to set <code>--network=host</code> but in this case, I get <code>404 status code</code> in the browser.
How can I solve this issue?</p>
"
"25444099","Why docker has ability to run different linux distribution?","<linux><docker>","61742110","How does host kernel supports various docker container OS flavours ? How kernel sharing happens?","<linux><docker><kernel><containers><lxc-docker>","<p>We can use docker to pull different images. And these images are different linux distribution.
But no matter which linux distro docker is running on, docker can run these different linux distribution just like in a virtual machine.</p>

<p>I know docker uses <a href=""http://en.wikipedia.org/wiki/Aufs"" rel=""noreferrer"">aufs</a> to control different read-write access level. So it can reuse some file on the host machine. But how can docker run <code>apt-get</code> in a container when my host runs <code>arch linux</code>? Does the image contain the <code>apt-get</code> binary? But different linux distribution have different libs and software version. Even the configuration file are different.How can docker ""run"" ubuntu in a arch linux?</p>
","<p>On a docker host, we can run multiple containers with different OS flavours<br>
For instance Ubuntu host, Fedora container, Centos container,... How Ubuntu kernel supports to Fedora OS and CentOS ? </p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61769017","connect php docker container to existing apache and mysql (both are not in a container)","<php><apache><docker><subdomain>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a server that does not use any docker containers at the moment. For testing I would like to introduce a php container that is working with the already present apache installation and the mysql database. I know that apache and mysql could live inside of docker containers as well, but I would like to only have a php container. The application (Laravel based) should be accessible via a subdomain.</p>

<p>The Dockerfile looks like this:</p>

<pre><code>FROM php:7.2-fpm-alpine
RUN docker-php-ext-install pdo pdo_mysql
</code></pre>

<p>Is something like this possible? I searched the internet, but everywhere I looked docker containers are usually used for mysql as well.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61773184","How to connect from docker-compose to host","<php><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>How can I connect from docker-compose to the host. In the docker-compose there is an nginx and php running and I would like for the php container to connect to a mysql database that is running on the host. the nginx and php containers are connected via a docker-compose network.</p>

<p>Here is the docker-compose file:</p>

<pre><code>version: '3'

networks:
    test:

services:
    nginx:
        some stuff
        networks:
            - test

    php:
        some stuff
        networks:
            - test
            - HOST?????
</code></pre>

<p>I think in the php application I would then connect to ""localhost"" and docker connects this the host.</p>

<p>Is this possible? I have read, that it is possible for individual containers to be connected to the host, but don't know if it is possible when docker-compose is used.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61986076","how does containered springboot image connect itself to mysql5.7 which runs locally?","<java><mysql><spring-boot><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have written a very simple demo project, you can download it by clicking <a href=""https://d613.top/filebedcc/static/d874ca730b814f3ba5bb90d67.zip"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The above java project will be containerized as an docker image. But it needs to connect to the local mysql database which is not containerized.</p>

<p>I tried changing the value of <code>spring.datasource.url</code> from <code>jdbc:mysql://localhost:3306/db?characterEncoding=utf8&amp;useSSL=false</code> to <code>jdbc:mysql://192.168.1.105:3306/db?characterEncoding=utf8&amp;useSSL=false</code> in the <code>application.yaml</code>. This change seems to bring success.</p>

<p>so I Change the value of <code>spring.datasource.url</code> back to the original and tried <code>docker run --network=host .....</code>,but this didn't success.. why?</p>

<p>Thanks a lot.</p>
"
"24095968","Docker for GUI-based environments?","<docker><qt><vagrant><configuration-management>","62475741","PyQt5 could not load the platform plugin xcb","<python><pyqt><pyqt5>","<p><strong>Problem</strong></p>

<p>I have a set of client machines that are a part of an enterprise web application. Each machine runs identical software, which is a PyQT-based web client that connects to a server. This client software is updated regularly and I would like to have some configuration/provisioning tool that allows to have the same environment on each machine and hence provide easy deployment and configuration of the software onto each of the clients' machines.</p>

<p>The problem is that I have tried to use Chef, but it takes a lot of effort to actually maintain Chef knowledge and skills (we do not have a dedicated Ops guy) and moreover a Chef recipe can fail if some third party repository <a href=""http://blog.relateiq.com/why-docker-why-not-chef/"">is no longer available</a> (this is a main stopper).</p>

<p>I would like to try Docker to solve the problem, but I <a href=""https://www.docker.io/the_whole_story/"">still do not know</a> if it is possible to set up images/containers that allow for some GUI based software to operate.</p>

<p><strong>Question</strong></p>

<p>Is it possible to use Docker to have a development/production environment for a GUI-based application (PyQt/QT)? If yes, what would be the first steps to approach that?</p>
","<p>Running the following Qt example on a fresh new dedicated <code>virtualenv</code> session:</p>

<pre><code>from PyQt5 import QtWidgets
import sys

app = QtWidgets.QApplication(sys.argv)
hello = QtWidgets.QPushButton(""Hello World!"", None)
hello.show()
app.exec_()
</code></pre>

<p>triggers the following error:</p>

<pre><code>qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.
</code></pre>

<p>I read some posts related to this <a href=""https://stackoverflow.com/questions/33051790/could-not-find-or-load-the-qt-platform-plugin-xcb"">here</a> and <a href=""https://stackoverflow.com/questions/30430845/pyqt5-error-could-not-find-or-load-qt-platform-plugin-xcb"">there</a> but they apply to conda and do not resolve my problem. Would you have any idea ?</p>

<p>EDIT: running the script with debug support triggers the following output:</p>

<pre><code>QFactoryLoader::QFactoryLoader() checking directory path ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms"" ...
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""eglfs""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QEglFSIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""eglfs"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqlinuxfb.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqlinuxfb.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""linuxfb""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QLinuxFbIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""linuxfb"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqminimal.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqminimal.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""minimal""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QMinimalIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""minimal"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqminimalegl.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqminimalegl.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""minimalegl""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QMinimalEglIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""minimalegl"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqoffscreen.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqoffscreen.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""offscreen""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QOffscreenIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""offscreen"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqvnc.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqvnc.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""vnc""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QVncIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""vnc"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-egl.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-egl.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""wayland-egl""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QWaylandEglPlatformIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""wayland-egl"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-generic.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-generic.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""wayland""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QWaylandIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""wayland"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-egl.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-egl.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""wayland-xcomposite-egl""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QWaylandXCompositeEglPlatformIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""wayland-xcomposite-egl"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-glx.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-glx.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""wayland-xcomposite-glx""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QWaylandXCompositeGlxPlatformIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""wayland-xcomposite-glx"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwebgl.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqwebgl.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""webgl""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QWebGLIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""webgl"")
QFactoryLoader::QFactoryLoader() looking at ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so""
Found metadata in lib /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so, metadata=
{
    ""IID"": ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"",
    ""MetaData"": {
        ""Keys"": [
            ""xcb""
        ]
    },
    ""archreq"": 0,
    ""className"": ""QXcbIntegrationPlugin"",
    ""debug"": false,
    ""version"": 331520
}


Got keys from plugin meta data (""xcb"")
QFactoryLoader::QFactoryLoader() checking directory path ""/home/pellegrini/venvs/inspigtor/bin/platforms"" ...
Cannot load library /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)
QLibraryPrivate::loadPlugin failed on ""/home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so"" : ""Cannot load library /home/pellegrini/venvs/inspigtor/lib/python3.5/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)""
qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.
</code></pre>
"
"24095968","Docker for GUI-based environments?","<docker><qt><vagrant><configuration-management>","62880804","Could not load QT platform plugin ""xcb""","<python><pyqt5><ubuntu-16.04><conda>","<p><strong>Problem</strong></p>

<p>I have a set of client machines that are a part of an enterprise web application. Each machine runs identical software, which is a PyQT-based web client that connects to a server. This client software is updated regularly and I would like to have some configuration/provisioning tool that allows to have the same environment on each machine and hence provide easy deployment and configuration of the software onto each of the clients' machines.</p>

<p>The problem is that I have tried to use Chef, but it takes a lot of effort to actually maintain Chef knowledge and skills (we do not have a dedicated Ops guy) and moreover a Chef recipe can fail if some third party repository <a href=""http://blog.relateiq.com/why-docker-why-not-chef/"">is no longer available</a> (this is a main stopper).</p>

<p>I would like to try Docker to solve the problem, but I <a href=""https://www.docker.io/the_whole_story/"">still do not know</a> if it is possible to set up images/containers that allow for some GUI based software to operate.</p>

<p><strong>Question</strong></p>

<p>Is it possible to use Docker to have a development/production environment for a GUI-based application (PyQt/QT)? If yes, what would be the first steps to approach that?</p>
","<p>I am trying to run a napari-based python program. I am getting the following error message</p>
<pre><code>WARNING: Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.
01:04:11 WARNING:Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.
WARNING: This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.

01:04:11 WARNING:This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.

Aborted (core dumped)
</code></pre>
<p>I am not really sure what QT is and why it is causing these problems. I have tried installing QT5 directly from the website, but that did not resolve it either.</p>
<p>I have looked at old threads for these issues and they seem to have slightly different issues, and I am finding it difficult to apply their answers to mine.</p>
<p>Any assistance would be appreciated. I am doing all of my work in a conda environment running Python3.</p>
"
"23460980","Set up npm credentials over `npm login` without reading input from interactively from stdin","<node.js><bash><npm><docker>","63136093","How do I fill the interactive prompts using bash script? | Getting errors with password field","<node.js><linux><bash><npm>","<p>I'm trying to automate <code>npm publish</code> inside a Docker container but I have trouble when the <code>npm login</code> command tries to read the username and email from prompts:</p>

<pre><code>npm login &lt;&lt; EOF
username
password
email
EOF
</code></pre>

<p>It works in a Bash terminal but not in a container without stdin open, and shows the following error message:</p>

<pre><code>Username: Password: npm ERR! cb() never called!
npm ERR! not ok code 0
</code></pre>

<p>According to <a href=""https://www.npmjs.org/doc/cli/npm-adduser.html"" rel=""noreferrer"">npm-adduser</a>:</p>

<blockquote>
  <p>The username, password, and email are read in from prompts.</p>
</blockquote>

<p>So how can I run <code>npm login</code> without stdin open?</p>
","<p>Im running this command:</p>
<pre><code>$npm login --registry=https://npm.pkg.github.com
&gt; Username: USERNAME 
&gt; Password: TOKEN
&gt; Email: PUBLIC-EMAIL-ADDRESS
</code></pre>
<p>It asks for username, password and email with interactive prompt.
I want bash command to autofill these values.</p>
<p>I tried:</p>
<pre><code>echo -e &quot;$username\n$password\n$email&quot; | npm login --registry=https://npm.pkg.github.com
</code></pre>
<pre><code>printf &quot;%s\n&quot; &quot;$username $password $email&quot; | npm login --registry=https://npm.pkg.github.com
</code></pre>
<p>It fills the username but gives error while filling password.
Got the same error in each case:</p>
<pre><code>Username: username
Password: npm ERR! cb() never called!

npm ERR! This is an error with npm itself. Please report this error at:
npm ERR!     &lt;https://npm.community&gt;

npm ERR! A complete log of this run can be found in:
npm ERR!     /home/ubuntu/.npm/_logs/2020-07-28T13_23_22_829Z-debug.log
</code></pre>
"
"24309526","How to change the docker image installation directory?","<docker>","63315904","Docker does not see all disks - No space left on device","<docker><dockerfile><docker-build><linux-df>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>If i run</p>
<pre><code>df -h
</code></pre>
<p>inside the Dockerfile of an image I want to build, it sees:</p>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
overlay         118G  112G     0 100% /
tmpfs            64M     0   64M   0% /dev
tmpfs            16G     0   16G   0% /sys/fs/cgroup
shm              64M     0   64M   0% /dev/shm
/dev/sda1       118G  112G     0 100% /etc/hosts
tmpfs            16G     0   16G   0% /proc/asound
tmpfs            16G     0   16G   0% /proc/acpi
tmpfs            16G     0   16G   0% /proc/scsi
tmpfs            16G     0   16G   0% /sys/firmware
</code></pre>
<p>but if I run it outside, it sees:</p>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
udev             16G     0   16G   0% /dev
tmpfs           3.2G  355M  2.8G  12% /run
/dev/sda1       118G  112G     0 100% /
tmpfs            16G  300M   16G   2% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            16G     0   16G   0% /sys/fs/cgroup
/dev/sda3       308G  4.8G  288G   2% /mnt/ssd
/dev/sdb1       1.8T  1.2T  605G  66% /home
tmpfs           3.2G   16K  3.2G   1% /run/user/1002
tmpfs           3.2G  4.0K  3.2G   1% /run/user/1038
tmpfs           3.2G     0  3.2G   0% /run/user/1037
/dev/loop1       97M   97M     0 100% /snap/core/9436
/dev/loop0       97M   97M     0 100% /snap/core/9665
/dev/loop3      163M  163M     0 100% /snap/blender/42
/dev/loop2      163M  163M     0 100% /snap/blender/43
tmpfs           3.2G     0  3.2G   0% /run/user/1039
</code></pre>
<p>I got error &quot;No space left on device&quot; and I would like to use my /dev/sdb1 disk with 600 free GB, why docker does not see it? How can I make it use that space?</p>
"
"24095968","Docker for GUI-based environments?","<docker><qt><vagrant><configuration-management>","63579417","Could not load Qt platform plugin ""Ubuntu 20.04""","<python><qt><ubuntu><pyqt5>","<p><strong>Problem</strong></p>

<p>I have a set of client machines that are a part of an enterprise web application. Each machine runs identical software, which is a PyQT-based web client that connects to a server. This client software is updated regularly and I would like to have some configuration/provisioning tool that allows to have the same environment on each machine and hence provide easy deployment and configuration of the software onto each of the clients' machines.</p>

<p>The problem is that I have tried to use Chef, but it takes a lot of effort to actually maintain Chef knowledge and skills (we do not have a dedicated Ops guy) and moreover a Chef recipe can fail if some third party repository <a href=""http://blog.relateiq.com/why-docker-why-not-chef/"">is no longer available</a> (this is a main stopper).</p>

<p>I would like to try Docker to solve the problem, but I <a href=""https://www.docker.io/the_whole_story/"">still do not know</a> if it is possible to set up images/containers that allow for some GUI based software to operate.</p>

<p><strong>Question</strong></p>

<p>Is it possible to use Docker to have a development/production environment for a GUI-based application (PyQt/QT)? If yes, what would be the first steps to approach that?</p>
","<p>I have a python script that I tried to run on my newly installed ubuntu 20.04 but it is giving an error that I could not find any solution for.
I tried entering the debug command <em><strong>export QT_DEBUG_PLUGINS=1</strong></em> and then I tried to run my script by  <em><strong>python3 ImageCompressor.py</strong></em> and this is what I got after that(I still don't understand what's the error for) :</p>
<pre><code>QFactoryLoader::QFactoryLoader() checking directory path &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms&quot; ...
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqeglfs.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;eglfs&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QEglFSIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;eglfs&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqlinuxfb.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqlinuxfb.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;linuxfb&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QLinuxFbIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;linuxfb&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqminimal.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqminimal.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;minimal&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QMinimalIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;minimal&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqminimalegl.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqminimalegl.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;minimalegl&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QMinimalEglIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;minimalegl&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqoffscreen.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqoffscreen.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;offscreen&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QOffscreenIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;offscreen&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqvnc.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqvnc.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;vnc&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QVncIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;vnc&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-egl.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-egl.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;wayland-egl&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QWaylandEglPlatformIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;wayland-egl&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-generic.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-generic.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;wayland&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QWaylandIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;wayland&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-egl.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-egl.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;wayland-xcomposite-egl&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QWaylandXCompositeEglPlatformIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;wayland-xcomposite-egl&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-glx.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwayland-xcomposite-glx.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;wayland-xcomposite-glx&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QWaylandXCompositeGlxPlatformIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;wayland-xcomposite-glx&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwebgl.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqwebgl.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;webgl&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QWebGLIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;webgl&quot;)
QFactoryLoader::QFactoryLoader() looking at &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so&quot;
Found metadata in lib /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so, metadata=
{
    &quot;IID&quot;: &quot;org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3&quot;,
    &quot;MetaData&quot;: {
        &quot;Keys&quot;: [
            &quot;xcb&quot;
        ]
    },
    &quot;archreq&quot;: 0,
    &quot;className&quot;: &quot;QXcbIntegrationPlugin&quot;,
    &quot;debug&quot;: false,
    &quot;version&quot;: 331520
}


Got keys from plugin meta data (&quot;xcb&quot;)
QFactoryLoader::QFactoryLoader() checking directory path &quot;/usr/bin/platforms&quot; ...
Cannot load library /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)
QLibraryPrivate::loadPlugin failed on &quot;/home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so&quot; : &quot;Cannot load library /home/my_user_name/.local/lib/python3.8/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)&quot;
qt.qpa.plugin: Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.

Aborted (core dumped)
</code></pre>
<p>Can you please help me out regarding this issue. I am a newbie to Linux and I don't understand anything about this error.
Thank you.</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","63813136","How to locally export docker image from a machine X and import and run it on machine Y?","<docker><dockerfile><docker-container><docker-image>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>I have a requirement to export docker image on machine X (locally) without pushing to the docker hub or any repo, and I have to use the exported image (my_docker_image.gz or something) on machine Y as a container. How to do it? Pls help on commands</p>
"
"22049212","Docker: Copying files from Docker container to host","<docker><docker-container><file-copying>","63993955","using docker how to copy files from docker to host using docker run","<docker>","<p>I'm thinking of using Docker to build my dependencies on a Continuous Integration (CI) server, so that I don't have to install all the runtimes and libraries on the agents themselves. </p>

<p>To achieve this I would need to copy the build artifacts that are built inside the container back into the host. Is that possible?</p>
","<p>I have a docker image which has all the required files when I say docker run I would like for it to copy the contents of a folder in docker image to a host directory. After which the docker is not needed and stops</p>
"
"22944631","How to get the IP address of the docker host from inside a docker container","<docker><ip>","64479931","In .net core in docker, how can I see the host's IP adress?","<docker><asp.net-core>","<p>As the title says. I need to be able to retrieve the IP address the docker hosts and the portmaps from the host to the container, and doing that inside of the container. </p>
","<p>The scenario is like this:</p>
<p>I have a .net Core docker image that runs on a laptop, where other laptops should be able to access it, ususally on an ad hoc local network with no internet access.</p>
<p>On the server laptop it is just:  http://localhost:23456</p>
<p>But when other laptops connects it is something like: <a href=""http://192.168.1.12:23456"" rel=""nofollow noreferrer"">http://192.168.1.12:23456</a></p>
<p>So I would like that when at app is started up on the server laptop on: http://localhost:23456
the app could resolve the hosts IP address and write:</p>
<p>&quot;For client laptops connecting to this server use this address: <a href=""http://192.168.1.12:23456%22"" rel=""nofollow noreferrer"">http://192.168.1.12:23456&quot;</a></p>
"
"23071214","Use environment variables in CMD","<environment-variables><docker>","64829758","How to assign property value with environment variable in docker file","<spring-boot><docker><dockerfile>","<p>Can I use environment variables in my CMD stanza in a Dockerfile?</p>

<p>I want to do something like this:</p>

<pre><code>CMD [""myserver"", ""--arg=$ARG"", ""--memcache=$MEMCACHE_11211_TCP_ADDR:$MEMCACHE_11211_TCP_PORT""]
</code></pre>

<p>Where $MEMCACHE_11211_TCP_* would be set automatically by the inclusion of the --link parameter of my <code>docker run</code> command.  And $ARG would be configurable by the user at runtime, maybe by the ""-e"" parameter?</p>

<p>This doesn't seem to be working for me, it seems to be literally passing through the string ""$ARG"" for example.</p>
","<p>I am working on docker file with springboot. This line in docker file</p>
<pre><code>ENTRYPOINT [&quot;java&quot;, &quot;-Dbootstrap.servers=$DB_PATH&quot;,&quot;org.springframework.boot.loader.JarLauncher&quot;]
</code></pre>
<p>when I run <code>&quot;docker run -p 9000:9000 -e DB_PATH=newvalue imaginename&quot;</code></p>
<p>I want bootstrap.servers to take value of variable $DB_PATH, however, it will not work and only takes variable name &quot;$DB_PATH&quot;</p>
<p>Or how can i let bootstrap.servers take variables in docker container ?
please help.</p>
<p>Thanks</p>
"
"23692470","Why can't I use Docker CMD multiple times to run multiple services?","<docker>","64911830","docker run only displays last CMD?","<docker>","<p>I have built a base image from Dockerfile named centos+ssh. In centos+ssh's Dockerfile, I use CMD to run ssh service.</p>

<p>Then I want to build a image run other service named rabbitmq,the Dockerfile:</p>

<pre><code>FROM centos+ssh
EXPOSE 22
EXPOSE 4149
CMD /opt/mq/sbin/rabbitmq-server start
</code></pre>

<p>To start rabbitmq container，run：</p>

<pre><code>docker run -d -p 222:22 -p 4149:4149 rabbitmq
</code></pre>

<p>but ssh service doesn't work, it sense rabbitmq's Dockerfile CMD override centos's CMD.</p>

<ol>
<li>How does CMD work inside docker image?</li>
<li>If I want to run multiple service, how to? Using supervisor?</li>
</ol>
","<p>I create a docker image with two echos but only the last one is displayed:</p>
<p><em>dockerfile</em>:</p>
<pre><code>FROM ubuntu:20.04
ENV DEBIAN_FRONTEND=noninteractive

CMD echo &quot;HELLO WORLD&quot;
CMD echo &quot;GOODBYE WORLD&quot;
</code></pre>
<p><em>build and run</em>:</p>
<pre><code>docker build -t junk:junk .
docker run -it junk:junk
</code></pre>
<p>The output is</p>
<pre><code>GOODBYE WORLD
</code></pre>
<p>How can I control the output of <code>docker run -it</code>?</p>
"
"21889053","What is the runtime performance cost of a Docker container?","<performance><docker>","65626587","Container vs Host (Native) Performance","<docker><kubernetes><containers><google-kubernetes-engine><amazon-ecs>","<p>I'd like to comprehensively understand the run-time performance cost of a Docker container. I've found references to <a href=""https://stackoverflow.com/questions/21691540/how-to-optimize-performance-for-a-docker-container/21707838#21707838"">networking anecdotally being ~100µs slower</a>.</p>

<p>I've also found references to the run-time cost being ""negligible"" and ""close to zero"" but I'd like to know more precisely what those costs are. Ideally I'd like to know what Docker is abstracting with a performance cost and things that are abstracted without a performance cost. Networking, CPU, memory, etc.</p>

<p>Furthermore, if there are abstraction costs, are there ways to get around the abstraction cost. For example, perhaps I can mount a disk directly vs. virtually in Docker.</p>
","<ul>
<li><p>Since containers are lightweight os virtualization, can we get the same performance as native (host)?</p>
</li>
<li><p>What would be the difference in performance?</p>
</li>
</ul>
<p>Any leads are highly appreciated or if you have any analysis reports or any reference with host vs containers performance comparison will help.</p>
<p>IA</p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","65792745","How to copy file from local to a directory in namenode Docker?","<docker>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>I'm trying to copy a file from local to a directory in the namenode in <code>docker</code>. I tried this command : <code>docker cp ./ventes.txt ID:/~/direc1</code> but the file doesn't exists when i access the directory in the namenode bash</p>
"
"23513045","How to check if a process is running inside docker container?","<shell><docker><containers>","65895493","How to determine whether a process is running in a container or not (in linux)?","<c><linux><docker><linux-kernel><containers>","<p>[Updated1] I have a shell which will change TCP kernel parameters in some functions, but now I need to make this shell run in Docker container, that means, the shell need to know it is running inside a container and stop configuring the kernel. </p>

<p>Now I'm not sure how to achieve that, here is the contents of <code>/proc/self/cgroup</code> inside the container: </p>

<pre><code>9:hugetlb:/
8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>Any flags above can I use to figure out if this process is running inside a container?</p>

<p>[Updated2]: I have also noticed <a href=""https://stackoverflow.com/questions/20010199/determining-if-a-process-runs-inside-lxc-docker"">Determining if a process runs inside lxc/Docker</a>, but it seems not working in this case, the content in <code>/proc/1/cgroup</code> of my container is:</p>

<pre><code>8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>No /lxc/containerid</p>
","<p>I am trying to determine whether a process is running in a container or not? Is there any method to see from the point of host view? Is there any method to check it by using linux kernel namespace?</p>
"
"22049212","Docker: Copying files from Docker container to host","<docker><docker-container><file-copying>","66307489","Copy folder from Dockerfile to host","<docker><dockerfile>","<p>I'm thinking of using Docker to build my dependencies on a Continuous Integration (CI) server, so that I don't have to install all the runtimes and libraries on the agents themselves. </p>

<p>To achieve this I would need to copy the build artifacts that are built inside the container back into the host. Is that possible?</p>
","<p>I have a docker file</p>
<pre><code>FROM ubuntu:20.04
################################
### INSTALL Ubuntu build tools and prerequisites
################################
# Install build base

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update &amp;&amp; apt-get install -y \
    build-essential \
    git \
    subversion \
    sharutils \
    vim \
    asciidoc \
    binutils \ 
    bison \
    flex \
    texinfo \
    gawk \
    help2man \
    intltool \
    libelf-dev \
    zlib1g-dev \
    libncurses5-dev \
    ncurses-term \
    libssl-dev \
    python2.7-dev \
    unzip \
    wget \
    rsync \
    gettext \
    xsltproc &amp;&amp; \
    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*
 
ARG  FORCE_UNSAFE_CONFIGURE=1
RUN  git clone https://git.openwrt.org/openwrt/openwrt.git
WORKDIR /openwrt
RUN ./scripts/feeds update -a &amp;&amp; ./scripts/feeds install -a
COPY .config /openwrt/.config
RUN mkdir files
WORKDIR /files
RUN mkdir etc
WORKDIR /etc
RUN mkdir uci-defaults
WORKDIR /uci-defaults
COPY xx_custom /openwrt/files/etc/uci-defaults/xx_custom
WORKDIR /openwrt
RUN make -j 4
RUN ls /openwrt/bin/targets/ramips/mt76x8

WORKDIR /root
CMD [&quot;bash&quot;]
</code></pre>
<p>I want to copy all the files inside the folder mt76x8 to the host. I want to that inside the dockerfile so that when I run the docker file I should get the generated files in my host.</p>
<p>How can I do that?</p>
"
"22466255","Is it possible to answer dialog questions when installing under docker?","<ubuntu><installation><docker><apt-get>","66333915","Docker package installation expects data entry","<docker><opencv><installation>","<p>Is it possible to somehow answer the questions that are presented as dialogs when installing some packages using apt-get?</p>

<p>For instance I'm trying to setup a container containing the <code>mail-stack-delivery</code> package with:</p>

<pre><code>FROM ubuntu

RUN apt-get install -y mail-stack-delivery
</code></pre>

<p>However that dockerfile generates dozens of errors when built that are along the lines of:</p>

<pre><code>debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (Can't locate Term/ReadLine.pm in @INC (@INC contains: /etc/perl /usr/local/lib/perl/5.14.2 /usr/local/share/perl/5.14.2 /usr/lib/perl5 /usr/share/perl5 /usr/lib/perl/5.14 /usr/share/perl/5.14 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7, &lt;&gt; line 11.)
debconf: falling back to frontend: Teletype
dpkg-preconfigure: unable to re-open stdin: 
</code></pre>

<p>From what I understand I just simply can't respond to the dialogs, but is there some way that I can pass a parameter to answer each question in advance? I know that it's just changing some configurations, so I could do it after the fact, but presumably it's better to let the install scripts do it so everything gets set properly.</p>
","<p>I'm trying to install opencv on docker image, but docker not reads stdin</p>
<p>Dockerfile</p>
<pre><code>FROM ubuntu:18.04
RUN apt-get update \
    &amp;&amp; apt-get install tesseract-ocr -y \
    tesseract-ocr-rus \
    ffmpeg \
    python3 \
    #python-setuptools \
    python3-pip \
    python3-opencv \
    &amp;&amp; apt-get clean \
    &amp;&amp; apt-get autoremove


RUN mkdir -p /usr/src/app 

WORKDIR /usr/src/app

COPY ./server /usr/src/app
COPY ./requirements.txt /usr/src/app

RUN python3 -m pip install --upgrade pip
RUN pip3 install -r requirements.txt

RUN ls

EXPOSE 8000

CMD [ &quot;python3&quot;, &quot;./manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot; ]
</code></pre>
<p>Terminal:</p>
<pre><code>...
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
</code></pre>
<p>He expects data entry</p>
<p>How can i pass data entry\ or how to input data?</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","66362392","What is the use of Expose in docker file for ASP.NET Core application","<docker><asp.net-core><asp.net-core-mvc><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I created a dockerfile from Visual Studio for an ASP.NET Core app. The dockerfile code is:</p>
<pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.

FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;FirstDockerApp.csproj&quot;, &quot;&quot;]
RUN dotnet restore &quot;./FirstDockerApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;FirstDockerApp.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;FirstDockerApp.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;FirstDockerApp.dll&quot;]
</code></pre>
<p>There comes 2 expose ports for the container these are:</p>
<pre><code>EXPOSE 80
EXPOSE 443
</code></pre>
<p>So according to the docs, once the container is run for this app I should be able to access the app by any of these 2 url on the browser.</p>
<ol>
<li>localhost: 80</li>
<li>localhost: 443</li>
</ol>
<p>So I ran 2 command which builds the image from the dockerfile and creates a container.</p>
<ol>
<li><p>Command 1</p>
<p>docker build -t myapp -f Dockerfile .</p>
</li>
<li><p>Command 2</p>
<p>docker run myapp</p>
</li>
</ol>
<p>Now when I try to access the app on the container with the url - <code>localhost:443</code>* in the browser. Then I get a message that the website can't be reached. What is the problem here?</p>
<p><strong>Expose has no effect</strong></p>
<p>Next I removed the expose from the dockerfile.</p>
<pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging. See the dockerfile code after removing the expose.

FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;FirstDockerApp.csproj&quot;, &quot;&quot;]
RUN dotnet restore &quot;./FirstDockerApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;FirstDockerApp.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;FirstDockerApp.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;FirstDockerApp.dll&quot;]
</code></pre>
<p>Now I build the image and created the container. This time i used &quot;-p&quot; option with the run command. See below</p>
<ol>
<li><p>Command 1</p>
<pre><code> docker build -t myapp -f Dockerfile .
</code></pre>
</li>
<li><p>Command 2</p>
<pre><code> docker run -p 5001:80 myapp
</code></pre>
</li>
</ol>
<p>Now I can access the app from the url <code>localhost:5001</code>.</p>
<p>So what is the use of expose in the ASP.NET Core docker app? According to me it serves no purpose.</p>
<p>There is also another thing. My dockerfile has EXPOSE 443. Now if i expose the port of the container by the run command - <code>docker run -p 5004:443 firstdockerapp</code>. Next I open the url on the browser - <code>localhost:5004</code> then app could not be reached by the browser. Why it is so?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62539446","Docker TCP client","<docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have TCP server outside and docker container , docker container is an TCP client.how to handle this can anyone help on this. How to connect to server and communicate. Can any one help me on this.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62590973","Connection to localhost refused while running dockerised app","<java><docker><apache-pulsar>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a simple Java application that runs in the command line and connects to a local Apache Pulsar on port 6650 (<code>pulsar://localhost:6650</code>). I've jar'ed the application and dockerised it. The Apache Pulsar server is on the same machine as the dockerised app. However, when I run the docker container, it says <code>Connection refused: localhost/127.0.0.1:6650</code>.</p>
<p>I'm new to Docker so I believe it may be something I'm doing wrong with Docker. I've tried the following commands to run my container:</p>
<pre><code>docker run -it -p 6650:6650 pulsar_logging_consumer
docker run -it --network=&quot;host&quot; pulsar_logging_consumer
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM openjdk:latest
COPY target/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar /usr/src/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar
EXPOSE 6650
CMD java -jar /usr/src/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar
</code></pre>
"
"27068596","How to include files outside of Docker's build context?","<docker>","62713079","dockefile-copy from absolute path","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>Dockerfile is in <code>/home/user/docker</code> directory</p>
<p>I want to copy all files on host from <code>/app/publish</code> to the same path on docker container</p>
<pre><code>WORKDIR /app
RUN cp /app/publish /app/
</code></pre>
<p>But getting</p>
<pre><code>cp: cannot stat '/app/publish/': No such file or directory
</code></pre>
<p>I need to copy files outside of build context</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62849739","docker windows : how to access my localhost","<docker><ssl><ssl-certificate><docker-for-windows>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Under Windows, I have a software running into a docker image, this one need access to my localhost machine, on another API.</p>
<p>Let's say that I have a local CDN service locally running (not in a docker image)</p>
<pre><code>https://localhost:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg
</code></pre>
<p>So, the software in the container needs access to &quot;localhost&quot;, the localhost outside the docker image.</p>
<p>Noticed that my local server use SSL, then the container has to access the resource:</p>
<p><a href=""https://host.docker.internal:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg"" rel=""nofollow noreferrer"">https://host.docker.internal:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg</a></p>
<p>And I got the following error message:</p>
<blockquote>
<p>SSL certificate problem: unable to get local issuer certificate</p>
</blockquote>
<p>PS: my localhost=192.168.65.1</p>
<p>I have tried several kinds of stuff, like:</p>
<p>1 - create a network</p>
<pre><code>docker network create -d bridge --subnet 192.168.0.0/24 --gateway 192.168.0.1 dockernet
docker run -d --network dockernet -p 8080:8080 jolibrain/deepdetect_cpu
</code></pre>
<p>2 - use loopback names, like:</p>
<pre><code>host.docker.internal
docker.for.win.localhost
host.docker.internal
10.0.0.2
</code></pre>
<p>3 - tried to use --network host, but then the container failed to start</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63068066","How to share local Postgres database with docker?","<django><postgresql><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a django project with PostgreS database. I need to use my local database in docker container. How to share localhost of the machine with docker container? My <em><strong>docker-compose.yml</strong></em> :</p>
<pre><code>  version: '3'
  services:
    web:
      build: ./
      network_mode: &quot;host&quot;
      command: bash -c &quot;python manage.py makemigrations &amp;&amp; python manage.py migrate &amp;&amp; python manage.py collectstatic --noinput --i rest_framework &amp;&amp; gunicorn orion-amr.wsgi:application --bind 0.0.0.0:8000 --workers=2&quot;
      ports:
        - &quot;${webport}:8000&quot;
        - &quot;${pgport}:${pgport}&quot;
      env_file:
        - ./.env
</code></pre>
<p>Django <em><strong>settings.py</strong></em>:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.contrib.gis.db.backends.postgis',
        'NAME': os.environ.get('pgdatabase'),
        'USER': os.environ.get('pguser'),
        'PASSWORD': os.environ.get('pgpassword'),
        'HOST': os.environ.get('pghostname'),
        'PORT': os.environ.get('pgport'),
    }
}
</code></pre>
<p>I added</p>
<pre><code>listen_addresses = '*'  
</code></pre>
<p>to <em><strong>postgresql.conf</strong></em> file and below is my <em><strong>pg_hba.conf</strong></em> file:</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

# IPv4 local connections:
host    all             all             0.0.0.0/0               trust
# IPv6 local connections:
host    all             all             ::/0                    trust
# Allow replication connections from localhost, by a user with the
# replication privilege.
host    replication     all             127.0.0.1/32            trust
host    replication     all             ::1/128                 trust
</code></pre>
<p>Now I have an error:</p>
<pre><code>web_1  | django.db.utils.OperationalError: could not connect to server: Connection refused
web_1  |        Is the server running on host &quot;127.0.0.1&quot; and accepting
web_1  |        TCP/IP connections on port 5432?
web_1  |
</code></pre>
<p>Who can help me? Please!</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63186064","How to connect from Docker container parse-server to a host Postgresql DB?","<postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have installed an instance of parse-server with docker. I have my DB on the Host, but when I run the docker, it says <code>error on connecting to the DB.</code></p>
<pre><code>docker run -t -d --name parse-server parseplatform/parse-server --appName appname --appId appid--masterKey masterkey --clientKey clientkey --databaseURI postgres://example:1234563@localhost/parse-server --serverURL http://sync.example.com:1337/parse --verbose --publicServerURL http://sync.example.com
</code></pre>
<p>And I am getting this error:</p>
<pre><code>Error: connect ECONNREFUSED 127.0.0.1:5432
at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1141:16) {
  errno: 'ECONNREFUSED',
  code: 'ECONNREFUSED',
  syscall: 'connect',
  address: '127.0.0.1',
  port: 5432
}
</code></pre>
<p>I tried with <code>--network host</code> but parse-server is giving me <code>{&quot;error&quot;:&quot;unauthorized&quot;}</code> using this docker.</p>
<p>I tried also using <code>-o 5432:5432</code>, but it said the port is used, because the instance running on the host.</p>
<p>How can I connect to the PostgreSQL on the host from the docker parse-server without using <code>--network host</code>?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63412022","Connect Docker image to local Postgres","<java><postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a Spring app that connects to local Postgres db. It works fine.</p>
<p>I then created a Docker image from the Spring app. When running the container it throws a db connection error.</p>
<p>I have followed the solution suggested <a href=""https://gist.github.com/MauricioMoraes/87d76577babd4e084cba70f63c04b07d"" rel=""nofollow noreferrer"">here</a>, but still getting the same error.</p>
<pre><code>[ERROR ] [task-1] c.z.h.p.HikariPool c.z.h.p.HikariPool.throwPoolInitializationException(HikariPool.java:593) – HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:285)
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:217)
    at org.postgresql.Driver.makeConnection(Driver.java:458)
    at org.postgresql.Driver.connect(Driver.java:260)
</code></pre>
<p>I have run the docker image in several ways with the same result.</p>
<p><code>docker run app-image</code></p>
<p><code>docker run --network=host app-image</code></p>
<p>Dockerfile:</p>
<pre><code>FROM java:8-jdk-alpine

COPY ./build/libs/runtime-0.0.1-SNAPSHOT.jar /usr/app/
WORKDIR /usr/app

ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;runtime-0.0.1-SNAPSHOT.jar&quot;]
# Expose standard tomcat port

EXPOSE 9888
</code></pre>
<p>postgresql.conf:</p>
<pre><code>#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

listen_addresses = '*'
</code></pre>
<p>pg_hba.conf</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

host        postgres        postgres     0.0.0.0/0              trust
host        postgres        postgres     172.17.0.0/16          trust
</code></pre>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","63736258","Make a new container with a combination of two separate containers","<docker><dockerfile><containers><jenkins-docker>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I want a container which have both, docker application and jenkins application installed.<br/>
I have tried building a new container from a Dockerfile -
`
FROM :docker
FROM :jenkins/jenkins</p>
<p>but it only seems to have jenkins but no docker .
`</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63764895","Redis connect ECONNREFUSED 127.0.0.1:6379","<javascript><node.js><docker><redis><containers>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Im running redis on my server (Redis is not running in a container)  and I am trying to connect to redis inside of my Container which has a Node.js app runninng. When starting the Docker Container with the node app from my Dockerfile in the logs there is the following error: <code>Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63828252","How to use services such as Kafka, Zookeeper and Postgres of the host sever inside a docker using docker compose?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am using docker compose and trying to use services such as Kafka, Zookeeper and Postgres of the host sever inside the container.</p>
"
"24309526","How to change the docker image installation directory?","<docker>","63993702","Run Docker image on /dev/shm partition","<docker><filesystems>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>Is there any way to run Docker image on <code>/dev/shm</code> partition? when running docker by default, the &quot;overlay&quot; entry is added in filesystem which has the same size as <code>/dev/sda3</code> partition, where the root is mounted. But since size of <code>/dev/shm</code> is bigger on the linux server I use, I was wondering if I could somehow run the image on that partition instead of <code>/dev/sda3</code>.</p>
"
"25135897","How to automatically start a service when running a docker container?","<docker>","64043175","Can I have Postgres + Python in a single Docker container for testing?","<python><postgresql><docker><pytest>","<p>I have a <a href=""https://github.com/larazest/db/blob/master/Dockerfile"">Dockerfile</a> to install MySQL server in a container, which I then start like this:</p>

<pre><code>sudo docker run -t -i 09d18b9a12be /bin/bash
</code></pre>

<p>But the MySQL service does not start automatically, I have to manually run (from within the container):</p>

<pre><code>service mysql start
</code></pre>

<p>How do I automatically start the MySQL service when I run the docker container?</p>
","<p>I would like to run pytest on some routes of a Flask application which need a database. Sadly, sqlite is not working as the setup uses database schemas. It should by Postgres.</p>
<p>I want to integrate this into the CI pipeline and I don't think docker-compose works there. But Docker does. So I need pytest + postgres in a single Docker container. How do I do that?</p>
<h2>What I tried</h2>
<pre><code>FROM python:3.7.9-slim-buster
RUN apt-get update
RUN apt-get -y install postgresql
RUN service postgresql start
COPY setup_dev_user.sql ./
RUN chmod 777 setup_dev_user.sql

USER postgres
RUN psql --file=setup_dev_user.sql
RUN psql createdb dev_db
ENV SQLALCHEMY_DATABASE_URI postgresql://dev_user:dev_pw@db:5432/dev_db

USER root
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install --no-cache-dir psycopg2-binary
COPY app /app/app
WORKDIR /app
CMD pytest
</code></pre>
<p>where <code>setup_dev_user.sql</code> is</p>
<pre><code>create user dev_user password 'dev_pw'
</code></pre>
<p>I get</p>
<pre><code>psql: could not connect to server: No such file or directory
        Is the server running locally and accepting
        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
The command '/bin/sh -c psql --file=setup_dev_user.sql' returned a non-zero code: 2
</code></pre>
<p>for the step <code>RUN psql --file=setup_dev_user.sql</code>. How can I fix that?</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","64065824","Dockerfile Service failed to build: COPY failed: Forbidden path outside the build context: ../../../../*","<docker><jenkins><docker-compose><jenkins-pipeline><wildfly>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I want to specify files outside of the dockerfile location.  Below is my dockerfile and I get this error: &quot;Step 2/3 : COPY ../../../../wildfly/standalone /opt/jboss/wildfly/
Service 'wildfly' failed to build: COPY failed: Forbidden path outside the build context: ../../../../wildfly/standalone ()&quot;
How can I reference the root of the directory or reference a file outside of the dockerfile workspace?</p>
<p>`FROM jboss/wildfly:10.1.0.Final</p>
<p>COPY ../../../../wildfly/standalone /opt/jboss/wildfly/
`</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","64410123","Dockerfile - How to copy files from a local folder?","<docker><dockerfile><docker-volume>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>Below is the <code>Dockerfile</code>:</p>
<pre><code>FROM golang:1.14.10
MAINTAINER xyz

COPY ~/go/bin/product-api /go/bin/product-api

COPY ~/go/bin/swagger /go/bin/swagger

ENTRYPOINT [&quot;/go/bin/product-api&quot;]
</code></pre>
<hr />
<p>on <code>docker build -t cloud-native-product-api:1.0.0 .</code>, gives error:</p>
<pre><code>Step 3/5 : COPY ~/go/bin/product-api /go/bin/product-api
COPY failed: stat /var/lib/docker/tmp/docker-builder398080099/~/go/bin/product-api: no such file or directory
</code></pre>
<hr />
<p>I think the image build process takes <code>/var/lib/docker/tmp/docker-builder398080099</code> as workspace and refer from that path.</p>
<p>How to copy files from specific folder of local machine into Docker image?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64622713","docker access outer localhost (host) ip from inside container [windows]","<docker><nginx>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am running <code>nginx</code> server inside a docker container and in <code>nginx.conf</code> file I need to access certain addresses (localhost for now) with different ports.</p>
<p>Currently I'm putting <code>127.0.0.1</code> address which is the nginx container's localhost address.</p>
<p>So I need to replace that.</p>
<p>Here's <code>nginx.conf</code>:</p>
<pre><code>http {
    upstream allbackend {
        #ip_hash;
        server 127.0.0.1:9000; &lt;- this is a container localhost
        server 127.0.0.1:9001;     
        server 127.0.0.1:9002;
        server 127.0.0.1:9003;
    }    

    server {
        listen 80;
        location / {
            proxy_pass http://allbackend/;
        }

        location /admin {
            return 403;
        }
    }
}
events {}
</code></pre>
<p>In <code>server</code> directive, how can I connect to the outer <code>localhost</code>?</p>
<p>I get this error now in container's log:</p>
<pre><code>2020/10/31 14:00:34 [error] 103#103: *11 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: &quot;GET / HTTP/1.1&quot;, upstream: &quot;http://127.0.0.1:9003/&quot;, host: &quot;localhost:4000&quot;
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64817110","How to access host machine's mongo db inside docker container","<php><laravel><mongodb><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am a bit new to docker. I am developing a laravel application, I have my app container running.
Inside app container I want to connect to my locally hosted mongo db server, which is generally <code>localhost:27017</code>.</p>
<p>This is my <strong>docker-compose.yml</strong> file</p>
<pre><code>version: '3'
services:

  #PHP Service
  app:
    build:
      context: .
      dockerfile: Dockerfile
    image: digitalocean.com/php
    container_name: bns_app
    restart: unless-stopped
    tty: true
    environment:
      SERVICE_NAME: app
      SERVICE_TAGS: dev
    working_dir: /var/www
    volumes:
      - ./:/var/www
      - ./docker_files/php/local.ini:/usr/local/etc/php/conf.d/local.ini
    networks:
      - app-network

  #Nginx Service
  webserver:
    image: nginx:alpine
    container_name: bns_web
    restart: unless-stopped
    tty: true
    ports:
      - &quot;80:80&quot;
      - &quot;443:443&quot;
    volumes:
      - ./:/var/www
      - ./docker_files/nginx/conf.d/:/etc/nginx/conf.d/
    networks:
      - app-network

  #MySQL Service
  db:
    image: mysql:5.7.22
    container_name: bns_db
    restart: unless-stopped
    tty: true
    ports:
      - &quot;3306:3306&quot;
    environment:
      MYSQL_DATABASE: laravel
      MYSQL_ROOT_PASSWORD: 12345
      SERVICE_TAGS: dev
      SERVICE_NAME: mysql
    volumes:
      - dbdata:/var/lib/mysql/
      - ./docker_files/mysql/my.cnf:/etc/mysql/my.cnf
    networks:
      - app-network

  #REDIS Service
  redis:
      build:
        context: ./docker_files/redis
        dockerfile: Dockerfile
      container_name: bns_redis
      volumes:
        - ./docker_files/redis:/data
      ports:
        - &quot;6379:6379&quot;
      networks:
        - app-network

  # Laravel Echo Server
  laravel-echo-server:
        build:
          context: ./docker_files/laravel-echo-server
          dockerfile: Dockerfile
        container_name: bns_echo_server

        volumes:
          - ./laravel-echo-server.json:/var/www/laravel-echo-server.json:ro
        ports:
          - &quot;6001:6001&quot;
        links:
          - redis
        networks:
          - app-network

  # PHP-WORKER
  php-worker:
    build:
      context: ./docker_files/php-worker
      dockerfile: Dockerfile
    container_name: bns_worker
    volumes:
      - ./php-worker/supervisord.d:/etc/supervisord.d
    depends_on:
      - app
    networks:
      - app-network


#Docker Networks
networks:
  app-network:
    driver: bridge
#Volumes
volumes:
  dbdata:
    driver: local
</code></pre>
<p>So inside my app container when I run artisan commands such as <code>docker-compose exec app php artisan migrate:fresh --seed</code> mysql tables are migrated successfully but mongo db (which is not in any container but it is in host machine only) documents are not migrate/seeded and I get error :</p>
<pre><code>No suitable servers found (`serverSelectionTryOnce` set): [connection refused calling ismaster on '172.18.0.3:27017']

  at /var/www/vendor/mongodb/mongodb/src/functions.php:431
    427|         // TODO: PHPLIB-476: Read transaction read preference once PHPC-1439 is implemented
    428|         $readPreference = new ReadPreference(ReadPreference::RP_PRIMARY);
    429|     }
    430| 
  &gt; 431|     return $manager-&gt;selectServer($readPreference);
    432| }
    433| 

  Exception trace:

  1   MongoDB\Driver\Manager::selectServer(Object(MongoDB\Driver\ReadPreference))
      /var/www/vendor/mongodb/mongodb/src/functions.php:431

  2   MongoDB\select_server(Object(MongoDB\Driver\Manager), [])
      /var/www/vendor/mongodb/mongodb/src/Database.php:419

  Please use the argument -v to see more details.
</code></pre>
<p>This is my ENV file:</p>
<pre><code>MONGO_DATABASE=exchange
MONGO_HOST=172.18.0.3 # &lt;&lt; This is the IP address of my app container  &lt;&lt;
MONGO_PORT=27017
MONGO_USERNAME=
MONGO_PASSWORD=
</code></pre>
<p>Please do help me. Where am I going wrong ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64962558","connect kafka from host inside python app with docker","<python><docker><apache-kafka>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>i have python app that run with docker container that consume kafka, i also run kafka on my host machine, i run kafka manually not from docker.</p>
<p>when i run my python app container it won't connect to kafka from host, i run it with this command:</p>
<pre><code>docker run -d --network=&quot;host&quot; --name myptm-rating -p 8002:8002 myptm-rating-command
</code></pre>
<p>i set <code>--network</code> flag to connect my app with host.</p>
<p>this is my consumer configuration (i use confluent kafka package):</p>
<pre><code>consumer_conf = {
        'bootstrap.servers': 'localhost:9092',
    }
</code></pre>
<p>then i see container logs, it shows errors:</p>
<pre><code>[2020-11-23 04:09:44 +0000] [10] [INFO] Starting gunicorn 20.0.4
[2020-11-23 04:09:44 +0000] [10] [INFO] Listening at: http://0.0.0.0:8002 (10)
[2020-11-23 04:09:44 +0000] [10] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2020-11-23 04:09:44 +0000] [13] [INFO] Booting worker with pid: 13
[2020-11-23 04:09:44 +0000] [14] [INFO] Booting worker with pid: 14
[2020-11-23 04:09:44 +0000] [15] [INFO] Booting worker with pid: 15
%3|1606104584.818|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT)
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.826|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT)
%3|1606104584.827|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
[2020-11-23 04:09:44 +0000] [13] [INFO] Started server process [13]
[2020-11-23 04:09:44 +0000] [13] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [13] [INFO] Application startup complete.
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.841|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
%3|1606104584.845|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.849|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
[2020-11-23 04:09:44 +0000] [14] [INFO] Started server process [14]
[2020-11-23 04:09:44 +0000] [14] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [14] [INFO] Application startup complete.
[2020-11-23 04:09:44 +0000] [15] [INFO] Started server process [15]
[2020-11-23 04:09:44 +0000] [15] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [15] [INFO] Application startup complete.
%3|1606104585.819|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)
</code></pre>
"
"27017715","Does virtualenv serve a purpose (in production) when using docker?","<python><virtualenv><docker>","65262769","Do we need to create virtual environment if we are using docker, in django project?","<python-3.x><django><docker>","<p>For development we use virtualenv to have an isolated development when it comes to dependencies. From <a href=""https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server"">this question</a> it seems deploying Python applications in a <a href=""/questions/tagged/virtualenv"" class=""post-tag"" title=""show questions tagged 'virtualenv'"" rel=""tag"">virtualenv</a> is recommended.</p>

<p>Now we're starting to use <a href=""/questions/tagged/docker"" class=""post-tag"" title=""show questions tagged 'docker'"" rel=""tag"">docker</a> for deployment. This provides a more isolated environment so I'm questioning the use of virtualenv inside a docker container. In the case of a single application I do not think virtualenv has a purpose as docker already provides isolation. In the case where multiple applications are deployed on a single docker container, I do think virtualenv has a purpose as the applications can have conflicting dependencies. </p>

<p>Should virtualenv be used when a single application is deployed in a docker container?</p>

<p>Should docker contain multiple applications or only one application per container?</p>

<p>If so, should virtualenv be used when deploying a container with multiple applications?</p>
","<p>Do we need virtual environment if we are using docker, in django project?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65264773","Docker django connect non docker postgresql","<django><postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have django project and  trying to dockerize the project.</p>
<p>Everything is working very well but the problem with postgresql.</p>
<p>I have installed my postgresql in my local machine, so postgresql host is localhost and my django app running in my local machine docker.</p>
<p>I dont want to install postgresql with docker, it's already installed without docker. so i tried to connect this with following infomrmatin:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'mydbname',
        'USER': 'postgres',
        'PASSWORD': 'secret',
        'HOST': 'localhost',
        'PORT': '5432',
    }
}
</code></pre>
<p>and it is not working, it's firing error.</p>
<p>Can anyone help me what can i do in this case? I dont want to install postgres by docker, the datbase should be in my direct local machine.</p>
<p>and this is my dockerFile</p>
<pre><code>FROM python:3.8.1-slim-buster


RUN useradd wagtail


EXPOSE 8000


ENV PYTHONUNBUFFERED=1 \
    PORT=8000

# Install system packages required by Wagtail and Django.
RUN apt-get update --yes --quiet &amp;&amp; apt-get install --yes --quiet --no-install-recommends \
    build-essential \
    libpq-dev \
    libmariadbclient-dev \
    libjpeg62-turbo-dev \
    zlib1g-dev \
    libwebp-dev \
 &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN pip install &quot;gunicorn==20.0.4&quot;

# Install the project requirements.
COPY requirements.txt /
RUN pip install -r /requirements.txt

WORKDIR /app

RUN chown wagtail:wagtail /app


COPY --chown=wagtail:wagtail . .

USER wagtail


RUN python manage.py collectstatic --noinput --clear

CMD set -xe; python manage.py migrate --noinput; gunicorn projectile.wsgi:application
</code></pre>
"
"27068596","How to include files outside of Docker's build context?","<docker>","65297277","COPY command in Dockerfile. How to copy from one level up","<docker><copy>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I am trying to copy files in Dockerfile.</p>
<p>Here is how my folder looks like.</p>
<pre><code>main.R &lt;- my main executable file
app &lt;- folder with Dockerfile.
</code></pre>
<p>In Dockerfile I have the following commands:</p>
<pre><code>RUN mkdir -p ~/application
COPY &quot;../&quot; &quot;application/&quot;
WORKDIR &quot;application/&quot; 
ENTRYPOINT [&quot;Rscript&quot;, &quot;main.R&quot;]
</code></pre>
<p>This does not work, it gives me error `Fatal error: cannot open file 'main.R': No such file or directory</p>
<p>If I move <code>main.R</code> file inside <code>app</code> folder everything works fine. How can I make it work without moving files.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65489632","Sending a request from inside a docker container to a host spring server","<docker><rest><containers><devops><restapi>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I'm new to docker and I was wondering if it was possible for an application inside a container to send a request to a REST server located outside of the container (in my case, a Spring Boot server located on the host port 8080), and how I should do it ?</p>
<p>There would be several containers trying to do the same so, sharing the whole network with the &quot;net=host&quot; option wouldn't be possible, I guess ?</p>
<p>Thanks !</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65489723","How to get rid of the port when `host.docker.internal` to interact with a service running locally on my machine","<docker><networking><containers><devops>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>To make a service running in a container interact with another one running locally at let's say <code>localhost:3000</code>, we can use <code>host.docker.internal:3000</code> in the container code to make the required connection. But is there a way I can get rid of the port while using <code>host.docker.internal</code> so that it automatically routes to <code>localhost:3000</code>? That is, I just use <code>host.docker.internal</code> in my code instead of <code>host.docker.internal:3000</code>?</p>
<p>Clarification: I am not asing how to connect to localhost from inside the container. What I am asking is how to route <code>host.docker.internal</code> to the correct port without specifying it explicitly like: <code>host.docker.internal:3000</code>.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65576088","How can I configure my docker-compose for connect my 'app' running in docker containner to my 'database' running in physical machine","<docker><networking><docker-compose><gateway>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<pre><code>version: '3'
services:
   my_app:
     build: ./api
     environments:
       - DB_HOST=192.168.10.1 #Physical IP database server
</code></pre>
<blockquote>
<p>Docker Engine Server IP: 192.168.20.1</p>
</blockquote>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65651130","Dockerized Django project not able to connect to host's postgres database","<python><django><postgresql><docker><postgresql-12>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>So I have a Django project which is running in Docker, which is trying to connect postgres which is running on host machine. But I am getting error</p>
<pre><code>web_1  | django.db.utils.OperationalError: could not connect to server: Connection refused
web_1  |    Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
web_1  |    TCP/IP connections on port 5432?
web_1  | could not connect to server: Cannot assign requested address
web_1  |    Is the server running on host &quot;localhost&quot; (::1) and accepting
web_1  |    TCP/IP connections on port 5432?
</code></pre>
<p>I know that we need to make postgres to listen for requests from other IP addresses.
I have already made changes in postgres settings.</p>
<p>Added few lines in following files.</p>
<ol>
<li><p>/etc/postgresql/12/main/postgresql.conf</p>
<pre><code>      listen_addresses = '*'
</code></pre>
</li>
<li><p>/etc/postgresql/12/main/pg_hba.conf</p>
<pre><code>     host  all  all  172.17.0.0/16  trust
</code></pre>
</li>
</ol>
<p>In Django project settings.py has</p>
<pre><code>DATABASES = {
    'default': {
         'ENGINE': 'django.db.backends.postgresql_psycopg2',
         'NAME': 'django_docker',
         'USER': '&lt;postgres_user&gt;',
         'PASSWORD': '&lt;password&gt;',
         'HOST': 'localhost',
         'PORT': '5432',
         'ATOMIC_REQUESTS': True
     }
}
</code></pre>
<p>I am not sure why I am getting this error.</p>
<p>I also tried to use <code>127.0.0.1</code> and <code>&lt;public_ip&gt;</code> of host in Django database <code>HOST</code> settings, but I still get same error.</p>
<p>All versions</p>
<pre><code>Django : 3.0.5
PostgreSQL : 12.5
Docker : 20.10.1
docker-compose : 1.25.0
</code></pre>
<p>I am guessing that I am missing very small thing here, but I'm not sure what it is.</p>
<p>Please let me know if someone has any solutions, suggestions for this. Thank you.</p>
"
"25540711","docker postgres pgadmin local connection","<postgresql><docker><pgadmin>","65654441","PgAdmin Connecting to Postgres through Docker-compose results in ""Unable to connect to server : timeout expired""","<postgresql><docker><docker-compose><pgadmin><pgadmin-4>","<p>I have created an ubuntu image with nginx, php and postgres.</p>

<p>I want to connect the postgres database in my current image with <code>pgadmin</code> located on my local machine. </p>

<p>I have tried using docker inspector to try to use the image ip to make a connection with my local pgadmin but without much success. I've also tried configuring some ports on local to make connection work.</p>
","<p>Consider the Docker-compose :</p>
<pre><code>version: &quot;3&quot;
services:
  postgres_svc:
    image: postgres:12.0-alpine
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: myCoolPassword
      POSTGRES_DB: postgres      
    ports:
      - &quot;5432:5432&quot;


   ... More services
</code></pre>
<p>When I run <code>Docker-compose up</code> and run :</p>
<pre><code>docker inspect &lt;POSTGRES_ID&gt; | grep Gateway &gt;&gt;&gt; 192.168.16.1
</code></pre>
<p>or</p>
<pre><code>docker inspect &lt;POSTGRES_ID&gt;| grep IPAddress  &gt;&gt;&gt; 192.168.16.3
</code></pre>
<p>I get the IP/Gateway of the postgres container , and then in <strong>PGADMIN</strong> :</p>
<p><a href=""https://i.stack.imgur.com/XCe8r.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XCe8r.gif"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/MaCZy.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MaCZy.gif"" alt=""enter image description here"" /></a></p>
<p>And the same thing for the IP <code>192.168.16.3</code>.</p>
<p>How can we solve this connection problem ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65804473","How connect to localhost mysql base from docker in spring application","<java><mysql><spring-boot><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I try connecting spring boot application in docker with MySQL base on localhost</p>
<p>It's my application.properties, but the application cannot find the base</p>
<pre><code>server.port = 8080


spring.jpa.hibernate.ddl-auto=update
spring.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/TestDB?useUnicode=true&amp;serverTimezone=UTC&amp;useSSL=true&amp;verifyServerCertificate=false
spring.datasource.username=test
spring.datasource.password=test
</code></pre>
<p>I think the IP for MySQL in the container is different from the IP on the localhost</p>
<p>How I can connect to MySQL on localhost?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65822002","How to connect External mysql DB to Docker Web Container","<mysql><spring-boot><docker><containers>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am trying to access the MySQL database in the Docker container but is not allowing me to access it.</p>
<p>The Mysql database is running on my local machine.</p>
<p>I tried with <strong>docker run --network=&quot;127.0.0.1&quot;  -it -p 8080:8080</strong> but no luck</p>
<p>my docker file</p>
<pre><code>FROM tomcat:latest
ADD target/sample.war /usr/local/tomcat/webapps/
EXPOSE 8080
CMD [&quot;catalina.sh&quot;, &quot;run&quot;]
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65861021","connect ECONNREFUSED 127.0.0.1:3000 error using Docker container","<javascript><node.js><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a javascript file (client.js) that runs node-fetch to make GET requests to my server. My server is listening on port 3000. Whenever I create a Docker container to run the client file, I get the following error:</p>
<pre><code>(node:9) UnhandledPromiseRejectionWarning: FetchError: request to http://localhost:3000/selectItem/ failed, reason: connect ECONNREFUSED 127.0.0.1:3000
</code></pre>
<p>When I run the client file without any container, it works fine. I just want to have multiple container instances that run the client.js file.</p>
<p>I tried solutions like <a href=""https://stackoverflow.com/questions/14168433/node-js-error-connect-econnrefused"">Node.js Error: connect ECONNREFUSED</a>, but they didn't work.</p>
<p>My Dockerfile:</p>
<pre><code>FROM node:10

WORKDIR /client

COPY package*.json ./

RUN npm install

COPY . .

CMD node client
EXPOSE 3000
</code></pre>
<p>Thank you :)</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66250872","Connecion to mysql on host machine from a docker container not possible","<mysql><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am running a SQL Server on my host machine (not inside a docker container). I bound it to 127.0.0.1 and all applications can access it, which are running native on the host.</p>
<p>Now I have a nodejs app inside a docker container which needs to connect to the mysql Server, too.</p>
<p><code>sudo docker inspect</code> says my gateway is <code>172.18.0.1</code></p>
<p>I tried to use <code>172.18.0.1</code> or <code>host.docker.internal</code> as hostname inside the container to connect. But I get alway a host not reachable error. Even if I bind mysql to 0.0.0.0 I can't connect.</p>
<p>I am using docker on a linux machine:</p>
<pre><code>Server: Docker Engine - Community
 Engine:
  Version:          20.10.3
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       46229ca
  Built:            Fri Jan 29 14:31:32 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.3
  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b
 runc:
  Version:          1.0.0-rc92
  GitCommit:        ff819c7e9184c13b7c2607fe6c30ae19403a7aff
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
docker-compose version 1.25.0, build unknown
docker-py version: 4.1.0
CPython version: 3.8.5
OpenSSL version: OpenSSL 1.1.1f  31 Mar 2020
</code></pre>
<p>That is my docker-compose.yml:</p>
<pre><code>version: &quot;3&quot;
services:
    mailadmin-backend:
            image: mailadmin-backend
            ports:
                    - &quot;10600:8080&quot;
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66271108","Docker - Connect containers to local database","<java><docker><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am in process to dockerize my local server, on that have databases, application servers and all additional configuration to deploy my applications. Currently, i have created 3 docker containers that run application servers like Tomcat or WildFly, and exposed <code>80</code>, <code>8130</code> and <code>8080</code> ports this deployed applications are in Java and read and write data from/to my local database. On the <code>persistence.xml</code> file for each app, i have configured the access via <code>localhost</code> in my actual configuration that works ok without docker.</p>
<p>I want to connect the apps on the containers to database that correspond on the host machine, how can i do that?</p>
<p>I have created the containers like this:</p>
<pre><code>docker run -d --name app_server1 -p 80:80 wildfly_website:1.0
docker run -d --name app_server2 -p 8130:8130 geoserver:1.0
</code></pre>
<p>This image illustrates what i want to do
<a href=""https://i.stack.imgur.com/B2Yhp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B2Yhp.png"" alt=""enter image description here"" /></a></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66386178","how to enable containers from docker-compose ""bridge"" network to reach out to service running on Docker host?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a <code>docker-compose.yml</code> file, that defines three services, using shared network. Network is defined only by name. <code>docker network list</code> shows that it's a <code>bridge</code> network. <code>docker inspect network</code> gives the IP addresses (<code>172.23.0.x</code>, with <code>172.23.0.1</code> defined as a gateway). My understanding is that this <code>gateway</code> is my Docker host (a Windows 10 machine).</p>
<p>When I execute <code>wget http://containerA-ip:PORT//</code> from <code>bash</code> running in containerB, I get response alright.</p>
<p>When I execute <code>wget http://gateway-ip:PORT//</code> from <code>bash</code> running in containerB, I get response alright, too. (port PORT is published as PORT:PORT, so I assume that's the reason it works).</p>
<p><strong>BUT</strong>. If I try to <code>wget http://gateway-ip:OTHER-PORT</code>, where OTHER-PORT is a port number of some process that is listening on my Windows 10 machine, I get <code>Connection refused</code>.</p>
<p>I wonder, is it FW configuration on my Windows machine, that's preventing this connection, or is it some Docker configuration/implementation detail that's preventing this kind of connection.</p>
<p>I was unable to find an answer to &quot;how to connect to Docker host services from within a container using <code>bridge</code> network&quot; anywhere in the documentation..</p>
<p>Thanks to anyone, who could assist.</p>
<p>UPD: I've been suggested to check which interfaces this service is listening on, here's the output of <code>netstat -na | grep 8088</code>:</p>
<pre><code>TCP    0.0.0.0:8088           0.0.0.0:0              LISTENING                                                                                                                      
TCP    [::]:8088              [::]:0                 LISTENING          
</code></pre>
<p>to verify it's the case, I've run <code>curl --noproxy &quot;*&quot; -X POST &quot;http://192.168.0.13:8088/&quot;</code> and it produced expected response</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","66442522","Running docker when dockerfile is in folder inside a project and the solution includes multiple projects","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I added a docker support to a dot.net core application, this is what I got</p>
<blockquote>
<p>#See <a href=""https://aka.ms/containerfastmode"" rel=""nofollow noreferrer"">https://aka.ms/containerfastmode</a> to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.</p>
</blockquote>
<pre><code>FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;SMSys.csproj&quot;, &quot;&quot;]
COPY [&quot;../DataLayer/DataLayer.csproj&quot;, &quot;../DataLayer/&quot;]
COPY [&quot;../Utilities/Utilities.csproj&quot;, &quot;../Utilities/&quot;]
COPY [&quot;../ServiceLayer/ServiceLayer.csproj&quot;, &quot;../ServiceLayer/&quot;]
RUN dotnet restore &quot;./SMSys.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;SMSys.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;SMSys.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;SMSys.dll&quot;]
</code></pre>
<p>My docker file is where 'SMSys.csproj' is.</p>
<p>If I run docker inside that directory, I get:</p>
<pre><code>COPY failed: forbidden path outside the build context:
</code></pre>
<p>If I change some of the references of the project after the copy commands and run docker from the outside of the directory where all my projects are, I get multiple over a thousand of errors relating to my solution. The errors relate to missing things (assemblies and packages) as if all the projects are oblivious to one another when in fact they should be well referenced to one another as they do when I launch the project through visual studio.</p>
<p>This is an example of a solution that I followed that didnt work.
<a href=""https://www.jamestharpe.com/docker-include-files-outside-build-context/"" rel=""nofollow noreferrer"">https://www.jamestharpe.com/docker-include-files-outside-build-context/</a></p>
<p>Whats the best solution to implement in order to run my project through docker?</p>
<p>UPDATE</p>
<pre><code>    FROM mcr.microsoft.com/dotnet/aspnet:5.0.3-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

 
FROM mcr.microsoft.com/dotnet/sdk:5.0.103-buster-slim AS build 
WORKDIR /src 

# Prevent 'Warning: apt-key output should not be parsed (stdout is not a terminal)'
ENV APT_KEY_DONT_WARN_ON_DANGEROUS_USAGE=1

# install NodeJS 13.x
# see https://github.com/nodesource/distributions/blob/master/README.md#deb
RUN apt-get update -y
RUN apt-get upgrade -y
RUN apt-get install -y mc
#RUN apt-get install curl gnupg -yq 
#RUN curl -sL https://deb.nodesource.com/setup_12.x | bash -
RUN apt-get install -y npm
 
COPY [&quot;SMSysSolution/SMSys.csproj&quot;, &quot;SMSysSolution/&quot;]
COPY [&quot;DataLayer/DataLayer.csproj&quot;, &quot;DataLayer/&quot;]
COPY [&quot;Utilities/Utilities.csproj&quot;, &quot;Utilities/&quot;]
COPY [&quot;ServiceLayer/ServiceLayer.csproj&quot;, &quot;ServiceLayer/&quot;]
COPY [&quot;XUnitIntegrationTests/XUnitIntegrationTests.csproj&quot;, &quot;XUnitIntegrationTests/&quot;]
COPY [&quot;XUnitTestProject1/XUnitTestProject1.csproj&quot;, &quot;XUnitTestProject1/&quot;]
RUN dotnet restore &quot;./SMSysSolution/SMSys.csproj&quot;
COPY . .
WORKDIR &quot;/src/SMSysSolution&quot;
RUN dotnet build &quot;SMSys.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;SMSys.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;SMSys.dll&quot;]
</code></pre>
<p>I had to put a few changes to the paths to make it work.</p>
"
"27937185","Assign static IP to Docker container","<docker>","59606281","Docker how to specify an ip address with docker-compose","<docker><docker-compose>","<p>I'm now trying to assign a static IP 172.17.0.1 when a Docker container be started up. </p>

<p>I use port 2122 as the ssh port of this container so that I let this container listen port 2122.</p>

<pre><code>sudo docker run -i -t -p 2122:2122 ubuntu
</code></pre>

<p>This command will run a Docker container with a random IP like 172.17.0.5, but I need to assign a specific IP to the container.</p>

<p>The following shell script is what I reference Docker documentation in advanced network settings.</p>

<pre><code>pid=$(sudo docker inspect -f '{{.State.Pid}}' &lt;container_name&gt; 2&gt;/dev/null)
sudo rm -rf /var/run/netns/*
sudo ln -s /proc/$pid/ns/net /var/run/netns/$pid
sudo ip link add A type veth peer name B
sudo brctl addif docker0 A
sudo ip link set A up
sudo ip link set B netns $pid
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link delete eth0
sudo ip netns exec $pid ip link set dev B name eth0
sudo ip netns exec $pid ip link set eth0 address 12:34:56:78:9a:bc
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link set eth0 up
sudo ip netns exec $pid ip addr add 172.17.0.1/16 dev eth0
sudo ip netns exec $pid ip route add default via 172.17.42.1
</code></pre>

<p>This shell script will assign a static IP 172.17.0.1 and link to the world fine. But whenever I try to ssh to this container from my local, it didn't work. What's the problem possibly I met?</p>
","<p>I use docker-compose to charge my container, I need to connect between container and host, so I need an ip that I always could access, I know docker's default ip on linux is <code>172.17.0.x</code>, but something strange is that every time I start up the container the ip will change, from <code>172.18.0.1</code> to <code>172.19.0.1</code> to <code>172.20.0.1</code> and etc ..., step by step</p>
"
"32486779","apt-add-repository: command not found error in Dockerfile","<docker>","59614258","Installing python using Dockerfile not working","<docker><ubuntu><dockerfile>","<p>I just made a very simple Docker file in my terminal, basically I did the following:</p>

<pre><code>mkdir pgrouted
cd pgrouted
touch Dockerfile
</code></pre>

<p>Now I open the Docker file in the <a href=""https://en.wikipedia.org/wiki/GNU_nano"" rel=""noreferrer"">nano</a> editor, and I add the following commands to the Docker file:</p>

<pre><code>FROM ubuntu

MAINTAINER Gautam &lt;gautamx07@yahoo.com&gt;

LABEL Description=""pgrouting excercise"" Vendor=""skanatek"" Version=""1.0""

ENV BBOX=""-122.8,45.4,-122.5,45.6""

# Add pgRouting launchpad repository
RUN sudo apt-add-repository -y ppa:ubuntugis/ppa
RUN sudo apt-add-repository -y ppa:georepublic/pgrouting
RUN sudo apt-get update

# Install pgRouting package (for Ubuntu 14.04)
RUN sudo apt-get install postgresql-9.3-pgrouting

# Install osm2pgrouting package
RUN sudo apt-get install osm2pgrouting

# Install workshop material (optional, but maybe slightly outdated)
RUN sudo apt-get install pgrouting-workshop

# For workshops at conferences and events:
# Download and install from http://trac.osgeo.org/osgeo/wiki/Live_GIS_Workshop_Install
RUN wget --no-check-certificate https://launchpad.net/~georepublic/+archive/pgrouting/+files/pgrouting-workshop_2.0.6-ppa1_all.deb

RUN sudo dpkg -i pgrouting-workshop_2.0.6-ppa1_all.deb

# Review: Not sure weather this should be in the dockerfile
RUN cp -R /usr/share/pgrouting/workshop ~/Desktop/pgrouting-workshop

# Log in as user ""user""
RUN psql -U postgres

# Create routing database
RUN CREATE DATABASE routing;

# Add PostGIS functions
RUN CREATE EXTENSION postgis;

# Add pgRouting core functions
CREATE EXTENSION pgrouting;

# Download using Overpass XAPI (larger extracts possible than with default OSM API)
wget --progress=dot:mega -O ""sampledata.osm"" ""http://www.overpass-api.de/api/xapi?*[bbox=${BBOX}][@meta]""
</code></pre>

<p>The entire Dockerfile can be see <strong><a href=""http://chopapp.com/#yd9pqtlj"" rel=""noreferrer"">HERE</a></strong> at a glance.</p>

<p>Now when I try to build the Dockerfile, like so:</p>

<pre><code>docker build -t gautam/pgrouted:v1 .
</code></pre>

<p>The Dockerfile runs and then I get the below error:</p>

<pre><code>Step 4 : RUN sudo apt-add-repository -y ppa:ubuntugis/ppa
 ---&gt; Running in c93c3c5fd5e8
sudo: apt-add-repository: command not found
The command '/bin/sh -c sudo apt-add-repository -y ppa:ubuntugis/ppa' returned a non-zero code: 1
</code></pre>

<p>Why am I getting this error?</p>
","<p>I'm trying to run the below Dockerfile contents on ubuntu image.</p>

<pre><code>    FROM ubuntu
    RUN apt-get update
    RUN apt-get install -y python
    RUN apt-get install -y python-pip
    RUN pip install flask
    COPY app.py /opt/app.py
    ENTRYPOINT FLASK_APP=/opt/app.py flask run --host=0.0.0.0
</code></pre>

<p>But im getting the below error at layer 3</p>

<pre><code>tep 1/7 : FROM ubuntu
 ---&gt; 549b9b86cb8d
Step 2/7 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 78d87d6d9188
Step 3/7 : RUN apt-get install -y python
 ---&gt; Running in a256128fde51
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package python

</code></pre>

<p>Athough while i run the below command individually</p>

<p><strong>sudo apt-get install -y python</strong></p>

<p>it's successfully getting installed. </p>

<p>Can anyone please help me out. </p>

<p>Thanks,
Sourav</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","59678512","Copy file with absolute path to Docker Container using a Dockerfile","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have a Dockerfile located at path:
<code>/Users/userx/Library/Documents/dockerProject</code></p>

<p>I want to copy a file into the container from:
<code>/Users/userx/Library/temp/otherfiles/myFile.txt</code></p>

<p>I would like to do this from with in the Dockerfile. I tried using <code>COPY</code> but it only works in the context of the project.</p>

<p>some people implied I can do it with the <code>RUN</code> command chaining bash commands but when I use:</p>

<p><code>RUN 'cd /Users/userx/Library/temp/otherfiles/;pwd'</code></p>

<p>I get the error <code>returned a non-zero code</code></p>

<p>How can I copy a file using a Dockerfile from a system absolute path.</p>

<p>thanks</p>
"
"28212380","Why docker container exits immediately","<docker>","59736415","Why Docker container always exited after start?","<docker><dockerfile>","<p>I run a container in the background using</p>

<pre><code> docker run -d --name hadoop h_Service
</code></pre>

<p>it exits quickly. But if I run in the foreground, it works fine. I checked logs using</p>

<pre><code>docker logs hadoop
</code></pre>

<p>there was no error. Any ideas?</p>

<p><strong>DOCKERFILE</strong></p>

<pre><code> FROM java_ubuntu_new
 RUN wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
 RUN dpkg -i cdh4-repository_1.0_all.deb
 RUN curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | apt-key add -
 RUN  apt-get update
 RUN apt-get install -y hadoop-0.20-conf-pseudo
 RUN dpkg -L hadoop-0.20-conf-pseudo
 USER hdfs
 RUN hdfs namenode -format
 USER root
 RUN apt-get install -y sudo
 ADD . /usr/local/
 RUN chmod 777 /usr/local/start-all.sh
 CMD [""/usr/local/start-all.sh""]
</code></pre>

<p>start-all.sh</p>

<pre><code> #!/usr/bin/env bash
 /etc/init.d/hadoop-hdfs-namenode start
 /etc/init.d/hadoop-hdfs-datanode start
 /etc/init.d/hadoop-hdfs-secondarynamenode start
 /etc/init.d/hadoop-0.20-mapreduce-tasktracker start
 sudo -u hdfs hadoop fs -chmod 777 /
 /etc/init.d/hadoop-0.20-mapreduce-jobtracker start
 /bin/bash
</code></pre>
","<p>I'm a new to Docker. Tried to start docker container with simple java code.</p>

<p>I saw the printing <code>Hello-World</code>, that means that my code is executed, but the container always exits.</p>

<p>This is my Dockerfile:</p>

<pre><code>FROM nicb/alpine-openjdk8-jre
COPY HelloDockerTest.jar /
CMD java -jar HelloDockerTest.jar
</code></pre>

<p><code>FROM nicb/alpine-openjdk8-jre</code> - this is image from remote repository</p>

<p>The steps that I do are:</p>

<pre><code>1. docker build -t testtt .
2. docker run -i testtt
3. docker start 2520e85b333a
</code></pre>

<p>In the last step I see the printing ""Hello World"" but when I do <code>docker ps</code> I see nothing.</p>

<p>What did I do wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","59759034","Flask + Nginx reverse proxy refuses to connect in docker containers","<docker><nginx><flask><docker-compose>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a React app that uses Flask as a back-end. I am trying to move the react app to be served from an Nginx server. Both the Nginx and the Flask server are inside docker containers. The static content works fine but when the site tries to proxy a call to the flask server the connection is refused. </p>

<p>I don't see any logs on the Flask server so I don't think the requests are getting into the Flask container. I've tried changing the proxy to hit a different port and got the same issue. Shouldn't the containers be allowed to communicate with each other or do I have to explicitly allow certain ports to be open? Do I need a WSGI server between them?</p>

<p>Nginx default.conf</p>

<pre><code>server {

   listen        80;
   server_name   localhost;

   location / {
      index      index.html;
      root       /usr/share/nginx/html;
   }

   location /api/ {
      proxy_pass http://flask:5000;
      proxy_http_version 1.1;
      proxy_set_header Connection """";
      proxy_set_header Host ""localhost"";
   }

}
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: ""3""
services:
    nginx:
        image: nginx
        volumes:
            - /home/cookery/Cookery/dining/build:/usr/share/nginx/html
            - /home/cookery/Cookery/docker/docker.conf:/etc/nginx/conf.d/default.conf
        ports:
            - ""8080:80""
    flask:
        build: ../kitchen
        ports:
            - ""5000:5000""
</code></pre>

<p>This is the error I get on the request to the flask server</p>

<pre><code>2020/01/15 21:18:08 [error] 6#6: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 172.26.0.1, server: localhost, request: ""GET /api/recipes/all HTTP/1.1"", upstream: ""http://172.26.0.2:5000/api/recipes/all"", host: ""localhost:8080"", referrer: ""http://localhost:8080/""
</code></pre>
"
"30383845","What is the best practice of docker + ufw under Ubuntu","<ubuntu><docker><containers><firewall><iptables>","59803456","UFW Rules Ignored","<ubuntu><firewall><ufw>","<p>I just tried out Docker. It is awesome but seems not work nicely with ufw. By default, docker will manipulate the iptables a little bit. The outcome is not a bug but not what I expected.
For more details you can read <a href=""http://blog.viktorpetersson.com/post/101707677489/the-dangers-of-ufw-docker"" rel=""noreferrer"">The dangers of UFW + Docker</a></p>

<p>My goal is to set up a system like</p>

<pre><code>    Host (running ufw) -&gt; docker container 1 - nginx (as a reverse proxy)
                       -&gt; docker container 2 - node web 1
                       -&gt; docker container 3 - node web 2
                       -&gt; .......
</code></pre>

<p>I want to manage the incoming traffic (e.g. restrict access) through ufw therefore I don't want docker to touch my iptables. Here is my test</p>

<p><strong>Environment:</strong></p>

<ul>
<li>a newly installed Ubuntu 14.04 (kernel: 3.13.0-53 )</li>
<li>Docker 1.6.2 </li>
<li>ufw forwarding is enabled.( <a href=""https://docs.docker.com/installation/ubuntulinux/#enable-ufw-forwarding"" rel=""noreferrer"" title=""Enable UFW forwarding"">Enable UFW forwarding</a> )</li>
<li><code>--iptables=false</code> was added to the Docker daemon.</li>
</ul>

<p><strong>First Attempt</strong></p>

<pre><code>docker run --name ghost -v /home/xxxx/ghost_content:/var/lib/ghost -d ghost
docker run --name nginx -p 80:80 -v /home/xxxx/nginx_site_enable:/etc/nginx/conf.d:ro --link ghost:ghost -d nginx
</code></pre>

<p>No luck. The first command is fine but the second command will throw an error</p>

<pre><code>Error response from daemon: Cannot start container
</code></pre>

<p><strong>Second Attempt</strong></p>

<p>Then I found this: <a href=""https://github.com/docker/docker/issues/12701"" rel=""noreferrer"">unable to link containers with --iptables=false #12701</a></p>

<p>After running the following command, everything looks OK.</p>

<pre><code>sudo iptables -N DOCKER
</code></pre>

<p>However, I noticed that I can not establish any outbound connections inside containers. For example:</p>

<pre><code>xxxxg@ubuntu:~$ sudo docker exec -t -i nginx /bin/bash
root@b0d33f22d3f4:/# ping 74.125.21.147
PING 74.125.21.147 (74.125.21.147): 56 data bytes
^C--- 74.125.21.147 ping statistics ---
35 packets transmitted, 0 packets received, 100% packet loss
root@b0d33f22d3f4:/# 
</code></pre>

<p>If I remove <code>--iptables=false</code> from the Docker daemon, then the internet connection of containers will be back to normal but the ufw will not work 'properly'  (well...by my definition).</p>

<p>So, what is the best practice of docker + ufw? Can anyone provide some help?</p>

<p>Thanks.</p>

<p>Bart.</p>
","<p>I have ufw enabled as you can see here:</p>

<pre><code>Status: active
Logging: on (low)
Default: deny (incoming), allow (outgoing), allow (routed)
New profiles: skip

To                         Action      From
--                         ------      ----
22/tcp                     LIMIT IN    Anywhere                  
22/tcp (v6)                LIMIT IN    Anywhere (v6) 
</code></pre>

<p>For some reason, when I started an HTTP Server on port 80, I was able to view the webpage, even though I have no rule set to allow port 80.</p>

<p>When I tried to deny port 80 as such:</p>

<pre><code>Status: active
Logging: on (low)
Default: deny (incoming), allow (outgoing), allow (routed)
New profiles: skip

To                         Action      From
--                         ------      ----
22/tcp                     LIMIT IN    Anywhere                  
80/tcp                     DENY IN     Anywhere                  
22/tcp (v6)                LIMIT IN    Anywhere (v6)             
80/tcp (v6)                DENY IN     Anywhere (v6)
</code></pre>

<p>I was still able to view the webpage.</p>

<p>Can someone please tell me what am I doing wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60273477","docker not able to start flask application","<docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>The flask application is started but I am not able to view anything from browser.</p>

<pre><code>root@1ec89cb3dad1:/app# python main.py
 * Serving Flask app ""main"" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 231-587-342
</code></pre>

<p>The Dockerfile looks like this:</p>

<pre><code>FROM ubuntu
RUN apt-get update -y &amp;&amp; \
    apt-get install -y python-pip python-dev  libhunspell-dev
WORKDIR /app
COPY ./requirements.txt /app/requirements.txt
RUN pip install -r requirements.txt
COPY . /app
ENTRYPOINT [ ""python"" ]
CMD [ ""main.py"" ]
</code></pre>

<p>The files are here...
<a href=""https://github.com/shantanuo/Spell-Checker/tree/master"" rel=""nofollow noreferrer"">https://github.com/shantanuo/Spell-Checker/tree/master</a></p>

<p>I am not sure if I should tag docker or flask!</p>

<hr>

<p>Update:</p>

<p>I used these commands to build and run the image...</p>

<pre><code>git clone https://github.com/shantanuo/Spell-Checker.git

cd Spell-Checker/

docker build -t shantanuo/flask .

docker run -p 5000:5000 shantanuo/flask
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60299144","Not able to access running server on docker in uvicorn","<python><docker><fastapi><uvicorn>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have already ran a uvicorn server in docker, its running fine too but i am not able to see it</p>

<pre><code>FROM python
RUN pip install fastapi uvicorn mongoengine
EXPOSE 8002
COPY ./app /app
ENV PYTHONPATH ""${PYTONPATH}:/app""
VOLUME /app
RUN cd app
CMD [""uvicorn"", ""main:app"", ""--host"", ""127.0.0.1"", ""--port"", ""8002""]
</code></pre>

<p>This ran successfully and i am seeing this success logs as well</p>

<pre><code>D:\***\****\****&gt;docker logs a1b
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)
</code></pre>

<p>On visiting </p>

<ul>
<li><a href=""http://127.0.0.1:8002/"" rel=""nofollow noreferrer"">http://127.0.0.1:8002/</a></li>
</ul>

<p>its saying
<strong>This page isn’t working</strong></p>

<p>How should i access this then..</p>

<p>On <code>docker ps -a</code>:</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                                 NAMES
832bf82a0fb1        edmaskbackend6      ""uvicorn main:app ...""   5 seconds ago       Up 4 seconds                   127.0.0.1:32772-&gt;8002/tcp             infallible_cray
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60326180","How to view Dash app running in docker container","<docker><flask><docker-compose><hyphen>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<ol>
<li>I Execute <code>docker-compose up</code></li>
<li>The Dash app seems to work fine (see the output)</li>
<li>Dash app cannot be accessed through the local browser <code>localhost:8050</code></li>
</ol>

<p><strong><em>Dash file:</em></strong></p>

<pre><code>app = dash.Dash(__name__)

//...

if __name__ == '__main__':
    app.run_server(debug=True, host='0.0.0.0', port=8050)
</code></pre>

<p><strong><em>docker-compose file:</em></strong></p>

<pre><code>version: ""3.7""
services:
  dashapp:
    image: my_dash_image
    container_name: my-dash
    ports:
      - 8050:8050
    volumes:
      - /tmp/files:/usr/src/app
</code></pre>

<p><strong><em>Output execution:</em></strong></p>

<pre><code>my-dash | Hello!
my-dash | Running on http://127.0.0.1:8050/
my-dash | Debugger PIN: 576-910-730
my-dash |  * Serving Flask app ""global_overview_indicators"" (lazy loading)
my-dash |  * Environment: production
my-dash |    WARNING: This is a development server. Do not use it in a production deployment.
my-dash |    Use a production WSGI server instead.
my-dash |  * Debug mode: on
</code></pre>
"
"27068596","How to include files outside of Docker's build context?","<docker>","60344899","How to copy files when files are in different folder than the dockerfile?","<windows><docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I need to build a docker image and need to copy a file from the different folder which don't have docker file</p>

<p>Basically this is my dir structure</p>

<pre><code>C:\AGENT\_WORK\2
├───a
│   └───s
└───s
    └───Docker
            Dockerfile
</code></pre>

<p>so I need to copy all files from <code>C:\AGENT\_WORK\2\a\s</code> but my docker file is in  <code>C:\AGENT\_WORK\2\s\Docker\Dockerfile</code></p>

<p>I'm building the image by this command, I have given context as C:\agent_work`</p>

<pre><code>docker build -t testapp:latest -f C:\AGENT\_WORK\2\s\Docker\Dockerfile C:\agent\_work
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM mcr.microsoft.com/windows/nanoserver:1709 AS base
WORKDIR /app
ARG BINARY_SRC
EXPOSE 80
EXPOSE 443
ADD C:\AGENT\_WORK\2\a\s .
RUN ls
FROM base AS final
ENTRYPOINT [""dotnet"", ""TestApp.dll""]
</code></pre>

<p>Error:</p>

<blockquote>
  <p>ADD failed: CreateFile
  \?\C:\ProgramData\Docker\tmp\docker-builder076789264\agent_work: The
  system cannot find the path specified.</p>
</blockquote>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60577240","Dockerized Python Flask REST API displays ""The page isn't working","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I am trying to run Flask in Docker. The containers seem to be running but the browser can't connect.</p>

<p><code>docker-compose.yml</code>:</p>

<pre><code>version: '3'

services: 
  api:
    build: .
    ports:
      - ""5000:5000""
    volumes:
      - ./src:/usr/src
</code></pre>

<p><code>Dockerfile</code>:</p>

<pre><code>FROM python:3.8.2
COPY ./src /usr/src
WORKDIR /usr/src
RUN pip install -r requirements.txt
ENTRYPOINT [""python""]
CMD [""run.py""]
</code></pre>

<pre><code>from flask import Flask

def create_app(config_filename):
    app = Flask(__name__)
    app.config.from_object(config_filename)

    from app import api_bp
    app.register_blueprint(api_bp, url_prefix='/api')

    return app

if __name__ == ""__main__"":
    app = create_app(""config"")
    app.run(debug=True)
</code></pre>

<p>When I try to go to <code>http://localhost:5000/api/Hello</code>, I get:</p>

<p><a href=""https://i.stack.imgur.com/jzGmT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jzGmT.png"" alt=""chrome page isn&#39;t working error""></a></p>
"
"33799885","How to stop all containers when one container stops with docker-compose?","<docker><docker-compose>","60671912","Docker-compose - wait for a container to exit","<docker><docker-compose><dockerfile>","<p>Up until recently, when one was doing <code>docker-compose up</code> for a bunch of containers and one of the started containers stopped, all of the containers were stopped. This is not the case anymore since <a href=""https://github.com/docker/compose/issues/741"" rel=""noreferrer"">https://github.com/docker/compose/issues/741</a> and this is a really annoying for us: We use docker-compose to run selenium tests which means starting application server, starting selenium hub + nodes, starting tests driver, then exiting when tests driver stops.</p>

<p>Is there a way to get back old behaviour?</p>
","<p>I have a docker-compose setup, where I have 4 services.  One of these services runs tests, and it then exits after the tests finish.  I would like the other services to listen to this service exiting, so that they can exit themselves automatically. How can I achieve this?</p>
"
"29556879","Is it possible change date in docker container?","<tomcat><docker>","60818445","How to change the date of a docker container without using libfaketime?","<database><docker><db2><containers>","<p>I have a container with a running program inside tomcat. I need to change date only in this container and test my program behaviour. I have time sensitive logic, and sometimes need to see what happens in a few days or months later.
Is it possible in docker? I read that if I change date in container, date will get changed on the host system. But it is a bad idea for me. I need to have a few instances of this application on one server and have possibilities of setting up different time for each instance.</p>

<p>But when I try to change date inside the container I get the error:</p>

<pre><code>sudo date 04101812
date: cannot set date: Operation not permitted
Fri Apr 10 18:12:00 UTC 2015
</code></pre>
","<p>I am trying to launch a container at runtime with a different date :</p>

<p><code>docker run -it db container /bash/bin date +%T -s ""20190322 10:00:00""</code></p>

<p>This doesn't changes also the date of the system.</p>
"
"30401448","Docker vs Vagrant","<docker><vagrant>","60915782","Docker-machine vs Vagrant?","<docker><vagrant><docker-machine>","<p>Every Docker image, as I understand, is based on <strong>base image</strong> - for example, Ubuntu.</p>

<p>And if I want to isolate any process I should deploy ubuntu docker base image (<strong>where is difference with Vagrant here?</strong>), and create a necessary subimage after it installing on ubuntu image?</p>

<p>So, if Ubuntu is launched on Vagrant and on Docker, where is practice difference?
And if to use docker provider in Vagrant - where here is difference between Vagrant and Docker?</p>

<p>And, in Docker is it possible to isolate processes on some PC without base image without it's sharing to another PC?</p>
","<p>I am reading about docker-machine and Vagrant. From what I am reading its looks like docker-machine is light version of Vagrant, but Vagrant gives you more flexibility and control. Am I right?</p>

<p>Today I am using docker on my local machine with MacOS. I use Docker for simple stuff like run database server and etc. So Docker is great to have in many ways. </p>

<p>But if I want to seperate my dev environments?</p>

<p>Can anyone who have experience with both tell me which one I should prefer if I want to seperate my dev environments and pros/cons with both?</p>

<p>I want to understand which one I should use, docker-machine or Vagrant?</p>

<p>Thank you for helping me understand pros/cons with both and concept of it :)</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60952484","Can't see my running Docker Container on localhost","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a simple python application that I want to run in a Docker Image. The application looks like this</p>

<pre><code>from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World!'

if __name__ == '__main__':
    app.run()
</code></pre>

<p>and a dockerfile that looks like this:</p>

<pre><code>FROM python:3
RUN pip install --upgrade pip
RUN pip install flask
CMD [""python"",""app.py""]
COPY app.py /app.py
</code></pre>

<p>In addition to these two files the rest of the folder structure looks like this:</p>

<pre><code>├───.idea
│   └───inspectionProfiles
├───static
├───templates
├───venv
│   ├───Include
│   ├───Lib
│   │   ├───site-packages
│   │   │   └───pip-19.0.3-py3.7.egg
│   │   │       ├───EGG-INFO
│   │   │       └───pip
│   │   │           ├───_internal
│   │   │           │   ├───cli
│   │   │           │   ├───commands
│   │   │           │   ├───models
│   │   │           │   ├───operations
│   │   │           │   ├───req
│   │   │           │   ├───utils
│   │   │           │   └───vcs
│   │   │           └───_vendor
│   │   │               ├───cachecontrol
│   │   │               │   └───caches
│   │   │               ├───certifi
│   │   │               ├───chardet
│   │   │               │   └───cli
│   │   │               ├───colorama
│   │   │               ├───distlib
│   │   │               │   └───_backport
│   │   │               ├───html5lib
│   │   │               │   ├───filters
│   │   │               │   ├───treeadapters
│   │   │               │   ├───treebuilders
│   │   │               │   ├───treewalkers
│   │   │               │   └───_trie
│   │   │               ├───idna
│   │   │               ├───lockfile
│   │   │               ├───msgpack
│   │   │               ├───packaging
│   │   │               ├───pep517
│   │   │               ├───pkg_resources
│   │   │               ├───progress
│   │   │               ├───pytoml
│   │   │               ├───requests
│   │   │               ├───urllib3
│   │   │               │   ├───contrib
│   │   │               │   │   └───_securetransport
│   │   │               │   ├───packages
│   │   │               │   │   ├───backports
│   │   │               │   │   └───ssl_match_hostname
│   │   │               │   └───util
│   │   │               └───webencodings
│   │   └───tcl8.6
│   └───Scripts
└───__pycache__

</code></pre>

<p>From Powershell I then build the Docker image by writing the command:</p>

<pre><code>docker build . -t myusername/flaskapp
</code></pre>

<pre><code>PS C:\Users\mypcuser\projects\flask_docker_test&gt; docker build . -t myusername/flaskapp
Sending build context to Docker daemon  19.49MB
Step 1/5 : FROM python:3
 ---&gt; f88b2f81f83a
Step 2/5 : RUN pip install --upgrade pip
 ---&gt; Running in 56dc287d7501
Requirement already up-to-date: pip in /usr/local/lib/python3.8/site-packages (20.0.2)
Removing intermediate container 56dc287d7501
 ---&gt; 2dff8ebf09c6
Step 3/5 : RUN pip install flask
 ---&gt; Running in 5b59f8968a63
Collecting flask
  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)
Collecting Werkzeug&gt;=0.15
  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)
Collecting click&gt;=5.1
  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)
Collecting Jinja2&gt;=2.10.1
  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)
Collecting itsdangerous&gt;=0.24
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting MarkupSafe&gt;=0.23
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Installing collected packages: Werkzeug, click, MarkupSafe, Jinja2, itsdangerous, flask
Successfully installed Jinja2-2.11.1 MarkupSafe-1.1.1 Werkzeug-1.0.0 click-7.1.1 flask-1.1.1 itsdangerous-1.1.0
Removing intermediate container 5b59f8968a63
 ---&gt; 7583bc2d8be6
Step 4/5 : CMD [""python"",""app.py""]
 ---&gt; Running in 9394be530612
Removing intermediate container 9394be530612
 ---&gt; 53e72fb77552
Step 5/5 : COPY app.py /app.py
 ---&gt; 5925b08ae09e
Successfully built 5925b08ae09e
Successfully tagged myusername/flaskapp:latest
SECURITY WARNING: You are building a Docker image from Windows against a non-Windows Docker host. All files and directories added to build context will have '-rwxr-xr-x' permissions. It is recommended to double check and reset permissions for sensitive files and directories.
PS C:\Users\mypcuser\projects\flask_docker_test&gt;
</code></pre>

<p>I then go ahead and run my app using this command:</p>

<pre><code>docker run -p 5001:5000 -t myusername/flaskapp
</code></pre>

<p>And get this output:</p>

<pre><code> * Serving Flask app ""app"" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
</code></pre>

<p>But when I go to this URL in both Firefox, Google Chrome and Postman I get this:</p>

<p><a href=""https://i.stack.imgur.com/MYGkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MYGkZ.png"" alt=""enter image description here""></a></p>
"
"32052975","How could I bind docker container to specific external interface","<docker>","61058370","How to bind secured PHP source code running inside Docker to a specific IP or MAC address?","<php><docker><security><ioncube>","<p>I have two network interfaces, <code>eth0</code> and <code>eth1</code>,</p>

<p>How could I bind all docker container to <code>eth1</code>, and let all network traffic go out and in via the <code>eth1</code></p>

<p>Thanks~</p>

<h1>update</h1>

<p>I tried to bind to the <code>eth1</code> with 133.130.60.36.</p>

<p>But i still got no luck, i still get the eth0 IP as the public IP in the container. the network flow is not go out via eth1</p>

<pre><code>➜  ~  docker run -d --name Peach_1 -p 133.130.60.36::9998 -ti sample/ubuntu-vnc-selenium-firefox

➜  ~  docker ps
CONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                     NAMES
eb28f0d1c337        sample/ubuntu-vnc-selenium-firefox   ""/opt/bin/run_sele_s   4 minutes ago       Up 4 minutes        5901/tcp, 133.130.60.36:32768-&gt;9998/tcp   Peach_1

➜  ~  docker exec -ti Peach_1 zsh

➜  /  curl ipecho.net/plain ; echo
133.130.101.114
</code></pre>
","<p>There are tools like <a href=""https://www.ioncube.com/"" rel=""nofollow noreferrer"">ioncube</a> or <a href=""https://www.sourceguardian.com"" rel=""nofollow noreferrer"">php encoder</a>, which make it possible to restrict PHP source code usage to specific IP addresses or MAC addresses. In theory it works perfectly fine, when the source code is deployed on a server (or servers) with specific static address. But I do not understand, how to secure source code, when it is deployed, using tools like Docker.</p>

<p>The problem is that Docker dynamically assigns IP and MAC addresses to running containers, so it seems like such tools as ioncube or php encoder can not work in this environment. Am I right or is it still possible to bind dockerized source code to specific addresses?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","61395555","I have a web app running inside a docker container, how do I access it on localhost?","<python><docker><flask><docker-compose><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a python flask web app running inside a docker container, how do I access it on my host?</p>

<p>When I run the up, it is ready and listening on port 5000 inside the container, but I want this exposed so I can access it
xxxsssddd  | 2020-04-23 19:28:48,416  * Running on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</p>

<p>below is my docker-compose.yml</p>

<pre><code>version: '2.2'

services:
    mywebapp:
        build:
            context: .
            dockerfile: mywebabb.yml
        ports:
            - 5000:5000
        networks:
            default:

        volumes:
            - .:/opt/mywebapp
</code></pre>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","61401777","Two docker images into a single docker image","<mongodb><docker><rabbitmq>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I have a requirement to create a single docker image by merging both ""RabbitMQ-management"" and ""MongoDB"" images. Is it possible to do it? </p>

<p>I've tried with below Dockerfile and can able to access RabbitMQ, but mongodb is not working. Could you please help me. </p>

<pre><code># cat Dockerfile 
FROM rabbitmq:3.7.17

RUN rabbitmq-plugins enable --offline rabbitmq_management

# extract ""rabbitmqadmin"" from inside the ""rabbitmq_management-X.Y.Z.ez"" plugin zipfile
# see https://github.com/docker-library/rabbitmq/issues/207
RUN apt-get update &amp;&amp; apt-get install -y gnupg2
RUN gpg --keyserver ha.pool.sks-keyservers.net --recv-keys 0C49F3730359A14518585931BC711F9BA15703C6 &amp;&amp; \
gpg --export $GPG_KEYS &gt; /etc/apt/trusted.gpg.d/mongodb.gpg
ARG MONGO_PACKAGE=mongodb-org
ARG MONGO_REPO=repo.mongodb.org
ENV MONGO_PACKAGE=${MONGO_PACKAGE} MONGO_REPO=${MONGO_REPO}
ENV MONGO_MAJOR 3.4
ENV MONGO_VERSION 3.4.18
RUN echo ""deb http://$MONGO_REPO/apt/debian jessie/${MONGO_PACKAGE%-unstable}/$MONGO_MAJOR main"" | tee ""/etc/apt/sources.list.d/${MONGO_PACKAGE%-unstable}.list""
RUN echo ""/etc/apt/sources.list.d/${MONGO_PACKAGE%-unstable}.list""
RUN apt-get update
RUN apt-get install -y ${MONGO_PACKAGE}=$MONGO_VERSION
RUN set -eux; \
        erl -noinput -eval ' \
                { ok, AdminBin } = zip:foldl(fun(FileInArchive, GetInfo, GetBin, Acc) -&gt; \
                        case Acc of \
                                """" -&gt; \
                                        case lists:suffix(""/rabbitmqadmin"", FileInArchive) of \
                                                true -&gt; GetBin(); \
                                                false -&gt; Acc \
                                        end; \
                                _ -&gt; Acc \
                        end \
                end, """", init:get_plain_arguments()), \
                io:format(""~s"", [ AdminBin ]), \
                init:stop(). \
        ' -- /plugins/rabbitmq_management-*.ez &gt; /usr/local/bin/rabbitmqadmin; \
        [ -s /usr/local/bin/rabbitmqadmin ]; \
        chmod +x /usr/local/bin/rabbitmqadmin; \
        apt-get update; apt-get install -y --no-install-recommends python ca-certificates; rm -rf /var/lib/apt/lists/*; \
        rabbitmqadmin --version

EXPOSE 15671 15672 27017
</code></pre>

<p>I'm using below command to run RabbitMQ (it is working)</p>

<pre><code># docker run -it -p 15672:15672 -p 5672:5672  --hostname my-rabbitmq image
</code></pre>

<p>Command to start Mongodb container (it is not working)</p>

<pre><code>docker run -d -v /tmp/mongodb:/data/db -p 27017:27017 image mongod
</code></pre>
"
"28302178","How can I add a volume to an existing Docker container?","<docker>","61620738","docker: make folder of running container persistent","<docker><docker-compose>","<p>I have a Docker container that I've created simply by installing Docker on Ubuntu and doing:</p>
<pre><code>sudo docker run -i -t ubuntu /bin/bash
</code></pre>
<p>I immediately started installing Java and some other tools, spent some time with it, and stopped the container by</p>
<pre><code>exit
</code></pre>
<p>Then I wanted to add a volume and realised that this is not as straightforward as I thought it would be. If I use <code>sudo docker -v /somedir run ...</code> then I end up with a fresh new container, so I'd have to install Java and do what I've already done before just to arrive at a container with a mounted volume.</p>
<p>All the documentation about mounting a folder from the host seems to imply that mounting a volume is something that can be done when creating a container. So the only option I have to avoid reconfiguring a new container from scratch is to commit the existing container to a repository and use that as the basis of a new one whilst mounting the volume.</p>
<p>Is this indeed the only way to add a volume to an existing container?</p>
","<p>In <code>docker-compose.yml</code> we are able to define persistent volumes:</p>

<pre><code>volumes:
  -foo:bar
</code></pre>

<p>For this to work, the container has to be rebuild.</p>

<p>Now I need to ""save"" the data on a non-persistent folder in a running container. Is there a way to do this. </p>

<p>Can I add a persistent volume to an existing and mounted container?</p>
"
"27563460","Let two Containers getting linked to eachother","<docker>","61650502","How to establish comminication between microservices in different docker containers?","<python><linux><docker><flask><microservices>","<p>I have a couple of docker Containers and one special case that two of them have to talk to each other, so they must know each other at best via link.
I can link one Container to the other but the problem is, that i can't tell them that the second one can talk back to the first.</p>

<p>I tried to create and run the first container and stopped it, then i created started the second container and stopped it as well. Next up i started the first container again with the link to the second and started the second one linked to the first.
After this my machine went crazy the docker process took all CPU and Memory and neither of the containers was accessible. When you killed the process a new one poped with the same.
Even when i deinstalled docker restarted the machine and installed docker again it get back to to the crazy state without even starting one of the containers.</p>

<p>Got anyone a solution how to link two containers to each other or let them talk to each other in both directions? </p>
","<p>I have two microservices made using Flask, running on localhost:3000 and localhost:5000, they work fine and communicate well on local machine but after containerizing them I'm getting below error.</p>

<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=3000): Max retries exceeded with url: /update (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2a08eaa850&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
</code></pre>

<p>The steps I followed for containerization are as below.    </p>

<pre><code>$ sudo docker build -t history:0.1  
$ sudo docker build -t calculation:0.1  
$ sudo docker run -d -p 0.0.0.0:3000:3000 --name history history:0.1  
$ sudo docker run -d -p 0.0.0.0:5000:5000 --name calculation calculation:0.1  
$ curl -X GET 0.0.0.0:3000/hist  
{  
""hist"": []  
}  
$ curl -X GET 0.0.0.0:5000/sum/5/6  

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2309, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2295, in wsgi_app
    response = self.handle_exception(e)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1741, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.8/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.8/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/app/calculate.py"", line 15, in add
    update(a,b,ans,'+')
  File ""/app/calculate.py"", line 10, in update
    requests.get('http://localhost:3000/update',json={""a"":a,""b"":b,""ans"":ans,""operator"":operator})
  File ""/usr/local/lib/python3.8/site-packages/requests/api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/sessions.py"", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/sessions.py"", line 637, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=3000): Max retries exceeded with url: /update (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2a08904040&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))

--&gt;

</code></pre>

<p>Here calculation service is calling the history service to update the data in history.</p>

<p>I am new to docker. How can I solve it?</p>
"
"32750748","How to edit files in stopped/not starting docker container","<bash><docker>","61724371","Change file contents within a stopped Apache Docker container","<apache><docker><httpserver>","<p>Trying to fix errors and debug problems with my application that is split over several containers, I frequently edit files in containers:</p>

<ul>
<li><p>either I am totally lazy and install nano and edit directly in container or</p></li>
<li><p>I docker cp the file out of the container, edit it, copy it back and restart the container</p></li>
</ul>

<p>Those are intermediate steps before coming to new content for container build, which takes a lot longer than doing the above (which of course is only intermediate/fiddling around).</p>

<p>Now I frequently break the starting program of the container, which in the breaking cases is either a node script or a python webserver script, both typically fail from syntax errors.</p>

<p>Is there any way to save those containers? Since they do not start, I cannot docker exec into them, and thus they are lost to me. I then go the rm/rmi/build/run route after fixing the offending file in the build input.</p>

<p><strong>How can I either edit files in a stopped container, or cp them in or start a shell in a stopped container - anything that allows me to fix this container?</strong></p>

<p>(It seems a bit like working on a remote computer and breaking the networking configuration - connection is lost ""forever"" this way and one has to use a fallback, if that exists.)</p>

<p><a href=""https://stackoverflow.com/questions/24553790/how-to-edit-docker-container-files-from-the-host"">How to edit Docker container files from the host?</a> looks relevant but is outdated.</p>
","<p>I was running an Apache HTTP server from a Docker container. I made a few changes to the httpd.conf file (using docker exec -it and then editing the file) and saved it. When I restarted the Docker container from host OS, the container is failed owing to syntax errors on the httpd.conf file.</p>

<p>Now, I cannot enter the container through docker exec -it (since it is in stopped state) and correct my errors! How do I edit the file again to correct the errors? Or is there a way to revert the changes I made?</p>
"
"31153394","Check Resources Used by each Docker Container","<ubuntu><docker><ubuntu-14.04>","61729268","Docker-compose: show containers usage of resources","<docker><docker-compose>","<p>How do you check the amount of resources (CPU, memory etc) being used by each Docker container that is running on the (Ubuntu) server?</p>
","<p>How to see resource usage by docker containers?</p>

<p>I am using docker-compose version 2.1 and 2.4.
I would like to see running containers and their resources usage: RAM, CPU.
<code>docker-compose ps</code> returns containers, images, but not resources.</p>

<pre><code>$ docker-compose ps
             Name                           Command                  State                                  Ports
--------------------------------------------------------------------------------------------------------------------------------------------
service-1                        /app.sh                         Up (healthy)     8001/tcp
service-2                        /app.sh                         Up (healthy)     0.0.0.0:80-&gt;4000/tcp
kafka                            /etc/confluent/docker/run       Up               0.0.0.0:29092-&gt;29092/tcp, 0.0.0.0:7071-&gt;7071/tcp, 9092/tcp
mongo                            /init.sh                        Up (unhealthy)   0.0.0.0:27017-&gt;27017/tcp
zookeeper                        /etc/confluent/docker/run       Up               2181/tcp, 2888/tcp, 0.0.0.0:32181-&gt;32181/tcp, 3888/tcp

</code></pre>
"
"29261811","Use docker-compose env variable in Dockerbuild file","<docker><docker-compose>","61764279","Accessing docker compose arm variable in Docker file","<node.js><docker><docker-compose><swarm>","<p>Having the following docker-compose file:</p>

<pre><code>db:
    build: .
    environment:
        - MYSQL_ROOT_PASSWORD=password
        - ENV=test
    env_file: .env
</code></pre>

<p>Is there any way to use the env variables declared in docker-compose.yml (either as environment or declared in the env_file) as part of Dockerfile without declaring them in the Dockerfile? Something like this:</p>

<pre><code>FROM java:7
ADD ${ENV}/data.xml /data/
CMD [""run.sh""]
</code></pre>
","<p>My Docker compose file is as follows</p>

<pre class=""lang-yaml prettyprint-override""><code>
  environment:
    external:
      name: dev
services:
   app:
      deploy:
         replicas: 1
      environment:
         NODE_ENV: development

</code></pre>

<p>Compose Swarm is as follows:</p>

<pre class=""lang-yaml prettyprint-override""><code>
version: ""3.3""

services:
  app:
    image: ""${IMAGE_NAME}""
    networks:
      - environment
    environment:
      NODE_ENV: ${ENVIRONMENT}
      PORT: 8080
    deploy:
      resources:
        limits:
          cpus: ""0.20""
          memory: 1500M
        reservations:
          cpus: ""0.20""
          memory: 100M
</code></pre>

<p>My docker file is as follows:</p>

<pre><code>FROM node:10.16.3-alpine
USER node
RUN npm install
RUN pwd
RUN ls  -lah

RUN npm run buildconfigs ${NODE_ENV}

EXPOSE 8080

CMD [""node"", ""app.js""]
`
</code></pre>

<p>I would like my RUN npm run buildconfigs ${NODE_ENV} in the docker file to be replaced to </p>

<p>RUN npm run buildconfigs development. </p>

<p>I am not sure why ${NODE_ENV} is not getting picked up from compose file. Would appreciate a direction on  how to capture compose file variable in the docker file. Thank you</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","62080978","Dockerfile copy to one folder up","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have below structure</p>

<pre><code>folder1
    Dockerfile
    start.sh
folder2
    config.ini
someotherfiles
</code></pre>

<p>inside the Dockerfile</p>

<p>i have added</p>

<pre><code>COPY ../folder2/config.ini /
COPY ../somotherfiles /
</code></pre>

<p>Now when I build the docker, i'm getting error <code>file not found.</code> </p>

<p>how can I copy the files which is one folder up where the Dockerfile exists? </p>
"
"29905953","How to correctly link php-fpm and Nginx Docker containers?","<php><nginx><docker><dockerfile><docker-compose>","62262377","Nginx and php-fpm most optimal setup configuration with docker","<php><docker><nginx><fpm>","<p>I am trying to link 2 separate containers:</p>

<ul>
<li><a href=""https://registry.hub.docker.com/_/nginx/"">nginx:latest</a></li>
<li><a href=""https://registry.hub.docker.com/_/php/"">php:fpm</a></li>
</ul>

<p>The problem is that php scripts do not work. Perhaps the php-fpm configuration is incorrect.
Here is the source code, which is in my <a href=""https://github.com/bocharsky-bw/docker"">repository</a>. Here is the file <code>docker-compose.yml</code>:</p>

<pre><code>nginx:
    build: .
    ports:
        - ""80:80""
        - ""443:443""
    volumes:
        - ./:/var/www/test/
    links:
        - fpm
fpm:
    image: php:fpm
    ports:
        - ""9000:9000""
</code></pre>

<p>and <code>Dockerfile</code> which I used to build a custom image based on the nginx one:</p>

<pre><code>FROM nginx

# Change Nginx config here...
RUN rm /etc/nginx/conf.d/default.conf
ADD ./default.conf /etc/nginx/conf.d/
</code></pre>

<p>Lastly, here is my custom Nginx virtual host config:</p>

<pre><code>server {
    listen  80;

    server_name localhost;
    root /var/www/test;

    error_log /var/log/nginx/localhost.error.log;
    access_log /var/log/nginx/localhost.access.log;

    location / {
        # try to serve file directly, fallback to app.php
        try_files $uri /index.php$is_args$args;
    }

    location ~ ^/.+\.php(/|$) {
        fastcgi_pass 192.168.59.103:9000;
        fastcgi_split_path_info ^(.+\.php)(/.*)$;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param HTTPS off;
    }
}
</code></pre>

<p>Could anybody help me configure these containers correctly to execute php scripts?</p>

<p><strong>P.S.</strong>
I run containers via docker-composer like this:</p>

<p><code>docker-compose up</code></p>

<p>from the project root directory.</p>
","<p>I have a question regarding the most optimal way to set up NGINX and PHP-FPM for my application with docker. So in my set up, my application code and PHP-FPM live in one docker container (the web container), and Ngnix on a separate docker container. </p>

<p>The trick here is that Nginx does not have access to the application files, because they are in the web container, which resulted in some configuration issues with Nginx.
One of those issues is that I can't use the 'root' directive in the Nginx config file, I had to use 'alias' instead. But I managed to get Nginx to point to the proper 'index.php' file in the end (see the config file at the end).</p>

<p>But I was wondering what is the most optimal way to define the relationship between this services?</p>

<ul>
<li>Should I just give access to the application code to Nginx?</li>
<li>Should I set up one more Nginx service in the web container, so that it will accept the requests that come from the Nginx container?</li>
<li>Or maybe there is some other option that would be even better for my case?</li>
</ul>

<p>This is my current nginx confg file (I've remove some irrelevant parts):</p>

<pre><code>upstream phpserver {
  server web:9000;
}
server {
    listen 443 ssl http2;
    server_name app;
    gzip off;   

    location ~ ^/index\.php(/|$) {
        alias /web/index.php;  

        fastcgi_pass phpserver;
        fastcgi_split_path_info ^(.+\.php)(/.*)$;
        #include fastcgi_params;

        fastcgi_param   QUERY_STRING        $query_string;
        fastcgi_param   REQUEST_METHOD      $request_method;
        fastcgi_param   CONTENT_TYPE        $content_type;
        fastcgi_param   CONTENT_LENGTH      $content_length;

        fastcgi_param   SCRIPT_FILENAME     $request_filename;
        fastcgi_param   SCRIPT_NAME     $fastcgi_script_name;
        fastcgi_param   REQUEST_URI     $request_uri;
        fastcgi_param   DOCUMENT_URI        $document_uri;
        fastcgi_param   DOCUMENT_ROOT       $document_root;
        fastcgi_param   SERVER_PROTOCOL     $server_protocol;

        fastcgi_param   GATEWAY_INTERFACE   CGI/1.1;
        fastcgi_param   SERVER_SOFTWARE     nginx/$nginx_version;

        fastcgi_param   REMOTE_ADDR     $remote_addr;
        fastcgi_param   REMOTE_PORT     $remote_port;
        fastcgi_param   SERVER_ADDR     $server_addr;
        fastcgi_param   SERVER_PORT     $server_port;
        fastcgi_param   SERVER_NAME     $server_name;

        internal;
        http2_push_preload on;
    }
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","62599588","unable to connect to flask docker image","<python><docker><flask><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I'm a beginner in docker and I have a folder called <code>MovieFlix</code> that contains a <code>flask web service</code> app with some <code>html templates</code> that I <code>connect to a mongodb image from docker</code> . I want to run my flask app as a docker image . For that I have succesfully created a <code>DockerFile</code> which I list below :</p>
<pre><code>FROM ubuntu:16.04
MAINTAINER bill &lt;bill@gmailcom&gt;
RUN apt-get update
RUN apt-get install -y python3 python3-pip 
RUN pip3 install flask pymongo flask_bcrypt 
EXPOSE 80
WORKDIR &quot;/MovieFlix&quot;
COPY webservice.py . //copy the flask app i want to run 
COPY templates . //copy my html templates
CMD [&quot;python3&quot;, &quot;webservice.py&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;80&quot;] 
</code></pre>
<p>My image is built succesfully however when I run <code>docker -p 5000:80 flask(my image name )</code>
and copy the url <code>http://127.0.0.1:5000/</code> in my search engine I get that <code>the page does not work and 127.0.0.1 did not send data</code>  I can run my <code>webservice.py</code> normally on vscode and the mongodb image is running so I do not think the problem is there and I run my flask app with</p>
<pre><code>if __name__ == '__main__':
    app.run(debug=True, host='127.0.0.1', port=5000)
</code></pre>
<p>I would appreciate your help with guiding me to solve this issue .  Thank you in advance .</p>
"
"32353055","How to start a stopped Docker container with a different command?","<docker>","62637213","How to start a docker container without executing the specified command","<docker>","<p>I would like to start a stopped Docker container with a different command, as the default command crashes - meaning I can't start the container and then use 'docker exec'. </p>

<p>Basically I would like to start a shell so I can inspect the contents of the container.</p>

<p>Luckily I created the container with the -it option!</p>
","<p>I made a Docker container to act as a Jupyter server</p>
<pre><code>docker run -i -t -p 8888:8888 continuumio/anaconda3 /bin/bash -c &quot;/opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks &amp;&amp; /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser&quot;
</code></pre>
<p>Is it possible to start the stopped container without executing the specified command?
At startup it stops when trying to create an existing directory.</p>
<pre><code>kosh@LinuxPC:~$ docker ps -a --no-trunc
CONTAINER ID                                                       IMAGE               COMMAND                                                                                                                                                                                                    CREATED             STATUS                     PORTS               NAMES
bb9ff79baf4b2a18289e14338cecdd3cdfa3bbe2a84cba0a63430de1e624b769   condaim             &quot;/bin/bash -c '/opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks &amp;&amp; /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser --allow-root'&quot;   24 hours ago        Exited (1) 2 minutes ago                       jovial_clarke

kosh@LinuxPC:~$ docker start -i jovial_clarke
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... done

# All requested packages already installed.

mkdir: cannot create directory ‘/opt/notebooks’: File exists
</code></pre>
"
"33443912","Commit to jenkins docker image does not save changes","<jenkins><docker>","62772216","using Jenkins in DockerToolbox in Windows, what's changed in image after docker commit a container?why installed plugins not commit to the image?","<windows><docker><jenkins><svn><offline>","<p>I pull the official Jenkins docker image from <a href=""https://hub.docker.com/_/jenkins/"">here</a>.
From Jenkins UI I create a new job , install the github plugin and set the repo urls in the job configuration.</p>

<p>Finally I save the changes from Jenkins.</p>

<p>I want to create a new image as it is. I stop the container, and commit it to a new image.</p>

<p>Then I start a new container from the new image...and Jenkins does not contain any of my changes.</p>

<p>I use <code>Docker version 1.6.2, build 7c8fca2</code></p>
","<p>using Jenkins in DockerToolbox in Windows, what's changed in image after docker commit a container?why installed plugins not commit to the image?
and what's the best way to include all the plugins(including pre-installed) in a new image which will be used in the other offline windows pc.
thank you very much!</p>
"
"29663459","Python app does not print anything when running detached in docker","<python><docker><dockerfile>","62804511","Nohup doesn't print to stdout until termination","<python><python-3.x><cx-oracle><nohup>","<p>I have a Python (2.7) app which is started in my dockerfile:</p>

<pre><code>CMD [""python"",""main.py""]
</code></pre>

<p><em>main.py</em> prints some strings when it is started and goes into a loop afterwards:</p>

<pre><code>print ""App started""
while True:
    time.sleep(1)
</code></pre>

<p>As long as I start the container with the -it flag, everything works as expected:</p>

<pre><code>$ docker run --name=myapp -it myappimage
&gt; App started
</code></pre>

<p>And I can see the same output via logs later:</p>

<pre><code>$ docker logs myapp
&gt; App started
</code></pre>

<p>If I try to run the same container with the -d flag, the container seems to start normally, but I can't see any output:</p>

<pre><code>$ docker run --name=myapp -d myappimage
&gt; b82db1120fee5f92c80000f30f6bdc84e068bafa32738ab7adb47e641b19b4d1
$ docker logs myapp
$ (empty)
</code></pre>

<p>But the container still seems to run;</p>

<pre><code>$ docker ps
Container Status ...
myapp     up 4 minutes ... 
</code></pre>

<p>Attach does not display anything either:</p>

<pre><code>$ docker attach --sig-proxy=false myapp
(working, no output)
</code></pre>

<p>Any ideas whats going wrong? Does ""print"" behave differently when ran in background?</p>

<p>Docker version:</p>

<pre><code>Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.2
Git commit (client): a8a31ef
OS/Arch (client): linux/arm
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.2
Git commit (server): a8a31ef
</code></pre>
","<p>I have a Python script like the following:</p>
<pre><code>#!/usr/bin/env python

print(&quot;Start&quot;)
sql = get_sql_from_file(...)
# The following function takes a long time to run
execute_oracle_driver(sql)
print(&quot;Finished&quot;)
</code></pre>
<p>I execute it with:</p>
<pre><code>(my-env) $ time nohup python script.py &amp;
</code></pre>
<p>Then I check if there's any initial output, but nothing:</p>
<pre><code>(my-env) $ cat nohup.out
(my-env) $ 
</code></pre>
<p>I would at least expect to see &quot;Start&quot; after no more than a few seconds. Even after waiting a few minutes, then killing it, I get no output. If possible, how can I fix this?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","62894597","set up flask with docker","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I got an issue I'm able to launch flask container but when I go in my browser I got &quot;
This site is inaccessible&quot;, my api is not loaded</p>
<p>docker-compose :</p>
<pre><code>version: &quot;3&quot;

services:
  api:
    container_name: api
    build: ./api
    environment:
      - FLASK_APP=main.py
    command: flask run
    ports:
      - &quot;5000:5000&quot;
</code></pre>
<p>Dockerfile :</p>
<pre><code>FROM python:3

# Install and setup flask
WORKDIR /app
COPY requirements.txt requirements.txt
RUN pip3 install -r requirements.txt
COPY . .

EXPOSE 5000
</code></pre>
<p><strong>UPDATE</strong></p>
<p>basic app</p>
<pre><code>from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'
</code></pre>
<p>Result of <code>docker-compose up --build</code>
<a href=""https://i.stack.imgur.com/mVDwD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVDwD.png"" alt=""enter image description here"" /></a></p>
<p>When I go to localhost:5000 I got &quot;This site is inaccessible&quot;</p>
<p>I don't have access to my app in my browser but why ? I exposed port 5000 in dockerfile and map port 5000:5000</p>
<p>Thank's in advance</p>
"
"33054369","How to change the default docker registry from docker.io to my private registry?","<docker><docker-registry>","62897750","is it possible to write a dockerfile without registry in FROM statements?","<docker><dockerfile><docker-registry>","<p>By default, if I issue command: </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from the docker.io offical site by default.</p>

<pre><code>Pulling repository docker.io/library/ruby
</code></pre>

<p>How do I change it to my private registry. That means if I issue </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from my own private registry, the output is something like:</p>

<pre><code>Pulling repository my_private.registry:port/library/ruby
</code></pre>
","<p>For example, I have a base dockerfile:</p>
<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:3.1
...
</code></pre>
<p>which I build <code>docker build -t my-base</code>, then build a second dockerfile:</p>
<pre><code>FROM my-base
...
</code></pre>
<p>which works because <code>my-base</code> is available locally.  However, if I push these to a registry, it seems I am forced to replace the second docker file with:</p>
<pre><code>FROM my-dockerhub-accountname/my-base
...
</code></pre>
<p>or if not using dockerhub:</p>
<pre><code>FROM 1234.myregistryprovider/something/my-base
...
</code></pre>
<p>And likewise doing a <code>docker push -t my-dockerhub-accountname/my-base</code> or <code>1234.myregistryprovider/something/my-base</code> for the first image.</p>
<p>But docker is only ever connected to one registry (this still applies if it was not), why do these registry provder uris have to poison the docker files I write?  This makes doing CI considerably harder, binding whatever registry provider we choose to declarations in source code.</p>
<p>By comparison, when I reference a nuget/npm package, I do so by package name while registry is handled by the package source manager.  It does not and should not come into the question :/</p>
<p>Is there a way to avoid all this?</p>
"
"33913020","Docker remove <none> TAG images","<docker>","63054339","Docker remove image and container before creating","<docker>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>I have the following script, which creates the docker image and container as expected.</p>
<pre><code>echo &quot;password&quot; | sudo -S docker stop /nexct-approval-service-container | xargs sudo docker rm
echo &quot;docker build ...&quot;
echo &quot;password&quot; | sudo -S docker build . -t nexct-approval-service-image:latest
echo &quot;docker run ...&quot;
echo &quot;password&quot; | sudo -S docker run -t --network=host -p 8081:8081 --name nexct-approval-service-container nexct-approval-service-image
</code></pre>
<p>As you can see I am creating a container and image with the following names:</p>
<pre><code>nexct-approval-service-container
nexct-approval-service-image
</code></pre>
<p>However, what I cannot get to work is the following line:</p>
<pre><code>echo &quot;password&quot; | sudo -S docker stop /nexct-approval-service-container | xargs sudo docker rm
</code></pre>
<p>My intention is to remove the container and image if it exists before I create a new one. What I am finding however, when I do <code>sudo docker images</code> is that my list of images continues to grow.</p>
<p><strong>Question</strong></p>
<p>How can I build my images with the specific name, but stop a new one from being built each time?</p>
<p>More info:</p>
<p>As you can see, a number of images called <code>&lt;none&gt;</code> are left below after each build.</p>
<pre><code>$ docker images
REPOSITORY                     TAG                 IMAGE ID            CREATED              SIZE
nexct-approval-service-image   latest              7359e1ff2138        21 seconds ago       557MB
&lt;none&gt;                         &lt;none&gt;              c4f98256b401        About a minute ago   557MB
&lt;none&gt;                         &lt;none&gt;              908eebf0227a        2 minutes ago        557MB
&lt;none&gt;                         &lt;none&gt;              8b6b0cb01773        3 minutes ago        557MB
openjdk                        14                  cdc43cc23d2d        6 days ago           511MB
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63201965","Container works but localhost says unable to connect","<docker>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Hi I have made my flask app and I have exposed port 5001 in Docker file.
I pushed it to dockerhub repo and ran on different machine by</p>
<pre><code>docker container run --name XYZ &lt;username&gt;/&lt;repo_name&gt;:&lt;tag&gt;
</code></pre>
<p>The log says that app is running on <a href=""http://127.0.0.1:5001/"" rel=""nofollow noreferrer"">http://127.0.0.1:5001/</a></p>
<p>But if I open that localtion in browser its says</p>
<pre><code>Unable to connect
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM ubuntu:18.04

RUN apt-get update &amp;&amp; apt-get -y upgrade \
    &amp;&amp; apt-get -y install python3.8 \
    &amp;&amp; apt -y install python3-pip \
    &amp;&amp; pip3 install --upgrade pip

WORKDIR /app

COPY . /app

RUN pip3 --no-cache-dir install -r requirements.txt 

EXPOSE 5001

ENTRYPOINT  [&quot;python3&quot;]

CMD [&quot;app.py&quot;]
</code></pre>
"
"31961934","Why docker login command saves unencrypted password on my computer?","<docker><docker-machine>","63357792","Unecrypted Docker password","<docker><dockerfile>","<p>I've used <code>docker login</code> command and passed my credentials and I got warning <code>WARNING: login credentials saved in /Users/{my_username}/.docker/config.json</code>. I've checked that file and indeed my password is there unencrypted (base64 encoded only).</p>

<p>Why docker saved my password unencrypted? Shouldn't it save it in my key-chain (I am on Mac OS) or instead of saving password just generate some access token or something like that?</p>
","<p>I recently built a docker image and upon completion docker throws a message that states:</p>
<p>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</p>
<p>I tried to navigate to the folder to see whether this was true but the file path doesn't exist.What could be the issue here? Is my password safe?</p>
"
"33322103","Multiple FROMs - what it means","<docker><dockerfile>","63367929","Dockerfile: Are multiple FROM (s) allowed?","<docker><dockerfile><cypress>","<p>I want to build a docker image for the <a href=""https://github.com/Linkurious/linkurious.js"" rel=""noreferrer"">Linkurious</a> project on github, which requires both the Neo4j database, and Node.js to run.</p>

<p>my first approach was to declare a base image for my image, containing Neo4j.  The reference docs do not define ""base image"" in any helpful manner:</p>

<blockquote>
  <p>Base image:
  An image that has no parent is a base image</p>
</blockquote>

<p>from which I read that I may only have a base image if that image has no base image itself.</p>

<p>but what is a base image? does it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?</p>

<p>reading the Docker reference (see: <a href=""https://docs.docker.com/reference/builder/#from"" rel=""noreferrer"">https://docs.docker.com/reference/builder/#from</a>) I see: </p>

<blockquote>
  <p>FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command.</p>
</blockquote>

<p>do I want to create multiple images? it would seem what I want is to have a single image that contains the contents of other images e.g. neo4j and node.js</p>

<p>I've found no directive to declare dependencies in the reference manual.  are there no dependencies like in RPM where in order to run my image the calling context must first install the images it needs?</p>

<p>I'm confused...</p>
","<p>I am creating a Docker image that needs to contain both:</p>
<ol>
<li>Ex: Company Foo's image of Jenkins support functions: <code>docker.foo.com/jenkins-build:latest</code></li>
<li><a href=""https://github.com/cypress-io/cypress-docker-images/tree/master/included/4.12.1"" rel=""nofollow noreferrer"">cypress/included:4.12.1</a></li>
</ol>
<p>This image will be labeled <code>docker.foo.com/cypress-build</code></p>
<p><code>Dockerfile</code> for this image is scripted as follows:</p>
<pre><code># Image containing company Foo's Jenkins build
FROM docker.foo.com/jenkins-build:latest

# Main image that include all operating system dependencies necessary to run Cypress,
# but NOT the test runner itself.
FROM cypress/included:4.12.1
</code></pre>
<p>Is this <code>Dockerfile</code> valid in scripting with multiple <code>FROM</code>?</p>
<p>If not, then what must be done to correct it?</p>
<p>Thank you, much appreciate any feedback!</p>
"
"30198189","Can I run Docker directly on a non VT-X machine (no Virtual Machine used)?","<docker>","63408215","Docker without VTX or AMD-V","<docker><virtualization><amd>","<p>I would like to use Docker on servers with 80546k Xeon Irwindale CPUs. These CPUs are 64bit but don't support Intel VT-X virtualization. VirtualBox won't run on this machine. 
I'm planning to install Linux directly on the hardware (No VM layer) and use Docker to virtualize applications. Is this possible? I found a lot of discussions about Docker and VT-x, but all of them concern a Virtual Machine such as VirtualBox besides Docker. </p>

<p>Thanks,
Alan</p>
","<p>I have laptop ACER aspire E15 553g F1j2, having processor AMD-FX9800p.
My computer's bios doesn't have any option to enable virtualization.</p>
<p>I'm not able to install Android Emulator.</p>
<p>Can I run Docker for MERN stack development without enabling the virtualization ?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63471692","Imports not working with Flask in Docker (no gunicorn!)","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a flask app, with Flask RestX, that works fine with <code>flask run</code> but has import issues with Docker. I've seen some questions and answers about this but they all involve Gunicorn, but <strong>I'm not using Gunicorn</strong>.</p>
<p>Here is my folder structure. I'm running the <code>docker-compose</code> file which uses the <code>flask.dev.dockerfile</code> as well as a Redis container.</p>
<p><a href=""https://i.stack.imgur.com/91cKl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/91cKl.png"" alt=""enter image description here"" /></a></p>
<p>So, I want to use my redis wrapper, <code>RedisDatabase</code>, from my <code>database.py</code> file. PyCharm inserts this as</p>
<pre><code>from ivd_app.database import RedisDatabase
</code></pre>
<p>And the rest of the <code>__init__.py</code> file, in summary:</p>
<pre class=""lang-py prettyprint-override""><code>from flask import Flask, request
from flask_cors import CORS
from flask_restx import Api, Resource, fields

def create_app(db=None):
    app = Flask(__name__, instance_relative_config=True)

    if (db is None):
        db = RedisDatabase()

    api = Api(app, version='0.0.1', title='IVD')

    config_endpoint = api.namespace(
        'config', description='APIs to Send configuration to and from the Front End'
    )

    config_model = api.model('Configuration', {
        # the model
    })

    @config_endpoint.route('/')
    class ConfigurationEndpoint(Resource):

        @config_endpoint.expect(config_model, validate=True)
        def put(self):
            # the put endpoint

        def get(self):
            # the get endpoint

    return app

# if __name__ == &quot;__main__&quot;:
#    create_app().run(host=&quot;0.0.0.0&quot;, debug=True)
</code></pre>
<p>When I use <code>CMD flask run</code> from the dockerfile after setting the environment variables, the application runs. But for some reason I can't access the flask server, so instead I uncomment the lines at the bottom, since I know those work, and use <code>CMD python ivd_app/__init__.py</code> in the Dockerfile. But the container exits:</p>
<pre><code>flask_1  | Traceback (most recent call last):
flask_1  |   File &quot;ivd_app/__init__.py&quot;, line 38, in &lt;module&gt;
flask_1  |     from ivd_app.database import RedisDatabase
flask_1  | ModuleNotFoundError: No module named 'ivd_app'
</code></pre>
<p>If I remove the import statement and put the <code>RedisDatabase</code> class in the <code>__init__.py</code> file, the entire app works, of course. But I want to separate it into a different file.</p>
<p>How do I fix this?</p>
"
"28662021","In fedora container systemctl gives Failed to get D-Bus connection","<docker><fedora>","63507412","Failed to get D-Bus connection: Operation not permitted [docker]","<docker><nginx><systemctl>","<p>When I in a fedora container systemctl use, I get:</p>

<p>Failed to get D-Bus connection:: Unknown error -1</p>

<p>Does someone know how to fix this? Or can systemctl not be used in a docker container? </p>
","<p>I'm trying to create a docker image (centos7) with an nginx web server running on it. I have created a <code>Dockerfile</code> which looks like this:</p>
<pre><code>FROM centos:centos7

RUN yum update -y

# -------- OPENSSL -------- 

#ADD install-openssl.sh /
#RUN chmod +x install-openssl.sh
#RUN /install-openssl.sh

# -------- NGINX --------

RUN yum install epel-release -y
RUN yum install nginx -y

# Copy a configuration file from the current directory 
ADD nginx.conf /etc/nginx/

# Append &quot;daemon off:&quot; to the beginning of the configuration
RUN echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf

# Expose ports
EXPOSE 80

RUN systemctl start nginx
</code></pre>
<p>I build my docker image with the following command:</p>
<pre><code>docker build -t nginx-img .
</code></pre>
<p>And I run it like this:</p>
<pre><code>docker -v run --name nginx-cont -p 80:80 -i  nginx-img
</code></pre>
<p>But I get the following error:</p>
<pre><code>Failed to get D-Bus connection: Operation not permitted
</code></pre>
"
"35218194","what is 'z' flag in docker container's volumes-from option?","<docker>","63634293","docker volume on file system","<docker><docker-compose>","<p>While going through the docker docs, I came across volumes-from (<a href=""https://docs.docker.com/engine/reference/commandline/run/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/commandline/run/</a>)   option for docker run command. 
I didn't understand the differences between <code>ro, rw, and z</code> option provided as-<br>
 <code>$ docker run --volumes-from ba8c0c54f0f2:ro -i -t ubuntu pwd</code><br>
In the above command the <code>ro</code> option is replaced with <code>z</code>. I will be thankful if anyone explores on differences of using these options.</p>
","<p>I have a docker-compose file like that :</p>
<pre><code>version: &quot;2&quot;
services:
  orthanc:
    build: orthanc
    restart: unless-stopped
    ports: [&quot;${PORT}:8042&quot;]
    volumes: [&quot;orthanc-storage:/var/lib/orthanc/db:Z&quot;]
    [...]
volumes:
  orthanc-storage:
</code></pre>
<p>1/ I can't understand how to define <code>orthanc-storage</code> with something like <code>C:/tmp/db</code></p>
<p>2/ what means the <code>Z</code> ?</p>
<p>Thank you for your answers</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63652631","Flask app not exposed properly on localhost","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a flask app, that i want to dockerize, so i have used the following Dockerfile</p>
<pre><code>FROM python:3.8.0-alpine
WORKDIR /project
ADD . /project
RUN pip install -r requirements.txt
EXPOSE 5000
CMD [&quot;python&quot;,&quot;application.py&quot;]
</code></pre>
<p>I built the container with a tag:
<code>docker build -t flask-sms-service:latest .</code>
and run it with the following command:
<code>docker run -d -p 5000:5000 flask-sms-service</code></p>
<p>I made sure to expose port 5000, for the server to be accessible. But when i access it on <code>127.0.0.1:5000</code> i can't access the container.</p>
<p>What did i do wrong?</p>
"
"31324981","How to access host port from docker container","<docker><docker-container>","63653049","How would I connect to a MySQL on the host machine from inside a docker kubernetes pod?","<mysql><docker><kubernetes><windows-subsystem-for-linux><kind>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<p>I have a MySQL server running on my local machine, accessible at <code>localhost:3306</code>.</p>
<p>I also have a KinD cluster set up on WSL2, with Docker integration. How would I access the server from a python application inside a kubernetes pod?</p>
"
"31870222","How can I keep a container running on Kubernetes?","<docker><containers><kubernetes><google-kubernetes-engine>","63709872","why the kubernetes pod shows Back-off restarting failed container","<kubernetes>","<p>I'm now trying to run a simple container with shell (/bin/bash) on a Kubernetes cluster.</p>

<p>I thought that there was a way to keep a container running on a Docker container by using <code>pseudo-tty</code> and detach option (<code>-td</code> option on <code>docker run</code> command).</p>

<p>For example,</p>

<pre><code>$ sudo docker run -td ubuntu:latest
</code></pre>

<p>Is there an option like this in Kubernetes?</p>

<p>I've tried running a container by using a <code>kubectl run-container</code> command like:</p>

<pre><code>kubectl run-container test_container ubuntu:latest --replicas=1
</code></pre>

<p>But the container exits for a few seconds (just like launching with the <code>docker run</code> command without options I mentioned above). And ReplicationController launches it again repeatedly.</p>

<p>Is there a way to keep a container running on Kubernetes like the <code>-td</code> options in the <code>docker run</code> command?</p>
","<p>I want to build a troube shooting pod, this is my Dockerbuild file:</p>
<pre><code>FROM alpine:3.11

MAINTAINER jiangxiaoqiang (jiangtingqiang@gmail.com)

ENV LANG=en_US.UTF-8 \
    LC_ALL=en_US.UTF-8 \
    TZ=Asia/Shanghai


RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \
    &amp;&amp; echo $TZ &gt; /etc/timezone \
    &amp;&amp; apk add --no-cache curl jq \
    nmap \
    bind-tools \
    busybox-extras \
    bash


CMD [&quot;/bin/bash&quot;,&quot;-l&quot;]
</code></pre>
<p>but when I start it in kubernetes cluster, it shows: <code>Back-off restarting failed container</code>, and always restart all the time. so simple docker container ,why give me this tips? this is the descibe output:</p>
<pre><code>[root@k8smaster ~]# kubectl describe pod ts-7d754488b9-jqqh9
Name:         ts-7d754488b9-jqqh9
Namespace:    default
Priority:     0
Node:         k8sslave2/192.168.31.31
Start Time:   Wed, 02 Sep 2020 12:28:48 -0400
Labels:       k8s-app=ts
              pod-template-hash=7d754488b9
Annotations:  cni.projectcalico.org/podIP: 10.11.125.135/32
Status:       Running
IP:           10.11.125.135
IPs:
  IP:           10.11.125.135
Controlled By:  ReplicaSet/ts-7d754488b9
Containers:
  ts:
    Container ID:   docker://0c810ed8f8ec1cde6c0249edde59fc28a169d5730e87c423403f802cd12df6dd
    Image:          registry.cn-shanghai.aliyuncs.com/jiangxiaoqiang/dolphin/k8s-ts:v0.0.1
    Image ID:       docker-pullable://registry.cn-shanghai.aliyuncs.com/jiangxiaoqiang/dolphin/k8s-ts@sha256:68edaed45c1fadee71abbe7bdaad23f2400f352f1b6309142689a197367f3ae9
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 02 Sep 2020 12:30:13 -0400
      Finished:     Wed, 02 Sep 2020 12:30:13 -0400
    Ready:          False
    Restart Count:  4
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79w95 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-79w95:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-79w95
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                Message
  ----     ------     ----                 ----                -------
  Normal   Scheduled  &lt;unknown&gt;            default-scheduler   Successfully assigned default/ts-7d754488b9-jqqh9 to k8sslave2
  Normal   Created    96s (x4 over 2m17s)  kubelet, k8sslave2  Created container ts
  Normal   Started    95s (x4 over 2m16s)  kubelet, k8sslave2  Started container ts
  Warning  BackOff    69s (x7 over 2m15s)  kubelet, k8sslave2  Back-off restarting failed container
  Normal   Pulling    54s (x5 over 2m17s)  kubelet, k8sslave2  Pulling image &quot;registry.cn-shanghai.aliyuncs.com/jiangxiaoqiang/dolphin/k8s-ts:v0.0.1&quot;
  Normal   Pulled     54s (x5 over 2m17s)  kubelet, k8sslave2  Successfully pulled image &quot;registry.cn-shanghai.aliyuncs.com/jiangxiaoqiang/dolphin/k8s-ts:v0.0.1&quot;
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63712527","Python package run in Docker results in ERR_EMPTY_RESPONSE","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have created a python package which is a Flask application. I want to run that application in a Docker container. This is my Dockerfile:</p>
<pre><code>FROM python:3.7
FROM pytorch/pytorch

MAINTAINER Nikolay Valkov nikolay1499@gmail.com

# set a directory for the app
WORKDIR /usr/app/

# copy all the files to the container
COPY . .

# install dependencies
RUN pip install --no-cache-dir -r requirements.txt

WORKDIR /usr/app/src/

RUN pip install -e .

WORKDIR /usr/app

# tell the port number the container should expose
EXPOSE 5000

ENV FLASK_APP packagename

# run the command
CMD flask run

</code></pre>
<p>My app is bound to 0.0.0.0 so it can be accessed from outside the Docker container like this:</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == '__main__':
    app.run(host='0.0.0.0')
</code></pre>
<p>The app variable is declared in <code>app.py</code> and imported in <code>__init__.py</code> if that information is required.
When I run the flask package locally without Docker everything works but when I run the container localhost:5000 gives me ERR_EMPTY_RESPONSE. I used the command <code>docker run -p 5000:5000 nameofimage</code>.
Any ideas why this happens? What am I missing?</p>
<p>Edit I was asked to post the python code:
<code>setup.py</code></p>
<pre><code>from setuptools import setup

def readme():
    with open(&quot;README.rst&quot;) as f:
        return f.read()

setup(name = &quot;generateme&quot;,
      version = &quot;0.1.2&quot;,
      description = &quot;Flask application to generate images with Generative Adversarial networks&quot;,
      long_description = readme(),
      url = &quot;https://github.com/Nikolay1499/GenerateMe&quot;,
      author = &quot;Nikolay Valkov&quot;,
      author_email = &quot;nikolay1499@gmail.com&quot;,
      license = &quot;MIT&quot;,
      packages = [&quot;generateme&quot;],
      install_requires = [
          &quot;flask&quot;,
          &quot;gevent&quot;,
          &quot;numpy&quot;,
          &quot;Pillow&quot;,
          &quot;matplotlib&quot;,
          &quot;future&quot;,
      ],
      zip_safe = False,
      include_package_data = True,
)
</code></pre>
<p><code>__init__.py</code></p>
<pre><code>from flask import Flask
import os
from generateme.app import app
from generateme.app import index, showImageConv, showImageLinear, showImageStyle
IMAGE_FOLDER = os.path.join(&quot;static&quot;, &quot;Photos&quot;)

def create_app():
  app = Flask(__name__)
  app.config[&quot;UPLOAD_FOLDER&quot;] = IMAGE_FOLDER
  app.config[&quot;TEMPLATES_AUTO_RELOAD&quot;] = True
  app.add_url_rule(&quot;/&quot;, &quot;index&quot;, index)
  app.add_url_rule(&quot;/index&quot;, &quot;index&quot;, index)
  app.add_url_rule(&quot;/getStyleImage&quot;, &quot;showImageStyle&quot;, showImageStyle)
  app.add_url_rule(&quot;/getConvImage&quot;, &quot;showImageConv&quot;, showImageConv)
  app.add_url_rule(&quot;/getLinearImage&quot;, &quot;showImageLinear&quot;, showImageLinear)
  return app
</code></pre>
<p><code>app.py</code></p>
<pre><code>from flask import Flask, render_template, send_file, Response, url_for
from PIL import Image
import numpy as np
import io
import os
from generateme.LinearGan import getLinearImage
from generateme.DCGan import getConvImage
from generateme.StyleGan import getStyleImage

IMAGE_FOLDER = os.path.join(&quot;static&quot;, &quot;Photos&quot;)

app = Flask(__name__)
app.config[&quot;UPLOAD_FOLDER&quot;] = IMAGE_FOLDER
app.config[&quot;TEMPLATES_AUTO_RELOAD&quot;] = True
folder = os.path.dirname(os.path.abspath(__file__))

@app.route(&quot;/&quot;)
@app.route(&quot;/index&quot;)
def index():
    return render_template(&quot;index.html&quot;)

@app.route(&quot;/getStyleImage&quot;)
def showImageStyle():
    getStyleImage()
    return getImage()
    
@app.route(&quot;/getConvImage&quot;)
def showImageConv():
    getConvImage()
    return getImage()
    
@app.route(&quot;/getLinearImage&quot;)
def showImageLinear():
    getLinearImage()
    return getImage()

def getImage():
    file = my_file = os.path.join(folder, &quot;static/Photos/image.png&quot;)
    img = Image.open(file)
    file_object = io.BytesIO()

    img.save(file_object, &quot;PNG&quot;)  
    file_object.seek(0)

    response = send_file(file_object, mimetype=&quot;image/PNG&quot;)
    response.headers[&quot;Cache-Control&quot;] = &quot;no-store, no-cache, must-revalidate, max-age=0&quot;
    return response
    
    
if __name__ == '__main__':
    app.run(host=&quot;0.0.0.0&quot;,port=5000)
</code></pre>
"
"27273412","Cannot install packages inside docker Ubuntu image","<docker><ubuntu-14.04>","63756071","How to use apt install in docker FROM python:3.6","<python><docker>","<p>I installed Ubuntu 14.04 image on docker. After that, when I try to install packages inside the ubuntu image, I'm getting unable to locate package error: </p>

<pre><code>apt-get install curl

Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package curl
</code></pre>

<p>How to fix this error?</p>
","<p>I make a Dockerfile and use <code>FROM python:3.6</code></p>
<p>And now, I want to use <code>apt</code> to install some <code>dependence</code> what should I do?</p>
<p>Dockerfile</p>
<pre><code>FROM python:3.6

COPY . /app

WORKDIR /app

RUN pip install -r requirements.txt

CMD python app.py
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63816090","Can't access React and Flask services created by Docker-Compose on Windows 10","<reactjs><docker><flask><docker-compose><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a docker-compose file that defines services for both a Flask backend and a React frontend. It looks like that everything is working correctly, but I can't access either applications. I get no responses returned for both services.</p>
<p>The backend and frontend services both build and run successfully. Here is the flask service output when I run <code>docker-compose up</code>:</p>
<pre><code>flask_1     |  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
flask_1     |  * Restarting with stat
flask_1     |  * Debugger is active!
flask_1     |  * Debugger PIN: 319-453-363
</code></pre>
<p>But when I try to access <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a>, I get the following</p>
<p><a href=""https://i.stack.imgur.com/OR9JG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OR9JG.png"" alt=""enter image description here"" /></a></p>
<p>It looks like Docker is exposing the ports correctly:</p>
<p><a href=""https://i.stack.imgur.com/CFJSF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CFJSF.png"" alt=""enter image description here"" /></a></p>
<p>docker-compose.yml</p>
<pre><code>version: '3.8'

services:
  flask:
    build: 
      context: ./backend/
      dockerfile: Dockerfile
    ports:
      - &quot;5000:5000&quot;
  frontend:
    build: ./frontend/webapp/
    stdin_open: true
    volumes:
      - './frontend/webapp:/app/webapp'
      - '/app/frontend/node_modules'
    ports:
      - &quot;3002:3002&quot;
</code></pre>
<p>Dockerfile for frontend:</p>
<pre><code>FROM node:latest
WORKDIR /app/frontend
COPY package.json /app/frontend
COPY . /app/frontend
RUN npm install
EXPOSE 3002
CMD [&quot;npm&quot;, &quot;start&quot;]

</code></pre>
<p>Dockerfile for backend:</p>
<pre><code>FROM anaconda3:5.0.1
WORKDIR /app/Backend
COPY . .
EXPOSE 5000
CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p><strong>UPDATE:</strong></p>
<p>Turned out the reason that this wasn't working was because React runs default on port 3000. In my docker-compose file, I had</p>
<pre><code>ports:
      - &quot;3002:3002&quot;
</code></pre>
<p>I changed the first '3002' to '3000' and was able to access it.</p>
"
"31210973","How do I seed a mongo database using docker-compose?","<mongodb><docker><docker-compose>","63845803","How can I use my database in a container after push the container into Dockerhub?","<database><docker>","<p>I am trying to distribute a set of connected applications running in several linked containers that includes a mongo database that is required to:</p>

<ul>
<li>be distributed containing some seed data;</li>
<li>allow users to add additional data.</li>
</ul>

<p>Ideally the data will also be persisted in a linked data volume container.</p>

<p>I can get the data into the <code>mongo</code> container using a <code>mongo</code> base instance that doesn't mount any volumes (dockerhub image: <code>psychemedia/mongo_nomount</code> - this is essentially the base mongo Dockerfile without the <code>VOLUME /data/db</code> statement) and a <code>Dockerfile</code> config along the lines of:</p>

<pre><code>ADD . /files
WORKDIR /files
RUN mkdir -p /data/db &amp;&amp; mongod --fork --logpath=/tmp/mongodb.log &amp;&amp; sleep 20 &amp;&amp; \
mongoimport  --db testdb --collection testcoll  --type csv --headerline --file ./testdata.csv  #&amp;&amp; mongod --shutdown
</code></pre>

<p>where <code>./testdata.csv</code> is in the same directory (<code>./mongo-with-data</code>) as the Dockerfile.</p>

<p>My docker-compose config file includes the following:</p>

<pre><code>mongo:
  #image: mongo
  build: ./mongo-with-data
  ports:
    - ""27017:27017""
  #Ideally we should be able to mount this against a host directory
  #volumes:
  #  - ./db/mongo/:/data/db
  #volumes_from:
  #  - devmongodata

#devmongodata:
#    command: echo created
#    image: busybox
#    volumes: 
#       - /data/db
</code></pre>

<p>Whenever I try to mount a VOLUME it seems as if the original seeded data - which is stored in <code>/data/db</code> - is deleted. I guess that when a volume is mounted to <code>/data/db</code> it replaces whatever is there currently. </p>

<p>That said, the <a href=""https://docs.docker.com/userguide/dockervolumes/"" rel=""noreferrer"">docker userguide</a> suggests that: <em>Volumes are initialized when a container is created. If the container’s base image contains data at the specified mount point, that existing data is copied into the new volume upon volume initialization</em>? So I expected the data to persist if I placed the VOLUME command after the seeding <code>RUN</code> command?</p>

<p>So what am I doing wrong?</p>

<p>The long view is that I want to automate the build of several linked containers, and then distribute a <code>Vagrantfile</code>/docker-compose YAML file that will fire up a set of linked apps, that includes a pre-seeded <code>mongo</code> database with a (partially pre-populated) persistent data container.</p>
","<p>I'm trying to save a database into one container. How can I save my database when I push the container to Dockerhub?</p>
"
"33001750","Connect to mysql in a docker container from the host","<mysql><docker><dockerfile>","63904851","In Javascript, how would I connect to a MySQL database that is inside a docker?","<javascript><mysql><docker>","<p><em>(It's probably a dumb question due to my limited knowledge with Docker or mysql administration, but since I spent a whole evening on this issue, I dare to ask it.)</em></p>

<p><strong>In a nutshell</strong></p>

<p>I want to run mysql in a docker container and connect to it from my host. So far, the best I have achieved is:</p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
</code></pre>

<p><strong>More details</strong></p>

<p>I'm using the following <code>Dockerfile</code>:</p>

<pre><code>FROM ubuntu:14.04.3
RUN apt-get update &amp;&amp; apt-get install -y mysql-server

# Ensure we won't bind to localhost only
RUN grep -v bind-address /etc/mysql/my.cnf &gt; temp.txt \
  &amp;&amp; mv temp.txt /etc/mysql/my.cnf

# It doesn't seem needed since I'll use -p, but it can't hurt
EXPOSE 3306

CMD /etc/init.d/mysql start &amp;&amp; tail -F /var/log/mysql.log
</code></pre>

<p>In the directory where there is this file, I can succesfully build the image and run it with:</p>

<pre><code>&gt; docker build -t my-image .
&gt; docker run -d -p 12345:3306 my-image
</code></pre>

<p>When I attach to the image, it seems to work just fine:</p>

<pre><code># from the host
&gt; docker exec -it &lt;my_image_name&gt; bash

#inside of the container now
$ mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
[...]
</code></pre>

<p>However I don't have that much success from the host:</p>

<pre><code>&gt; mysql -P 12345 -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
</code></pre>

<p><strong>Even more details</strong></p>

<ul>
<li>I've seen that there's a question which <a href=""https://stackoverflow.com/q/25069860/1796345"">looks like mine</a>. However, it isn't the same (and it doesn't have any answers anyway)

<ul>
<li>I've seen that there are images <a href=""https://github.com/sameersbn/docker-mysql"" rel=""noreferrer"">dedicated to mysql</a>, but I didn't have more success with them</li>
<li>My <code>grep -v</code> may feel weird. Admittedly, there may be cleaner way to do it. But when I attach my image, I can observe it actually worked as expected (ie: removed the <code>bind-address</code>). And I can see in the container <code>/var/log/mysql/error.log</code>:</li>
</ul></li>
</ul>

<blockquote>
  <p>Server hostname (bind-address): '0.0.0.0'; port: 3306
        - '0.0.0.0' resolves to '0.0.0.0';
      Server socket created on IP: '0.0.0.0'.</p>
</blockquote>
","<p>I've written scripts to connect to mysql databases in Javascript. One I've done is like:</p>
<pre><code>function connectToSQL() {
    return new Promise((resolve,reject)=&gt;{
        try {
            var connection = mysql.createConnection({
                host:&quot;0.0.0.0&quot;,
                user: &quot;username&quot;,
                password: &quot;password&quot;,
                database: &quot;mydb&quot;
            });
        } catch(e) {
            reject(e);
        }
        if(!connection) {
            reject(new Error(&quot;No Connection&quot;));
        }
        connection.connect(function(err) {
            if(err) reject(err);
            resolve(connection);
        });
    });
}
</code></pre>
<p>However, that was a standalone Mysql, not inside a docker container, I was connecting to. Inside my Docker, the Sql container can be found:</p>
<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                     NAMES
fcd8a5277248        mysql:5.6           &quot;docker-entrypoint...&quot;   2 weeks ago         Up 2 weeks          0.0.0.0:3306-&gt;3306/tcp                                                    db-mysql
</code></pre>
<p>How would I write a JS script to get into this to run queries?</p>
"
"28302178","How can I add a volume to an existing Docker container?","<docker>","64120664","How do I attach a volume to a running container starting from the Docker Run command?","<amazon-web-services><docker><docker-volume>","<p>I have a Docker container that I've created simply by installing Docker on Ubuntu and doing:</p>
<pre><code>sudo docker run -i -t ubuntu /bin/bash
</code></pre>
<p>I immediately started installing Java and some other tools, spent some time with it, and stopped the container by</p>
<pre><code>exit
</code></pre>
<p>Then I wanted to add a volume and realised that this is not as straightforward as I thought it would be. If I use <code>sudo docker -v /somedir run ...</code> then I end up with a fresh new container, so I'd have to install Java and do what I've already done before just to arrive at a container with a mounted volume.</p>
<p>All the documentation about mounting a folder from the host seems to imply that mounting a volume is something that can be done when creating a container. So the only option I have to avoid reconfiguring a new container from scratch is to commit the existing container to a repository and use that as the basis of a new one whilst mounting the volume.</p>
<p>Is this indeed the only way to add a volume to an existing container?</p>
","<p>I need to attach a volume to the running container which has started from the docker run command on my AWS EC2 server (Ubuntu 18.04 LTS) with the following command</p>
<pre><code>$ docker run --name example -p 8080:80 -dt example-image:latest
</code></pre>
<p>I'm using Docker version 19.03.12.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","64219058","docker-compose ports not mapping; logs shows app is running","<docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>When I run <code>flask run</code> the localhost port is exposed and the application loads with no errors, but when I run <code>docker-compose up -d --build</code> the image is built and running but the localhost is not exposed. inspecting the container shows that it built with no errors and is still running. I've reviewed <a href=""https://github.com/docker/compose/issues/4799"" rel=""nofollow noreferrer"">this github issue</a> and <a href=""https://stackoverflow.com/questions/35429837/docker-compose-port-mapping"">this stackoverflow</a> which suggested <code>network_host mode</code> was the problem but I am not using it. It also suggested manually using 0.0.0.0 instead of 127.0.0.1 in the ports mapping but that didn't help.</p>
<p>docker-compose.yml</p>
<pre><code>#~/Projects/mysite/docker-compose.yml
version: '3.7'
services:
  flask-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    image: &quot;site-development&quot;
    ports:
      - &quot;5000:5000&quot;
    volumes:
      - &quot;.:/personal_site&quot;
    tty: true
</code></pre>
<p>Dockerfile</p>
<pre><code>#~/Projects/mysite/Dockerfile
# multi build stages
#https://docs.docker.com/develop/develop-images/multistage-build/

# ==================================== BASE ====================================
FROM python:3.8-slim-buster AS base

WORKDIR /personal_site

ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV FLASK_APP mysite
ENV FLASK_ENV development
ENV FLASK_DEBUG 1


RUN apt-get update
RUN apt-get install -y \
    gcc

RUN pip install --upgrade pip
COPY . .

# ================================= DEVELOPMENT ================================
FROM base as development
COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
RUN pip install -r requirements-dev.txt

CMD [&quot;flask&quot;, &quot;run&quot;]

# ================================= PRODUCTION =================================
FROM base as production
COPY ./requirements-prod.txt /personal_site/requirements-prod.txt
RUN pip install -r requirements-prod.txt

# =================================== MANAGE ===================================
FROM base as manage
COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
RUN pip install -r requirements-dev.txt
</code></pre>
<p>Docker versions:</p>
<pre><code>docker-compose version 1.25.5, build 8a1c60f6
Docker version 19.03.13, build 4484c46d9d
</code></pre>
<p>Folder Structure:</p>
<pre><code>#~/Projects/mysite
- docker-compose.yml
- Dockerfile
- mysite/__init__.py
</code></pre>
<p>Running from cmd with <code>flask run</code></p>
<pre><code> * Serving Flask app &quot;mysite&quot; (lazy loading)
 * Environment: development
 * Debug mode: on
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 331-834-605
</code></pre>
<p>Docker-compose stout</p>
<pre><code>stephen@stephen:~/Projects/mysite$ docker-compose up -d --build
docker-compose up -d --build
Building flask-dev
Step 1/15 : FROM python:3.8-slim-buster AS base
 ---&gt; 6cf621cb1327
Step 2/15 : WORKDIR /personal_site
 ---&gt; Using cache
 ---&gt; 751c8aa117f3
Step 3/15 : ENV PYTHONDONTWRITEBYTECODE 1
 ---&gt; Using cache
 ---&gt; 8b29a6862b7a
Step 4/15 : ENV PYTHONUNBUFFERED 1
 ---&gt; Using cache
 ---&gt; a5f8751c9c12
Step 5/15 : ENV FLASK_APP mysite
 ---&gt; Using cache
 ---&gt; bc212892cc17
Step 6/15 : ENV FLASK_ENV development
 ---&gt; Using cache
 ---&gt; 9ffa29dcea47
Step 7/15 : ENV FLASK_DEBUG 1
 ---&gt; Using cache
 ---&gt; 9b2e876d307a
Step 8/15 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; bd323216e65a
Step 9/15 : RUN apt-get install -y  gcc
 ---&gt; Using cache
 ---&gt; 3105c1727b5d
Step 10/15 : RUN pip install --upgrade pip
 ---&gt; Using cache
 ---&gt; 6f8fb5bc0015
Step 11/15 : COPY . .
 ---&gt; 76ff5e26c0a9

Step 12/15 : FROM base as development
 ---&gt; 76ff5e26c0a9
Step 13/15 : COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
 ---&gt; 06e9dac6dec6
Step 14/15 : RUN pip install -r requirements-dev.txt
 ---&gt; Running in f4743afd3a31
Collecting click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
Collecting Flask==1.1.2
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting itsdangerous==1.1.0
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2==2.11.2
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting MarkupSafe==1.1.1
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Collecting Werkzeug==1.0.1
  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Collecting pytest==6.0.2
  Downloading pytest-6.0.2-py3-none-any.whl (270 kB)
Collecting black==19.10b0
  Downloading black-19.10b0-py36-none-any.whl (97 kB)
Collecting flake8==3.8.3
  Downloading flake8-3.8.3-py2.py3-none-any.whl (72 kB)
Collecting flake8-blind-except==0.1.1
  Downloading flake8-blind-except-0.1.1.tar.gz (2.5 kB)
Collecting flake8-debugger==3.2.1
  Downloading flake8-debugger-3.2.1.tar.gz (4.2 kB)
Collecting flake8-docstrings==1.5.0
  Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl (5.5 kB)
Collecting flake8-isort==4.0.0
  Downloading flake8_isort-4.0.0-py2.py3-none-any.whl (14 kB)
Collecting isort==5.5.2
  Downloading isort-5.5.2-py3-none-any.whl (95 kB)
Collecting pep8-naming==0.11.1
  Downloading pep8_naming-0.11.1-py2.py3-none-any.whl (8.4 kB)
Collecting pluggy&lt;1.0,&gt;=0.12
  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)
Collecting toml
  Downloading toml-0.10.1-py2.py3-none-any.whl (19 kB)
Collecting more-itertools&gt;=4.0.0
  Downloading more_itertools-8.5.0-py3-none-any.whl (44 kB)
Collecting attrs&gt;=17.4.0
  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)
Collecting py&gt;=1.8.2
  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)
Collecting iniconfig
  Downloading iniconfig-1.0.1-py3-none-any.whl (4.2 kB)
Collecting packaging
  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)
Collecting appdirs
  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting regex
  Downloading regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675 kB)
Collecting pathspec&lt;1,&gt;=0.6
  Downloading pathspec-0.8.0-py2.py3-none-any.whl (28 kB)
Collecting typed-ast&gt;=1.4.0
  Downloading typed_ast-1.4.1-cp38-cp38-manylinux1_x86_64.whl (768 kB)
Collecting pyflakes&lt;2.3.0,&gt;=2.2.0
  Downloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)
Collecting mccabe&lt;0.7.0,&gt;=0.6.0
  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)
Collecting pycodestyle&lt;2.7.0,&gt;=2.6.0a1
  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from flake8-blind-except==0.1.1-&gt;-r requirements-dev.txt (line 10)) (50.3.0)
Collecting pydocstyle&gt;=2.1
  Downloading pydocstyle-5.1.1-py3-none-any.whl (35 kB)
Collecting testfixtures&lt;7,&gt;=6.8.0
  Downloading testfixtures-6.14.2-py2.py3-none-any.whl (89 kB)
Collecting flake8-polyfill&lt;2,&gt;=1.0.2
  Downloading flake8_polyfill-1.0.2-py2.py3-none-any.whl (7.3 kB)
Collecting six
  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting pyparsing&gt;=2.0.2
  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Collecting snowballstemmer
  Downloading snowballstemmer-2.0.0-py2.py3-none-any.whl (97 kB)
Building wheels for collected packages: flake8-blind-except, flake8-debugger
  Building wheel for flake8-blind-except (setup.py): started
  Building wheel for flake8-blind-except (setup.py): finished with status 'done'
  Created wheel for flake8-blind-except: filename=flake8_blind_except-0.1.1-py3-none-any.whl size=3045 sha256=ee8ec372f4ef9ba5aa472cc70cf2b34462c2e5742c4431a88d4811ba3c6941b9
  Stored in directory: /root/.cache/pip/wheels/7f/70/4f/6002af5c24a072eec72570bf0a265bf95f1968c4508563b6cd
  Building wheel for flake8-debugger (setup.py): started
  Building wheel for flake8-debugger (setup.py): finished with status 'done'
  Created wheel for flake8-debugger: filename=flake8_debugger-3.2.1-py3-none-any.whl size=4201 sha256=0399bc5b93a43d4f70912c25e9251000b1003a04215c0d3b0656da88f0fd828b
  Stored in directory: /root/.cache/pip/wheels/39/b8/cc/049c8f7f02a46a88305db6b9ed26799fded45f98e20728640c
Successfully built flake8-blind-except flake8-debugger
Installing collected packages: click, MarkupSafe, Jinja2, Werkzeug, itsdangerous, Flask, pluggy, toml, more-itertools, attrs, py, iniconfig, six, pyparsing, packaging, pytest, appdirs, regex, pathspec, typed-ast, black, pyflakes, mccabe, pycodestyle, flake8, flake8-blind-except, flake8-debugger, snowballstemmer, pydocstyle, flake8-docstrings, isort, testfixtures, flake8-isort, flake8-polyfill, pep8-naming
Successfully installed Flask-1.1.2 Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 appdirs-1.4.4 attrs-20.2.0 black-19.10b0 click-7.1.2 flake8-3.8.3 flake8-blind-except-0.1.1 flake8-debugger-3.2.1 flake8-docstrings-1.5.0 flake8-isort-4.0.0 flake8-polyfill-1.0.2 iniconfig-1.0.1 isort-5.5.2 itsdangerous-1.1.0 mccabe-0.6.1 more-itertools-8.5.0 packaging-20.4 pathspec-0.8.0 pep8-naming-0.11.1 pluggy-0.13.1 py-1.9.0 pycodestyle-2.6.0 pydocstyle-5.1.1 pyflakes-2.2.0 pyparsing-2.4.7 pytest-6.0.2 regex-2020.9.27 six-1.15.0 snowballstemmer-2.0.0 testfixtures-6.14.2 toml-0.10.1 typed-ast-1.4.1
Removing intermediate container f4743afd3a31
 ---&gt; c63db68e69d5
Step 15/15 : CMD [&quot;flask&quot;, &quot;run&quot;]
 ---&gt; Running in c0f90d383fb2
Removing intermediate container c0f90d383fb2
 ---&gt; 8552c5d064ca

Successfully built 8552c5d064ca
Successfully tagged site-development:latest
Recreating mysite_flask-dev_1 ... 
stephen@stephen:~/Projects/mysite$ docker-compose ps
docker-compose ps
       Name           Command    State           Ports         
---------------------------------------------------------------
mysite_flask-dev_1   flask run   Up      0.0.0.0:5000-&gt;5000/tcp
stephen@stephen:~/Projects/mysite$ docker-compose logs
docker-compose logs
Attaching to mysite_flask-dev_1
flask-dev_1  |  * Tip: There are .env or .flaskenv files present. Do &quot;pip install python-dotenv&quot; to use them.
flask-dev_1  |  * Serving Flask app &quot;mysite&quot; (lazy loading)
flask-dev_1  |  * Environment: development
flask-dev_1  |  * Debug mode: on
flask-dev_1  |  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
flask-dev_1  |  * Restarting with stat
flask-dev_1  |  * Tip: There are .env or .flaskenv files present. Do &quot;pip install python-dotenv&quot; to use them.
flask-dev_1  |  * Debugger is active!
flask-dev_1  |  * Debugger PIN: 124-299-806
</code></pre>
"
"35290901","How to populate a Mysql docker container during build of another container?","<docker><dockerfile><docker-compose>","64247215","Automatically create the database with all tables during the Docker build","<mysql><database><docker>","<p>I am trying to build a docker-compose file that automatically builds a container for my application and a mysql-container that stores all the data.</p>
<p>Within my dockerfile for my application I have a script, that sets up all the database tables and preset values, that are necessary for the application to run. Is it possible to somehow pass the reference of the mysql container to myapplication that during build of myapplication it can access the mysql container?</p>
<p>If so, how can I do that, and if not, how should it be done then?</p>
<p>docker-compose.yml</p>
<pre class=""lang-yaml prettyprint-override""><code>mysql:
  image: mysql/mysql-server
  ports:
    - &quot;3306:3306&quot;
  environment:
    MYSQL_USER: root
    MYSQL_ROOT_PASSWORD: mysql-root-password

myapplication:
  command: /home/myapplication/startup.sh
  build: ./..
  dockerfile: ./dockerfiles/myapplication
  ports:
    - &quot;443:443&quot;
    - &quot;8090:8090&quot;
  links:
    - mysql:docker-mysql
  environment:
    MYSQL_SERVER: docker-mysql
</code></pre>
<p>myapplication-dockerfile</p>
<pre><code>FROM ubuntu:latest

EXPOSE 443

ENV applicationPath=&quot;/home/application“

COPY . ${applicationPath}

#Execute installationScript
RUN /bin/bash -c &quot;./${applicationPath}/Tools/install_all.sh&quot;
</code></pre>
","<p>I'm currently starting to work with docker. For my application I need to provide a MySQL database, which is already there and the mysql server works also. My problem is that I want Docker to automatically import the database (e.g. from an sql file) during the build process, so that it's available from the beginning so I dont have to import it manually afterwards.</p>
<p>Thanks for your answers!</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","64284575","docker container on windows does not give any response","<azure><docker><flask><docker-compose><flask-restful>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Nice time of the day for everybody and Hello,
I am new at docker and I have faced a lot of problems with deploying of a Flask app.
My Dockerfile:</p>
<pre><code># For more information, please refer to https://aka.ms/vscode-docker-python
FROM ubuntu:20.04

EXPOSE 5000

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE 1

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED 1

# Install pip requirements
RUN apt-get update &amp;&amp; apt-get install -y python3-pip unixodbc-dev python3-dev
ADD requirements.txt .
RUN pip3 install -r requirements.txt
RUN pip3 install flask-restful
RUN pip3 install werkzeug

WORKDIR /app
ADD . /app

# Switching to a non-root user, please refer to https://aka.ms/vscode-docker-python-user-rights
RUN useradd appuser &amp;&amp; chown -R appuser /app
USER appuser

# During debugging, this entry point will be overridden. For more information, please refer to https://aka.ms/vscode-docker-python-debug
CMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;127.0.0.1:5000&quot;, &quot;main:app&quot;]
</code></pre>
<p>Docker-compose:</p>
<pre><code>version: '3.4'

services:
  tutorial:
    image: tutorial
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 5000:5000
</code></pre>
<p>main.py:</p>
<pre><code>api.add_resource(Video,&quot;/videos/&lt;int:video_id&gt;&quot;) 
if __name__=='__main__':
    app.run(host=&quot;127.0.0.1&quot;, port=5000,debug=False)
</code></pre>
<p>One if the main problems is that I cannot get a respose from container when it is locally runned on the local host.
In my docker log I see that <code>gunicorn</code> listening to a <code>port 5000</code></p>
<pre><code>[2020-10-09 14:20:19 +0000] [1] [INFO] Starting gunicorn 20.0.4

[2020-10-09 14:20:19 +0000] [1] [INFO] Listening at: http://127.0.0.1:5000 (1)

[2020-10-09 14:20:19 +0000] [1] [INFO] Using worker: sync

[2020-10-09 14:20:19 +0000] [7] [INFO] Booting worker with pid: 7
</code></pre>
<p>But according infromation from terminal I  see my container port opened.But it is not binded as it is specified in docker-compose file <code>ports:- 5000:5000.</code></p>
<pre><code>C:\Users\fele&gt;docker ps
CONTAINER ID        IMAGE                                                                       COMMAND                  CREATED             STATUS              PORTS               NAMES
f3e0bfae76ec        containerregistr.azurecr.io/restvideoshop:1 &quot;gunicorn --bind 127…&quot;   5 hours ago         Up 33 seconds       5000/tcp            beautiful_chatterjee
</code></pre>
<p>Furthermore if I open my windows docker desktop and inspect my container I see one port opened in not binded state.
<a href=""https://i.stack.imgur.com/oG9h5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oG9h5.png"" alt=""enter image description here"" /></a></p>
<p>So when I run my test.py</p>
<pre><code>import requests

BASE=&quot;http://127.0.0.1:5000/&quot;
data=[{&quot;name&quot;:&quot;Duke Nuken&quot;,&quot;views&quot;:1000,&quot;likes&quot;:10},
    {&quot;name&quot;:&quot;Never Again&quot;,&quot;views&quot;:20,&quot;likes&quot;:80},
    {&quot;name&quot;:&quot;Belarus&quot;,&quot;views&quot;:500,&quot;likes&quot;:70}]

for i in range(len(data)):
    response=requests.put(BASE+&quot;videos/&quot;+str(i),data[i])
    print(response.json())
</code></pre>
<p>I see folowing log in the terminal:</p>
<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /videos/0 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x02FB8C30&gt;: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer 
die Verbindung verweigerte'))
</code></pre>
<p>Can anybody give my advice and tell me why I cannot get a response from container? Should I do any other configuration in docker-compose file or set some additional code for gunicorn?One more question is not directly related to docker, I am planning to deploy container with Azure container instances so is it requiered to use inside of container a Nginx or Azure takes itself things to be done with Nginx?
Thanks in advance</p>
"
"33913020","Docker remove <none> TAG images","<docker>","64330661","How To Remove Anonymous Image In Docker?","<docker><dockerfile><docker-cli>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>So, I want to create an image for my app using</p>
<pre><code>docker build .
</code></pre>
<p>But I forgot to give it a name and the name became <code>&lt;none&gt;</code>. When I tried to remove this with <code>docker image rm &lt;none&gt;</code>
The output is</p>
<pre><code>bash: syntax error near unexpected token `newline'
</code></pre>
<p>How to remove this anonymous image?</p>
"
"31381322","Docker in Docker cannot mount volume","<linux><jenkins><docker><containers>","64358000","Jenkins in docker-compose run another docker-compose","<docker><jenkins><docker-compose>","<p>I am running a Jenkins cluster where in the Master and Slave, both are running as a Docker containers. </p>

<p>The Host is latest boot2docker VM running on MacOS.</p>

<p>To allow Jenkins to be able to perform deployment using Docker, I have mounted the docker.sock and docker client from the host to the Jenkins container like this :-</p>

<pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker -v $HOST_JENKINS_DATA_DIRECTORY/jenkins_data:/var/jenkins_home -v $HOST_SSH_KEYS_DIRECTORY/.ssh/:/var/jenkins_home/.ssh/ -p 8080:8080 jenkins
</code></pre>

<p>I am facing issues while mounting a volume to Docker containers that are run inside the Jenkins container. For example, if I need to run another Container inside the Jenkins container, I do the following :-</p>

<pre><code>sudo docker run -v $JENKINS_CONTAINER/deploy.json:/root/deploy.json $CONTAINER_REPO/$CONTAINER_IMAGE 
</code></pre>

<p>The above runs the container, but the file ""deploy.json"" is NOT mounted as a file, but instead as a ""Directory"". Even if I mount a Directory as a Volume, I am unable to view the files in the resulting container. </p>

<p>Is this a problem, because of file permissions due to Docker in Docker case?</p>
","<p>I am try to start a docker-compose project from Jenkins launched in another docker-compose.
This is the docker-compose of jenkins:</p>
<pre><code>version: '3.7'
services:
  jenkins:
    image: jenkins/jenkins:lts
    privileged: true
    user: root
    ports:
      - 8081:8080
      - 50000:50000
    container_name: jenkins
    volumes:
      - ./jenkins:/var/jenkins_home
      - ./test_deploy:/test_deploy
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker
      - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose
</code></pre>
<p>I have a pipeline like this:</p>
<pre><code>pipeline {
    agent any

    stages {
        ... clone project etc ...
        stage('Run APP') {
            steps {
                dir('/test_deploy/app') {
                    sh 'docker-compose up -d'
                }
            }
        }
    }
}

</code></pre>
<p>the docker-compose that I launch via pipeline not load correctly the volumes, for example this is a fragment of the docker-file I launch</p>
<pre><code>rabbitmq:
    image: rabbitmq:3-management
    container_name: app_rabbitmq
    restart: unless-stopped
    user: root
    hostname: &quot;rabbit&quot;
    environment:
      RABBITMQ_DEFAULT_USER: &quot;rabbitmq&quot;
      RABBITMQ_DEFAULT_PASS: &quot;rabbitmq&quot;
      RABBITMQ_DEFAULT_VHOST: &quot;/&quot;
    ports: 
      - &quot;5672:5672&quot;
      - &quot;15672:15672&quot;
    volumes:
      - ./rabbitdata:/var/lib/rabbitmq
    networks:
      - APP
</code></pre>
<p>the <code>rabbitdata</code> folder is created on root of host machine on <code>/test_deploy/rabbitdata</code> but I need that is created in the <code>./test_deploy</code> of volume mounted on jenkis.</p>
<p>I have other services that I starts in my docker-file and the attached volume with file and code necessary for run. The error I receive is there are not found. How I can fix that?</p>
"
"33933107","Change Docker machine location - Windows","<windows><docker><docker-machine><docker-toolbox>","64402436","How to change Docker Image location on Windows 10 Home","<windows><docker>","<p>I am using docker toolbox on Windows 7 to run docker. (docker version 1.9.1)</p>

<p>As it turns out, the docker machine creates its VM at C:\Users\username\.docker\machine\machines\default. As I commit the images, the size of VM at this location bloats up. Since it's Windows, I can't afford the luxury of space on the C drive. </p>

<p>Is there any way I can change the location of the default machine?</p>
","<p>I have searched everywhere and all solutions are mostly outdated. I want to either change the whole docker dir from C Drive to D drive or only image from C to D. Because I have less space on C drive.
I am currently using Windows 10 Home.</p>
"
"34132237","Container NAMES when deploying with Docker","<docker><dokku>","64409647","NAMES column of ""docker ps"" output","<docker><docker-container>","<p>I've just done my first ever Docker deployment, when i run this command to see the status of recent processes...</p>

<p><code>docker ps -a</code></p>

<p>I get this output</p>

<p><a href=""https://i.stack.imgur.com/66qrY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66qrY.png"" alt=""docker output""></a></p>

<p>My question is; what are those name referring to?</p>
","<p>Below is the docker file:</p>
<pre><code>FROM golang:1.14.10

MAINTAINER xyz

ENV SOURCES /product-api

COPY . ${SOURCES}

WORKDIR /product-api

RUN make swagger

ENTRYPOINT product-api
</code></pre>
<hr />
<p>After running the below commands:</p>
<p><code>$ docker build -t cloud-native-product-api:1.0.0</code></p>
<p><code>$ docker run -it -p 8080:8080 cloud-native-product-api:1.0.0</code></p>
<hr />
<p><code>docker ps</code> gives below output:</p>
<pre><code>$ docker ps -a
CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                    NAMES
9aa144b7873c        cloud-native-product-api:1.0.0   &quot;/bin/sh -c product-…&quot;   3 minutes ago       Up 3 minutes        0.0.0.0:9090-&gt;9090/tcp   trusting_matsumoto
</code></pre>
<hr />
<p>Where does value <code>trusting_matsumoto</code> derive from in <code>NAMES</code> column?</p>
"
"33827342","How to connect mysql workbench to running mysql inside docker?","<mysql><docker><mysql-workbench>","64422760","Connect to MySQL in docker from Datagrip","<mysql><docker><datagrip>","<p>I am using mysql server inside docker container and able to access inside docker. How to create connection in mysql workbench running on my local(Host Machine).  </p>
","<p>I installed MYSQL in docker using this command</p>
<pre><code>docker pull mysql/mysql-server:latest
</code></pre>
<p>and then run docker image via this</p>
<pre><code>docker run --name=mysql -d mysql/mysql-server:latest
</code></pre>
<p>I try to connect to container like this</p>
<p><a href=""https://i.stack.imgur.com/hJgPv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hJgPv.png"" alt=""enter image description here"" /></a></p>
<p>I got an error</p>
<blockquote>
<p>[08S01] Communications link failure</p>
<p>The last packet sent successfully to the server was 0 milliseconds
ago. The driver has not received any packets from the server.
java.net.ConnectException: Connection refused: connect.</p>
</blockquote>
<p>Password and User are correct.</p>
<p>How I can solve this problem?</p>
"
"28996907","docker: ""build"" requires 1 argument. See 'docker build --help'","<docker><containers>","64502213","""docker build"" requires exactly 1 argument","<docker><dockerfile><docker-build><docker-buildkit>","<p>Trying to follow the instructions for building a docker image from the docker website.</p>

<p><a href=""https://docs.docker.com/examples/running_redis_service/"">https://docs.docker.com/examples/running_redis_service/</a></p>

<p>this is the error I get will following the instructions on the doc and using this Dockerfile</p>

<pre><code>FROM        ubuntu:14.04
RUN         apt-get update &amp;&amp; apt-get install -y redis-server
EXPOSE      6379
ENTRYPOINT  [""/usr/bin/redis-server""]


sudo docker build -t myrepo/redis
docker: ""build"" requires 1 argument. See 'docker build --help'.
</code></pre>

<p>How do  resolve?</p>
","<p>I just want to build a dockerfile from a different directory, I tried the following command</p>
<p><code>docker build -f C:/Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile</code></p>
<p>and</p>
<p><code>docker build -f C://Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile</code></p>
<p>Both of them yield the same error</p>
<p><code>&quot;docker build&quot; requires exactly 1 argument.</code></p>
<p>Am I missing something ?</p>
"
"29181032","Add a volume to Docker, but exclude a sub-folder","<docker><dockerfile>","64575774","Add a volume to Docker, but exclude a specific file","<docker><docker-compose><dockerfile><mounted-volumes>","<p>Supposed I have a Docker container and a folder on my host <code>/hostFolder</code>. Now if I want to add this folder to the Docker container as a volume, then I can do this either by using <code>ADD</code> in the <code>Dockerfile</code> or mounting it as a volume.</p>

<p>So far, so good.</p>

<p>Now <code>/hostFolder</code> contains a sub-folder, <code>/hostFolder/subFolder</code>.</p>

<p>I want to mount <code>/hostFolder</code> into the Docker container (whether as read-write or read-only does not matter, works both for me), but I do <strong>NOT</strong> want to have it included <code>/hostFolder/subFolder</code>. I want to exclude this, and I also want the Docker container be able to make changes to this sub-folder, without the consequence of having it changed on the host as well.</p>

<p>Is this possible? If so, how?</p>
","<p>Hi this might be flagged as a duplicate but none of the other solutions have really worked for me.</p>
<p>So I have a directory structure as (code is top most with the rest subfolders and subfiles)</p>
<ul>
<li>code/
<ul>
<li>task_queue/</li>
<li>tasks/</li>
<li>scheduler/</li>
<li>static/</li>
<li>core/</li>
<li>run_scheduler.sh</li>
<li>scheduler.cfg</li>
</ul>
</li>
</ul>
<p>scheduler.cfg is a config file in which secrets and variables are dynamically inserted to avoid  sensitive information from being compromised. Recently, we started mounting the code volume so that we can edit python code without rebuilding the container every time. That's all good and well but as a side affect any dynamic secrets inserted into the scheduler.cfg file now persist locally as well.</p>
<p>Is there a good way to exclude or avoid changing the local copy of this file? It cannot be gitignore I believe as we typically want to change scheduler.cfg to add new configs and commit to git. It is the dynamic secret insertion part that's being propagated locally that seems to be the problem.</p>
<p>Thank you and apologies if it is a duplicate. I'm not too handy with docker stuff.</p>
"
"32723111","How to remove old and unused Docker images","<docker><docker-image>","64628988","How to remove multiple <none> docker images blocked by stopped containers quickly","<docker><containers>","<p>When running Docker for a long time, there are a lot of images in system. How can I remove all unused Docker images at once safety to free up the storage?</p>

<p>In addition, I also want to remove images pulled months ago, which have the correct <code>TAG</code>.</p>

<p>So, I'm not asking for removing untagged images only. I'm searching for a way to remove general unused images, which includes both untagged and other images such as pulled months ago with correct <code>TAG</code>.</p>
","<p>I was trying to remove all dangling images including the images which have <code>&lt;none&gt;</code> as repo name or tag while listing with <code>docker images</code> command.</p>
<p><a href=""https://i.stack.imgur.com/tc6ML.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tc6ML.png"" alt=""enter image description here"" /></a></p>
<p>Now, if you try to remove the images , you may have seen that the <code>docker rmi &lt;image_id&gt;</code> commands might fail due to presence of invalid docker containers which were Exited abruptly and persisted as below.</p>
<p><a href=""https://i.stack.imgur.com/OG1NI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OG1NI.jpg"" alt=""enter image description here"" /></a></p>
<p>It is tedious to remove all the containers and dependent images one by. Hence, is there a good way of removing such images quickly or in automated way ?</p>
"
"33992867","How do you perform Django database migrations when using Docker-Compose?","<django><docker><docker-compose>","64696850","Make docker automatically run collecstatic and migrate in Django","<django><docker>","<p>I have set up a Docker Django/PostgreSQL app closely following the <a href=""https://docs.docker.com/compose/django/"" rel=""noreferrer"">Django Quick Start instructions on the Docker site</a>. </p>

<p>The first time I run Django's manage.py migrate, using the command <code>sudo docker-compose run web python manage.py migrate</code>, it works as expected. The database is built inside the Docker PostgreSQL container just fine.</p>

<p>Changes made to the Django app itself are likewise reflected in the Docker Django container, the moment I save them. It's great!</p>

<p>But if I then change a model in Django, and try to update the Postgres database to match the model, no changes are detected so no migration happens no matter how many times I run <code>makemigrations</code> or <code>migrate</code> again.</p>

<p>Basically, every time I change the Django model, I have to delete the Docker containers (using <code>sudo docker-compose rm</code>) and start afresh with a new migration.</p>

<p>I'm still trying to get my head around Docker, and there's an awful lot I don't understand about how it works, but this one is driving me nuts. Why doesn't migrate see my changes? What am I doing wrong?</p>
","<p>I have a following question.</p>
<p>I have dockerized <code>Django</code> project and instead of manually run <code>collectstatic</code> and <code>migrate</code> each and every time I would like to docker do it for me. Is it possible to make <code>Docker</code> automatically run:</p>
<pre><code>python manage.py collectstatic --noinput

python manage.py migrate

</code></pre>
<p>every time when image is created? Or alternatively before container is started…</p>
<p>My <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.8.6-alpine

LABEL project=&quot;short_urls&quot;

ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

WORKDIR /code

COPY Pipfile Pipfile.lock /code/
RUN pip install pipenv &amp;&amp; pipenv install --system --ignore-pipfile

COPY short_urls /code/

</code></pre>
<pre><code>Docker-Compose.yaml
</code></pre>
<pre><code>version: '3.8'

volumes:
    redis_data:

services:
  web:
    build: .
    command: python /code/manage.py runserver 0.0.0.0:8000
    volumes:
      - .:/code
    ports:
      - target: 8000
        published: 8000
        protocol: tcp
        mode: host
    depends_on:
      - redis
    restart: always

  redis:
    image: redis:6.0.9-alpine
    command:
      redis-server
    ports:
      - target: 6379
        published: 6380
        protocol: tcp
        mode: host
    volumes:
        - redis_data:/data
    restart: always
    environment:
      - REDIS_REPLICATION_MODE=master

  celery:
    build: .
    command: celery -A short_urls worker --loglevel=INFO -E
    restart: always
    environment:
      - C_FORCE_ROOT=1
    volumes:
      - .:/code
    links:
      - redis
    depends_on:
      - web
      - redis
    hostname: celery-main

  celery-beat:
    build: .
    command: celery -A short_urls beat --loglevel=INFO --pidfile=
    restart: always
    volumes:
        - .:/code
    depends_on:
        - web
        - redis
    hostname: celery-beat

  flower:
    image: mher/flower
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/3
      - FLOWER_PORT=8888
    depends_on:
      - celery
      - celery-beat
      - redis
    restart: always
    ports:
      - target: 8888
        published: 8888
        protocol: tcp
        mode: host
</code></pre>
<p>Thank you.</p>
"
"33322103","Multiple FROMs - what it means","<docker><dockerfile>","64721365","MultiStage dockerfile skips first step and only runs second one","<docker><dockerfile><python-3.7><google-cloud-sdk>","<p>I want to build a docker image for the <a href=""https://github.com/Linkurious/linkurious.js"" rel=""noreferrer"">Linkurious</a> project on github, which requires both the Neo4j database, and Node.js to run.</p>

<p>my first approach was to declare a base image for my image, containing Neo4j.  The reference docs do not define ""base image"" in any helpful manner:</p>

<blockquote>
  <p>Base image:
  An image that has no parent is a base image</p>
</blockquote>

<p>from which I read that I may only have a base image if that image has no base image itself.</p>

<p>but what is a base image? does it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?</p>

<p>reading the Docker reference (see: <a href=""https://docs.docker.com/reference/builder/#from"" rel=""noreferrer"">https://docs.docker.com/reference/builder/#from</a>) I see: </p>

<blockquote>
  <p>FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command.</p>
</blockquote>

<p>do I want to create multiple images? it would seem what I want is to have a single image that contains the contents of other images e.g. neo4j and node.js</p>

<p>I've found no directive to declare dependencies in the reference manual.  are there no dependencies like in RPM where in order to run my image the calling context must first install the images it needs?</p>

<p>I'm confused...</p>
","<p>Docker file that's supposed to install cloud sdk and python 3.7 is skipping the cloud sdk and only running the python3.7 step.</p>
<p>Dockerfile:</p>
<pre><code>FROM google/cloud-sdk:247.0.0

FROM python:3.7
WORKDIR /test
COPY . .
RUN python3 -m pip install -U pip
</code></pre>
<ul>
<li>build image: <code>docker build -t test -t test .</code>
check python3.7 installation: <code>docker run test python3 --version</code> .
output: <code>Python 3.7.9</code></li>
<li>check gcloudsdk installation: <code>docker run test gcloud version</code>. output:  <code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;gcloud\&quot;: executable file not found in $PATH&quot;: unknown. ERRO[0000] error waiting for container: context canceled</code></li>
</ul>
"
"32163955","How to run shell script on host from docker container?","<docker>","64818493","How to callculate the free space on a host machine and get the information inside a docker container","<linux><docker><shell><webserver>","<p>How to control host from docker container?</p>

<p>For example, how to execute copied to host bash script?</p>
","<p>I want to run a shell script which calculates the free disc space on the host machine so that the docker container can use the information in one of its REST endpoints.</p>
<p>Is there a clean way to do this or should I try to solve the problem differently. Because I'm quite sure that it is not a good idea to create this coupling between host and container.</p>
"
"32163955","How to run shell script on host from docker container?","<docker>","64980331","Execute Host OS Command from Flask container","<python><linux><docker><kubernetes>","<p>How to control host from docker container?</p>

<p>For example, how to execute copied to host bash script?</p>
","<p>I have Flask application which is running docker container, I have to run host OS commands from Flask Application,</p>
<p>How to execute Host OS commands from Docker container?</p>
<p>My Docker image File:</p>
<pre><code>FROM python:3.8-slim
RUN mkdir /app
WORKDIR /app
ADD requirements.txt /app
RUN pip3 install -r requirements.txt
ADD . /app
RUN chmod +x ./entrypoint.sh
ENTRYPOINT [&quot;sh&quot;, &quot;entrypoint.sh&quot;]
</code></pre>
<p>app.py</p>
<pre><code>import subprocess
from flask import Flask
app = Flask(__name__)

app.config['DEBUG'] = True

# Run host command from container
def run_os_cmd():
    return subprocess.checkout([&quot;/usr/bin/some-cmd&quot;, &quot;few-options&quot;])

@app.route(&quot;/&quot;)
def hello_world():
    cmd_output = run_os_cmd()
    return cmd_output

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0')
</code></pre>
<p>Host is Linux</p>
<p>Note, I cannot install host command package inside docker container.</p>
"
"27273412","Cannot install packages inside docker Ubuntu image","<docker><ubuntu-14.04>","65007040","Can't find curl or sudo package in a ubuntu pod","<ubuntu><curl><kubernetes><sudo><apt-get>","<p>I installed Ubuntu 14.04 image on docker. After that, when I try to install packages inside the ubuntu image, I'm getting unable to locate package error: </p>

<pre><code>apt-get install curl

Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package curl
</code></pre>

<p>How to fix this error?</p>
","<p>I deployed a service on kubernetes and then I wanted to test it out inside the cluster itself, before I put an ingress rule to access it outside.</p>
<p>So, after I verified from the log that the service is running.</p>
<p>I tried to create a pod and get to the shell in it via</p>
<p><code>kubectl run -i --tty ubuntu --image=ubuntu --restart=Never -- sh</code></p>
<p>This seems to work fine. I am shown an ubuntu shell, and it seems to work fine.</p>
<p>But then if I do a curl: example</p>
<p><code>curl --location --request GET 'http://127.0.0.1:9000/hello'</code></p>
<p>I get a response
<code>sh: 4: curl: not found</code></p>
<p>So then I tried,</p>
<p><code>apt-get install curl</code>
which gives me:</p>
<pre><code>Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package curl
</code></pre>
<p>Thought this was because of sudo. so I did <code>sudo apt-get install curl</code>, but that tells me
<code>sh: 6: sudo: not found</code></p>
<p>Tried installing sudo with <code>apt-get install sudo</code> but that just gives</p>
<pre><code>Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package sudo
</code></pre>
<p>What am I missing here?</p>
"
"33443912","Commit to jenkins docker image does not save changes","<jenkins><docker>","65074264","Creating Docker image from a running container with its volume","<docker><jenkins>","<p>I pull the official Jenkins docker image from <a href=""https://hub.docker.com/_/jenkins/"">here</a>.
From Jenkins UI I create a new job , install the github plugin and set the repo urls in the job configuration.</p>

<p>Finally I save the changes from Jenkins.</p>

<p>I want to create a new image as it is. I stop the container, and commit it to a new image.</p>

<p>Then I start a new container from the new image...and Jenkins does not contain any of my changes.</p>

<p>I use <code>Docker version 1.6.2, build 7c8fca2</code></p>
","<p>I am trying to crate an image from one of my Jenkins container where I have create a pipeline. Now I want to share that whole Jenkins with those pipeline jobs as an image to my other teammates.</p>
<p>I tried  using docker commit</p>
<pre><code>docker commit -m &quot;Jenkins with AWS pipeline setup&quot; -a &quot;User&quot; 2b01xdxx9d8f pipeline/jenkins:v1
</code></pre>
<p>then I did <code>docker save pipeline/jenkins:v1 &gt; jenkins.tar</code>
and shared the .tar file with my team but when they loaded the .tar file  <code>docker load &lt; jenkins.tar</code>
they got back to fresh Jenkins my pipeline my users where not there . is there any way to load the data too in the image .</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65087636","""Docker-compose run"" shows backend running but can't access from host","<docker><flask><docker-compose><dockerfile><flask-sqlalchemy>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I was trying to use docker for my flask backend. I've written a docker file that uses <code>python:3.8 as build-python</code> then install all packages from <code>requirements.txt</code> file. I was  <code>PostgreSQL</code> database then written in <code>docker-compose.yml</code> file. When I put command in terminal</p>
<blockquote>
<p>sudo docker-compose run</p>
</blockquote>
<p>It shows <code>api_1  |  Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) </code> which looks good. But I can't access this from my host pc It shows <strong>This site can’t be reached</strong>
Here is my <code>docker-compose.yml</code> file</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;2&quot;

services:
  api:
    ports:
      - 5000:5000
    build:
      context: ./mymeds
      dockerfile: ./Dockerfile
    restart: unless-stopped
    networks:
      - mymeds-backend-tier
    depends_on:
      - db
    volumes:
      - ./mymeds/app/:/app/app:Z
    command: python manage.py run
    env_file: common.env

  db:
    image: library/postgres:11.1-alpine
    ports:
      - 5432:5432
    restart: unless-stopped
    networks:
      - mymeds-backend-tier
    volumes:
      - mymeds-db:/var/lib/postgresql
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=user123

volumes:
  mymeds-db:
    driver: local

networks:
  mymeds-backend-tier:
    driver: bridge

</code></pre>
<p>Here is my <code>Dockerfile</code> for <code>flask</code> backend</p>
<pre><code>FROM python:3.8 as build-python

RUN apt-get -y update \
  &amp;&amp; apt-get install -y gettext \
  &amp;&amp; apt-get clean \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY requirements.txt /app/
WORKDIR /app
RUN pip install -r requirements.txt

FROM python:3.8-slim


RUN apt-get update \
  &amp;&amp; apt-get install -y \
    libxml2 \
    libssl1.1 \
    libcairo2 \
    libpango-1.0-0 \
    libpangocairo-1.0-0 \
    libgdk-pixbuf2.0-0 \
    shared-mime-info \
    mime-support \
  &amp;&amp; apt-get clean \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY . /app
COPY --from=build-python /usr/local/lib/python3.8/site-packages/ /usr/local/lib/python3.8/site-packages/
COPY --from=build-python /usr/local/bin/ /usr/local/bin/
WORKDIR /app

EXPOSE 5000
ENV PORT 5000
</code></pre>
<p><a href=""https://i.stack.imgur.com/Auq3a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Auq3a.png"" alt=""enter image description here"" /></a></p>
"
"28721699","Root password inside a Docker container","<docker>","65117674","Docker Image payara/server-full:latest: How is the root password of this image","<docker><dockerhub><payara>","<p>I'm using a Docker image which was built using the USER command to use a non-root user called <code>dev</code>.
Inside a container, I'm ""dev"", but I want to edit the <code>/etc/hosts</code> file.</p>

<p>So I need to be root. I'm trying the su command, but I'm asked to enter the root password.</p>

<p>What's the default root user's password inside a Docker container?</p>
","<p>when I open a bash with</p>
<p><strong><code>docker exec -it &lt;IMAGE-ID&gt; bash</code></strong></p>
<p>I have a bash shell without root rights. Does this image has a default root password?</p>
<p>How can I get SU rights?</p>
"
"27989751","Mount SMB/CIFS share within a Docker container","<linux><windows><docker><mount><cifs>","65141704","Permission denied while mounting inside of container","<linux><docker><unix><permissions><docker-container>","<p>I have a web application running in a Docker container. This application needs to access some files on our corporate file server (Windows Server with an Active Directory domain controller). The files I'm trying to access are image files created for our clients and the web application displays them as part of the client's portfolio.</p>

<p>On my development machine I have the appropriate folders mounted via entries in <code>/etc/fstab</code> and the host mount points are mounted in the Docker container via the <code>--volume</code> argument. This works perfectly.</p>

<p>Now I'm trying to put together a production container which will be run on a different server and which doesn't rely on the CIFS share being mounted on the host. So I tried to add the appropriate entries to the <code>/etc/fstab</code> file in the container &amp; mounting them with <code>mount -a</code>. I get <code>mount error(13): Permission denied</code>.</p>

<p>A little research online led me to <a href=""http://opensource.com/business/14/9/security-for-docker"" rel=""noreferrer"">this article about Docker security</a>. If I'm reading this correctly, it appears that Docker explicitly denies the ability to mount filesystems within a container. I tried mounting the shares read-only, but this (unsurprisingly) also failed.</p>

<p>So, I have two questions:</p>

<ol>
<li><p>Am I correct in understanding that Docker prevents any use of <code>mount</code> inside containers?</p></li>
<li><p>Can anyone think of another way to accomplish this <strong>without</strong> mounting a CIFS share on the host and then mounting the host folder in the Docker container?</p></li>
</ol>
","<p>I've to mount external remote volume inside of container. To achieve this, I created a directory(where to mount) - <code>TMG</code> at root level with giving permission with command - <code>chmod -R 777 TMG</code>. Once entered in container, I hit command -</p>
<pre><code>sudo mount -t cifs '\\abcdomain\Data\Products\XYG\' /TMG -o username=username,dom=doamin,password=password
</code></pre>
<p>But this results into an error  - <code>mount: /TMG: permission denied.</code></p>
<p>Even though all permissions are given to the directory, I'm unable to get why this error is occurring. Please help to resolve this or any other way to achieve this..</p>
"
"32257830","What are the potential security problems running untrusted code in a Docker container as a non-root user?","<security><docker><multi-tenant>","65153042","Securing a Docker Container","<docker><security>","<p>I've seen plenty of ink spilled by now about how Docker is not sufficiently isolated to allow arbitrary containers to be run in a multi-tenant environment, and that makes sense. ""If it's root in Docker, consider it root in the host machine."" What about non-root though?</p>

<p>If I want to take some untrusted code and run it in a container, can it be done safely so long as the container is running as a non-root non-sudo user? What are the potential security pitfalls of doing something like that?</p>

<p>I'm fairly sure there are production applications doing this today (CI systems, runnable pastebins), but are they just lucky not to have had a determined attacker or is this a reasonable thing to do in a production system?</p>
","<p>Suppose I have a docker file which creates an image that may execute a possibly malicious python program.
The goal is to get the output of this program, but I don't want my host machine to become compromised. The
<code>Dockerfile</code> is as follows:</p>
<pre><code> FROM python3.7-alpine
 COPY possiblyMalicious.py .
 ENTRYPOINT [&quot;python3&quot;,&quot;possiblyMalicious.py&quot;]
</code></pre>
<p>So then I build an image from this docker file and run a respective container. My question is: Is there a way to configure my host system such that the effects of <code>possiblyMalicious.py</code> stays and dies within the container, and if so how?</p>
"
"33357567","ECONNREFUSED for Postgres on nodeJS with dockers","<node.js><postgresql><docker><sequelize.js>","65156757","Cannot connect to PostgreSQL when running docker compose","<postgresql><docker-compose><dockerfile>","<p>I'm building an app running on NodeJS using postgresql.
I'm using SequelizeJS as ORM.
To avoid using real postgres daemon and having nodejs on my own device, i'm using containers with docker-compose.</p>

<p>when I run <code>docker-compose up</code>
it starts the pg database</p>

<pre><code>database system is ready to accept connections
</code></pre>

<p>and the nodejs server.
but the server can't connect to database.</p>

<pre><code>Error: connect ECONNREFUSED 127.0.01:5432
</code></pre>

<p>If I try to run the server without using containers (with real nodejs and postgresd on my machine) it works.</p>

<p>But I want it to work correctly with containers. I don't understand what i'm doing wrong.</p>

<p>here is the <code>docker-compose.yml</code> file</p>

<pre><code>web:
  image: node
  command: npm start
  ports:
    - ""8000:4242""
  links:
    - db
  working_dir: /src
  environment:
    SEQ_DB: mydatabase
    SEQ_USER: username
    SEQ_PW: pgpassword
    PORT: 4242
    DATABASE_URL: postgres://username:pgpassword@127.0.0.1:5432/mydatabase
  volumes:
    - ./:/src
db:
  image: postgres
  ports:
  - ""5432:5432""
  environment:
    POSTGRES_USER: username
    POSTGRES_PASSWORD: pgpassword
</code></pre>

<p>Could someone help me please?</p>

<p>(someone who likes docker :) )</p>
","<p>I am trying to dockerize my server and db with docker compose. However, I cannot make the server connect to db when I try to run migration with Sequelize. Below are my Dockerfile and docker-compose.yml:</p>
<p>Dockerfile:</p>
<pre><code>FROM node:14-alpine
RUN mkdir -p /app
WORKDIR /app
ADD . /app
RUN yarn install
RUN yarn sequelize db:migrate
EXPOSE 9000
CMD [ &quot;yarn&quot;, &quot;start&quot; ]
</code></pre>
<p>docker-compose.yml:</p>
<pre><code>version: &quot;3&quot;
services:
  db:
    image: postgres:10
    container_name: &quot;postgres&quot;
    environment: 
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: mydatabase
    ports:  
      - '5432:5432'
    volumes: 
      - my-db:/var/lib/postgresql/data
  server:
    build: 
      context: .
    environment: 
      DB_USER: postgres
      DB_PASSWORD: postgres
      DB_NAME: mydatabase
    depends_on:
      - db
    ports: 
      - 8100:8100
volumes:
  my-db:
</code></pre>
<p>And here is the error log while doing the migration:</p>
<pre><code>ERROR: connect ECONNREFUSED 127.0.0.1:5432

error Command failed with exit code 1.
</code></pre>
<p>I already try the suggestions from <a href=""https://stackoverflow.com/questions/33357567/econnrefused-for-postgres-on-nodejs-with-dockers"">here</a>, but change the db url cannot solve my issue. It returns</p>
<blockquote>
<p>ERROR: getaddrinfo ENOTFOUND postgres://username:pgpassword@127.0.0.1:5432/mydatabase</p>
</blockquote>
<p>(I already change the username, password, and db name).</p>
<p>Could someone figure out what goes wrong and how to fix this?</p>
<p><strong>UPDATE:</strong>
I try to change DB_URL to db which is same as the service name of db. Then the docker runs successfully. The idea came from the setup in this <a href=""https://www.reddit.com/r/docker/comments/8szjw0/how_to_connect_to_postgresql_using_dockercompose/"" rel=""nofollow noreferrer"">post</a>.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65271662","build docker image for datetime flask api in python","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I make a docker image for datetime flask api in python. I cannot see the output on the specified port when I visit browser on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a></p>
<p>flask api python code <strong>app.py</strong></p>
<pre><code>from flask import Flask,jsonify
from datetime import datetime
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    now = datetime.now()
    return jsonify( {&quot;Date and Time: &quot;: now})

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM python:3.7.4
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
ENTRYPOINT [&quot;python&quot;]
CMD [&quot;app.py&quot;]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>flask==1.1.2
</code></pre>
<p>The three files [ 'app.py', 'Dockerfile', 'requirements.txt'] are in 'my_docker_flask' folder. I run the following commands inside the folder</p>
<p>I build and run the docker</p>
<pre><code>docker build -t my_docker_flask:latest .
docker run  -p 5000:5000 my_docker_flask:latest
</code></pre>
<p>I cannot see the output on the specified port when I visit browser on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a>
What am I doing wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65273595","Flask API endpoint is not accessible from docker","<python><python-3.x><docker><flask><docker-compose>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have simple flask app which uses SQLLite database.</p>
<p>Here is my <code>app.py</code> code</p>
<pre><code>from flask import Flask
from flask_restful import Api
from flask_jwt import JWT
from datetime import timedelta

from db import db
from security import authenticate, identity
from resources.user import UserRegister
from resources.product import Product, ProductList


app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///data.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.secret_key = '@hsgvTyb@3gd###123'
api = Api(app)


@app.before_first_request
def create_tables():
  db.create_all()


jwt = JWT(app, authenticate, identity)  # /auth
app.config['JWT_EXPIRATION_DELTA'] = timedelta(seconds=1800)

api.add_resource(Product, '/product/&lt;string:name&gt;')
api.add_resource(ProductList, '/products')
api.add_resource(UserRegister, '/register')


if __name__ == '__main__':
  db.init_app(app)
  app.run(port=5000, debug=False)
</code></pre>
<p>DockerFile as below:</p>
<pre><code>FROM python:3
ADD app/ /app
WORKDIR /app
RUN apt-get update
RUN pip install -r requirements.txt
CMD python app.py
</code></pre>
<p>docker-compose.yml as below:</p>
<pre><code>version: &quot;2&quot;
services:
  app:
    container_name: test-146957-app
    build:
      context: ../
      dockerfile: deployments/app/Dockerfile
    ports:
      - &quot;5000:5000&quot;
</code></pre>
<p>Now when I execute <code>docker-compose up</code> command it is showing container is running but I hit any endpoints from postman, it does not return any response.</p>
<p><a href=""https://i.stack.imgur.com/PTa9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PTa9c.png"" alt=""enter image description here"" /></a></p>
"
"32257830","What are the potential security problems running untrusted code in a Docker container as a non-root user?","<security><docker><multi-tenant>","65278153","run user supplied python code inside docker. Is it safe and also speed up the process","<python><docker><subprocess>","<p>I've seen plenty of ink spilled by now about how Docker is not sufficiently isolated to allow arbitrary containers to be run in a multi-tenant environment, and that makes sense. ""If it's root in Docker, consider it root in the host machine."" What about non-root though?</p>

<p>If I want to take some untrusted code and run it in a container, can it be done safely so long as the container is running as a non-root non-sudo user? What are the potential security pitfalls of doing something like that?</p>

<p>I'm fairly sure there are production applications doing this today (CI systems, runnable pastebins), but are they just lucky not to have had a determined attacker or is this a reasonable thing to do in a production system?</p>
","<p>I have a user supplied python code.</p>
<p>Intially i wanted to use exec, but later learning the security loopholes i dropped</p>
<p>Now I am thinking of using docker to run in an isolation, so nothing bad can happen outside it</p>
<p>But the problem is i have more than 100K custom codes to run</p>
<p>so i wanted to use multiprocessing.</p>
<p>So putting together multiprocesing and running code using docker container with python i have to do something like</p>
<pre><code>def getScoreForEachMethod(customCode):
    process = subprocess.Popen(docker pythonImage sh python -c customCode)
    process.wait
    return score (i.e capture the output from subprocess)

if __name__ == &quot;__main__&quot;:
    #pool = multiprocessing.Pool(joblib.cpu_count()) 
    pool = multiprocessing.Pool(4)
    results = pool.map(getScoreForEachMethod, [list of custom methods])
    pool.close()
    pool.join()

</code></pre>
<p>But inside the multiprocess calling subprocess will definitely make it slow.</p>
<p>the other option run the entire multiprocessing inside docker i.e</p>
<pre><code>def getScoreForEachMethod(customCode):
    score =  output from customcode
    return score 

if __name__ == &quot;__main__&quot;:
    #pool = multiprocessing.Pool(joblib.cpu_count()) 
    pool = multiprocessing.Pool(4)
    results = pool.map(getScoreForEachMethod, [list of custom methods])
    pool.close()
    pool.join()

</code></pre>
<p>but here any customcode is malacious (like delete some system files inside docker), it will effect the running of rest of the scirpts.</p>
<p>Can we run docker command using subprocess more faster so that we can get benefit of noninterruption and isolation, where in the later speed is gauranteed by interuption is not</p>
"
"29663459","Python app does not print anything when running detached in docker","<python><docker><dockerfile>","65435090","When the docker startup parameter is -d, the flask application can print on the docker console, but not when the startup parameter is -it","<python><docker><flask><dockerfile>","<p>I have a Python (2.7) app which is started in my dockerfile:</p>

<pre><code>CMD [""python"",""main.py""]
</code></pre>

<p><em>main.py</em> prints some strings when it is started and goes into a loop afterwards:</p>

<pre><code>print ""App started""
while True:
    time.sleep(1)
</code></pre>

<p>As long as I start the container with the -it flag, everything works as expected:</p>

<pre><code>$ docker run --name=myapp -it myappimage
&gt; App started
</code></pre>

<p>And I can see the same output via logs later:</p>

<pre><code>$ docker logs myapp
&gt; App started
</code></pre>

<p>If I try to run the same container with the -d flag, the container seems to start normally, but I can't see any output:</p>

<pre><code>$ docker run --name=myapp -d myappimage
&gt; b82db1120fee5f92c80000f30f6bdc84e068bafa32738ab7adb47e641b19b4d1
$ docker logs myapp
$ (empty)
</code></pre>

<p>But the container still seems to run;</p>

<pre><code>$ docker ps
Container Status ...
myapp     up 4 minutes ... 
</code></pre>

<p>Attach does not display anything either:</p>

<pre><code>$ docker attach --sig-proxy=false myapp
(working, no output)
</code></pre>

<p>Any ideas whats going wrong? Does ""print"" behave differently when ran in background?</p>

<p>Docker version:</p>

<pre><code>Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.2
Git commit (client): a8a31ef
OS/Arch (client): linux/arm
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.2
Git commit (server): a8a31ef
</code></pre>
","<p>I have a flask application that is very simple, without any operation, the code is as follows, just for testing:</p>
<pre><code>from flask import Flask
app1 = Flask(__name__)
@app1.route('/', methods=['POST'])
def send():
    print(1111111111111111)
    a = &quot;ok&quot;
    print(1111111111111111)
    return a
app1.run(debug=False,host='0.0.0.0',port=8060)
</code></pre>
<p>Then I built him into a image through <code>Dockerfile</code>:</p>
<pre><code>From python:3.6.12
COPY ./a.py /usr/
RUN pip3 install -i https://mirrors.aliyun.com/pypi/simple/ flask
CMD python3 /usr/a.py
</code></pre>
<p>My understanding is that when I pass <code>docker run -p12345:8060 -d image_name</code>, Then I access the interface through the following simple code, I think the interface should output two lines of &quot;111111111111&quot; logs, but it is not</p>
<pre><code>import requests

url=&quot;http://127.0.0.1:12345&quot;
data = {
    &quot;server_id&quot;: 1,
}
header = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;charset&quot;: &quot;utf-8&quot;
}
res = requests.post(url=url,json=data,headers=header)
print(res.text)
</code></pre>
<p><a href=""https://i.stack.imgur.com/m8QHG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m8QHG.png"" alt=""enter image description here"" /></a></p>
<p>We can see in the picture when the startup parameter is <code>-it</code>, there is print, but when the parameter is <code>-d</code>, I get the log through docker logs, but I don't see print. The CMD of my image is to start this python code, so the print in the code should be output on the docker console, so no matter I start docker through <code>-it</code> or <code>-d</code>, the docker logs command can get the output of print.</p>
<p>Isn't it?</p>
"
"30740828","Commit data in a mysql container","<mysql><linux><database><docker><virtualization>","65482012","Docker commit does not actually commit","<mysql><docker><docker-compose><joomla><commit>","<p>I created a mysql container using the officially supported <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql image</a>. I ran the image mounting a folder that contains a sql dump, then I created a new database in the container and imported the .sql dump in it:</p>

<pre><code>sudo docker run --name mysql-psat1 -v /opt/Projets/P1/sqldumps:/mnt -e MYSQL_ROOT_PASSWORD=secret -d mysql:latest
sudo docker exec -it mysql-psat1 bash
&gt; mysql -uroot -psecret -e 'create database liferay_psat1;'
&gt; mysql -uroot -psecret liferay_psat1 &lt; /mnt/liferay_sql_dump.sql
</code></pre>

<p>Then I listed the running containers to get that container's id:</p>

<pre><code>sudo docker ps -a
</code></pre>

<p>Then, I commited the container (with the imported sql) as a new container image</p>

<pre><code>sudo docker commit -m ""Imported liferay sql dump"" &lt;id-of-the-container&gt; jihedamine/mysql-psat1:v1
</code></pre>

<p>However, when if I start a container using that new image, the mysql database doesn't contain the newly created database liferay_psat1.</p>

<pre><code>sudo docker run -ti jihedamine/mysql-psat1:v1 bash
&gt; mysql -uroot -psecret
# show databases;
</code></pre>

<p>What am I doing wrong?</p>

<p>Thanks for your help!</p>
","<p>(I am new to docker so patience please)
NOTE: I DO NOT USE VOLUMES so the answers in <a href=""https://stackoverflow.com/questions/30740828/commit-data-in-a-mysql-container"">this question</a> do not apply</p>
<ol>
<li>I am creating two containers from two docker images (jomla+apache+php and mysql) using the docker-compose file listed at the end. Everything works fine.</li>
<li>I can connect to the Joomla installation page I go through the initial configuration, connect to the database, all is well again. (I can browse to the home &amp; administration pages now). Even the installation folder is removed just fine (mandatory step in joomla). I can see the new files and new database created/modified fine within the containers.</li>
<li>I now commit the two running containers to two new images:  <em>docker commit CONTAINER_ID new_image_name</em></li>
<li>The new images are created fine</li>
<li>I remove the containers with <em>docker-compose down</em></li>
<li>I modify the docker-compose.yml file to use the newly committed images respectively for joomla and mysql</li>
<li>I issue <em>docker-compose up -d</em></li>
<li>And here comes the frustration: when I browse to the page, I am back to the Joomla installation page, the config files are gone, the database is empty and installation folder reappeared. In other words, nothing was committed in either image.
I even removed the older images to exclude any possibility of accidentally using them.</li>
</ol>
<p>Why is docker not committing these changes?</p>
<p>If I do manual changes (like <em>docker cp ...</em> or <em>docker exec ...</em> or even inside the container) those stick and are committed just fine.
I spent two full days on this, any help is appreciated.</p>
<p>docker-compose.yml:</p>
<pre><code>version: '3.8'

networks:
  frontend:
  backend:

services:

  mysql_db:
    container_name: mysql
    image: mysql_me:latest
    command: mysqld --innodb-buffer-pool-size=20M
    restart: on-failure
    environment:
      MYSQL_DATABASE: 'joomla'
      MYSQL_USER: 'joomla_user'
      MYSQL_PASSWORD: 'JPassword'
      MYSQL_ROOT_PASSWORD: 'RPassword'
      MYSQL_ROOT_HOST: '%'
    ports:
      - '3306:3306'
    expose:
      - '3306'
    networks:
      - backend

  joomla:
    container_name: joomla
    image: joomla_me:latest
    restart: always
    ports:
      - &quot;443:443&quot;
      - &quot;8080:80&quot;
    environment:
      JOOMLA_DB_HOST: 'mysql_db:3306'
      JOOMLA_DB_USER: 'joomla_user'
      JOOMLA_DB_PASSWORD: 'JPassword'
      JOOMLA_DB_NAME: 'joomla'
    links:
      - mysql_db:3306
    networks:
      - frontend
      - backend
    depends_on:
      - mysql_db
</code></pre>
"
"31324981","How to access host port from docker container","<docker><docker-container>","65648295","Connect to a local PostgreSQL database from a docker-compose service","<python><postgresql><docker><docker-compose>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<h3>Context</h3>
<p>I need to connect from inside a compose service to a PostgreSQL (10) database which is on my local host machine (Ubuntu 18.04).</p>
<h3>Description of the errors</h3>
<p>I have set <code>service &gt; app &gt; build &gt;</code> to <code>network: host</code> in my compose file:</p>
<pre><code>version: '3.8'
  app:
    image: myservice:0.0.1
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
      network: host
</code></pre>
<p>The Dockerfile is build <code>FROM python:3.9.1-slim-buster</code>.</p>
<p>Then, from inside my compose service (<code>docker exec -it composefilefolder_app_1 bash</code>) I try to connect to a PostgreSQL database on the host machine with <code>psycopg2</code> (after having started <code>python</code>):</p>
<pre><code>import psycopg2 

DBPARAMS = {
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost',
    'port': '5432',
    'dbname': 'mydatabase'
}

conn = psycopg2.connect(**DBPARAMS)
</code></pre>
<p>but this returns:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
    Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
    TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
    Is the server running on host &quot;localhost&quot; (::1) and accepting
    TCP/IP connections on port 5432?
</code></pre>
<p>Then I tried to use my host machine IP (given by <code>ifconfig</code>):</p>
<pre><code>import psycopg2 

DBPARAMS = {
    'user': 'postgres',
    'password': 'password',
    'host': '192.168.x.1',
    'port': '5432',
    'dbname': 'mydatabase'
}

conn = psycopg2.connect(**DBPARAMS)
</code></pre>
<p>but now I got a time-out after one minute:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection timed out
    Is the server running on host &quot;192.168.x.1&quot; and accepting
    TCP/IP connections on port 5432?
</code></pre>
<p>All other parameters are correct as I can connect to this database by running the same command from the host machine directly.</p>
<h3>Question</h3>
<p>How can I connect to my host machine PostgreSQL database from my compose service?</p>
<h3>What else I've tried</h3>
<ol>
<li><a href=""https://stackoverflow.com/questions/51313852/docker-compose-cannot-connect-to-database"">This</a> thread seemed promising but it doesn't work either.</li>
<li>And <a href=""https://stackoverflow.com/questions/48350843/how-to-connect-from-docker-compose-to-host-postgresql"">this one</a> is for Mac users.</li>
<li>I also tried to set <code>list_adressess='*'</code> in my local machine <code>postgres.conf</code> file AND to set <code>host all all 0.0.0.0/0 md5</code> into the <code>pg_hba.conf</code> file without any changes. But if I can avoid to tweak my local PostgreSQL config that would be great.</li>
<li><a href=""https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container"">This</a> thread also did not help as it makes use of my local machine IP, for which the connection is timing-out as I said.</li>
</ol>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","65660287","Use features from different base dockers","<docker>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>Imagine that I want to build and run an application in docker. This application requires 2 things:</p>
<ol>
<li>ROS</li>
<li>NVIDIA</li>
</ol>
<p>Docker containers already exist that have the features that I need:</p>
<ol>
<li><code>ros:noetic-ros-base-focal</code></li>
<li><code>nvidia/opengl:1.0-glvnd-runtime-ubuntu20.04</code></li>
</ol>
<p>I want everything that both these dockers install using <code>apt-get</code>, is it possible without rewriting one of them to use the base of another?</p>
"
"33913020","Docker remove <none> TAG images","<docker>","65667913","docker image prune not removing intermediate images","<docker>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>i have a problem with docker images.
If i run <code>docker info</code> i get this output</p>
<p><a href=""https://i.stack.imgur.com/lyptp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lyptp.png"" alt=""enter image description here"" /></a></p>
<p>saying 22 images are available.
When i run <code>docker images -a</code> indeed 22 images are shown:</p>
<p><a href=""https://i.stack.imgur.com/aX88G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aX88G.png"" alt=""enter image description here"" /></a></p>
<p>All those image tagged with <code>None</code> i suppose that are intermediate images.
I try to run <code>docker image prune</code> but it says</p>
<blockquote>
<p>Total reclaimed space: 0B</p>
</blockquote>
<p>Why prune command isn't working? Thanks</p>
<p>Docker version: 19.03.5.
Host: centos 7</p>
<p>Grazie</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65698602","Can't connect to docker container on localhost","<docker>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I am trying to put a very simple Python API that returns some text and logs the timestamps of sent requests and what not. And if ran on the machine directly, it works fine, but when I try to use it in a Docker, I get no connection of any kind while trying to connect to 127.0.0.1 or localhost.</p>
<p>Here's the Dockerfile</p>
<pre><code>FROM alpine
FROM python
FROM tiangolo/uwsgi-nginx-flask:python3.8
LABEL MAINTAINER=&quot;Asd &lt;asd@asd.com&quot;
COPY ./app /app
EXPOSE 5000


ENTRYPOINT [&quot;python3&quot;, &quot;/app/main.py&quot;]
</code></pre>
<p>I run the container with</p>
<pre><code>docker run --name unicorns -p 127.0.0.1:5000:5000 unicorn-image:latest
</code></pre>
<p>Thanks in Advance</p>
<p>EDIT1: Python code</p>
<pre><code># coding=utf8
import flask,json,sys,socket

import logging
log = logging.getLogger('werkzeug')
log.setLevel(logging.INFO)
logging.basicConfig(filename='./demo.log', level=logging.INFO)

from datetime import datetime


app = flask.Flask(__name__)
app.config[&quot;DEBUG&quot;] = True

@app.route('/', methods=['GET'])


def home():
    test_date = datetime.now()
    app.logger.info('%s', json.dumps(test_date, indent=4, sort_keys=True, default=str))
    return &quot;&lt;h1&gt;You are a unicorn!&lt;/h1&gt;&quot;

if __name__ == '__main__':
    app.run()
</code></pre>
"
"34254200","How to pass arguments to a Dockerfile?","<docker><dockerfile><docker-compose><docker-registry><dockerhub>","65931367","How to pass arguments to a bash script through Dockerfile","<bash><docker>","<p>I am using RUN instruction within a <code>Dockerfile</code> to install a rpm</p>

<pre><code>RUN yum -y install samplerpm-2.3
</code></pre>

<p>However, I want to pass the value ""2.3"" as an argument.
My RUN instruction should look something like:</p>

<pre><code>RUN yum -y install samplerpm-$arg
</code></pre>

<p>where <code>$arg=2.3</code></p>
","<p>I have a simple <code>Dockerfile</code> as below:</p>
<pre><code>FROM kalilinux/kali-rolling

WORKDIR /attack

COPY . /attack
RUN ls
RUN chmod +x attack.sh
RUN ./attack.sh $aws_access_key_id $aws_secret_access_key $default_region $bucket
</code></pre>
<p>And I have this <code>attack.sh</code> and it contains content as below:</p>
<pre><code>#!/bin/bash

# Getting the host passed as an argument to the script
/usr/share/zaproxy/zap.sh -cmd -addoninstall exportreport
mkdir -p /root/test

test() {
    echo &quot;access key = ${1}&quot;
    echo &quot;secret key = ${2}&quot;
    echo &quot;default region = ${3}&quot;
    echo &quot;bucket = ${4}&quot;
    echo &quot;line = ${5}&quot;

}

while IFS= read -r line; do
    # echo $line
    test $1 $2 $3 $4 $line 
done &lt; domains.txt

</code></pre>
<p>And if you are wondering how I pass the value to the <code>Dockerfile</code>, it is like this:</p>
<pre><code>docker build --build-arg aws_access_key_id=${SPOT_RUNNER_ACCESS_KEY} --build-arg aws_secret_access_key=${SPOT_RUNNER_SECRET_KEY} --build-arg default_region=ap-southeast-2 --build-arg bucket=${BUCKET_NAME} -t run-test .
</code></pre>
<p>So the arguments are passed in below order</p>
<pre><code>docker build ---&gt; Dockerfile ---&gt; attack.sh
</code></pre>
<p>But, this is not working, it gives an empty values for these variables.</p>
<p>Can someone please help me?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","66071282","Docker Container server not sending data","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I've just started learning docker and I'm trying to run a very simple flask server in a container. Whenever I run it, it works just fine. However, when I access the website, chrome displays this:</p>
<pre><code>This page isn’t working
127.0.0.1 didn’t send any data.
ERR_EMPTY_RESPONSE
</code></pre>
<p>Here is my code:</p>
<pre><code>from flask import Flask

app = Flask(__name__)

@app.route(&quot;/&quot;)
def index():
    return &quot;Hello World&quot;

if __name__ == &quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
<p>The Dockerfile:</p>
<pre><code>FROM python:3

RUN mkdir /test
WORKDIR /test
RUN pip3 install flask

COPY . .
EXPOSE 5000

cmd [&quot;python3&quot;, &quot;application.py&quot;]
</code></pre>
<p>And the commands I'm using to build the image and run the container:</p>
<pre><code>docker build . -t test
docker start -p 5000:5000 test
</code></pre>
<p>Could someone help me figure this out?</p>
"
"28089344","Docker, what is it and what is the purpose","<docker>","66073824","why is 'sys.platform' output different between 'docker run python' and just 'python'?","<linux><docker>","<p>I've heard about Docker some days ago and wanted to go across.</p>

<p>But in fact, I don't know what is the purpose of this ""container""? </p>

<p>What is a container?</p>

<p>Can it replace a virtual machine dedicated to development?</p>

<p>What is the purpose, in simple words, of using Docker in companies? The main advantage?</p>
","<p>I'm really new to Docker, and I'm using Mac.
I heard that docker is not VM, and docker is built upon the existing OS of the machine.
Then why does that difference occur? Are all docker containers running on Linux? Then what is the difference btw VM and docker?</p>
<pre><code>python
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.platform
'darwin'

docker run --rm -it python
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.platform
'linux'
</code></pre>
"
"35406213","How to copy docker volume to local?","<docker><boot2docker><docker-volume>","66243623","How to switch from docker named volumes to path based volumes without losing the container data?","<docker><persistent-volumes>","<p>I create a docker volume ""hello"", and it contains some data . </p>

<p>How can i copy it to local ?</p>

<p><strong>First :</strong> </p>

<pre><code>kerydeMacBook-Pro:~ hu$ docker volume create --name hello
hello
</code></pre>

<p><strong>checking :</strong></p>

<pre><code>kerydeMacBook-Pro:~ hu$ docker volume ls
DRIVER              VOLUME NAME
local               hello
</code></pre>

<p><strong>volume ""hello"" inspect</strong></p>

<pre><code>kerydeMacBook-Pro:~ hu$  docker volume inspect hello
[
    {
        ""Name"": ""hello"",
        ""Driver"": ""local"",
        ""Mountpoint"": ""/mnt/sda1/var/lib/docker/volumes/hello/_data""
    }
]
</code></pre>

<p><strong>How can i copy volume ""hello"" to local ?</strong></p>

<blockquote>
  <p>I try :</p>
</blockquote>

<pre><code>kerydeMacBook-Pro:~ hu$  docker cp hello:/mnt/sda1/var/lib/docker/volumes/hello/_data /Users/hu/Desktop/12
Error response from daemon: no such id: hello
</code></pre>

<p><strong>It not works as expect !</strong></p>

<p>Who can help me ?</p>
","<p>Im runing several docker containers on my raspberry pi. All containers are using named volumes to store persistent data. But since I often need to edit config files etc from the docker volumes I prefer to use path based volumes instead of named volumes that are managed by docker.</p>
<p>I first tought that I just could copy all the content from</p>
<pre><code>/var/lib/docker/volumes/
</code></pre>
<p>to a folder on my home directory and remove all containers and rerun them with the new path based volumes.
But unfortunately this seems not to work. For example if I rerun portainer with the new path based volume (which is just the folder that I copied from /var/lib/docker/volumes/ ) I need to create a new user etc. as if portainer could not use the copied data. I already used chown to add permissions for the current user.</p>
<p>Hope someone can help.</p>
"
"32723111","How to remove old and unused Docker images","<docker><docker-image>","66361693","How do I delete multiple images in docker?","<docker>","<p>When running Docker for a long time, there are a lot of images in system. How can I remove all unused Docker images at once safety to free up the storage?</p>

<p>In addition, I also want to remove images pulled months ago, which have the correct <code>TAG</code>.</p>

<p>So, I'm not asking for removing untagged images only. I'm searching for a way to remove general unused images, which includes both untagged and other images such as pulled months ago with correct <code>TAG</code>.</p>
","<p>I have created a lot of images but I don't know how to delete them efficiently.</p>
<p>Does anyone know good command to remove the images smartly?</p>
<p>Thank you very much!</p>
"
"31448821","How to write data to host file system from Docker container","<docker>","66397825","upload file with java to root of linux from docker container","<java><docker><api><file>","<p>I have a Docker container which is running some code and creating some HTML reports. I want these reports to be published into a specific directory on the host machine, i.e. at <code>/usr/share/nginx/reports</code></p>

<p>The way I have gone about doing this is to mount this host directory as a data volume, i.e. <code>docker run -v /usr/share/nginx/reports --name my-container com.containers/my-container</code></p>

<p>However, when I ssh into the host machine, and check the contents of the directory <code>/usr/share/nginx/reports</code>, I don't see any of the report data there.</p>

<p>Am I doing something wrong?</p>

<p>The host machine is an Ubuntu server, and the Docker container is also Ubuntu, no boot2docker weirdness going on here.</p>
","<p>i want to save the files that i upload to the java spring boot applicatiob to the root of ubuntu 18.
my application in docker container and start with docker compose.
after upload, my files save to</p>
<p>var/lib/docker/overlay2/jdjdsbe737bdj7837bdjwhs73828ndjdjd......</p>
<p>but i like save to this :</p>
<p>~/attachment/my-files    // without hash names and with my chosen name in my application</p>
<p>my code:</p>
<pre><code>String UPLOADED_FOLDER = &quot;/root/attachments/&quot;;
Files.createDirectories(Paths.get(UPLOADED_FOLDER + FileOwnerType.valueOfIndex(fileOwnerType).getTitle()));
            bytes = file.getBytes();
            Path path = Paths.get(UPLOADED_FOLDER + FileOwnerType.valueOfIndex(fileOwnerType).getTitle() + &quot;/&quot; + fileName + &quot;.&quot; + fileExtension);
            Files.write(path, bytes);
</code></pre>
"
"39445047","Python Tkinter in Docker .TclError: couldn't connect to display","<python><ubuntu><docker><tkinter><xfce>","59576282","Running a Tkinter application inside a Docker container","<python><python-3.x><docker><tkinter>","<p>I am new to python and am trying to build a small app. It needs to be a GUI app and I was wanting to containerise it with docker. I am getting the following error and can not find a solution</p>

<pre><code>No protocol specified
No protocol specified
Traceback (most recent call last):
  File ""tkinker.py"", line 7, in &lt;module&gt;
    tinker = Tk()
  File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 1818, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display "":0.0""
</code></pre>

<p>It starts locally but wont start in docker. My OS is Xubuntu.</p>

<p>I have created a sample app (below), see run-test.sh <a href=""https://github.com/jeremysells/test/tree/master/docker-tkinter"" rel=""nofollow"">https://github.com/jeremysells/test/tree/master/docker-tkinter</a></p>
","<p>I wrote a small application in Python3 using <a href=""https://docs.python.org/3/library/tkinter.html"" rel=""nofollow noreferrer"">Tkinter</a>. I would like to containerize this application in order to reduce any installation and dependency hassle. </p>

<p>I wrote the following Dockerfile:</p>

<pre><code>FROM python:3.6-buster

WORKDIR /home/python/app

COPY . .

RUN pip3 install -r requirements.txt

# Install my package
RUN python3 -m pip install --user --upgrade setuptools wheel
RUN python3 setup.py install

CMD [ ""charts-app"" ]
</code></pre>

<p>If I run the same commands outside Docker, everything works. </p>

<p>Inside Docker, I am able to build the image, but when I run it, the following message is displayed:</p>

<pre><code>File ""/usr/local/lib/python3.6/tkinter/__init__.py"", line 2023, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable
</code></pre>

<p>I understand that as the application is running inside the container, it has not directly access to a Graphical Interface. But, is there any way that I can ""force"" the container to use the <code>host</code> as its Graphical Interface? </p>
"
"35511604","Docker: unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx","<docker><dockerfile><docker-toolbox><docker-build>","59589257","Make command failing in RHEL but passes in ubuntu","<linux><makefile><cmake><centos>","<p>I just downloaded Docker Toolbox for Windows 10 64bit today.  I'm going through the tutorial.  I'm receving the following error when trying to build an image using a Dockerfile.</p>

<p>Steps:</p>

<ul>
<li>Launched Docker Quickstart terminal.</li>
<li>testdocker after creating it.</li>
<li>Prepare Dockerfile as documented in ""Build your own image"" web link</li>
<li>ran below command</li>
</ul>

<p><code>docker build -t docker-whale .</code></p>

<p><code>Error: $ docker build -t docker-whale .</code></p>

<p><code>unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx C:\Users\Villanueva\Test\testdocker\Dockerfile: The system cannot find the file specified.</code></p>

<p><strong>BTW:</strong> I tried several options mentioned @ <a href=""https://github.com/docker/docker/issues/14339"" rel=""noreferrer"">https://github.com/docker/docker/issues/14339</a></p>

<pre><code>    $ docker info
    Containers: 4
     Running: 0
     Paused: 0
     Stopped: 4
    Images: 2
    Server Version: 1.10.1
    Storage Driver: aufs
     Root Dir: /mnt/sda1/var/lib/docker/aufs
     Backing Filesystem: extfs
     Dirs: 20
     Dirperm1 Supported: true
    Execution Driver: native-0.2
    Logging Driver: json-file
    Plugins:
     Volume: local
     Network: bridge null host
    Kernel Version: 4.1.17-boot2docker
    Operating System: Boot2Docker 1.10.1 (TCL 6.4.1); master : b03e158 - Thu Feb 11 22:34:01 UTC 2016
    OSType: linux
    Architecture: x86_64
    CPUs: 1
    Total Memory: 996.2 MiB
    Name: default
    ID: C7DS:CIAJ:FTSN:PCGD:ZW25:MQNG:H3HK:KRJL:G6FC:VPRW:SEWW:KP7B
    Debug mode (server): true
     File Descriptors: 32
     Goroutines: 44
     System Time: 2016-02-19T17:37:37.706076803Z
     EventsListeners: 0
     Init SHA1:
     Init Path: /usr/local/bin/docker
     Docker Root Dir: /mnt/sda1/var/lib/docker
    Labels:
     provider=virtualbox
</code></pre>
","<p>Stuck with weird problem while building image of <a href=""https://github.com/kubernetes-sigs/service-catalog"" rel=""nofollow noreferrer"">Service-Catalog</a> on red hat linux, after running <code>make</code> following error occurs while same <code>make</code> works perfectly fine on ubuntu. Following is the error</p>

<pre><code>sed ""s/GO_VERSION/1.12/g"" &lt; build/build-image/Dockerfile | \
  docker build -t scbuildimage -f - .
unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /root/go_projects/src/github.com/kubernetes-sigs/service-catalog/-: no such file or directory
make: *** [.scBuildImage] Error 1

</code></pre>

<p>Its failing while executing following line.</p>

<pre><code>.scBuildImage: build/build-image/Dockerfile $$(shell sh -c ""docker inspect scbuildimage"" &gt; /dev/null 2&gt;&amp;1 || echo .forceIt)
    mkdir -p .cache
    mkdir -p .pkg
    sed ""s/GO_VERSION/$(GO_VERSION)/g"" &lt; build/build-image/Dockerfile | \
      docker build -t scbuildimage -f - .
    touch $@
</code></pre>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","59596564","Unable to install Psycopg2 in Docker alpine image","<python><postgresql><docker><docker-compose><dockerfile>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>I am trying to run docker for an alpine image but unable to install psycopg2 from requirements.txt, tried all the solutions for similar problems but none worked.</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM frolvlad/alpine-miniconda3:python3.7 as base

FROM base as builder
# RUN apt-get update \
#     &amp;&amp; apt-get install -y build-essential \
#     &amp;&amp; apt-get install -y nginx 
WORKDIR /usr/src/app
COPY ./requirements.txt /usr/src/app/requirements.txt
RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=""/opt/venv/bin:$PATH""
RUN pip install gunicorn


RUN pip install -r requirements.txt


FROM base
RUN apk add nginx bash
# RUN conda install python==3.7.1
COPY --from=builder /usr/src/app /usr/src/app
COPY --from=builder /opt/venv /opt/venv
ENV PATH=""/opt/venv/bin:$PATH""

COPY . /usr/src/app/rapid-nrn-backend
WORKDIR /usr/src/app/rapid-nrn-backend

CMD [""/bin/bash"", ""entrypoint.sh""]
EXPOSE 80
EXPOSE 8000
</code></pre>

<p><strong>requirements.txt</strong>
<a href=""https://drive.google.com/open?id=1XTOG9PxnzfwqI5sCb6UE5RdlLor590hn"" rel=""nofollow noreferrer"">requirements.txt</a></p>

<p>Any help is highly appreciated.</p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","59632583","Jenkins inside a container do not find docker command","<docker><jenkins><jenkins-pipeline>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I'm trying to run a container inside another. Where the first it's based in a jenkins image and the second in a gcc image (only for test).  </p>

<p>That's the steps I maked:  </p>

<ol>
<li>I upped a container jenkins based in the image <code>jenkins/jenkins</code> with the docker command:  

<ul>
<li><code>docker run -it -p 1234:8080 -v jenkins_home:/var/jenkins_home -u 0 jenkins/jenkins</code></li>
<li>where <code>jenkins_home</code> is a volume created with the command: <code>docker volume create jenkins_home</code></li>
</ul></li>
<li>I installed the plugins Docker, Docker Slaves and Docker Pipeline.</li>
<li>I created a pipeline job:</li>
</ol>

<pre><code>pipeline {
    stages {        
        stage('Container') {
            agent {
                docker {
                    image 'gcc'
                    args '-v ${nomedocurso}:/curso/'
                }
            }
            steps {
                sh 'ls -l'
            }
        }
    }
}
</code></pre>

<ul>
<li>In this case, the error is jenkins not find docker:  

<blockquote>
  <p>+ docker inspect -f . gcc /var/jenkins_home/workspace/curso-c@2@tmp/durable-ee333f1e/script.sh: 1: /var/jenkins_home/workspace/curso-c@2@tmp/durable-ee333f1e/script.sh: docker: not found</p>
</blockquote></li>
<li>I tried to include the <code>label</code> parameter:</li>
</ul>

<pre><code>pipeline {
    agent {
        label 'docker' 
    }
    stages {        
        stage('Container') {
            agent {
                docker {
                    label 'docker'
                    image 'gcc'
                    args '-v ${nomedocurso}:/curso/'
                }
            }
            steps {
                sh 'ls -l'
            }
        }
    }
}
</code></pre>

<ul>
<li>But jenkins still continued complaining:</li>
</ul>

<blockquote>
  <p>Still waiting to schedule task<br>
  ‘Jenkins’ doesn’t have label ‘docker’</p>
</blockquote>

<hr>

<ul>
<li>Why docker do not be find by the jenkins?</li>
<li>How do I set up and run a pipeline with docker in one of the stages?</li>
</ul>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","59645589","Unable to bind docker.sock to Docker Service","<docker><terraform><bind><docker-swarm>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I am trying to setup a docker service (via terraform) which will run jenkins as containers. I want to bind the host VM's docker docket to the docker service. I tried two options.</p>

<ol>
<li>Docker Service as command</li>
<li>Spawn docker service using terraform</li>
</ol>

<p>While the service gets instantiated successfully, the container doesn't recognize the docker socket. I can see the <code>/var/run/docker.sock</code> file inside the container but docker command wont work.</p>

<p>The code block for your reference.</p>

<p>DOCKER SERVICE COMMAND</p>

<pre><code>docker service  create --name aws --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock jenkins/jenkins:lts
</code></pre>

<p>TERRAFORM MAIN TF FILE</p>

<pre><code>resource ""docker_service"" ""jenkins_service"" {
  name = var.project_name
  task_spec {
    container_spec {
      image = docker_image.jenkins_image.name
      mounts {
        source = var.jenkins_volume
        target = ""/var/jenkins_home""
        type   = ""bind""
      }
      mounts {
        source = ""/var/run/docker.sock""
        target = ""/var/run/docker.sock""
        type   = ""bind""
      }
    }
    networks = [""${docker_network.jenkins_network.name}""]
  }

  endpoint_spec {
    ports {
      target_port    = ""8080""
      published_port = var.web_interface_port
      publish_mode   = ""ingress""
      name           = ""WEB_INTERFACE""
    }
    ports {
      target_port    = ""50000""
      published_port = var.api_interface_port
      publish_mode   = ""ingress""
      name           = ""API_INTERFACE""
    }
  }
}
</code></pre>

<p><strong>Error Message</strong></p>

<blockquote>
  <p>bash-4.2$ docker exec  -u 0 -it 9bc2a468174a bash<br>
  root@9bc2a468174a:/# docker<br>
  bash: docker: command not found<br>
  root@9bc2a468174a:/# ls -ltr /var/run/docker.sock<br>
  srwxrwxrwx. 1 root 167 0 Sep 24 11:02 /var/run/docker.sock  </p>
</blockquote>

<p><strong>Docker Version</strong></p>

<blockquote>
  <p>-bash-4.2$ docker version<br>
  Client:<br>
   Version:           18.09.8<br>
   API version:       1.39<br>
   Go version:        go1.10.8<br>
   Git commit:        0dd43dd87f<br>
   Built:             Wed Jul 17 17:40:31 2019<br>
   OS/Arch:           linux/amd64<br>
   Experimental:      false  </p>
  
  <p>Server: Docker Engine - Community<br>
   Engine:<br>
    Version:          18.09.8
    API version:      1.39 (minimum version 1.12)<br>
    Go version:       go1.10.8<br>
    Git commit:       0dd43dd<br>
    Built:            Wed Jul 17 17:10:42 2019<br>
    OS/Arch:          linux/amd64<br>
    Experimental:     false  </p>
</blockquote>

<p>Any inputs will be much appreciated.</p>

<p>Regards
Senthil Nathan M</p>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","59669845","Connecting to Postgres docker container from docker-compose file","<postgresql><docker><docker-compose>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have 2 projects. The first contains this <code>docker-compose.yml</code>:</p>

<pre><code># pg/docker-compose.yml
version: ""3""
services:
  postgres:
    image: postgres
    environment:
      POSTGRES_USER: pguser
      POSTGRES_DB: pg
    ports:
      - ""5432:5432""
    volumes:
      - ""postgres-data:/var/lib/postgresql/data""
    networks:
      - some_net

volumes:
  postgres-data:

networks:
  some_net:
    driver: bridge
</code></pre>

<p>The second project has this docker compose file:</p>

<pre><code># pg_client/docker-compose.yml
version: ""3""
services:
  postgres_client:
    image: tmaier/postgresql-client
    command: postgres://pguser@localhost:5432/pg
    networks:
      - pg_some_net

networks:
  pg_some_net:
    external: true
</code></pre>

<p>The <a href=""https://hub.docker.com/r/tmaier/postgresql-client"" rel=""nofollow noreferrer""><code>tmaier/postgresql-client</code></a> image is extremely simple, it has PostgresSQL installed and runs the command <code>psql DOCKER_COMMAND</code> at build time. In my case this is <code>psql postgres://pguser@localhost:5432/pg</code>.</p>

<p>I can connect to my Postgres db just fine from the command line:</p>

<pre><code>$ psql postgres://pgmuser@localhost:5432/pg
psql (12.1)
Type ""help"" for help.

pg=#
</code></pre>

<p>But when the <code>postgres_client</code> container attempts to connect (after running <code>docker-compose up</code>), it fails saying </p>

<pre><code>postgres_client_1  | psql: could not connect to server: Connection refused
postgres_client_1  |        Is the server running on host ""localhost"" (127.0.0.1) and accepting
postgres_client_1  |        TCP/IP connections on port 5432?
postgres_client_1  | could not connect to server: Address not available
postgres_client_1  |        Is the server running on host ""localhost"" (::1) and accepting
postgres_client_1  |        TCP/IP connections on port 5432?
</code></pre>

<p>Is there something special about docker-compose that I need to account for when writing my Postgres connection URI?</p>

<p>Any help is much appreciated.</p>
"
"37057468","Conditional ENV in Dockerfile","<docker><dockerfile>","59710802","conditional set of a variable based on another ENV variable","<docker><dockerfile>","<p>Is it possible to conditionally set an <code>ENV</code> variable in a Dockerfile based on the value of a build <code>ARG</code>?</p>

<p>Ex: something like</p>

<pre><code>ARG BUILDVAR=sad
ENV SOMEVAR=if $BUILDVAR -eq ""SO""; then echo ""hello""; else echo ""world""; fi
</code></pre>

<p>Update: current usage based on Mario's answer:</p>

<pre><code>ARG BUILD_ENV=prod
ENV NODE_ENV=production
RUN if [ ""${BUILD_ENV}"" = ""test"" ]; then export NODE_ENV=development; fi
</code></pre>

<p>However, running with <code>--build-arg BUILD_ENV=test</code> and then going onto the host, I still get </p>

<pre><code>docker run -it mycontainer bin/bash
[root@brbqw1231 /]# echo $NODE_ENV
production
</code></pre>
","<p>In a given Dockerfile, I want to set a variable based on content of another ENV variable (which is injected into the container beforehand, or defined within the Dockerfile)</p>

<p>I'm looking at something like this</p>

<pre><code>FROM centos:7
ENV ENABLE_REMOTE_DEBUG ""true""

ENV DEBUG_FLAG=""""
RUN if [ ""$ENABLE_REMOTE_DEBUG"" = ""true"" ] ; then echo ""set debug flag"" ;export DEBUG_FLAG=""some_flags""; else echo ""remote debug not set"" ; fi

RUN echo debug flags: ${DEBUG_FLAG}

## Use the debug flag in the Entrypoint : java jar ${DEBUG_FLAG} ...
</code></pre>

<p>the problem with this Dockerfile is <code>$DEBUG_FLAG</code> is not properly set (or is not being used in the next line? ) ... since output is empty:</p>

<pre><code>debug flags:

</code></pre>

<p>What am I missing here? (I prefer not to call external bash script)</p>
"
"43654656","Dockerfile if else condition with external arguments","<docker><dockerfile>","59834620","How to compare a variable and set the value in alpine linux","<shell><docker><alpine>","<p>I have dockerfile</p>

<pre><code>FROM centos:7
ENV foo=42
</code></pre>

<p>then I build it </p>

<pre><code>docker build -t my_docker .
</code></pre>

<p>and run it.</p>

<pre><code>docker run -it -d  my_docker
</code></pre>

<p>Is it possible to pass arguments from command line and use it with if else in Dockerfile? I mean something like</p>

<pre><code>FROM centos:7
if (my_arg==42)
     {ENV=TRUE}
else:
     {ENV=FALSE}
</code></pre>

<p>and build with this argument.</p>

<pre><code> docker build -t my_docker . --my_arg=42
</code></pre>
","<p>Never wrote any shell script before but after extensively googling it I came up with the following code for my docker file. But don't understand why is it doesn't work.</p>

<pre><code>###stage 2####################
FROM nginx:alpine

##########Calculate the environment type #########
ARG BUILD_TYPE

####echo of build build_type does gives me output of Development when passed argument is Development.
RUN if [ ""$BUILD_TYPE"" = ""Development"" ]; then BUILD_TYPE='dev'; fi
RUN if [ ""$BUILD_TYPE"" = ""Production"" ]; then BUILD_TYPE='prod'; fi
RUN echo ""UI BUILD_TYPE=$BUILD_TYPE---------""
##########Calculate the environment type #########
</code></pre>

<p>The above echo always comes as Development.</p>

<p>UPDATE</p>

<p>Now I built a sample in a separate docker file to isolate the issue. After this I realised that the assignment is not happening though the condition matched.</p>

<p>Here is the new sample docker file code.</p>

<pre><code>FROM nginx:alpine

ARG BUILD_TYPE
ARG ENV_TYPE

RUN if [ ""$BUILD_TYPE"" = ""Development"" ]; then ENV_TYPE='dev'; echo ""matched dev""; fi
RUN if [ ""$BUILD_TYPE"" = ""Production"" ]; then ENV_TYPE=""prod""; echo ""matched prod""; fi
RUN echo ""UI BUILD_TYPE=$BUILD_TYPE ENV_TYPE = $ENV_TYPE---------""
</code></pre>

<p>The output is</p>

<ol>
<li>matched dev </li>
<li>UI BUILD_TYPE=Development ENV_TYPE = ---------</li>
</ol>

<p>I see ENV_TYPE is empty.</p>
"
"36283908","Re-using environment variables in docker-compose.yml","<docker><docker-compose><environment-variables>","59894230","Sharing environment variables across multiple Docker services","<docker><docker-compose>","<p>Is it possible to re-use environment variables that are shared among multiple containers?</p>
<p>The idea is to avoid duplication, as illustrated in this example:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:

  db:
    image: example/db
    ports:
      - &quot;8443:8443&quot; 
    container_name: db
    hostname: db
    environment:
      - USER_NAME = admin 
      - USER_PASSWORD = admin 

svc:
  image: example/svc
  depends_on:
    - db
  ports:
    - &quot;9443:9443&quot;
  container_name: svc
  hostname: svc
  environment:
    - DB_URL = https://db:8443
    - DB_USER_NAME = admin
    - DB_USER_PASSWORD = admin 
</code></pre>
","<p>I was wondering, let's say I had the following .env file:</p>

<pre><code>STAGING_ALLOWED_DOMAINS=example.com
STAGING_SITES=example.com=web:8000
</code></pre>

<p>Is there a way to condense the following services to utilise the same environment settings, or do they always need to be explicitly listed for each service?</p>

<pre><code>services:
    web:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    nginx:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    celery:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    celery-beat:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
</code></pre>

<p>Is there a cleaner way to share environment configurations?</p>
"
"45681780","How to initialize mysql container when created on Kubernetes?","<mysql><docker><kubernetes>","60014936","Kubernetes - Create database in MySQL after pod start-up","<mysql><kubernetes><containers><database-connection><ddl>","<p>I want to set initial data on MySQL of container.
In docker-compose.yml, such code can create initial data when running container.</p>

<pre><code>volumes:
  - db:/var/lib/mysql
  - ""./docker/mysql/conf.d:/etc/mysql/conf.d""
  - ""./docker/mysql/init.d:/docker-entrypoint-initdb.d""
</code></pre>

<p>However, how can I create initial data on Kubernetes when running?</p>
","<p>This is my <code>Deployment</code> YAML for launching a MySQL pod and then create a database and a user. The MySQL commands do not run. If I remove the <code>-h localhost</code> argument, it tries to connect over Unix socket and fails. If I provide the <code>-h localhost</code> or <code>-h 127.0.0.1</code>, then I get an error as server not found. Since this set-up is for a quick demo, I am not keen on running script after start-up or creating a new <code>Dockerfile</code> of off base <code>mysql</code> image.</p>

<p>How do I get the MySQL commands to run with <code>heredoc</code>?</p>

<pre><code>kind: Deployment
apiVersion: apps/v1
metadata:
  name: mysql
  labels:
    run: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      run: mysql
  template:
    metadata:
      labels:
        run: mysql
    spec:
      containers:
        - name: mysql
          image: mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: R00t.Passw0rd
            - name: MYSQL_PASSWORD
              value: Dem0.Passw0rd
          command:
            - mysql
          args:
            - -h 127.0.0.1 
            - -u root 
            - -p $(MYSQL_ROOT_PASSWORD)
            - &lt;&lt; END
              create database demodb;
              create user demo identified with mysql_native_password by $(MYSQL_PASSWORD);
              grant all on demodb.* to demo;
              END
          resources:
            limits:
              memory: ""512Mi""
              cpu: ""200m""
          ports:
          - containerPort: 3306
            protocol: TCP          
          imagePullPolicy: Always
      restartPolicy: Always
</code></pre>
"
"46898193","docker exec or docker container exec","<docker>","60097132","Difference between docker exec and docker container exec","<docker><docker-container><docker-exec>","<p>I noticed in the latest Docker CLI documentation that Docker CLI command list has expanded. 
If I used <strong>docker exec</strong> earlier to start executable inside container now I can also use <strong>docker container exec</strong> command.</p>

<p><strong>docker container run</strong> command is similar to <strong>docker run</strong>, etc.</p>

<p>So which commands are preferrable now? Old syntax or new docker container syntax? Unfortunately I couldn't find any explanation in the docs.</p>

<p>Also what is the difference between docker container run and docker container create commands? And between docker container stop and docker container kill? The description and syntax are very similar.</p>

<p>Thanks.</p>
","<p>What is the difference between ""docker exec"" and ""docker container exec""?
The same for the difference between ""docker commit"" and ""docker container commit""?</p>

<p>I researched a lot but couldn't find any.
Your help is appreciated.</p>
"
"43467847","Setup git via windows docker file","<dockerfile><docker-for-windows><docker-desktop><docker>","60163945","Docker file with git installation for windows","<python><git><powershell><docker><dockerfile>","<p>I write <code>Dockerfile</code> which is based on <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>. I need to add to this image git. In order to achieve it I did the following:</p>

<pre><code>RUN Invoke-WebRequest 'https://github.com/git-for-windows/git/releases/download/v2.12.2.windows.2/Git-2.12.2.2-64-bit.exe'
RUN Invoke-Expression ""c:\Git-2.12.2.2-64-bit.exe""
</code></pre>

<p>But when I execute this lines via docker build, I receive following error message:</p>

<blockquote>
  <p>Invoke-Expression : The term 'c:\Git-2.12.2.2-64-bit.exe' is not
  recognized as the name of a cmdlet, function, script file, or operable
  program. Check the spelling of the name, or if a path was included,
  verify that the path is correct and try again.</p>
</blockquote>

<p>I realize that this error message indicates that due to console nature of windows docker images I'll not be able to execute GUI installers. Unfortunately git doesn't have console installer. <a href=""https://chocolatey.org/"" rel=""nofollow noreferrer"">Chocolatey</a> works fine under <a href=""https://hub.docker.com/r/microsoft/windowsservercore/"" rel=""nofollow noreferrer"">windowsservercore</a> image but doesn't work at <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>. In order to install git for <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a> I have idea to repeat in <code>Dockerfile</code> commands from <a href=""https://chocolatey.org/packages/git.install"" rel=""nofollow noreferrer"">chocolatey git installer</a> which is fine for me, but still I'd like to know is there any simpler way to install git on <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>? </p>
","<p>I need to create a docker file that does the following:</p>

<p>Install python. </p>

<p>Install git. </p>

<p>Clone git repo.</p>

<p>Run with latest commit.</p>

<p>I have the following for linux:</p>

<pre><code>FROM ubuntu:16.04

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends python-pip
RUN apt-get install -y --no-install-recommends python-setuptools
RUN apt-get install -y --no-install-recommends python-wheel
RUN apt-get install -y --no-install-recommends git

# Clone project github repository
WORKDIR /repo
RUN git clone https://github.com/&lt;repo&gt;.git
# Checkout version 
WORKDIR /repo/project
RUN git checkout &lt;latest commit&gt;
</code></pre>

<p>In the windows docker file creation I'll skip python installation by using <code>python:2.7-windowsservercore-1809</code> as the base image.</p>

<p>I tried to search online for ways to install git like the above in linux, but couldn't find any way that worked for me (like using chocolaty)</p>

<p>Any ideas how to create an equivalent docker file for windows container? </p>
"
"37526509","How to install pdo driver in php docker image?","<php><mysql><pdo><docker>","60384378","PDO could not find driver ( DOCKER )","<php><mysql><docker><pdo>","<p>I am using as a base the <a href=""https://hub.docker.com/_/php/"" rel=""noreferrer"">php docker container</a> with the tag:</p>



<pre class=""lang-none prettyprint-override""><code>php:5.6-apache
</code></pre>

<p>I linked it with a basic <code>mysql:5.6</code> image which I can reach at the host <code>mysql</code>. I created a DB, and filled a table with basic values.</p>

<p>Yet trying to access my app, I get:</p>

<pre class=""lang-none prettyprint-override""><code>Fatal error: Uncaught exception 'PDOException' with message
could not find driver' in /var/www/html/index.php:30 
Stack trace: #0 [internal function]: 
PDO-&gt;__construct('mysql:host=mysq...', 'root', 'root', Array) 
#1 [internal function]: Phalcon\Db\Adapter\Pdo-&gt;connect(Array)
#2 /var/www/html/index.php(30): Phalcon\Db\Adapter\Pdo-__construct(Array)
#3 [internal function]: {closure}()
#4 [internal function]: Phalcon\Di\Service-&gt;resolve(NULL, Object(Phalcon\Di\FactoryDefault))
#5 [internal function]: Phalcon\Di-&gt;get('db', NULL)
#6 [internal function]: Phalcon\Di-&gt;getShared('db')
#7 [internal function]: Phalcon\Mvc\Model\Manager-&gt;_getConnection(Object(Reviews), NULL)
#8 [internal function]: Phalcon\Mvc\Model\Manager-&gt;getReadConnection(Object(Reviews))
#9 [internal function]: Phalcon\Mvc\Model-&gt;getReadConnection()
#10 [internal function]: Phalcon\Mvc\Model\MetaData\Strategy\Introspection-&gt;getMetaData(Object(Reviews), Object(Phalcon\Di\FactoryDefault))
#11 [internal function]: Phalcon\Mvc\Model\MetaData-&gt;_initialize(Object(Rev in /var/www/html/index.php on line 30
</code></pre>

<p>Hence, I thought that the php container was lacking the <code>php-mysql</code> component I installed via:</p>

<pre class=""lang-none prettyprint-override""><code>apt-get install php5-mysql
</code></pre>

<p>I also added a mysql.ini at:</p>

<pre class=""lang-none prettyprint-override""><code>cat /usr/local/etc/php/conf.d/mysql.ini
; configuration for php MySQL module
; priority=20
extension=pdo_mysql.so
</code></pre>

<p>If I <code>echo phpinfo();die</code> it tells me that:</p>

<pre class=""lang-none prettyprint-override""><code>Additional .ini files parsed:
    /usr/local/etc/php/conf.d/mysql.ini,
    /usr/local/etc/php/conf.d/phalcon.ini
</code></pre>

<p>Yet still, the error persists.</p>

<p>Furthermore, when running:</p>

<pre class=""lang-none prettyprint-override""><code>php -i|grep PDO
</code></pre>

<p>I get:</p>

<pre class=""lang-none prettyprint-override""><code>PHP Warning:  PHP Startup: Unable to load dynamic library '/usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so' - /usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so: cannot open shared object file: No such file or directory in Unknown on line 0
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite
PDO Driver for SQLite 3.x =&gt; enabled
</code></pre>

<p>so it seems the mysql extension isn't even activated.</p>

<p>What am I doing wrong?</p>
","<p>I have the following docker.yml file</p>

<pre><code>version: ""3""
services:
  wwww:
    build: .
    ports:
      - ""8888:80""
    volumes:
      - ""./src:/var/www/html/""
    networks:
      - default
  db:
    image: mysql
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example

  adminer:
    image: adminer
    restart: always
    ports:
      - 8060:8080
</code></pre>

<p>and the docker file for php is </p>

<pre><code>FROM php:7.2-apache
RUN docker-php-ext-install mysqli pdo pdo_mysql
</code></pre>

<p>I checked with <code>phpinfo()</code> 
and the pdo driver is enabled for sqlite.</p>

<p>Even when I do docker-compose I get the following warning 
<code>warning: pdo (pdo.so) is already loaded!</code></p>

<p>However, I still get an exception that pdo is not found.</p>

<p>Is there anything that I am missing ?</p>

<p>The exact error is </p>

<pre><code>Fatal error: Uncaught PDOException: could not find driver in /var/www/html/index.php:5 

Stack trace: 
#0 /var/www/html/index.php(5): PDO-&gt;__construct('mysql:host=127....', 'root', 'example') 
#1 {main} thrown in /var/www/html/index.php on line 5
</code></pre>
"
"36283908","Re-using environment variables in docker-compose.yml","<docker><docker-compose><environment-variables>","60420088","Pass same variables to multiple services in docker-compose without duplicating instructions?","<docker><docker-compose><dockerfile>","<p>Is it possible to re-use environment variables that are shared among multiple containers?</p>
<p>The idea is to avoid duplication, as illustrated in this example:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:

  db:
    image: example/db
    ports:
      - &quot;8443:8443&quot; 
    container_name: db
    hostname: db
    environment:
      - USER_NAME = admin 
      - USER_PASSWORD = admin 

svc:
  image: example/svc
  depends_on:
    - db
  ports:
    - &quot;9443:9443&quot;
  container_name: svc
  hostname: svc
  environment:
    - DB_URL = https://db:8443
    - DB_USER_NAME = admin
    - DB_USER_PASSWORD = admin 
</code></pre>
","<p>I have two services in my docker-compose file. Both of them need the same set of environment variables. Is there a better way to do this than duplicating instructions?</p>

<blockquote>
  <p>PS: I have stored my variables in the ""<code>.env</code>"" file, which resides in the same directory as the docker-compose file.</p>
</blockquote>

<p><strong>example docker-compose file:</strong></p>

<pre><code>version: '3'

services: 
     service_1: 
           build: './s_1/'
           environment: 
                - variable_1 = ${variable_1}
                - variable_2 = ${variable_2}
                - variable_x = ${variable_y}
     service_2:
           build: './s_2/'
           environment: 
                - variable_1 = ${variable_1}
                - variable_2 = ${variable_2}
                - variable_x = ${variable_y}
</code></pre>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","60456516","How can I automatically start cron in a Docker container that is running PHP?","<php><linux><docker><cron>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>I'm trying to run some PHP scripts automatically and want to have cron running once the container starts. Below is my current Dockerfile but upon running the <code>docker-compose up -d</code> command, the PHP container crashes immediately and I get the errors <code>Cannot start service composer: Cannot link to a non running container</code> and <code>Cannot start service apache: Cannot link to a non running container</code> which makes sense, because the PHP container already crashed. I've also tried <code>CMD [""cron"", ""-f""]</code> in place of <code>CMD cron start &amp;&amp; tail -f /var/log/cron.log</code> and all the containers run, and the test job is executed, but I get a 
503 service unavailable error when trying to access the app. From what I've read this is because cron is running in the foreground and blocking PHP.</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM php:7.4.2-fpm-buster

RUN apt-get update &amp;&amp; apt-get install -y \
        freetds-bin \
        freetds-dev \
        freetds-common \
        libct4 \
        libsybdb5 \
        tdsodbc \
        libfreetype6-dev \
        libjpeg62-turbo-dev \
        libmcrypt-dev \
        libpng-dev \
        libldap2-dev \
        libpq-dev \
        zlib1g-dev \
        libc-client-dev \
        libzip-dev \
        zip \
        cron

RUN ln -s /usr/lib/x86_64-linux-gnu/libsybdb.a /usr/lib/
RUN ln -s /usr/lib/x86_64-linux-gnu/libpq-fe.h /usr/lib/

RUN docker-php-ext-install pdo pdo_mysql pdo_dblib pdo_pgsql gd zip

RUN apt-get install unixodbc unixodbc-dev -y \
 &amp;&amp; docker-php-ext-configure pdo_odbc --with-pdo-odbc=unixODBC,/usr \
 &amp;&amp; docker-php-ext-install pdo_odbc

COPY ./freetds.conf /etc/freetds/freetds.conf
COPY ./odbc.ini /etc/odbc.ini
COPY ./odbcinst.ini /etc/odbcinst.ini
COPY ./php.ini /usr/local/etc/php

# Install Composer
RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer

RUN mkdir /var/www/html/files &amp;&amp; chown -R www-data:www-data /var/www/html/files &amp;&amp; chmod -R 755 /var/www/html/files
RUN mkdir /var/www/html/exports &amp;&amp; chown -R www-data:www-data /var/www/html/exports &amp;&amp; chmod -R 755 /var/www/html/exports
RUN mkdir /var/www/html/scripts &amp;&amp; chown -R www-data:www-data /var/www/html/scripts &amp;&amp; chmod -R 755 /var/www/html/scripts

# copying cronjobs, a file I created with a test cronjob to /etc
COPY ./cronjobs /etc
# using the cronjobs file I created as the crontab
RUN crontab /etc/cronjobs

CMD cron start &amp;&amp; tail -f /var/log/cron.log

EXPOSE 9000
</code></pre>

<p><strong>docker-compose.yml</strong> (some info redacted ... the xxx parts)</p>

<pre><code>version: ""3.7""
services:
  php:
    build: './php/'
    links:
      - 'mysql'
    network_mode: ""bridge""
    ports:
      - ""xxxx:9000""
    volumes:
      - ./www/:/var/www/html/
      - files:/var/www/html/files
  apache:
    build: './apache/'
    depends_on:
      - php
    links:
      - 'php'
    network_mode: ""bridge""
    ports:
      - ""xxxx:80""
    volumes:
      - ./www/:/var/www/html/
      - files:/var/www/html/files
  mysql:
    image: mysql:latest
    command: --init-file /docker-entrypoint-initdb.d/init.sql
    environment:
      MYSQL_ROOT_PASSWORD: 'xxx'
      MYSQL_USER: 'xxx'
      MYSQL_PASS: 'xxx'
    ports:
      - ""xxxx:3306""
    network_mode: ""bridge""
    volumes:
      - data:/var/lib/mysql
      - ./mysql/init:/docker-entrypoint-initdb.d
  phpmyadmin:
    depends_on: 
      - mysql
    environment:
      PMA_HOST: mysql
      PMA_PORT: 3306
    image: phpmyadmin/phpmyadmin
    links:
      - mysql
    network_mode: ""bridge""
    ports:
      - ""xxxx:80""
  composer:
    container_name: composer
    depends_on:
      - php
    links:
      - 'php'
    image: composer
    network_mode: ""bridge""
    volumes:
      - ./www:/var/www/html/
    command: composer install --ignore-platform-reqs &amp;&amp; composer require phpoffice/phpspreadsheet
networks:
  default:
    external:
      name: bridge
volumes:
  data:
  files:
</code></pre>

<p><strong>cronjobs file</strong></p>

<pre><code># Edit this file to introduce tasks to be run by cron.
#
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
#
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').
#
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
#
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
#
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
#
# For more information see the manual pages of crontab(5) and cron(8)
#
# m h  dom mon dow   command

42 22 * * * /usr/local/bin/php /var/www/html/scripts/test.php
</code></pre>
"
"45122459","Mounts denied. The paths ... are not shared from OS X and are not known to Docker","<macos><docker><docker-for-mac>","60663629","How to use /usr/local/var/mysql for Docker volume","<mysql><docker>","<p>The command <code>docker run -v /var/folders/zz/...</code> produces the following error.</p>

<pre><code>docker: Error response from daemon: Mounts denied: 
The paths /var/folders/zz/... and /var/folders/zz/...
are not shared from OS X and are not known to Docker.
You can configure shared paths from Docker -&gt; Preferences... -&gt; File Sharing.
</code></pre>

<p>When I do open File Sharing, I see that /private is listed already.</p>

<p>If I attempt to add <code>/var/folder/</code>, it resolves to <code>/private/var/folders</code>, which is a subset of /private and hence the addition is rejected. </p>

<p>To summarize, it looks to me like the directory <code>/var/folders/..</code> is shared by OS X as a subdirectory of <code>/private</code> and hence must be known to Docker. Any help on resolving this would be appreciated.</p>

<p>As an experiment, I replaced the <code>/private</code> in File Sharing with <code>/private/var/folders</code> and restarted the docker but the result did not change. </p>

<p>Just for a more complete reference, this is the <a href=""https://github.com/tensorflow/cleverhans/blob/master/examples/nips17_adversarial_competition/run_attacks_and_defenses.sh"" rel=""noreferrer"">.sh script</a>, which runs <a href=""https://github.com/tensorflow/cleverhans/blob/master/examples/nips17_adversarial_competition/run_attacks_and_defenses.py"" rel=""noreferrer"">this python script</a>, which in turn runs the docker command. </p>
","<p>I'm trying to use <code>/usr/local/var/mysql</code> as the volume in my docker-compose set up. When I do, I receive the following error:</p>

<pre><code>ERROR: for dockerdb  Cannot start service dockerdb: Mounts denied: 
The path /usr/local/var/mysql
is not shared from OS X and is not known to Docker.
You can configure shared paths from Docker -&gt; Preferences... -&gt; File Sharing.
See https://docs.docker.com/docker-for-mac/osxfs/#namespaces for more info.
</code></pre>

<p>This is the entry in the <code>docker-compose</code> file:</p>

<pre><code>dockerdb:
    image: mysql:5.6
    restart: always
    environment:
      # MYSQL_DATABASE: ""smodindbdev""
      MYSQL_ROOT_PASSWORD: ""docker""
    volumes:
      - /usr/local/var/mysql:/var/lib/mysql
    ports:
      - ""3306:3306""
</code></pre>

<p>How can I add <code>/usr/local/var/mysql</code> to the volumes? This may not be something you're supposed to do, but if I can do it I would prefer to do it this way. </p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","60723357","Run docker container with official postgres image, failed to access the container","<postgresql><image><docker><containers><psql>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I wanted to run postgres database in docker container.
I pulled the newest image of postgres:</p>

<pre><code>docker pull postgres
</code></pre>

<p>And started container out of it:</p>

<pre><code>docker run --rm --name pg-docker -e POSTGRES_PASSWORD=docker -d -p 5432:5432 postgres
</code></pre>

<p>After that I tried to access the postgres:</p>

<pre><code>psql -h localhost -U postgres -d postgres
</code></pre>

<p>And connection failed:</p>

<pre><code>psql: error: could not connect to server: could not connect to server: Connection refused (0x0000274D/10061) Is the server running on host ""localhost"" (::1) and accepting TCP/IP connections on port 5432?

could not connect to server: Connection refused (0x0000274D/10061) Is the server running on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on port 5432?
</code></pre>

<p>I don't understand why?</p>

<p>[EDIT]
I tried it on Windows 10 Pro with Docker Toolbox and it fails, however on Ubuntu 18.04 works fine.</p>
"
"46778868","ng serve not working in Docker container","<angular><docker><docker-compose>","60929479","Unable to reach angular running in Docker container","<node.js><angular><docker><yaml><docker-container>","<p>I have this <a href=""https://github.com/Axiol/Docker-Angular-CLI"" rel=""noreferrer"">Docker Compose configuration</a> where I simply create a NodeJS container and install Angular CLI inside.</p>

<p>After a <code>docker-compose up -d</code>, I'm able to SSH inside the container with <code>docker-compose run node bash</code>. <code>ng new</code> works perfectly but <code>ng serve</code> does not seem to work. It's launched correctly, no error in the console. But, if I visit <code>localhost</code> (ad I mapped port 4200 to 80), nothing loads.</p>

<p>Am I missing something?</p>
","<p>I am brand new to Docker so pleas bear with me.</p>

<p><strong>Dockerfile:</strong></p>

<pre><code>FROM node:alpine
WORKDIR '/app'
COPY ./package.json .
EXPOSE 4200
RUN npm i
COPY . .
CMD [""npm"",""start""]
</code></pre>

<p><strong>Commands:</strong></p>

<pre><code>docker build -t angu .
docker run  -p 4300:4200 angu
</code></pre>

<p><a href=""https://i.stack.imgur.com/ruS9w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ruS9w.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Rf1nE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rf1nE.png"" alt=""enter image description here""></a></p>

<p><strong>I am not sure if I need to include <code>EXPOSE 4200</code> in Dockerfile. But it is not working either ways.</strong></p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","60954502","Docker container doesn't connect to host","<docker><flask><ip><dockerfile><port>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I am not able to connect to the docker container from my host machine (Windows 10 Home).</p>

<pre><code>#This is how my Dockerfile looks like

FROM ubuntu:latest
RUN apt-get update -y
RUN apt-get install -y python-pip python-dev build-essential
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
ENTRYPOINT [""python""]
CMD [""app.py""]
</code></pre>

<p>After running <code>docker build -t flask-sample:latest</code> and then <code>docker run -d -p 5000:5000 flask-sample</code>, it shows <code>Running on http://0.0.0.0:5000/</code>, but nothing on my browser shows up.
Also, when I connect to the docker ip address on the port 5000 <code>(i.e. 192.168.99.100:5000)</code> everything works fine.</p>

<p>What am I missing here?</p>
"
"46907584","Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error","<python><docker><dockerfile><docker-image>","61023611","Docker COPY newly created file in gitlab pipeline fails","<docker><gitlab><gitlab-ci><docker-in-docker>","<p>I want to create a docker image. This is my work directory:
Dockerfile.in   test.json   test.py</p>

<p>And this is my Dockerfile:</p>

<pre><code>COPY ./test.json /home/test.json
COPY ./test.py /home/test.py

RUN python test.py
</code></pre>

<p>When i launch this command: 
<code>docker build -f Dockerfile.in -t 637268723/test:1.0 .</code></p>

<p>It gives me this error: </p>

<pre><code>`Step 1/5 : COPY ./test.json /home/test.json
 ---&gt; Using cache
 ---&gt; 6774cd225d60
 Step 2/5 : COPY ./test.py /home/test.py
 COPY failed: stat /var/lib/docker/tmp/docker-builder428014112/test.py: 
 no such file or directory`
</code></pre>

<p>Can anyone help me?</p>
","<p>My Dockerfile includes a COPY command which copies a directory that was just created in the pipeline.</p>

<pre><code>COPY [""$CI_PROJECT_DIR/ui-dist/"", ""/content/ui-dist/""]
</code></pre>

<p>That COPY command fails with the following error:</p>

<pre><code>COPY failed: stat /var/lib/docker/tmp/docker-builder597514812/ui-dist: no such file or directory
</code></pre>

<p>However, I know the directory exists because right before I build the image, <code>ls $CI_PROJECT_DIR/ui-dist</code> prints <code>bundle.js</code></p>

<p>The runner is using the dind service. I'm not sure if that's relevant.</p>

<p><strong>EDIT:</strong></p>

<p>Okay, I tried removing the reference to the env variable. i.e. I changed the pipeline from</p>

<pre><code>- docker cp ui:/content/dist $CI_PROJECT_DIR/ui-dist
- docker build --tag ${IMAGE_ID} api
</code></pre>

<p>to</p>

<pre><code>- docker cp ui:/content/dist ui-dist
- docker build --tag ${IMAGE_ID} api
</code></pre>

<p>and I've updated the docker command to</p>

<pre><code>COPY [""ui-dist/"", ""/content/ui-dist/""]
</code></pre>

<p>and I'm still getting this error message:</p>

<pre><code>COPY failed: stat /var/lib/docker/tmp/docker-builder488922739/ui-dist: no such file or directory
</code></pre>
"
"45928842","multiple volumes to single target directory?","<docker><unionfs>","61154399","Is many to one (HOST<->Container) volume mapping possible?","<docker><docker-compose><docker-volume>","<p>Is there a way to mount multiple volumes from a host to form a single target mount point? A bit like this:</p>

<pre><code>docker run --name ubuntu_bash \
    --rm --interactive --tty \
    --volume=/media/Large/videos:/videos \
    --volume=/media/Small/videos:/videos \
    ubuntu find /videos
</code></pre>

<p>I'm guessing the answer is no but with ""overlay"" having so many meanings in the context of Docker it's a bit difficult to search for this on the web.</p>

<p>If not, is there a Docker Store image that might help? Unfortunately a lot of Docker images don't give sufficient instructions on how to use them.</p>
","<p>For example something link this (snippet below),</p>

<pre><code>ports:
      - ${MONGO_PORT}:${MONGO_PORT}
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    volumes:
      - qm-dbdata:/data/db/raw
      - /data/db/raw:/data/db/raw
</code></pre>
"
"39913757","Restrict Internet Access - Docker Container","<docker><docker-networking>","61237950","How to disable external network calls in docker compose?","<docker><networking><docker-compose><circleci>","<p>I have a situation to restrict internet access of the container in load balancer network. for example in that below picture</p>

<p><a href=""https://i.stack.imgur.com/tChKt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tChKt.png"" alt=""easy for your reference""></a></p>

<p>Only <strong>container4</strong> connects to the Internet; other <strong>three</strong> only communicate through <strong>container4</strong> with the outside world. For example if <strong>container1</strong> needs smtp support, it will forward smtp request to <strong>container4</strong> to get access.</p>

<p>No container other than <strong>container4</strong> should be allowed to access the Internet directly! This should be enforced on Docker level.</p>

<p>I believe it will be configurable on <strong>docker network creation</strong>, can any one explain how to achieve this?</p>
","<p>I am running a set of tests against a large legacy service that (for now) makes external network calls to other services over the internet.</p>

<p>The goal for this project is to mock these external services as other docker containers within the same <code>docker-compose</code> network. Some of these services are microservices run by our organization, others are third-party vendors.</p>

<p>We are logging external calls to identify which external endpoints are being called by this service, and mocking those calls accordingly. But because I'll be putting this in CI, and this service has many contributors in a large organization, I want to enforce that no external services are called by isolating the entire <code>docker-compose</code> network from the internet.</p>

<p>Short of turning off my wifi, How can I disable external calls from these services, while still allowing services in the same docker network to communicate with each other?</p>

<p>The options given here: <a href=""https://docs.docker.com/network/#network-drivers"" rel=""nofollow noreferrer"">https://docs.docker.com/network/#network-drivers</a> do not seem to be what I want.</p>

<p>We use CircleCI, so if there is a solution within that configuration thats fine. I just haven't found anything related to that in those docs either, and would prefer a <code>docker-compose</code> solution so I can test it locally.</p>
"
"40092796","libGL error: failed to load driver: swrast - Running Ubuntu Docker container on Mac OS X host","<docker>","61280681","Gtk-WARNING **: cannot open display ssh'ing Jetson from mac","<macos><nvidia-jetson><x11-forwarding>","<p>I'm running a GUI application in a container in privileged mode on a MAX OS X host. 
I'be been successfully able to start the GUI in the container using this link: <a href=""http://kartoza.com/en/blog/how-to-run-a-linux-gui-application-on-osx-using-docker/"" rel=""noreferrer"">http://kartoza.com/en/blog/how-to-run-a-linux-gui-application-on-osx-using-docker/</a></p>

<p>Now within my GUI application, I'm trying to pop up another window and I get the following:
Using Volk machine: avx_64_mmx_orc
libGL error: failed to load driver: swrast</p>

<p>How do I go about solving this?</p>
","<p>trying to get camera feed from jetson nano headless mode (no display connected), while ssh connect from mac.</p>

<p>I had a number of issues forwarding GUI from the nano over ssh. specifically I got this issue when following the instructions posted on <a href=""https://github.com/dusty-nv/jetbot_ros"" rel=""nofollow noreferrer"">https://github.com/dusty-nv/jetbot_ros</a>.</p>
"
"46892589","Get latest Docker image creation date from registry","<docker><docker-registry>","61289710","From local machine how to get creation Date and last downloaded date of Image (tag) from Docker private Registry","<docker><docker-registry>","<p>How is it possible to get latest creation date of Docker image that exists at registry? Recently we have a problem that Docker image was not automatically pulled at some of our cluster slave servers and the project was running at the very outdated container environment. So am expecting to run a cron-script once per day to check that pulled Docker image is not 24 hours older than registry Docker image.</p>
","<p>Is there any command or script (perl/shell) to get image (tags) details like <strong>creation Date</strong> and <strong>last downloaded date</strong> from Docker private Registry.</p>
"
"37584961","How to encrypt docker images or source code in docker images?","<encryption><docker>","61426490","encrypt access to docker image","<docker><encryption><containers>","<p>Say I have a docker image, and I deployed it on some server. But I don't want other user to access this image. Is there a good way to encrypt the docker image ? </p>
","<p>I'm relatively new to docker and created an image of my dash app and pushed it to my private repository. I would like to secure my app/ python script as one can easy pull the image from my private repository and run it.</p>

<p>Is there some easy way to do so (besides creating a private registry and so forth)? </p>

<ul>
<li>I thought about mounting a text file that contains some predefined credentials. However, as my app is really simple and runs locally there is no other way than hardcoding and comparing it with the submitted credentials from the txt file? Or is this complete non-sense? Is one able to inspect the code of my python script via docker inspect/ or creating an interactive shell and investigating all files? </li>
<li>Another thought: Is it possible to require some certificate from the user via certbot? How would this look like?</li>
</ul>

<p>Are there other ways to encrypt access to a docker image?</p>
"
"42345235","How to specify Memory & CPU limit in docker compose version 3","<docker><docker-compose>","61426882","How to specify max cpu and ram in docker compose 3.7","<docker><docker-compose>","<p>I am unable to specify CPU &amp; memory for services specified in version 3 .</p>

<p>With version 2 it works fine with ""mem_limit"" &amp; ""cpu_shares"" parameters under the services . But it fails while using version 3 , putting them under deploy section doesn't seem worthy unless i am using swarm mode .</p>

<p>Can somebody help ?</p>

<pre><code>version: ""3""
services:
  node:
    build:
     context: .
      dockerfile: ./docker-build/Dockerfile.node
    restart: always
    environment:
      - VIRTUAL_HOST=localhost
    volumes:
      - logs:/app/out/
    expose:
      - 8083
    command: [""npm"",""start""]
    cap_drop:
      - NET_ADMIN
      - SYS_ADMIN
</code></pre>
","<p>How to specify max cpu and ram in docker compose 3.7.?</p>

<p>My compose file is:</p>

<pre><code>version: ""3.7""
services:
  mongodb_container:
    image: mongo:latest
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: psw
      MONGO_INITDB_DATABASE: admin
    ports:
      - 27017:27017
    command: mongod --auth
    volumes:
      - ./data-docker/mongo:/data
</code></pre>
"
"39223249","Multiple RUN vs. single chained RUN in Dockerfile, which is better?","<docker><dockerfile>","61438916","DockerFile one-line vs multi-line instruction","<docker><dockerfile>","<p><code>Dockerfile.1</code> executes multiple <code>RUN</code>:</p>
<pre><code>FROM busybox
RUN echo This is the A &gt; a
RUN echo This is the B &gt; b
RUN echo This is the C &gt; c
</code></pre>
<p><code>Dockerfile.2</code> joins them:</p>
<pre><code>FROM busybox
RUN echo This is the A &gt; a &amp;&amp;\
    echo This is the B &gt; b &amp;&amp;\
    echo This is the C &gt; c
</code></pre>
<p>Each <code>RUN</code> creates a layer, so I always assumed that fewer layers is better and thus <code>Dockerfile.2</code> is better.</p>
<p>This is obviously true when a <code>RUN</code> removes something added by a previous <code>RUN</code> (i.e. <code>yum install nano &amp;&amp; yum clean all</code>), but in cases where every <code>RUN</code> adds something, there are a few points we need to consider:</p>
<ol start=""0"">
<li><p>Layers are supposed to just add a diff above the previous one, so if the later layer does not remove something added in a previous one, there should not be much disk space saving advantage between both methods.</p>
</li>
<li><p>Layers are pulled in parallel from Docker Hub, so <code>Dockerfile.1</code>, although probably slightly bigger, would theoretically get downloaded faster.</p>
</li>
<li><p>If adding a 4th sentence (i.e. <code>echo This is the D &gt; d</code>) and locally rebuilding, <code>Dockerfile.1</code> would build faster thanks to cache, but <code>Dockerfile.2</code> would have to run all 4 commands again.</p>
</li>
</ol>
<p>So, the question: <strong>Which is a better way to do a Dockerfile?</strong></p>
","<p>To my knowledge of the way docker build works is that for each line of instruction, it creates a separate image/layer. However, it is very efficient in managing to reuse the layers or avoid rebuilding those layers if nothing has changed.</p>

<p>So does it matter if I put below instruction either on same line or multi-line? For convenience, I would prefer the single line option unless it is not an efficient option.</p>

<p><strong>Multi-Line Instruction</strong></p>

<pre><code>RUN apt-get -y update
RUN apt-get -y install ...
</code></pre>

<p><strong>Single-Line Instruction</strong></p>

<pre><code>RUN apt-get -y update &amp;&amp; apt-get -y install
</code></pre>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","61541234","Docker Desktop for Windows- Where exactly the volume is present","<docker>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I created a volume using Docker Desktop. Where does the volume is located. 
It is not intuitive as it was was the docker toolbox to locate the volume.</p>

<pre><code>PS C:\Users\admin&gt; docker volume inspect test                   
[                                                               
    {                                                           
        ""CreatedAt"": ""2020-05-01T10:59:22Z"",                    
        ""Driver"": ""local"",                                      
        ""Labels"": {},                                           
        ""Mountpoint"": ""/var/lib/docker/volumes/test/_data"",     
        ""Name"": ""test"",                                         
        ""Options"": {},                                          
        ""Scope"": ""local""                                        
    } 
</code></pre>

<p>Where does exactly "" /var/lib/docker/volumes/test/"" is present ?     I wanted to navigate to the path of it and run some commands for that path.</p>

<p>I referred in many articles, either they are not straightforward,                                                 </p>
"
"46749411","System.InvalidOperationException: 'A path base can only be configured using IApplicationBuilder.UsePathBase().'","<c#><docker><asp.net-core-2.0>","61545742","My app ASP.NET Core app crashs when i move the project to another directory","<c#><asp.net-core-3.1><rider>","<p>I have an ASP.Net Core 2 Solution running in Docker which is working fine on 1 machine the is running VS 2017 Professional but on another machine running VS 2017 Community I am getting the following error</p>

<blockquote>
  <p>""System.InvalidOperationException: 'A path base can only be
  configured using IApplicationBuilder.UsePathBase().'""</p>
</blockquote>

<p>when I try and start or debug the solution using docker. If I start the solution with IIS Express it works fine. </p>

<p>There is nothing special about my project:</p>

<pre><code>public class Program
{
    public static void Main(string[] args)
    {
        BuildWebHost(args).Run(); // &lt;- Exception happens here
    }

    public static IWebHost BuildWebHost(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .CaptureStartupErrors(true)
            .UseStartup&lt;Startup&gt;()
            .Build();
}

public class Startup
{
    public Startup(IHostingEnvironment env)
    {

    }

    public IConfiguration Configuration { get; }

    // This method gets called by the runtime. Use this method to add services to the container.
    public void ConfigureServices(IServiceCollection services)
    {
        services.AddMvc();
    }

    // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {

        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
        }

        app.UseMvc();
    }
}
</code></pre>

<p>I also get this pop up in a new window:</p>

<pre><code>crit: Microsoft.AspNetCore.Server.Kestrel[0]
      Unable to start Kestrel.
System.InvalidOperationException: A path base can only be configured using IApplicationBuilder.UsePathBase().
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAddressAsync&gt;d__7.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.AddressesStrategy.&lt;BindAsync&gt;d__2.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAsync&gt;d__0.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServer.&lt;StartAsync&gt;d__21`1.MoveNext()
</code></pre>

<p>Everything i've read on this issue don't seem related. Any ideas?</p>
","<p>I recently copied my project to a new directory when I try to run it crashes and shows this error:</p>

<blockquote>
  <p>Unable to start Kestrel.<br>
  System.InvalidOperationException: A path base can only be configured using IApplicationBuilder.UsePathBase().  </p>
  
  <p>at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.ParseAddress(String address, Boolean&amp; https)<br>
  at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.AddressesStrategy.BindAsync(AddressBindContext context)</p>
</blockquote>

<p>The original project runs without any problems. The error only occurs when I copy to a new location. I am using Rider ide. Any help will be highly appreciated - thanks.</p>

<p><a href=""https://i.stack.imgur.com/ILuIQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ILuIQ.png"" alt=""image of the error""></a></p>
"
"42660690","Is it possible to shut down the host machine by executing a command on one of its docker container?","<docker>","61632208","Is it possible to restart a computer from within a docker container on Linux?","<docker>","<p>I have a host machine which has one docker container. The container is active and running a particular service. On meeting a particular condition, I want to remove the container and shut down the machine also. Is it possible to do so?
I am planning to modify the code which runs the service to handle the shutting down of the machine? Any suggestions are welcome!</p>
","<p>Is it possible to run the command reboot on the host from container ? I tried to mount a volume with the command but without any success. </p>

<p>The idea would be to have a container with some logic inside that I can deploy anywhere on Debian based machines and allow me to reboot a server.  </p>
"
"41267305","Docker for Mac VM IP","<macos><docker><docker-machine>","61683926","default docker-machine ip","<macos><docker>","<p>I just migrated to using Docker for Mac, from previously using Docker Toolbox with virtualbox for OSX.</p>

<p>I used to get the machine IP address with <code>$(docker-machine ip default)</code>.</p>

<p>Is there a reliable way to get the Hyperkit IP address?</p>

<p>Thanks!</p>
","<p>normally, the default docker-machine ip is 192.168. 99.100. I work on MAC and want to check it , but when I type </p>

<pre><code>$docker-machine ip 
</code></pre>

<p>I have the following error message : </p>

<pre><code>Error: No machine name(s) specified and no ""default"" machine exists.
</code></pre>

<p>What can I do ?</p>
"
"40366192","Kubernetes how to make Deployment to update image","<docker><kubernetes>","61771351","Kubernetes not updates Docker image on redeployment","<kubernetes>","<p>I do have deployment with single pod, with my custom docker image like:</p>

<pre><code>containers:
  - name: mycontainer
    image: myimage:latest
</code></pre>

<p>During development I want to push new latest version and make Deployment updated.
Can't find how to do that, without explicitly defining tag/version and increment it for each build, and do</p>

<pre><code>kubectl set image deployment/my-deployment mycontainer=myimage:1.9.1
</code></pre>
","<p>Automation builds Docker image with microservice and push this image into JFrog Artifactory registry labeled by branch name <code>registry/service-name:branch</code>. At the next step it applies Kubernetes yaml manifest file and application starts after pulling image at the appropriate Kubernetes node.
The problem is following - when I push changes in microservice source code into repository the automation starts:</p>

<ul>
<li>rebuild the project and push updated docker image into registry with the same label(branch)</li>
<li>redeploy the microservice in Kubernetes</li>
<li>microservice redeployed but with old image</li>
</ul>

<p>I guess it is occurs because there are no changes in 'Deployment' section of Kubernetes yaml manifest file and Kubernetes not pull updated image from JFrog registry. As workaround, I implement inserting timestamp annotation into template section on each redeployment:</p>

<pre><code>  ""template"": {
      ""metadata"": {
          ""labels"": {
              ""app"": ""service-name""
          },
          ""annotations"": {
              ""timestamp"": ""1588246422""
</code></pre>

<p>But miracle is not happened - image updated only when I delete Kubernetes deployment and redeploy the application (may be in this case it just starts at the another node and docker pull is necessary).</p>

<p>Is it possible to setup Kubernetes or configure manifest file some how to force Kubernetes pull image on each redeployment? </p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","61963319","Where Docker store volume on Windows or Mac?","<docker>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I created a docker volume</p>

<pre><code>docker inspect mongodata                                                                                                [
{
    ""CreatedAt"": ""2020-05-22T20:02:02Z"",
    ""Driver"": ""local"",
    ""Labels"": {},
    ""Mountpoint"": ""/var/lib/docker/volumes/mongodata/_data"",
    ""Name"": ""mongodata"",
    ""Options"": {},
    ""Scope"": ""local""
}
</code></pre>

<p>]</p>

<p>Do you know where the docker stores this volume on Windows?</p>

<p>OS: Windows10 64x</p>

<p>Docker Desktop Version: 2.2.0.3</p>
"
"39525820","Docker port forwarding not working","<docker><portforwarding><docker-container>","61973619","Are there any port mapping specific features in a Spring application using Docker Compose?","<java><spring><docker><nginx><docker-compose>","<p>I have setup Docker container for access my machine docker container to another machine in local.</p>

<p>Create a container below command:</p>

<pre><code>    docker run -it -d --name containerName -h www.myhost.net -v /var/www/html -p 7000:8000 --net mynetwork --ip 172.11.0.10 --privileged myimagename bash
</code></pre>

<p>After Create A Container Details:</p>

<pre><code>        CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES

        1e1e5e9b74b4        myimgaename         ""bash""              21 minutes ago      Up 6 minutes        0.0.0.0:7000-&gt;8000/tcp   containername
</code></pre>

<p>NetWork Details:</p>

<pre><code>     ""NetworkSettings"": {
        ""Bridge"": """",
        ""SandboxID"": ""fe357c54c816fff0f9d642037dc9a173be7f7e42a80776d006572f6a1395969e"",
        ""HairpinMode"": false,
        ""LinkLocalIPv6Address"": """",
        ""LinkLocalIPv6PrefixLen"": 0,
        ""Ports"": {
            ""8000/tcp"": [
                {
                    ""HostIp"": ""0.0.0.0"",
                    ""HostPort"": ""7000""
                }
            ]
        }
</code></pre>

<p>if I access docker  ipaddr(172.11.0.10) or hostname(www.myhost.net) in mymachine(hostmachine) it working  </p>

<p>But if I access with <strong>Port</strong> doesn't work: hostmachine ip: 192.168.1.1</p>

<pre><code>  go to the browser  192.168.1.1:7000  hostmachine and locally connected anoter machine also.
</code></pre>

<p>But My 7000 port are listen in hostmachine:      </p>

<pre><code>        # ps aux | grep 7000
        root     10437  0.0  0.2 194792 24572 pts/0    Sl+  12:33   0:00 docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 7000 -container-ip 172.11.0.10 -container-port 8000
        root     10941  0.0  0.0 118492  2324 pts/3    R+   12:44   0:00 grep --color=auto 7000
</code></pre>

<p><strong>update 1:</strong></p>

<pre><code>      $ docker version
        Client:
         Version:      1.11.2
         API version:  1.23
         Go version:   go1.5.4
         Git commit:   b9f10c9
         Built:        Wed Jun  1 21:39:21 2016
         OS/Arch:      linux/amd64

        Server:
         Version:      1.11.2
         API version:  1.23
         Go version:   go1.5.4
         Git commit:   b9f10c9
         Built:        Wed Jun  1 21:39:21 2016
         OS/Arch:      linux/amd64
</code></pre>

<p>Suggest me Why Cannot access my Container to another machine. How to Resolve this Problem</p>
","<p>I have a problem with dockerizing Spring Boot app. My docker-compose project consists of 4 parts:</p>

<ul>
<li>back - it`s just Spring Boot application with Tomcat on 8080. Here are my controllers for front app.</li>
<li>front - Nginx + Angular</li>
<li>core - mainly consists of a TCP server for receiving some information to DB in database-app, implemented on a simple Java Socket.</li>
<li>database - Postgres, which I just download from the Docker Hub and configure to create the database necessary for the back-application.</li>
</ul>

<p>My goal is to use my front-app, which is open in the browser on the host machine, manipulate data from the database from the database-app, through the controllers of my back-app.</p>

<p>So, I don't have any problems with building and running. Ports mapping for core, database and front apps works excellent. But not for back. I don't have any access from host to back-container from <code>localhost:8080</code>(<code>curl</code> requests from the host to container return an empty response, but <code>curl</code> in container bash works fine). In back-app I used Spring Security, so CORS is configured to allow all requests, and CSRF is disabled, if it's matter.</p>

<p>Generously apologize for my broken English!</p>

<p>Back Dockerfile</p>

<pre><code>FROM maven:3.5-jdk-8 AS build
COPY src /usr/src/app/src  
COPY pom.xml /usr/src/app  
RUN mvn -f /usr/src/app/pom.xml clean package

FROM gcr.io/distroless/java  
ARG JAR_FILE=target/*.jar
COPY --from=build /usr/src/app/${JAR_FILE} /usr/app/back.jar
ENTRYPOINT [""java"",""-jar"",""/usr/app/back.jar""]
</code></pre>

<p>Core Dockerfile</p>

<pre><code>FROM maven:3.5-jdk-8 AS build
COPY src /usr/src/app/src  
COPY pom.xml /usr/src/app  
RUN mvn -f /usr/src/app/pom.xml clean package

FROM gcr.io/distroless/java  
ARG JAR_FILE=target/*.jar
COPY --from=build /usr/src/app/${JAR_FILE} /usr/app/core.jar
ENTRYPOINT [""java"",""-jar"",""/usr/app/core.jar""]
</code></pre>

<p>Front Dockerfile</p>

<pre><code>FROM node:12 as builder
COPY package.json package-lock.json ./
RUN npm install &amp;&amp; mkdir /app &amp;&amp; mv ./node_modules ./app
WORKDIR /app
COPY . .
RUN npm run ng build -- --deploy-url=/ --prod

FROM nginx
COPY ./.nginx/nginx.conf /etc/nginx/nginx.conf
RUN rm -rf /usr/share/nginx/html/*
COPY --from=builder /app/dist/snsr-front-app /usr/share/nginx/html
ENTRYPOINT [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>UPDATED 1</strong>:
Dockerfile(s) are still the same.</p>

<p>docker-compose.yml</p>

<pre><code>version: '3'

services:

    snsr-front-app:
        build: ./snsr-front-app
        ports: 
            - 4200:80
        depends_on: 
            - snsr-back-app
        image: mxmtrms/snsr-front-app
        networks: 
            - front-net

    snsr-back-app:
        build: ./snsr-back-app
        depends_on: 
            - database
        image: mxmtrms/snsr-back-app
        networks: 
            - back-net
            - front-net
        expose: 
            - 8080
        environment: 
            DB_URL: database
            DB_PORT: 5432


    snsr-core-app:
        build: ./snsr-core-app
        ports: 
            - 3000:3000
        depends_on: 
            - database
        image: mxmtrms/snsr-core-app
        networks:
            - back-net

    database: 
        image: postgres
        environment: 
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: masterkey
            POSTGRES_DB: snsr
        ports: 
            - 5432:5432
        networks: 
            - back-net
networks:
  back-net:
  front-net:
</code></pre>

<p>nginx.conf</p>

<pre><code>worker_processes 4;

events { worker_connections 1024; }

http {

    upstream frontend {
        server 0.0.0.0:80;
    }
    upstream backend {
        server snsr-back-app:8080;
    }

    server {

        listen 80;
        root  /usr/share/nginx/html;
        include /etc/nginx/mime.types;

        location / {
            proxy_pass http://frontend;
            try_files $uri /index.html;
        }

        location /api {
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_pass http://backend;
        }
    }
}
</code></pre>

<p><strong>UPDATE 2</strong>
Backend logs: <a href=""https://gist.github.com/mxmtrms/ff12e2481d0ccc2781f15a961de6eab9"" rel=""nofollow noreferrer"">https://gist.github.com/mxmtrms/ff12e2481d0ccc2781f15a961de6eab9</a></p>

<p><code>docker ps</code>:
<a href=""https://gist.github.com/mxmtrms/2baaadc0e4873fc8bb28453d5c6d04f4"" rel=""nofollow noreferrer"">https://gist.github.com/mxmtrms/2baaadc0e4873fc8bb28453d5c6d04f4</a></p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","62064151","How to use Docker in Jenkins from Docker (DIND: Docker in Docker)?","<docker><jenkins><docker-compose><jenkins-pipeline>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I create a jenkins with this docker-compose.yml:</p>

<pre><code>version: '3'
services:
  dind:
    image: docker:dind
    privileged: true
    expose:
      - 2375
      - 2376
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
  jenkins:
    image: jenkins/jenkins:lts
    environment:
      DOCKER_HOST: tcp://dind:2375
    ports:
      - 8081:8080
    links:
      - dind
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
      - .myProject:/home/project
</code></pre>

<p>My Jenkins is ready to use. After I use this JenkinsFile in my <code>.myProject</code> Git repository:</p>

<pre><code>pipeline {

    agent none

    stages {
        stage('Docker node test') {
          agent {
            docker {
              image 'node:7-alpine'
              args '--name docker-node' // list any args
            }
          }
          steps {
            // Steps run in node:7-alpine docker container on docker slave
            sh 'node --version'
          }
        }
        stage('Build') { 
            steps {
                sh 'npm install' 
            }
        }
    }
}
</code></pre>

<p>My result is <code>docker: not found</code>:</p>

<pre><code>[Pipeline] withEnv
[Pipeline] {
[Pipeline] isUnix
[Pipeline] sh
+ docker inspect -f . node:7-alpine
/var/jenkins_home/workspace/foo@tmp/durable-d4681405/script.sh: 1: /var/jenkins_home/workspace/foo@tmp/durable-d4681405/script.sh: docker: not found
[Pipeline] isUnix
[Pipeline] sh
+ docker pull node:7-alpine
/var/jenkins_home/workspace/foo@tmp/durable-6312e2a5/script.sh: 1: /var/jenkins_home/workspace/foo@tmp/durable-6312e2a5/script.sh: docker: not found
[Pipeline] }
[Pipeline] // withEnv
[Pipeline] }
[Pipeline] // node
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Build)
Stage ""Build"" skipped due to earlier failure(s)
[Pipeline] }
[Pipeline] // stage
[Pipeline] End of Pipeline
ERROR: script returned exit code 127
Finished: FAILURE
</code></pre>

<h2>EDIT:</h2>

<p>I custom my Jenkins:</p>

<pre><code>version: '3'
services:
  dind:
    image: docker:dind
    privileged: true
    expose:
      - 2375
      - 2376
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
  jenkins:
    build: .docker
    environment:
      DOCKER_HOST: tcp://dind:2375
    ports:
      - 8081:8080
      - 50000:50000
    links:
      - dind
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
      - .myProject:/home/project
</code></pre>

<p>Dokerfile:</p>

<pre><code>FROM jenkins/jenkins:lts

LABEL maintainer=""Antoine Descamps &lt;antoine.descamps@ineat-conseil.fr&gt;""

USER root

RUN echo ""deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main"" &gt;&gt; /etc/apt/sources.list
RUN apt-key adv --keyserver https://keyserver.ubuntu.com:443 --recv-keys 93C4A3FD7BB9C367

RUN apt-get update

RUN curl -fsSL https://get.docker.com | bash -
RUN curl -L ""https://github.com/docker/compose/releases/download/1.22.0/docker-compose-Linux-x86_64"" -o /usr/local/bin/docker-compose
RUN chmod +x /usr/local/bin/docker-compose

RUN usermod -aG docker jenkins

USER jenkins
</code></pre>

<p>Now, my error is:</p>

<pre><code>docker inspect -f . node:7-alpine

Cannot connect to the Docker daemon at tcp://dind:2375. Is the docker daemon running?
</code></pre>
"
"39404280","Make container accessible only from localhost","<docker><portforwarding>","62120030","Prevent public access to MySQL port running using Docker-compose","<mysql><docker><docker-compose>","<p>I have Docker engine installed on Debian Jessie and I am running there container with nginx in it. My ""run"" command looks like this:</p>

<pre><code>docker run -p 1234:80 -d -v /var/www/:/usr/share/nginx/html nginx:1.9
</code></pre>

<p>It works fine, problem is that now content of this container is accessible via <code>http://{server_ip}:1234</code>. I want to run multiple containers (domains) on this server so I want to setup reverse proxies for them. </p>

<p>How can I make sure that container will be only accessible via reverse proxy and not directly from <code>IP:port</code>? Eg.:</p>

<pre><code>http://{server_ip}:1234  # not found, connection refused, etc...
http://localhost:1234  # works fine
</code></pre>

<p><strong>//EDIT:</strong> Just to be clear - I am not asking how to setup reverse proxy, but how to run Docker container to be accessible only from localhost.</p>
","<p>I'm running MySQL using Docker-compose on my server.</p>

<p>This is the related part of my <code>docker-compose.yml</code>:</p>

<pre class=""lang-yaml prettyprint-override""><code>mysql:
    image: mysql:5.7.27
    environment:
      - MYSQL_DATABASE=app
      - MYSQL_ROOT_PASSWORD=secret
    ports:
      - 3306:3306
</code></pre>

<p>Now, it's accessible from the Internet ([Server Static IP]:3306) and it seems so insecure.</p>

<p>I want to <strong>prevent</strong> it from being accessible from the <strong>Internet</strong> and let it only be accessible <strong>inside the server</strong> and <strong>through SSH</strong>.</p>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","62167766","Will(or When) Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><docker-desktop><windows-container>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I'm using Wine to host windows appilcation on centos server. I want to replace it into docker. But currently, <a href=""https://stackoverflow.com/questions/42158596/can-windows-containers-be-hosted-on-linux"">it is impossible</a>. I just want to know if docker team has any plan for hosting Windows container on linux system.</p>
"
"38396139","Docker: change folder where to store docker volumes","<mongodb><amazon-ec2><docker><ubuntu-14.04><volumes>","62170193","Create docker volume in directory","<docker><docker-compose><docker-volume>","<p>On my <code>Ubuntu EC2</code> I host an application using docker containers. <code>db</code> data and <code>upload</code> data is being stored in volumes <code>CaseBook-data-db</code> and <code>CaseBook-data-uploads</code> which are being created with this commands:</p>

<pre><code>docker volume create --name=CaseBook-data-db
docker volume create --name=CaseBook-data-uploads
</code></pre>

<p>Volumes being attached through <code>docker-compose</code> file:</p>

<pre><code>version: '2'
services:
    mongo:
        container_name: ""CaseBook-db""
        restart: always
        image: mongo:3.2.7
        ports:
            - ""27017""
        volumes:
            - data_db:/data/db
        labels:
            - ""ENVIRONMENT_TYPE=meteor""

    app:
        container_name: ""CaseBook-app""
        restart: always
        image: ""meteor/casebook""
        build: .
        depends_on:
            - mongo
        environment:
            - MONGO_URL=mongodb://mongo:27017/CaseBook
        ports:
            - ""80:3000""
        volumes:
            - data_uploads:/Meteor-CaseBook-Container/.uploads
        labels:
            - ""ENVIRONMENT_TYPE=meteor""
volumes:
     data_db:
        external:
            name: CaseBook-data-db
     data_uploads:
        external:
            name: CaseBook-data-uploads
</code></pre>

<p>I need to store those docker volumes in different folder(for example <code>/home/ubuntu/data/</code>) of the host machine. How to change docker storage folder for volumes? Or there is a better way in doing this? Thank you in advance.</p>
","<p>It is possible to create docker volume in my destined directory? </p>

<p>I was not able to found it in documentation, I just saw some mountpoints, but I want to mount it localy, from my defined directory. For example <code>/data/docker/volume_name/</code></p>

<p><code>docker volume create</code>  won't let me do that.</p>
"
"40356259","Mount linux image in docker container","<linux><docker><mount>","62178026","Is it possible to mount ISO file while building image by Dockerfile?","<docker>","<p>For a project I need to mount a linux image inside a docker container running <a href=""https://hub.docker.com/_/ubuntu/"" rel=""noreferrer"">ubuntu</a>. The image I want to mount is Raspbian. I need to access the linux filesystem of the image and add a file.</p>

<p>I access the image by mounting the folder with the volume flag:</p>

<p><code>docker run -it -v /path/to/image/folder:/default ubuntu /bin/bash</code></p>

<p>With <code>fdisk -l raspbian.img</code> I found the offset:</p>

<pre><code>Disk raspbian.img: 1.3 GiB, 1389363200 bytes, 2713600 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x5a7089a1

Device        Boot  Start     End Sectors  Size Id Type
raspbian.img1        8192  137215  129024   63M  c W95 FAT32 (LBA)
raspbian.img2      137216 2713599 2576384  1.2G 83 Linux
</code></pre>

<p>Now when I try to mount the image with <code>mount -o loop,offset=$((137216*512)) raspbian.img /mnt/</code> I get <code>mount: /mnt/: mount failed: Unknown error -1</code>. Can someone explain if I can mount a linux image in a running docker container and if so how?</p>

<p><strong>Edit</strong></p>

<p>Doing the same mount operations in vagrant works perfectly. Are there some limitations to docker mounting filesystems?</p>
","<p>I'm trying to mount the image file in Dockerfile,
but it occurred error.</p>

<p>I think the error is related to there are no loop device in docker, is this suggestion right?</p>

<p>How can I resolve it, is it possible?</p>

<p>This is my Dockerfile as below:</p>

<pre><code>FROM ubuntu:bionic

RUN mkdir -p /mnt/rpi-sysroot                                                   
COPY ubuntu.img /root                  
RUN OFFSET=$(fdisk -l /root/ubuntu.img | grep Linux | awk '{print $2 ""* 512""}' | bc) 
RUN mount -o ro,loop,offset=$OFFSET -t auto /root/ubuntu.img /mnt/rpi-sysroot
</code></pre>
"
"37242217","Access docker container from host using containers name","<docker><docker-compose>","62279012","How do I publish and access jenkins docker container from browser?","<docker>","<p>I am developing a service and using there docker compose to spin services like postgres, redis, elasticsearch. I have a web application that is based on RubyOnRails and writes and reads from all those services.</p>

<p>Here is my <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      - frontapp

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      - frontapp

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      - frontapp

networks:
  frontapp:
    driver: bridge
</code></pre>

<p>And i can ping containers within this network</p>

<pre><code>$ docker-compose run redis /bin/bash
root@777501e06c03:/data# ping postgres
PING postgres (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.346 ms
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.047 ms
...
</code></pre>

<p>So far so good. Now I want to run ruby on rails application on my host machine but be able to access postgres instance with url like <code>postgresql://username:password@postgres/database</code> currently that is not possible</p>

<pre><code>$ ping postgres
ping: unknown host postgres
</code></pre>

<p>I can see my network in docker</p>

<pre><code>$ docker network ls
NETWORK ID          NAME                DRIVER
ac394b85ce09        bridge              bridge              
0189d7e86b33        elephant_default    bridge              
7e00c70bde3b        elephant_frontapp   bridge              
a648554a72fa        host                host                
4ad9f0f41b36        none                null 
</code></pre>

<p>And I can see an interface to it</p>

<pre><code>$ ifconfig 
br-0189d7e86b33 Link encap:Ethernet  HWaddr 02:42:76:72:bb:c2  
          inet addr:172.18.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:76ff:fe72:bbc2/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:36 errors:0 dropped:0 overruns:0 frame:0
          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2000 (2.0 KB)  TX bytes:8792 (8.7 KB)

br-7e00c70bde3b Link encap:Ethernet  HWaddr 02:42:e7:d1:fe:29  
          inet addr:172.20.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e7ff:fed1:fe29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1584 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1597 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:407137 (407.1 KB)  TX bytes:292299 (292.2 KB)
...
</code></pre>

<p>But i am not sure what should I do next. I tried to play a bit with <code>/etc/resolv.conf</code>, mainly with <code>nameserver</code> directive, but that had no effect.</p>

<p>I would appreciate any help of suggestions how to configure this setup correctly.</p>

<p><strong>UPDATE</strong></p>

<p>After browsing through Internet resources I managed to assign static IP addresses to boxes. For now it is enough for me to continue development. Here is my current <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      frontapp:
        ipv4_address: 172.25.0.11

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      frontapp:
        ipv4_address: 172.25.0.12

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      frontapp:
        ipv4_address: 172.25.0.10

networks:
  frontapp:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1
</code></pre>
","<p><strong>What I did:</strong></p>
<p>Created 2 docker containers using the below command and successfully navigated from the browser.</p>
<p><strong>Docker Setup:</strong></p>
<p><strong>Host Machine:</strong> Linux VM Image(Assume IP is <em>10.1.1.1</em>)</p>
<p><strong>Jenkins Docker Command:</strong></p>
<pre><code>sudo docker run --detach \
  --hostname jenkins.local \
  --publish 8080:8080 \
  --publish 50000:50000 \
  --name jenkins \
  --restart always \
  jenkins/jenkins
</code></pre>
<p><strong>GitLab CE Docker Command:</strong></p>
<pre><code>sudo docker run --detach \
  --hostname gitlab.local \
  --publish 50443:443 \
  --publish 7070:80 \
  --publish 5022:22 \
  --name gitlab \
  --restart always \
  --volume $GITLAB_HOME/gitlab/config:/etc/gitlab:Z \
  --volume $GITLAB_HOME/gitlab/logs:/var/log/gitlab:Z \
  --volume $GITLAB_HOME/gitlab/data:/var/opt/gitlab:Z \
  gitlab/gitlab-ce:latest
</code></pre>
<p><strong>Browser Navigation:</strong></p>
<ul>
<li>Jenkins: <a href=""http://10.1.1.1:8080/"" rel=""nofollow noreferrer"">http://10.1.1.1:8080/</a> - Success</li>
<li>GitLab : <a href=""http://10.1.1.1:7070/"" rel=""nofollow noreferrer"">http://10.1.1.1:7070/</a> - Success</li>
</ul>
<p><strong>What I need to achieve?</strong></p>
<p>I need to access the docker applications using a friendly URL without the port as given below.</p>
<ul>
<li><a href=""http://10.1.1.1:8080/"" rel=""nofollow noreferrer"">http://10.1.1.1:8080/</a> by --&gt; <a href=""http://gitlab.test.local"" rel=""nofollow noreferrer"">http://gitlab.test.local</a></li>
<li><a href=""http://10.1.1.1:7070/"" rel=""nofollow noreferrer"">http://10.1.1.1:7070/</a> by --&gt; <a href=""http://jenkins.test.local"" rel=""nofollow noreferrer"">http://jenkins.test.local</a></li>
</ul>
"
"41972328","Docker History Base Image Add:sha256hash","<linux><docker><hash><dockerfile><sha256>","62315791","In a Docker image's history what is ""ADD file:<digest> in /""?","<docker><dockerfile><containers><dockerhub>","<p>I'm trying to better understand the <code>docker history</code> output. When I run <code>docker history nginx:latest</code> I get output that nearly matches the <a href=""https://github.com/nginxinc/docker-nginx/blob/e950fa7dfcee74933b1248a7fe345bdbc176fffb/mainline/jessie/Dockerfile"" rel=""noreferrer"">Dockerfile</a>:</p>

<pre><code>/bin/sh -c #(nop) CMD [""nginx"" ""-g"" ""daemon off;""]
/bin/sh -c #(nop) EXPOSE 443/tcp 80/tcp/bin/sh -c ln -sf /dev/stdout /var/log/nginx/access.log  &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log
/bin/sh -c apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62  &amp;&amp; echo ""deb http://nginx.org/packages/mainline/debian/ jessie nginx"" &gt;&gt; /etc/apt/sources.list &amp;&amp; apt-get update &amp;&amp; apt-get install --no-install-recommends --no-install-suggests -y       ca-certificates nginx=${NGINX_VERSION} nginx-module-xslt nginx-module-geoip nginx-module-image-filter nginx-module-perl nginx-module-njs gettext-base  &amp;&amp; rm -rf /var/lib/apt/lists/*
/bin/sh -c #(nop) ENV NGINX_VERSION=1.11.9-1~jessie
/bin/sh -c #(nop) MAINTAINER NGINX Docker Maintainers ""docker-maint@nginx.com""
/bin/sh -c #(nop) CMD [""/bin/bash""]
/bin/sh -c #(nop) ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /
</code></pre>

<p>with three notable exceptions</p>

<ol>
<li>All of the lines start with <code>/bin/sh -c #(nop)</code> except for the third line which is the <code>RUN</code> command in the Dockerfile - no big deal</li>
<li>The commands are in reverse (the last command in the Dockerfile is the first command listed with <code>docker history</code>) - also no big deal</li>
<li><p>This one's the kicker - The <code>FROM debian:jessie</code> line from the Dockerfile is translated to:</p>

<p><code>ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /</code>
<code>CMD [""/bin/bash""]</code></p></li>
</ol>

<p>It took me a little while to realize that the last two commands above (the <code>ADD</code> and <code>CMD [""/bin/bash""]</code> lines) were carried over from the base image <code>debian:jessie</code>. Once I figured that out, I thought to myself, ""self, the <code>file:89ec...da957</code> must be the sha256 hash of the <a href=""https://github.com/tianon/docker-brew-debian/blob/b39d31635ca26c8b1f3d982090ba8d54167c4d85/jessie/rootfs.tar.xz"" rel=""noreferrer"">rootfs.tar.xz</a> included as the file system. But no, the sha256 hash of the rootfs.tar.xz is <code>467328e24c316fd058f086eb8eb77706f3f448ad8886d202e7c9687d30692eca</code>.</p>

<p>Herein lies my question: <strong>Where does the hash listed in <code>docker history</code> come from? And why is it different than the actual hash of rootfs.tar.xz?</strong></p>

<p>I've thoroughly reviewed much of Docker's documentation, with no luck, including:</p>

<ol>
<li><a href=""https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/"" rel=""noreferrer"">https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/</a></li>
<li><a href=""https://docs.docker.com/engine/reference/commandline/history/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/commandline/history/</a></li>
<li><a href=""https://docs.docker.com/engine/reference/builder/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/builder/</a></li>
</ol>

<p>The hash is consistent across all images that use <code>debian:jessie</code> as the base image. Even <code>docker history debian:jessie</code> shows the same hash:</p>

<pre><code>/bin/sh -c #(nop) CMD [""/bin/bash""]
/bin/sh -c #(nop) ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /
</code></pre>

<p>and I think you might agree, that there is only one file that could possibly have a hash in the <code>debian:jessie</code> Dockerfile:</p>

<pre><code>FROM scratch
ADD rootfs.tar.xz /
CMD [""/bin/bash""]
</code></pre>

<p>If anyone could provide some insight or point me to a resource I have yet to find, it would be much appreciated.</p>
","<p>On Docker Hub when viewing the ""Image History"" for an image, I commonly see this as the first line:</p>

<pre><code>ADD file:&lt;SHA256 digest&gt; in /
</code></pre>

<p>For example:</p>

<pre><code>ADD file:c3e6bb316dfa6b81dd4478aaa310df532883b1c0a14edeec3f63d641980c1789 in / 
</code></pre>

<p>I'm guessing it's some other docker image being used as the base, but I can't find any docs for that, and I can't find a way to easily search for that digest to better understand what it is.</p>

<ol>
<li>Can you please reference any docs that can help explain? </li>
<li>Is there a way to search for that digest to help find what it is?</li>
</ol>
"
"40454470","How can I use a variable inside a Dockerfile CMD?","<docker><dockerfile>","62322343","How to use ENV variables within Dockerfile CMD command","<docker><dockerfile>","<p>Inside my Dockerfile:</p>

<pre><code>ENV PROJECTNAME mytestwebsite
CMD [""django-admin"", ""startproject"", ""$PROJECTNAME""]
</code></pre>

<p>Error:</p>

<pre><code>CommandError: '$PROJECTNAME' is not a valid project name
</code></pre>

<p>What is the quickest workaround here?  Does Docker have any plan to ""fix"" or introduce this functionality in later versions of Docker?</p>

<p>NOTE:  If I remove the CMD line from the Docker file and then run the Docker container, I am able to manually run Django-admin startproject $PROJECTNAME from inside the container and it will create the project...</p>
","<p>Dockerfile</p>

<pre><code>ENV APP_JAR = ""/app/service/lib/service-1.0.0.jar""

# CMD [""java"", ""-jar"", ""/app/service/lib/service-1.0.0.jar""]
CMD [""java"", ""-jar"", $APP_JAR]
</code></pre>

<p>The above fails to start my jar, what should I do to use ENV var properly</p>
"
"43746782","Running nuxt js application in Docker","<javascript><docker><vue.js><nuxt.js>","62352284","Docker mapped port is not accessible outside","<docker><dockerfile><nuxt.js>","<p>I'm trying to run nuxt application in docker container. In order to do so, I created the following Dockerfile:</p>

<pre><code>FROM node:6.10.2

RUN mkdir -p /app

EXPOSE 3000

COPY . /app
WORKDIR /app
RUN npm install
RUN npm run build

CMD [ ""npm"", ""start"" ]
</code></pre>

<p>However, when I build the image and run the container (<code>docker run -p 3000:3000 &lt;image-id&gt;</code>) I get nothing while hitting <code>localhost:3000</code> in my browser. What could be the cause?</p>
","<p>I'm running:</p>

<p><code>sudo docker run -d -p 9001:9001 --rm --name &lt;cname&gt; &lt;img&gt;</code></p>

<p>then I go to my browser at <code>localhost:9001</code>, no connection.</p>

<p>If I run:</p>

<p><code>sudo docker run -d --network=host --rm --name &lt;cname&gt; &lt;img&gt;</code></p>

<p>I can access the application at <code>localhost:9001</code> from my browser.</p>

<p>Running the first command, I can verify it's running properly inside docker by running:</p>

<p><code>sudo docker exec &lt;cname&gt; wget localhost:9001</code> which returns a page as expected.</p>

<p>If it is useful: the application running is a standard <code>nuxt.js</code> that listens on port <code>9001</code>, the dockerfile used to generate the image is (ran npm build before docker image build)</p>

<pre><code>FROM node:lts-alpine

WORKDIR /app/
COPY . /app/

EXPOSE 9001

ENTRYPOINT npm start
</code></pre>

<p>The docker version I'm using is <code>19.03.8-ce</code>. How would I fix this ?</p>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","62357876","devilbox_httpd_1 error address already in use","<apache><docker><docker-compose><port>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I am using macOS Catalina 10.15 </p>

<p>I do: </p>

<pre><code>docker-compose up -d httpd php mysql 
</code></pre>

<p>And I get: </p>

<pre><code>Creating devilbox_httpd_1 ... error

ERROR: for devilbox_httpd_1  Cannot start service httpd: Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use

ERROR: for httpd  Cannot start service httpd: Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use
</code></pre>

<p>When I go to Network Utility Monitor/ Port Scan for localhost and also for my IP address I find:</p>

<pre><code>Port Scanning host: 127.0.0.1

     Open TCP Port:     80          http
     Open TCP Port:     1053        remote-as
</code></pre>

<p>Firewall is not activated. </p>

<p>What else can I do/check to solve this error?? </p>

<p>Thanks a ton </p>
"
"42153759","MySQL container crash after /etc/mysql/my.cnf change, how to edit back?","<mysql><docker><crash><debian><containers>","62379708","How to access the inside of the container without running the container","<macos><docker><docker-compose><mariadb>","<p>I changed some mysql config settings and set something wrong, now Docker container keeps restarting and I cannot find the <code>my.cnf</code> file to edit in host filesystem. I have tried aufs/diff folders but so far unable to find it. Also tried:</p>

<pre><code>find / -name my.cnf -exec nano {} \;
</code></pre>

<p>But it does not bring up the file I changed. And I tried to change <code>config.v2.json</code> to start <code>/bin/bash</code> instead of <code>mysqld</code> and restarted docker, but yet it started <code>mysqld</code> (due supervisor or something?) using official mysql container image.</p>
","<p>Good morning.
I am currently using Docker version 19.03 on Mac OS X Catalina.
MariaDB 10.3 was installed in Docker, vim was installed to set the character-set, and /etc/mysql/my.cnf file was modified.</p>

<p>After modification, an attempt was made to restart to reflect, but it was not executed normally.
When I checked it with docker ps -a command, STATUS showed an Exited (1) error.</p>

<p>When I checked the log showing the error, I could check the following log.</p>

<p>unknown variable'collection-server=utf8_unicode_ci'</p>

<p>Stupidly there was a typo in the settings.</p>

<p>So I am trying to modify this setting, but there is no way to modify it because the docker container is not loaded.</p>

<p>docker-compose.yml is not in use.</p>

<p>The simplest way is to delete the Docker Container, reset it, but I don't think this is the right way.</p>

<p>Is there a way to modify /etc/mysql/my.cnf inside Docker Container without using docker-compose.yml?</p>
"
"48034906","Node Docker Runs, but can't see the application","<node.js><docker><hapijs>","62499052","docker-compose up is not mapping port","<node.js><postgresql><docker><docker-compose>","<p>It appears that my Hapi app is running in a Docker container, but I can't hit it in the browser. I thought that <code>docker run -d -p 8080:3000</code> would have done it, but I guess not. I'm running boot to docker and neither <code>http://localhost:8080/hello</code> nor <code>http://192.168.99.100:8080/hello</code> is working.</p>

<p>I've tried tons of variations on this as well.</p>

<p>This is what I see when I run <code>docker inspect &lt;container id&gt;</code>:</p>

<pre><code>Server running at: http://localhost:8080
</code></pre>

<p>Here's my Hapi.js server:</p>

<pre><code>'use strict';

const Hapi = require('hapi');

// Create a server with a host and port
const server = Hapi.server({
    host: 'localhost',
    port: 3000
});

// Add the route
server.route({
    method: 'GET',
    path:'/hello',
    handler: function (request, h) {
        return 'hello world';
    }
});

async function start() {

    try {
        await server.start();
    }
    catch (err) {
        console.log(err);
        process.exit(1);
    }

    console.log(`App running at: ${server.info.uri}/hello`);
}

start();
</code></pre>

<p>Here's my Dockerfile:</p>

<pre><code>FROM node:8.9.3

MAINTAINER My Name &lt;email@email.com&gt;

ENV NODE_ENV=production
ENV PORT=3000
ENV user node

WORKDIR /var/www
COPY package.json yarn.lock ./

RUN cd /var/www &amp;&amp; yarn

COPY . .

EXPOSE $PORT

ENTRYPOINT [""yarn"", ""start""]
</code></pre>

<p>Here's my package.json:</p>

<pre><code>{
    ""name"": ""my-app"",
    ""version"": ""1.0.0"",
    ""repository"": ""https://github.com/myname/myrepo.git"",
    ""author"": ""My Name"",
    ""license"": ""MIT"",
    ""private"": true,
    ""dependencies"": {
        ""hapi"": ""17.2.0""
    },
    ""scripts"": {
        ""start"": ""node ./src/server""
    }
}
</code></pre>
","<p>I know this has been asked multiple times already, but I am not able to find any solution to this. I am creating an application with Node.js and is trying to use <code>postgres</code> as a database with it. I am using <code>docker-compose</code> to start both the web server as well as the postgres server container. Here's how my <code>docker-compose.yml</code> file looks like -</p>
<pre class=""lang-yaml prettyprint-override""><code>
version: '3'
services:
  web:
    build: .
    ports:
      - 3000:3000
  postgres:
    image: 'postgres:12'
    environment:
      - POSTGRES_USER=my_admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=my_db
    ports:
      - 5432:5432
    volumes:
      - my_db:/var/lib/postgresql/data
volumes:
  my_db:
</code></pre>
<p>Here's how my dockerfile looks like -</p>
<pre><code>FROM node:12.16.2

ENV NODE_ENV production
# Add Tini
ENV TINI_VERSION v0.18.0
RUN wget https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini
RUN chmod +x /tini
ENTRYPOINT [&quot;/tini&quot;, &quot;-e&quot;, &quot;143&quot;, &quot;--&quot;]

# Setup code
WORKDIR /app

COPY package.json yarn.lock /app/
RUN yarn install --frozen-lockfile

COPY . /app

# Run on port 3000
EXPOSE 3000
CMD [&quot;yarn&quot;, &quot;start&quot;]
USER node
</code></pre>
<p>When I run <code>docker-compose up</code>, both the containers start and I can see the output. But when I try to run <code>curl -i http://localhost:3000</code> it shows an error, but works if I ssh into the container.</p>
<p>Am I missing something here?</p>
<p><strong>Update:</strong></p>
<p>Running <code>docker ps</code> shows this -</p>
<pre class=""lang-sh prettyprint-override""><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
0b2a43079005        postgres:12         &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 7 minutes        0.0.0.0:5432-&gt;5432/tcp   todo_postgres_1
0b0f1afd4fa1        todo_web            &quot;/tini -e 143 -- yar…&quot;   7 minutes ago       Up 7 minutes        0.0.0.0:3000-&gt;3000/tcp   todo_web_1
</code></pre>
<p><strong>Update 2:</strong></p>
<p>Here's how my. <code>app.ts</code> file looks like (I am using <code>hapijs</code>) -</p>
<p>import Hapi from '@hapi/hapi';</p>
<pre class=""lang-js prettyprint-override""><code>const init = async () =&gt; {
  const server = Hapi.server({
    port: 3000,
    host: 'localhost',
  });

  server.route({
    method: 'GET',
    path: '/',
    handler: (request, reply) =&gt; {
      return reply.response('Hello');
    },
  });

  await server.start();
  console.log('Server running on %s', server.info.uri);
};

process.on('unhandledRejection', (err) =&gt; {
  console.log(err);
  process.exit(1);
});

init();
</code></pre>
"
"46879196","MySQLi not found dockerized php","<php><docker><mysqli><mysqlnd>","62513604","Docker- Call to undefined function mysqli_connect()","<php><docker><mysqli><docker-compose>","<p>I'm trying to dockerize my website. I've got Nginx and PHP up and running and it's working find except I can't connect to a db. When the page is loaded I get the error:</p>

<pre><code>Fatal error: Uncaught Error: Class 'MySQLi' not found in /private/conn.php:8 Stack trace: #0 /public_html/index.php(2): require() #1 {main} thrown in /private/conn.php on line 8
</code></pre>

<p>My connection line on line 8 is:</p>

<pre><code>$db = new mysqli(""$host"", ""$user"", ""$pass"", ""$db"", ""$port"");
</code></pre>

<p>Now this used to work fine on my old set up but since moving to a new one and installing php7.1.5 I can't get it running. Now, I haven't used the mysqlnd so this may be a misunderstanding on my part but the mysqlnd includes the mysqli driver.</p>

<p>Output of phpinfo:
<a href=""https://i.stack.imgur.com/SwS4S.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SwS4S.jpg"" alt=""config""></a>
<a href=""https://i.stack.imgur.com/RmAl0.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RmAl0.jpg"" alt=""mysqlnd""></a></p>

<p>How can I get this running? Or should I be using a different driver now?</p>
","<p>I am getting below error when trying to access from browser &quot;http://192.168.x.x:9091/testcon.php&quot;</p>
<pre><code>Fatal error: Uncaught Error: Call to undefined function mysqli_connect() in /var/www/html/testcon.php
</code></pre>
<p>Contents of testcon.php is like below,</p>
<pre><code>&lt;?php
$servername = &quot;db&quot;;
$username = &quot;root&quot;;
$password = &quot;pass1234&quot;;
$dbname='server';
$conn = mysqli_connect($servername, $username, $password, $dbname);

// Check connection
if ($conn-&gt;connect_error) {
 die(&quot;Connection failed: &quot; . $conn-&gt;connect_error);
}
echo &quot;Connected successfully&quot;;
?&gt;
</code></pre>
<p>But the same is working from inside container. If i run like &quot;php testcon.php&quot; it will work</p>
<pre><code>root@e951040c5265:/var/www/html# php testcon.php
Connected successfullyroot@e951040c5265:/var/www/html#
</code></pre>
<p>I have also edited /usr/local/etc/php/php.ini-development and uncommented line &quot;extension=mysqli&quot;, Still facing same issue</p>
<p>docker images used are below,</p>
<p>mysql:8.0.1</p>
<p>php:7.4.7-apache</p>
<p>phpmyadmin/phpmyadmin</p>
<p>TIA</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","62603737","How to fix Docker : ""Got permission denied while trying to connect to the Docker daemon socket ""","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>After installing the docker, if I try login to docker hub registry. I am getting following error:</p>
<pre><code>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/auth: dial unix /var/run/docker.sock: connect: permission denied
</code></pre>
<p>Just I would like to know, why I got this issue and how to resolve issue?</p>
"
"38682114","HostPath with minikube - Kubernetes","<docker><kubernetes><google-cloud-platform><dockerfile><docker-volume>","62659524","volume hostPath of a directory in the host machine","<kubernetes><volume>","<p>UPDATE:
I connected to the minikubevm and I see my host directory mounted but there is no files there. Also when I create a file there it will not in my host machine. Any link are between them</p>

<p>I try to mount an host directory for developing my app with kubernetes.</p>

<p>As the doc recommended, I am using minikube for running my kubernetes cluster on my pc. The goal is to create a develop environment with docker and kubernetes for develop my app. I want to mount a local directory so my docker will read the code app from there. But it is not work. Any help would be really appreciate.</p>

<p>my test app (server.js):</p>

<pre><code>var http = require('http');
var handleRequest = function(request, response) {
response.writeHead(200);
response.end(""Hello World!"");
}
var www = http.createServer(handleRequest);
www.listen(8080);
</code></pre>

<p>my Dockerfile:</p>

<pre><code>FROM node:latest
WORKDIR /code
ADD code/ /code
EXPOSE 8080
CMD server.js
</code></pre>

<p>my pod kubernetes configuration: (pod-configuration.yaml)</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: apiserver
spec:
  containers:
  - name: node
    image: myusername/nodetest:v1
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: api-server-code-files
      mountPath: /code
  volumes:
  - name: api-server-code-files
    hostPath:
      path: /home/&lt;myuser&gt;/Projects/nodetest/api-server/code
</code></pre>

<p>my folder are:</p>

<pre><code>/home/&lt;myuser&gt;/Projects/nodetest/
- pod-configuration.yaml
- api-server/
    - Dockerfile
    - code/
        - server.js
</code></pre>

<p>When I running my docker image without the hostPath volume it is of course works but the problem is that on each change I will must recreate my image that is really not powerful for development, that's why I need the volume hostPath.</p>

<p>Any idea ? why i don't success to mount my local directory ?</p>

<p>Thanks for the help.</p>
","<p>I am using kubernetes (minikube) inside a MAC machine , and for my project i need to set up a mountVolume , pointing to a folder ==&gt; /Users/macbookpro/Desktop/htmldocker/phpdocker</p>
<p>NB: Minikube runs by default in a VM (on MAC and windows) , people using Linux distribution won't have this issue</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
 name: html
spec:
 replicas: 1
 selector:
  matchLabels:
   app: app
 template:
   metadata:
    labels:
     app: app
   spec:
    containers:
    - name: php-mysqli
      image: walidledocker/php-mysqli
      volumeMounts:
       - mountPath: /var/www/html/
         name: volume
    volumes:
     - name: volume
      hostPath:
        path: /Users/macbookpro/Desktop/htmldocker/phpdocker
        type: Directory
</code></pre>
<p>i get this error :</p>
<pre><code>kubelet, minikube  MountVolume.SetUp failed for volume &quot;volume&quot; : hostPath type check failed: /Users/macbookpro/Desktop/htmldocker/phpdocker is not a directory
</code></pre>
"
"38980547","Multiple Docker containers, same image, different config","<docker><containers>","62673792","How to run multiple containers of docker image with different configuration in it","<python><docker>","<p>I'm totally new to Docker so I appreciate your patience.</p>

<p>I'm looking for a way to deploy multiple containers with the same image, however I need to pass in a different config (file) to each? </p>

<p>Right now, my understanding is that once you build an image, that's what gets deployed, but the problem for me is that I don't see the point in building multiple images of the same application when it's only the config that is different between the containers.</p>

<p>If this is the norm, then I'll have to deal with it however if there's another way then please put me out of my misery! :)</p>

<p>Thanks!</p>
","<p>I have a <code>opencv python</code> project where I am reading <code>rtsp</code> url from <code>config.json</code> and based on that I am doing some logic. Now I have created a docker image for this and its container runs fine.</p>
<p>Now I need to run 2 containers of the same docker image but both of the containers will use different rtsp url which means I need to have 2 config files in the project. But the python docker image I have uses only 1 config.json</p>
<p>Is there any way during run time I can provide <code>config1.json</code> <code>config2.json</code> respectively to both the containers and for each container it acts like <code>config.json</code>.</p>
"
"37540921","Push docker image to amazon ecs repository","<amazon-web-services><docker><containers>","62712502","How do I push a Docker image to amazon ECR?","<amazon-web-services><amazon-ecr>","<p>Im new to AWS. I want to set up a private docker repository on an AWS ECS container instance. I created a repository named <code>name</code>. The example push commands shown by AWS are working.</p>

<pre><code>aws ecr get-login --region us-west-2 
docker build -t name .
docker tag name:latest ############.dkr.ecr.us-west-2.amazonaws.com/name:latest 
docker push ############.dkr.ecr.us-west-2.amazonaws.com/name:latest
</code></pre>

<p>But with this commands I build and pushed an image named <code>name</code> and I want to build an image named <code>foo</code>. So I altered the commands to:</p>

<pre><code>docker build -t foo .
docker tag foo ###########.dkr.ecr.us-west-2.amazonaws.com/name/foo
docker push ###########.dkr.ecr.us-west-2.amazonaws.com/name/foo
</code></pre>

<p>This should work, but it doesn't. After a period of retrys I get the error:</p>

<pre><code>The push refers to a repository [###########.dkr.ecr.us-west-2.amazonaws.com/name/foo]
8cc63cf4528f: Retrying in 1 second
...
name unknown: The repository with name 'name/foo' does not exist in the registry with id '############'
</code></pre>

<p>Does AWS really require a dedicated repository for every image i want to push?</p>
","<p>I am using the cli and I want to push a docker image to AWS ECR. How can I do this? Thanks!</p>
"
"49754286","Multiple images, one Dockerfile","<docker><dockerfile>","62794200","Docker image build with shared library files","<docker><docker-compose><dockerfile>","<p>How to create two images in one Dockerfile, they only copy different files.</p>

<p>Shouldn't this produce two images <strong>img1</strong> &amp; <strong>img2</strong>, instead it produces two unnamed images <strong>d00a6fc336b3</strong> &amp; <strong>a88fbba7eede</strong></p>

<p>Dockerfile:</p>

<pre><code>FROM alpine as img1
COPY file1.txt .

FROM alpine as img2
COPY file2.txt .
</code></pre>

<p>Instead this is the result of <em>docker build .</em></p>

<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
&lt;none&gt;              &lt;none&gt;              d00a6fc336b3        4 seconds ago       4.15 MB
&lt;none&gt;              &lt;none&gt;              a88fbba7eede        5 seconds ago       4.15 MB
alpine              latest              3fd9065eaf02        3 months ago        4.15 MB
</code></pre>
","<p>Here is my file structure:</p>
<pre><code>-- Dir1
---- Dockerfile
---- index.php
-- Dir2
---- Dockerfile
---- index.php
-- SharedLibrary
---- class.php
</code></pre>
<p>I want to build multiple images with <code>docker-compose</code>, and they both need the shared file in the other directory.</p>
<p>But <code>Dockerfile</code> can't add files with a relative path.</p>
<p>What does one usually do to resolve this?</p>
<p>By the way, it's not only run with local docker, so it should not handle with volume.</p>
<hr />
<p>Additional Information:</p>
<ul>
<li>I want to make a <code>shellscript</code> to copy the required files to each directory and run <code>docker-compose</code>.</li>
<li>I just want to know how to solve this issue gracefully.</li>
</ul>
"
"42280792","Reuse inherited image's CMD or ENTRYPOINT","<shell><docker><dockerfile>","62869559","How do I inject a command to be executed prior to entrypoint in docker-compose?","<bash><docker><docker-compose>","<p>How can I include my own shell script <code>CMD</code> on container start/restart/attach, without removing the <code>CMD</code> used by an inherited image?</p>

<p>I am using this, which does execute my script fine, but appears to overwrite the PHP <code>CMD</code>:</p>

<pre><code>FROM php

COPY start.sh /usr/local/bin

CMD [""/usr/local/bin/start.sh""]
</code></pre>

<p>What should I do differently?  I am avoiding the prospect of copy/pasting the ENTRYPOINT or CMD of the parent image, and maybe that's not a good approach.</p>
","<p>I'm certain that I at some point read about how to run a specific command as a part of docker-compose, and then afterward still run whatever that docker-compose would be running by default. But now when I need it, I cannot find it.</p>
<p>I've got a dockerfile. It specifies some command that should be run when the rest of the docker-compose.yml file has been carried out. As I recall it, I would need to specify a bash script at the entrypoint, replacing whatever would be run by default. And then in that script, I could add something, as the final command that would run whatever the default command was.</p>
<p>example.sh:</p>
<pre><code>echo Hello World!
[Something that runs the default command]
</code></pre>
<p>Putting example.sh in my entrypoint, would then run the echo first, and then default command.</p>
<p>I would like to be able to do that. Or some other solution that achieves the same thing.</p>
"
"47269579","NaCl helper process running without a sandbox! error when running npm tests","<google-chrome><docker><npm>","62921654","How to run headless Chrome on ubuntu terminal?","<python><selenium><google-chrome><ubuntu>","<p>I'n having a problem when running some npm test. The error I'm receiving is: ""NaCl helper process running without a sandbox!"", which is true, as I'm running the browser with the ""--no-sandbox"" option. I have to run this option due to the fact that the browser runs as root, and I don't have an option to run it a different user at all(it's a docker image).
Can anyone please help me to sort it out?</p>

<p>P.S I'm installing the browser in the following way:</p>

<pre><code>RUN apt-get update
RUN apt-get install -y nodejs npm
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -
RUN sh -c 'echo ""deb https://dl.google.com/linux/chrome/deb/ stable main"" &gt;&gt; /etc/apt/sources.list.d/google.list'
RUN apt-get install -y apt-transport-https
RUN apt-get update
RUN apt-get install -y google-chrome-stable
</code></pre>

<p>Thanks in advance!</p>
","<p>I'm new to ubuntu and web scraping with selenium. Recently I ran into problems in running my python web scraping scripts on a Ubuntu server. I accessed the ubuntu server remotely via my terminal.</p>
<p>When I tried to run the code on Ubuntu server but got the error below:</p>
<p><code>selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally</code>
<code>(unknown error: DevToolsActivePort file doesn't exist)</code>
<code>(The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)</code>
<code>(Driver info: chromedriver=2.41.578700</code> <code>(2f1ed5f9343c13f73144538f15c00b370eda6706),platform=Linux 4.15.0-74-generic x86_64)</code></p>
<p>I've already installed chrome, chrome driver and selenium on the server. When I checked the location of Chrome on the server, it returns:</p>
<p><code>google-chrome: /usr/bin/google-chrome</code></p>
<p>When I tried to run google chrome via <code>/usr/bin/google-chrome</code>, it returns the error below:</p>
<p><code>~$ [0715/183414.382630:ERROR:nacl_helper_linux.cc(308)] NaCl helper process running without a sandbox!</code>
<code>Most likely you need to configure your SUID sandbox correctly</code></p>
<p>I suppose the problem is there is no display for the browser? How can I solve this problem ?</p>
<p>Thank you very much.</p>
"
"45858031","How to get Elastic Container Repository URI from Cloud Formation?","<amazon-web-services><docker><amazon-s3><amazon-cloudformation><amazon-ecs>","62934626","AWS stack output ECR image name","<amazon-cloudformation><amazon-ecr>","<p>I'm trying to create an Elastic Container Service (ECS) setup from Cloud Formation. </p>

<p>However I don't want the ECS repository to have the ugly autogenerated URI:</p>

<p><strong>111111111.dkr.ecr.us-east-1.amazonaws.com/docker-repo.company.com</strong></p>

<p>but instead I want it to have a nice and shiny </p>

<p><strong>docker-repo.company.com</strong></p>

<p>The repository itself does not allow setting the URI or <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=223934"" rel=""noreferrer"">even a CNAME</a>. So I'm trying to setup a S3 bucket to redirect to the repo. However unless I'm missing something, Cloud Formation doesn't support this since using !Ref or !GetAtt there's nothing I can query in the AWS::ECR::Repository object that will give me the repository URI.</p>

<p>Am I missing something? Thanks!</p>
","<p>I'm using cloudformation to create a set of ECR repositories to use in my root script. For the task defintions I need to have the image names of these created repositories in a format like this:</p>
<pre><code>{ACC_NR}.dkr.ecr.eu-central-1.amazonaws.com/{REPO_NAME}
</code></pre>
<p>When I output the created ECR in the outputs I only have 2 options. I can either get the {REPO_NAME} without the rest of the information in front of it or I will get the ARN, which contains more or less the same information but structured differently.</p>
<p>What is the best way to get the image name in a variable inside a CF template? Basically I need the string that you would get when nagivating to the ECR and pressing the &quot;copy&quot; button next to the repository.</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","63008434","How to login from one container to another","<docker><containers>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>I have 2 containers: <code>c1</code> and <code>c2</code>. How can I run a script in <code>c1</code> from <code>c2</code>? Thank you in advance!</p>
"
"41203970","Pull image Azure Container Registry - Kubernetes","<azure><docker><kubernetes><azure-container-service><azure-container-registry>","63012200","k8s deploymet with image from private registry","<azure><docker><kubernetes>","<p>Does anyone have any advice on how to pull from Azure container registry whilst running within Azure container service (kubernetes)</p>

<p>I've tried a sample deployment like the following but the image pull is failing:</p>

<pre><code>kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: jenkins-master
spec:
  replicas: 1
  template:
    metadata:
      name: jenkins-master
      labels:
        name: jenkins-master
    spec:
      containers:
      - name: jenkins-master
        image: myregistry.azurecr.io/infrastructure/jenkins-master:1.0.0
        imagePullPolicy: Always
        readinessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 20
          timeoutSeconds: 5
        ports:
        - name: jenkins-web
          containerPort: 8080
        - name: jenkins-agent
          containerPort: 50000
</code></pre>
","<p>I've k8s deployment yaml which I need to pull image
from private registry
where should I put the</p>
<pre><code>host 
user 
password 
</code></pre>
<p>deployment.yml</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: tra
  namespace: ba
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tra
  template:
    metadata:
      labels:
        app: tra
    spec:
      containers:
        - name: tra
          image: de/sec:0.0.10
          imagePullPolicy: Always
          ports:
            - containerPort: 5000
</code></pre>
<p>I found this but it doesnt really helps</p>
<p><a href=""https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</a></p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","63070313","New user account - permission denied to enter a docker","<docker><user-accounts>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I work on a server (Ubuntu 20.04 LTS (GNU/Linux 5.4.0-40-generic x86_64)).
Somebody else set-up some folders and mysql data bases in a docker <em>redcap-lamp</em> on this server.
In addition, he created a user account for me to enter the server, and the docker.</p>
<p>Every thing works fine with my account.<br />
As login on the server I use: <code>ssh [my user account]@[our server]</code>.<br />
To enter the docker I (was told to) use: <code>docker exec -it redcap-lamp bash</code>.</p>
<p>I now had to create a user account for somebody else (and am not used to working in terminals and dockers).</p>
<p>I created a new user account by using: <code>sudo adduser [newUser]</code>, what worked fine.
It is possible to log in with this new account to have access to our server.
Unfortunately, when I use <code>docker exec -it redcap-lamp bash</code> to enter the docker with the new account, the permission is always denied. I get the output:</p>
<p><code>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/redcap-lamp/json: dial unix /var/run/docker.sock: connect: permission denied</code></p>
<p>Do I have to give new users some 'privileges' or something? Does anybody know where the problem lies and how it can be fixed?</p>
<p>Thanks,<br />
Danny</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","63072408","is it possible to call command belonging to another container in docker","<php><docker><nginx><docker-compose><volumes>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>Is it possible to call a command inside docker which belongs to another container.</p>
<p>An example would be:</p>
<p>php container which contains php
app container which contains nginx</p>
<p>I would like to somehow share the php executable inside the nginx container.</p>
<p>Currently I have tried sharing the volumes with no luck.</p>
<p>Any ideas?</p>
"
"35560894","Is Docker ARG allowed within CMD instruction","<docker><docker-compose>","63122324","docker argument not getting detected","<docker><kubernetes><jenkins-pipeline>","<p>I have a Dockerfile where an <code>ARG</code> is used in the <code>CMD</code> instruction:</p>

<pre><code>ARG MASTER_NAME
CMD spark-submit --deploy-mode client --master ${MASTER_URL}
</code></pre>

<p>The arg is passed via docker-compose:</p>

<pre><code>  spark:
    build:
      context: spark
      args:
        - MASTER_URL=spark://master:7077
</code></pre>

<p>However, the <code>ARG</code> does not seem to get expanded for <code>CMD</code>. After I <code>docker-compose up</code>.</p>

<p>Here's what inspect shows:</p>

<pre><code>docker inspect  -f ""{{.Name}} {{.Config.Cmd}}"" $(docker ps -a -q)
/spark {[/bin/sh -c spark-submit --deploy-mode client --master ${MASTER_URL}]}
</code></pre>
","<p><strong>Problem</strong></p>
<p>I am trying to send a argument to dockerfile using <code>--build-arg</code> but my final <code>CMD</code> is not recognizing it.</p>
<p><strong>Code:</strong></p>
<pre><code>docker image build -t *image_name* --build-arg env=${ENVIRONMENT} .
</code></pre>
<p><strong>Dockerfile:</strong></p>
<pre><code>FROM openjdk:11-slim
VOLUME /tmp
ARG env
RUN echo ${env}
RUN echo &quot;config-${env}.yml&quot;

COPY configs/* ./
EXPOSE 8080
EXPOSE 8081
CMD java -jar something server config-$env.yml
</code></pre>
<p>The kubernetes clusters are giving <code>CrashLoopBackOff</code></p>
<p><strong>What I did:</strong></p>
<pre><code>kubectl -n abc logs -p xyz
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>java.io.FileNotFoundException: File config-.yml not found
</code></pre>
"
"39780422","Docker not mapping port using gunicorn","<docker><ports>","63134004","GUnicorn not showing up locally, not using any ports (Django)","<python><django><docker><dockerfile><gunicorn>","<p>I'm running gunicorn inside a docker container. I know this works because sshing into it and curling localhost:8000/things in docker container gives me the response I want, however, I am not able to reach this on my host, despite docker telling me the port has been mapped. What gives?</p>

<p>I ran </p>

<pre><code>docker run -d -p 80:8000 myapp:version1.1 /bin/bash -c 'gunicorn things:app'
</code></pre>

<p><code>docker ps</code> gives me</p>

<pre><code>CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                            NAMES
614df1f2708e        myapp:version1.1  ""/bin/bash -c 'gunico""   6 minutes ago       Up 6 minutes        5000/tcp, 0.0.0.0:80-&gt;8000/tcp   evil_stallman
</code></pre>

<p>On my host, curling locahost/things gives me</p>

<pre><code>curl: (52) Empty reply from server
</code></pre>

<p>However, when I <code>docker exec -t -i 614df1f2708e /bin/bash</code> and then <code>curl localhost:8000/things</code>, I succesfully get my correct response.</p>

<p>Why isn't docker mapping my port 8000 correctly?</p>
","<p>I am trying to run a Docker container via the command:</p>
<p><code>docker run --rm -d  projectname:latest</code></p>
<p>The gunicorn command which is executing is:</p>
<p><code>gunicorn projectname.wsgi 0.0.0.0:$PORT</code></p>
<p>And I get the following gunicorn lines:</p>
<pre><code>[2020-07-28 11:56:50 +0000] [10] [INFO] Listening at: http://127.0.0.1:8000 (10)
[2020-07-28 11:56:50 +0000] [10] [INFO] Using worker: sync
[2020-07-28 11:56:50 +0000] [12] [INFO] Booting worker with pid: 12
</code></pre>
<p>From this I thought it was working fine. However, nothing is showing up in 127.0.0.1:8000. After using Network Utility's port scanner, no ports above or including 8000 are being used.</p>
<p>When I right click on the container in Visual Studio Code (with Docker extension installed) and click 'Open in Browser' it states &quot;No valid ports are available. Source: Docker (Extension)&quot;.</p>
<p>How can I see the app running locally? What could the issue possibly be?</p>
"
"37461868","Difference between RUN and CMD in a Dockerfile","<docker><dockerfile>","63138328","I was creating a simple DOCKERFILE . I have attached code below . Can someone specify to start service in DOCKERFILE?","<apache><docker><containers><webserver>","<p>I'm confused about when should I use <code>CMD</code> vs <code>RUN</code>. For example, to execute bash/shell commands (i.e. <code>ls -la</code>) I would always use <code>CMD</code> or is there a situation where I would use <code>RUN</code>? Trying to understand the best practices about these two similar <code>Dockerfile</code> directives.</p>
","<pre><code>FROM centos:latest AS build

RUN echo $USER

RUN yum update -y

RUN yum install  -y  httpd 

USER root

ADD index.html /var/www/html/index.html

ENTRYPOINT [ &quot;/usr/sbin/httpd&quot; ]

CMD [ &quot;-D&quot;,&quot;BACKGROUND&quot; ]

EXPOSE 80
</code></pre>
<p>Can someone please explain me difference between <code>CMD</code>, <code>RUN</code>, <code>ENTRYPOINT</code>. It is so confusing. And How do I start httpd service inside the <code>Dockerfile</code>.</p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","63164968","Combine multiple docker containers without copying","<docker>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p><del>Suppose, for example, I have a project that needs to run both Python and Go. For Python I can use the <code>python:latest</code> container in DockerHub and for Go I can use <code>golang:latest</code>. However, how can I combine the two containers in order to access both languages (i.e. applications) at the same time? </del></p>
<p><del>Some possible solutions is to install using <code>apt-get install python3</code> in the go container or to do <code>COPY --from=python</code> in the Dockerfile. However, The actual problem I have involves something much more complicated (I have multiple applications and multiple versions to deal with, and they are large and difficult to copy). So ideally I can link all the files, instead of installing again or copying the files. </del></p>
<p>Edit: I have a project that is based on Python, go, swi-prolog, and some other programming languages. That project uses all these programming languages and is sensitive to the version of each programming language used. So I need to be able to change the version of each programming language quickly (e.g. from Python 3.8.5 to 3.7.8, while still using golang 1.15rc1).</p>
<p>I have limited amount of disk space for docker images (only around 1GB). Python takes 350 MB, Go takes 300MB, swi-prolog takes 50MB, etc. It is possible for me to pull the docker images, but I CANNOT use <code>COPY --from=</code> in my docker file (since copy will consume disk space, and I run out of disk space before I can finish a multi-stage build. So multi-stage builds is NOT a viable solution for my problem). I also want to just download container images from dockerhub, instead of building the images myself.</p>
<p>The ideal case is to have something similar to <code>COPY --from=</code> but hard-link all the files instead of copying. Or maybe mount a container inside another container. Is it possible?</p>
"
"42885538","Raspberry-pi docker error: standard_init_linux.go:178: exec user process caused ""exec format error""","<linux><docker><raspberry-pi><raspbian>","63401867","docker image error: standard_init_linux.go:211: exec user process caused “exec format error”","<reactjs><postgresql><spring-boot><docker><docker-compose>","<p>I've installed docker in rapsbian according to the official instructions (i.e., running <code>curl -sSL https://get.docker.com | sh</code>) but I'm not able to run the hello-world example (I've also tried other examples without success). This is the error I'm getting:</p>

<pre><code>pi@raspberrypi2:~ $ docker run hello-world
standard_init_linux.go:178: exec user process caused ""exec format error""
</code></pre>

<p>My environment is Raspberry Pi 2 Model B with Raspbian GNU/Linux 8 (jessie) and Docker version 17.03.0-ce, build 60ccb22. </p>

<p>Any hint about the problem or possible directions to solve the problem? </p>

<p>Many thanks!</p>
","<p>I created a few docker images on windows and when I try to run them (with or without docker-compose) on Raspbian I get the error described in the title. They work on windows perfectly but fail on linux. I'm totally new to docker. I looked up the error but didn't find anything that helped. Thanks for your answers.</p>
<p><strong>Dockerfiles:</strong></p>
<pre><code>FROM openjdk:8-jdk-alpine
COPY *.jar app.jar
CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]
</code></pre>
<pre><code>FROM node:12.7-alpine
COPY package.json ./
COPY package-lock.json ./
RUN npm install
RUN npm install react-scripts@3.4.0 -g
COPY . .
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<pre><code>FROM postgres
COPY sql ./docker-entrypoint-initdb.d
ENV POSTGRES_DB {db}
ENV POSTGRES_USER {user}
ENV POSTGRES_PASSWORD {pwd}
</code></pre>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: '3'
services:
  database:
    image: {database-image}
    ports:
      - 5432:5432
  service:
    image: {service-image}
    ports:
      - 8080:8080
    depends_on:
      - database
  web:
    image: {web-image}
    ports:
      - 3000:3000
    stdin_open: true
</code></pre>
"
"47977699","ADD failed : No such file/Directory while building docker image","<docker>","63410551","Can't build php docker image","<php><docker><docker-compose><dockerfile><docker-container>","<p>I have below Dockerfile:</p>

<pre><code>FROM python:3

RUN mkdir -p /test/code/
RUN mkdir -p /test/logs/
RUN mkdir -p /test/configs/

ADD test.py /test/code/
ADD test_output.txt /test/code/
ADD test_input.txt /test/configs/
ADD logfile.log /test/logs/


CMD [ ""python3"", ""/test/code/test.py"" ]
</code></pre>

<p>My directory structure is:</p>

<pre><code>/home/&lt;username&gt;/test/
                 |-&gt; code/Dockerfile, test_output.txt, test.py
                 |-&gt; logs/logfile.log
                 |-&gt; configs/test_input.txt
</code></pre>

<p>when I am building the docker image using below command:</p>

<pre><code>sudo docker build -t myimage .
</code></pre>

<p>It shows below error:</p>

<pre><code>Step 7/9 : ADD test_input.txt /test/configs/
ADD failed: stat /var/lib/docker/tmp/docker-builder562406652/test_input.txt: no such file or directory
</code></pre>

<p>Why it shows this error when I have the directory and my file is also present.</p>
","<p>I got this error when building docker php image</p>
<blockquote>
<p>Step 13/25 : ADD php.ini /usr/local/etc/php/php.ini</p>
<p>ERROR: Service 'phpt3' failed to build: ADD failed: stat
/var/lib/docker/tmp/docker-builder310748204/php.ini: no
such file or directory</p>
</blockquote>
<p>Below is the docker file:</p>
<pre><code>FROM php:7.3-fpm

# install the PHP extensions we need
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends msmtp mailutils vim curl debconf subversion git apt-transport-https apt-utils \
    build-essential locales acl mailutils wget nodejs \
    gnupg gnupg1 gnupg2 \
    zlib1g-dev zlib1g-dev libicu-dev g++ \
    sudo

# Install GD
RUN apt-get install -y libfreetype6-dev libjpeg62-turbo-dev libpng-dev
RUN docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ 
RUN docker-php-ext-install -j$(nproc) gd
RUN docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/
RUN docker-php-ext-install gd

# MYSQLI 
RUN docker-php-ext-install mysqli &amp;&amp; docker-php-ext-enable mysqli

# Install ext-zip
RUN apt-get install -y unzip libzip-dev
RUN docker-php-ext-install zip

RUN docker-php-ext-configure intl
RUN docker-php-ext-install pdo_mysql json calendar intl

ADD php.ini /usr/local/etc/php/php.ini
COPY additionnal.ini /usr/local/etc/php/conf.d/

COPY php-fpm-pool.conf  /usr/local/etc/php/pool.d/www.conf

RUN rm -rf /var/lib/apt/lists/* \
    echo &quot;en_US.UTF-8 UTF-8&quot; &gt; /etc/locale.gen &amp;&amp; \
    echo &quot;fr_FR.UTF-8 UTF-8&quot; &gt;&gt; /etc/locale.gen &amp;&amp; \
    locale-gen

# Install Composer
RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer
RUN composer --version

# set up sendmail config, see http://linux.die.net/man/5/ssmtp.conf for options
RUN echo &quot;hostname=localhost.localdomain&quot; &gt; /etc/msmtp/msmtp.conf
RUN echo &quot;mailhub=maildevt3&quot; &gt;&gt; /etc/msmtp/msmtp.conf
# The above 'maildevt3' is the name you used for the link command
# in your docker-compose file or docker link command.
# Docker automatically adds that name in the hosts file
# of the container you're linking MailDev to.

# Set up php sendmail config
RUN echo &quot;sendmail_path=sendmail -i -t&quot; &gt;&gt; /usr/local/etc/php/conf.d/php-sendmail.ini

# Fully qualified domain name configuration for sendmail on localhost.
# Without this sendmail will not work.
# This must match the value for 'hostname' field that you set in ssmtp.conf.
RUN echo &quot;localhost localhost.localdomain&quot; &gt;&gt; /etc/hosts 

WORKDIR /var/www/

EXPOSE 9000
CMD [&quot;php-fpm&quot;]
</code></pre>
<p>Can anyone help me to overcome this bug.</p>
<p>I'm doing this tutorial
<a href=""https://passions.miary.dev/2019/08/30/docker-maildev-fr/"" rel=""nofollow noreferrer"">https://passions.miary.dev/2019/08/30/docker-maildev-fr/</a></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63416186","Can not connect to Kafka outside container","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to implement blackbox testing of my app. So I created compose file</p>
<pre><code>version: '3.4'
services:  
  kafka:
    image: &quot;spotify/kafka&quot;
    environment:
      - ADVERTISED_HOST=kafka
      - ADVERTISED_PORT=9092
    ports:
      - &quot;9092:9092&quot;
      - &quot;2181:2181&quot;
    networks:
      testing_net:
        ipv4_address: 172.17.0.2
  app:
    build: ./blackbox
    image: app    
    networks:
      testing_net:
        ipv4_address: 172.17.0.4
networks:
  testing_net:
    ipam:
      driver: default
      config:
        - subnet: &quot;172.17.0.0/16&quot;
</code></pre>
<p>It works fine, application inside app container can connect to kafka using kafka:9092 host.
But app is consumer, and producer is located outside of containers. When I try to populate data outside containers I get error:</p>
<pre><code>[kafka-producer-network-thread | producer] WARN o.apache.kafka.clients.NetworkClient - [Producer clientId=producer] Error connecting to node kafka:9092 (id: 0 rack: null) 
</code></pre>
<p>I could add &quot;kafka&quot; host to hosts file and it maybe will work, but I have to run this scenario not only at local machine but at CI\CD</p>
<p>I tried to set ADVERTISED_HOST=172.17.0.2. It doesn't work as well.</p>
<p>I can not edit any kafka config files, I can only use docker-compose file</p>
<p>Any suggestions would be appreciated. Thanks</p>
<p>UPD.</p>
<p>Solved this issue using dirty hack. For windows for sure, for other systems need to check Docker adds following host to hosts file: kubernetes.docker.internal:127.0.0.1 In that case inside container I use</p>
<pre><code>kafka:
    image: &quot;spotify/kafka&quot;
    environment:
      - ADVERTISED_HOST=kubernetes.docker.internal
      - ADVERTISED_PORT=9092
    ports:
      - &quot;9092:9092&quot;
      - &quot;2181:2181&quot;
    networks:
      testing_net:
        ipv4_address: 172.16.238.102
    extra_hosts:
      - &quot;kubernetes.docker.internal:172.16.238.102&quot;
</code></pre>
<p>And kafka broker works for outside application because kubernetes.docker.internal is known host</p>
"
"49596760","How to login using ssh without any sort of authentication?","<docker><ssh><openssh>","63475287","Creating a password-less user to directly log in in SSH","<shell><ssh><raspberry-pi><raspbian>","<p>I have seen similar questions, but nothing helped.</p>
<p>Like here:</p>
<ul>
<li><a href=""https://serverfault.com/questions/627906/ssh-login-with-no-authentication"">SSH login with no authentication</a></li>
<li><a href=""https://stackoverflow.com/questions/24673155/ssh-session-without-any-authentication"">SSH session without ANY authentication</a></li>
</ul>
<p>My problem is as the question states. I want to setup ssh to work without any password prompt or any keys. Means, doing</p>
<pre><code>ssh computer@IP_address
</code></pre>
<p>should give me access to the remote machine.</p>
<p>Question ends here^^^^^^^^^^^. Details of what I'm trying to achieve:</p>
<p>I have a docker image of Ubuntu in which I'm trying to install ssh. This has 2 reasons: easy file transfer using scp and the other is, that I sometimes, by mistake I close docker without committing and end up losing all my progress/data. So I want to make it such that I run the docker container in the background and only access it using ssh. Also, I am working in a team and I'll need to some other outside people(who download my docker image) to be able to work with it easily as well, which is why I want it to be ssh-accessible without a login.</p>
","<p><strong>This is for testing purposes and not for any production system so I understand the attached security risks involved</strong></p>
<p>I would like to create a user which doesn't have a password. This would be used to do tests on a SSH login method that doesn't require using a password. Just as I type <code>ssh USER@HOST</code> I'd log in.</p>
<p>I know this &quot;seamless&quot; login without a password can be done with keys but that's not what I'd like.</p>
<p>I've followed some other questions like <a href=""https://raspberrypi.stackexchange.com/questions/3435/how-can-i-deactivate-the-login-password-on-my-raspberry-pi"">this</a> one without any luck. I've also created another user without a password but I haven't seen a consensus on how to do this.</p>
<p>The results I'm getting is getting locked out of my account as it only requires public key log in or I can't set up a user without a password. Is there any way to do this?</p>
<p><strong>Solution</strong>: For future reference in case I do stumble upon this again or anyone finds this, <a href=""https://askubuntu.com/a/1080575"">this</a> worked!</p>
"
"49037742","Why does it take ages to install Pandas on Alpine Linux","<pandas><numpy><docker><alpine>","63517400","Installing pandas=1.1.1 in a Docker container takes forever","<python><pandas><docker><raspberry-pi>","<p>I've noticed that installing Pandas and Numpy (it's dependency) in a Docker container using the base OS Alpine vs. CentOS or Debian takes much longer. I created a little test below to demonstrate the time difference. Aside from the few seconds Alpine takes to update and download the build dependencies to install Pandas and Numpy, why does the setup.py take around 70x more time than on Debian install?</p>

<p>Is there any way to speed up the install using Alpine as the base image or is there another base image of comparable size to Alpine that is better to use for packages like Pandas and Numpy?</p>

<p><strong>Dockerfile.debian</strong></p>

<pre><code>FROM python:3.6.4-slim-jessie

RUN pip install pandas
</code></pre>

<p><strong>Build Debian image with Pandas &amp; Numpy:</strong></p>

<pre><code>[PandasDockerTest] time docker build -t debian-pandas -f Dockerfile.debian . --no-cache
    Sending build context to Docker daemon  3.072kB
    Step 1/2 : FROM python:3.6.4-slim-jessie
     ---&gt; 43431c5410f3
    Step 2/2 : RUN pip install pandas
     ---&gt; Running in 2e4c030f8051
    Collecting pandas
      Downloading pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)
    Collecting numpy&gt;=1.9.0 (from pandas)
      Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)
    Collecting pytz&gt;=2011k (from pandas)
      Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)
    Collecting python-dateutil&gt;=2 (from pandas)
      Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
    Collecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)
      Downloading six-1.11.0-py2.py3-none-any.whl
    Installing collected packages: numpy, pytz, six, python-dateutil, pandas
    Successfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0
    Removing intermediate container 2e4c030f8051
     ---&gt; a71e1c314897
    Successfully built a71e1c314897
    Successfully tagged debian-pandas:latest
    docker build -t debian-pandas -f Dockerfile.debian . --no-cache  0.07s user 0.06s system 0% cpu 13.605 total
</code></pre>

<p><strong>Dockerfile.alpine</strong></p>

<pre><code>FROM python:3.6.4-alpine3.7

RUN apk --update add --no-cache g++

RUN pip install pandas
</code></pre>

<p><strong>Build Alpine image with Pandas &amp; Numpy:</strong></p>

<pre><code>[PandasDockerTest] time docker build -t alpine-pandas -f Dockerfile.alpine . --no-cache
Sending build context to Docker daemon   16.9kB
Step 1/3 : FROM python:3.6.4-alpine3.7
 ---&gt; 4b00a94b6f26
Step 2/3 : RUN apk --update add --no-cache g++
 ---&gt; Running in 4b0c32551e3f
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
(1/17) Upgrading musl (1.1.18-r2 -&gt; 1.1.18-r3)
(2/17) Installing libgcc (6.4.0-r5)
(3/17) Installing libstdc++ (6.4.0-r5)
(4/17) Installing binutils-libs (2.28-r3)
(5/17) Installing binutils (2.28-r3)
(6/17) Installing gmp (6.1.2-r1)
(7/17) Installing isl (0.18-r0)
(8/17) Installing libgomp (6.4.0-r5)
(9/17) Installing libatomic (6.4.0-r5)
(10/17) Installing pkgconf (1.3.10-r0)
(11/17) Installing mpfr3 (3.1.5-r1)
(12/17) Installing mpc1 (1.0.3-r1)
(13/17) Installing gcc (6.4.0-r5)
(14/17) Installing musl-dev (1.1.18-r3)
(15/17) Installing libc-dev (0.7.1-r0)
(16/17) Installing g++ (6.4.0-r5)
(17/17) Upgrading musl-utils (1.1.18-r2 -&gt; 1.1.18-r3)
Executing busybox-1.27.2-r7.trigger
OK: 184 MiB in 50 packages
Removing intermediate container 4b0c32551e3f
 ---&gt; be26c3bf4e42
Step 3/3 : RUN pip install pandas
 ---&gt; Running in 36f6024e5e2d
Collecting pandas
  Downloading pandas-0.22.0.tar.gz (11.3MB)
Collecting python-dateutil&gt;=2 (from pandas)
  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
Collecting pytz&gt;=2011k (from pandas)
  Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)
Collecting numpy&gt;=1.9.0 (from pandas)
  Downloading numpy-1.14.1.zip (4.9MB)
Collecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)
  Downloading six-1.11.0-py2.py3-none-any.whl
Building wheels for collected packages: pandas, numpy
  Running setup.py bdist_wheel for pandas: started
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/e8/ed/46/0596b51014f3cc49259e52dff9824e1c6fe352048a2656fc92
  Running setup.py bdist_wheel for numpy: started
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/9d/cd/e1/4d418b16ea662e512349ef193ed9d9ff473af715110798c984
Successfully built pandas numpy
Installing collected packages: six, python-dateutil, pytz, numpy, pandas
Successfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0
Removing intermediate container 36f6024e5e2d
 ---&gt; a93c59e6a106
Successfully built a93c59e6a106
Successfully tagged alpine-pandas:latest
docker build -t alpine-pandas -f Dockerfile.alpine . --no-cache  0.54s user 0.33s system 0% cpu 16:08.47 total
</code></pre>
","<p>I am trying to create a <code>Dockerfile</code> with <code>streamlit</code> and <code>pandas</code> but the building of pandas takes forever. This is my <code>Dockerfile</code>.</p>
<pre><code># For more information, please refer to https://aka.ms/vscode-docker-python
FROM python:3.8-slim

# This is needed for argon2-cffi==20.1.0
RUN apt-get update &amp;&amp; apt-get install libssl-dev libffi-dev -y

# Apparently the installation of pandas in Docker takes forever.
RUN apt-get install python3-pandas -y

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE 1

# Exposing Streamlit Default Port for Streamlit
EXPOSE 8501

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED 1

ADD requirements.txt .
RUN python -m pip install -r requirements.txt
</code></pre>
<p>But the building of the Dockerfile takes forever. I have tried to install pandas with <code>sudo apt-get install python3-pandas -y</code> and removed pandas=1.1.1 from the requirements.txt file but apparently the installed version of pandas is not 1.1.1 and this is the required version from <code>Streamlit</code> so it is still trying to install the latest <code>pandas</code>.</p>
<p>Do you have any ideas, how I can speed up the process of the building of the docker image? By the way, I am building it on top of a Raspberry Pi 4B.</p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","63700454","Restrict user from accessing code inside docker","<docker><docker-compose><dockerfile><password-protection><docker-machine>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I have created a docker image for my code which is an Django and Deeplearning application and I have to give that image to some client. What I want is that the client should not be permitted to access the code. So is there any chance to password protect the docker image so that the user has to enter the username and password from command line each time before trying to execute any docker command?
Docker version - 19.03.12 / June 18, 2020
Operating system - Ubuntu</p>
"
"39657058","Installing GD in Docker","<php><docker><dockerfile>","63795830","configure: error: Package requirements (libpng) were not met when install gd on docker image","<docker>","<p>I am a complete Docker novice but am having to maintain an existing system. The Dockerfile I am using is as below:</p>

<pre><code>FROM php:5.6-apache

RUN docker-php-ext-install mysql mysqli

RUN apt-get update -y &amp;&amp; apt-get install -y sendmail

RUN apt-get update &amp;&amp; \
    apt-get install -y \
        zlib1g-dev 

RUN docker-php-ext-install mbstring

RUN docker-php-ext-install zip

RUN docker-php-ext-install gd
</code></pre>

<p>When I run 'docker build [sitename]' everything seems ok until I get the error:</p>

<pre><code>configure: error: png.h not found.
The command '/bin/sh -c docker-php-ext-install gd' returned a non-zero code: 1
</code></pre>

<p>What is the cause of this error?</p>
","<p>I want to create an php image and install gd component on it. I have used following commands:</p>
<pre><code>RUN apt-get update &amp;&amp; \
    apt-get install -y \
        zlib1g-dev 

RUN docker-php-ext-install gd

</code></pre>
<p>but I get this error :</p>
<pre><code>configure: error: Package requirements (libpng) were not met:

No package 'libpng' found

Consider adjusting the PKG_CONFIG_PATH environment variable if you
installed software in a non-standard prefix.

Alternatively, you may set the environment variables PNG_CFLAGS
and PNG_LIBS to avoid the need to call pkg-config.
See the pkg-config man page for more details.

</code></pre>
"
"45682010","docker : invalid reference format","<docker>","63870803","How to build and run a Docker image?","<docker><machine-learning>","<p>I'm following this tutorial:
<a href=""https://medium.com/towards-data-science/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c"" rel=""noreferrer"">https://medium.com/towards-data-science/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c</a></p>
<p>and they use docker. When I tried to run docker (inside the run.sh script):</p>
<pre><code>docker run \
    -p 8888:8888 
    -v `pwd`/../src:/src \
    -v `pwd`/../data:/data -w /src supervisely_anpr \
    --rm \
    -it \
    bash
</code></pre>
<p>I got the error:</p>
<pre><code>docker: invalid reference format.
</code></pre>
<p>I spent 2 hours and I can't really understand what's wrong. Any idea really appreciated.</p>
","<p>I'm <em>new</em> to Docker, and I would like to reproduce a project from a <a href=""https://github.com/Zhongdao/Towards-Realtime-MOT"" rel=""nofollow noreferrer"">https://github.com/Zhongdao/Towards-Realtime-MOT</a>.</p>
<p>In order to run the project examples, I need to build an image, so I first cloned the repository locally.</p>
<p>The documentation tells me to run the following commands:</p>
<pre><code>docker build -t towards-realtime-mot docker/

docker run --rm --gpus all -v $(pwd)/:/Towards-Realtime-MOT -ti towards-realtime-mot /bin/bash
cd /Towards-Realtime-MOT;
python demo.py --input-video path/to/your/input/video --weights path/to/model/weights
               --output-format video --output-root path/to/output/root
</code></pre>
<p>I'm running on macOS (without gpu support), and I've done:</p>
<pre><code>➜ docker build -t towards-realtime-mot docker/
➜ docker run --rm -v -ti towards-realtime-mot /bin/bash
➜ cd /Towards-Realtime-MOT
cd: no such file or directory: /Towards-Realtime-MOT
</code></pre>
<p>I've successfully built the docker image, but I'm stuck from the second command onwards. I know that I should create a folder called <code>Towards-Realtime-MOT</code>, but <code>$(pwd)</code> confuses me since it gives me erros (docker: invalid reference format. See 'docker run --help'.). What is the correct way?</p>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","63883482","Docker build getting errors after I tried to install pip install -r requirements.txt file","<python><django><docker><dockerfile><docker-image>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>This is the error I get after I installed</p>
<p>RUN pip install -r requirements.txt</p>
<p>RROR: Service 'web' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
➜  django-docker pip install psycopg2
Defaulting to user installation because normal site-packages is not writeable
Collecting psycopg2
Using cached psycopg2-2.8.6.tar.gz (383 kB)
ERROR: Command errored out with exit status 1:
command: /usr/bin/python3.6 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-7xuxd9cm/psycopg2/setup.py'&quot;'&quot;'; <strong>file</strong>='&quot;'&quot;'/tmp/pip-install-7xuxd9cm/psycopg2/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(<strong>file</strong>);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, <strong>file</strong>, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-uon18vv4
cwd: /tmp/pip-install-7xuxd9cm/psycopg2/
Complete output (23 lines):
running egg_info
creating /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info
writing /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/PKG-INFO
writing dependency_links to /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/dependency_links.txt
writing top-level names to /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/top_level.txt
writing manifest file '/tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/SOURCES.txt'</p>
<pre><code>Error: pg_config executable not found.

pg_config is required to build psycopg2 from source.  Please add the directory
containing pg_config to the $PATH or specify the full executable path with the
option:

    python setup.py build_ext --pg-config /path/to/pg_config build ...

or with the pg_config option in 'setup.cfg'.

If you prefer to avoid building psycopg2 from source, please install the PyPI
'psycopg2-binary' package instead.

For further information please check the 'doc/src/install.rst' file (also at
&lt;https://www.psycopg.org/docs/install.html&gt;).

----------------------------------------
</code></pre>
<p>ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.<a href=""https://i.stack.imgur.com/GkScl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GkScl.png"" alt=""enter image description here"" /></a></p>
"
"37599128","docker - how do you disable auto-restart on a container?","<docker>","63939149","some containers are always running after reboot on docker-compose","<docker-compose>","<p>I can enable auto-restart with <code>--restart=always</code>, but after I stop the container, how do I turn off that attribute?</p>

<p>I normally run a webserver and typically map port 80:</p>

<pre><code>docker run -d --restart=always -p 80:80 -i -t myuser/myproj /bin/bash
</code></pre>

<p>But there are times when I want to run a newer version of my image, but I want to keep the old container around.  The problem is that if there are multiple containers with <code>--restart=always</code>, only one of them (random?) starts because they're all contending for port 80 on the host.</p>
","<p>After my PC reboot or shutdown and start, some docker containers are always started.</p>
<ul>
<li>OS: Windows 10</li>
<li>docker-compose version: 1.24.1, build 4667896b</li>
</ul>
<p>My step is :</p>
<ol>
<li>start docker toolbox</li>
<li>docker-compose up (container <code>A</code>)</li>
<li>docker-compose stop/down <code>A</code></li>
<li>reboot or shutdown and start PC.</li>
<li>start docker toolbox and execute <code>docker ps</code> command: <code>A</code> is running</li>
</ol>
<p>I've removed the <code>restart: always</code> config, but it's still running every time.</p>
<p>My question is how I can have no containers when I start the docker toolbox. Thanks in advance.</p>
"
"49991978","Kubernetes not copying data into mounted Volume","<mysql><docker><kubernetes>","64026599","Volume mount folder being erased by kubernetes","<docker><kubernetes>","<p>According to the documentation here:
<a href=""https://docs.docker.com/storage/volumes/"" rel=""nofollow noreferrer"">https://docs.docker.com/storage/volumes/</a></p>

<blockquote>
  <p>If you start a container which creates a new volume, as above, and the container has files or directories in the directory to be mounted (such as /app/ above), the directory’s contents are copied into the volume. The container then mounts and uses the volume, and other containers which use the volume also have access to the pre-populated content.</p>
</blockquote>

<p>In other words, the expectation is that, if i have files like so in /var/lib/mysql</p>

<pre><code>root@d8fa9a8b305a:/var/lib/mysql# ls
auto.cnf         xtz           ib_logfile0  ibdata1  mysql               sys
debian-5.7.flag  ib_buffer_pool  ib_logfile1  ibtmp1   performance_schema
</code></pre>

<p>Then, when I mount a volume into <code>/var/lib/mysql</code> then all the files from the container should be copied into my volume.</p>

<p>But I find this is not happening:</p>

<pre><code>/var/lib/mysql/mysql # ls
auto.cnf        ib_buffer_pool  ib_logfile0     ib_logfile1     ibdata1
</code></pre>

<p>This is content of the volume that I mounted into /var/lib/mysql, and as you can see the data is not the same as the one present in /var/lib/mysql of the docker image itself. So, as a result, there's a failure on startup.</p>

<p><strong>Note</strong>: The Volume in question is actually mounted by kubernetes. So, I'm making a major assumption here that </p>

<pre><code>volumeMounts:
        - name: xtz-persistent-storage
          mountPath: ""/var/lib/mysql/""
</code></pre>

<p>is the equivalent of doing this : <code>docker run -p 443:443 --rm -v mysql:/var/lib/mysql &lt;image&gt;</code></p>
","<p>I have a pod (whose deployment config I have included below whichever I guessed as relevant to this question. Please tell me if more is needed) which uses a persistent volume. And the container is a docker whose docker file creates this folder <code>/mounted/folder</code>  and also some hierarchy of subfolders. After this pod is deployed, I can see that this <code>/mounted/folder</code> is wiped clean of all the subfolders. I saw this <a href=""https://stackoverflow.com/questions/39751421/mountpath-overrides-the-rest-of-the-files-in-that-same-folder"">question</a> where it was mentioned that adding <code>subPath</code> field will ensure that the folder contents are not deleted. But it didn't work. I may have to mention here that I have another pod using the same pvc which this pod is using (for having a common shared storage). When this subPath didn't work, I tried (unsuccessfully) to add subPath in that pod deployment also. Please help! Thanks!</p>
<pre><code>spec:
  containers:
    - name: nginx
      image: docker-registry-address/imagename:version
      volumeMounts:
        - mountPath: /mounted/folder
          subPath: folder
          name: name-of-volume

  volumes:
    - name: name-of-volume
      persistentVolumeClaim:
        claimName: pv-claim
</code></pre>
"
"48700453","Docker image build with PHP zip extension shows ""bundled libzip is deprecated"" warning","<php><docker><php-extension><libzip>","64076482","how to install ZipArchive in the PHP Docker Container","<php><docker><docker-compose><dockerfile><php-7>","<p>I have a <code>Dockerfile</code> with a build command like this:</p>

<pre><code>#install some base extensions
RUN apt-get install -y \
        zlib1g-dev \
        zip \
  &amp;&amp; docker-php-ext-install zip
</code></pre>

<p>I get this warning from build output:</p>

<blockquote>
  <p>WARNING: Use of bundled libzip is deprecated and will be removed.<br>
  configure: WARNING: Some features such as encryption and bzip2 are not available.<br>
  configure: WARNING: Use system library and --with-libzip is
  recommended.</p>
</blockquote>

<p>What is the correct way to install the zip extension without these warnings?</p>

<p>My complete Dockerfile looks like:</p>

<pre><code>FROM php:7.2-apache

RUN apt-get clean
RUN apt-get update

#install some basic tools
RUN apt-get install -y \
        git \
        tree \
        vim \
        wget \
        subversion

#install some base extensions
RUN apt-get install -y \
        zlib1g-dev \
        zip \
  &amp;&amp; docker-php-ext-install zip

#setup composer
RUN curl -sS https://getcomposer.org/installer | php \
        &amp;&amp; mv composer.phar /usr/local/bin/ \
        &amp;&amp; ln -s /usr/local/bin/composer.phar /usr/local/bin/composer


WORKDIR /var/www/
</code></pre>
","<p>in my laravel project I have this error</p>
<pre><code>Class 'ZipArchive' not found
</code></pre>
<p>I need to install php-zip into my php container</p>
<p>I've changed the Dockerfile as I found some answers on the internet and added <code>extension=zip.so</code> to php.ini but it doesn't work.</p>
"
"46080290","ReactJS browser app cannot see things in the Docker Compose network","<reactjs><docker><axios>","64098159","HTTP requests between two docker services works fine but I don't know why","<node.js><reactjs><docker><docker-compose>","<p>I have a ReactJS project with its own Dockerfile, exposing port 3000:3000.</p>

<p>I also have a PHP project with its own Dockerfile, exposing port 80:80.  The PHP app also has containers for MySQL, Redis and Nginx</p>

<p>For the PHP app, I have a docker-compose file that creates a network (<code>my-net</code>) for PHP, Nginx, MySQL and Redis to communicate on.  However, I now want the ReactJS (which is in a separate project) to be able to communicate with the PHP app.</p>

<p>I added a docker-compose file to the React project, and added it to the network from the PHP project <code>my-net</code> and declared it as <code>external</code> so that it doesn't try to create it.</p>

<p>This seems to work: From the ReactJS container, I can ping <code>app</code> (the name of my backend service) and it works properly.  However, from the ReactJS code, if I use something like <code>axios</code> to try and hit the backend API, it can't resolve <code>app</code> or <code>http://app</code> or any variation.  It can however access the underlying IP address if I substitute that into in <code>axios</code>.</p>

<p>So there seems to be some issue with the hostname resolution, and presumably this is on the <code>axios</code> / JavaScript end.  is there something I'm missing or a reason this isn't working?</p>
","<p>I have two nodejs services:</p>
<ul>
<li><code>/server</code> =&gt; NodeJS API built using NestJS. Exposes one <code>GET</code> route: <code>/</code> that return &quot;Hello World&quot;</li>
<li><code>/client</code> =&gt; ReactJS application that fetches data(&quot;Hello World&quot;) from the <code>server</code> api.</li>
</ul>
<hr />
<p>This is my project structure:</p>
<pre><code>.
├── client
│   ├── Dockerfile
│   ├── package.json
│   ├── package-lock.json
│   ├── public
│   │   ├── favicon.ico
│   │   ├── index.html
│   │   ├── logo192.png
│   │   ├── logo512.png
│   │   ├── manifest.json
│   │   └── robots.txt
│   ├── README.md
│   ├── src
│   │   ├── App.css
│   │   ├── App.test.tsx
│   │   ├── App.tsx
│   │   ├── index.css
│   │   ├── index.tsx
│   │   ├── logo.svg
│   │   ├── react-app-env.d.ts
│   │   ├── serviceWorker.ts
│   │   └── setupTests.ts
│   ├── tsconfig.json
│   └── yarn.lock
├── docker-compose.yml
└── server
    ├── dist
    │   ├── app.controller.d.ts
    │   ├── app.controller.js
    │   ├── app.controller.js.map
    │   ├── app.module.d.ts
    │   ├── app.module.js
    │   ├── app.module.js.map
    │   ├── app.service.d.ts
    │   ├── app.service.js
    │   ├── app.service.js.map
    │   ├── main.d.ts
    │   ├── main.js
    │   ├── main.js.map
    │   └── tsconfig.build.tsbuildinfo
    ├── Dockerfile
    ├── nest-cli.json
    ├── package.json
    ├── package-lock.json
    ├── README.md
    ├── src
    │   ├── app.controller.spec.ts
    │   ├── app.controller.ts
    │   ├── app.module.ts
    │   ├── app.service.ts
    │   └── main.ts
    ├── test
    │   ├── app.e2e-spec.ts
    │   └── jest-e2e.json
    ├── tsconfig.build.json
    └── tsconfig.json
</code></pre>
<hr />
<p><code>server/Dockerfile</code></p>
<pre><code>FROM node:latest

WORKDIR /usr/src/app

COPY package.json .
COPY package-lock.json .

RUN npm install

COPY . .

CMD [&quot;npm&quot;, &quot;run&quot;, &quot;start:dev&quot;]
</code></pre>
<hr />
<p><code>client/Dockerfile</code></p>
<pre><code># pull official base image
FROM node:13.12.0-alpine

# set working directory
WORKDIR /usr/src/app

# add `/app/node_modules/.bin` to $PATH
ENV PATH /app/node_modules/.bin:$PATH

# install app dependencies
COPY package*.json ./

RUN npm install --silent
RUN npm install react-scripts@3.4.1 -g --silent

# add app
COPY . ./

# start app
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<hr />
<p><code>docker-compose.yml</code></p>
<pre class=""lang-yml prettyprint-override""><code>version: &quot;3.8&quot;
services:
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ''
      POSTGRES_DB: the-notebook-dev-db
  server:
    build: ./server
    volumes:
      - ./server:/usr/src/app
    ports:
      - '4000:4000'
    depends_on:
      - postgres
  client:
    tty: true
    build: ./client
    volumes:
      - ./client:/usr/src/app
    ports:
      - '3000:3000'
    depends_on:
      - server
</code></pre>
<p>Everything works as expected. Client makes a GET request to <code>http://localhost:4000</code>, receives the &quot;Hello World&quot; message. However, shouldn't <code>localhost</code>, when used by the client container, point to the client container itself? How is this setup working and why am I not able to use <code>http://service:4000</code> instead of <code>http://localhost:4000</code>?</p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","64098685","Can't mount docker container's working directory to host","<python><docker><dockerfile><docker-volume><docker-build>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I'm trying to build a docker container for my python application that allow edits to a config file in the application's working directory so I don't need to rebuild the image every time a change is made.</p>
<p>Dockerfile:</p>
<pre><code>FROM python:3

VOLUME /app
COPY . /app
WORKDIR /app

RUN pip install requirements.txt

CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p>Once the image is built I deploy the container with</p>
<pre><code>docker run -d --name app -v /path/on/host/app:/app app:latest
</code></pre>
<p>I get an error in the container, <code>python: can't open file 'main.py': [Errno 2] No such file or directory</code> and <code>/path/on/host/app</code> stays empty.</p>
<p>I can't figure out what I need in my Dockerfile to make this work.</p>
<h3>Update - Found a solution</h3>
<p>Since I only need access to the config file here are the changes I made to get this to work</p>
<p>Dockerfile:</p>
<pre><code>FROM python:3
COPY . /app
WORKDIR /app
RUN pip install requirements.txt
VOlUME /app/config
CMD [&quot;python&quot;,&quot;main.py&quot;]
</code></pre>
<p>Now when I run the container I copy the config to <code>/path/on/host/app</code></p>
<pre><code>docker run -d --name app --mount type=bind,source=/path/on/host/app,target=/app/config
</code></pre>
<p>This solved my issue and now I can update the config and restart the container to get the changes to apply.</p>
"
"48561981","Activate python virtualenv in Dockerfile","<python><docker><virtualenv><dockerfile>","64132470","How to install pip packages in container","<python><docker><pip><virtualenv>","<p>I have a Dockerfile where I try to activate python virtualenv after what, it should install all dependencies within this env. However, everything still gets installed globally. I used different approaches and non of them worked. I also do not get any errors. Where is a problem?</p>

<p>1.
<code>ENV PATH $PATH:env/bin</code></p>

<p>2.
<code>ENV PATH $PATH:env/bin/activate</code></p>

<p>3.
<code>RUN . env/bin/activate</code></p>

<p>I also followed <a href=""https://github.com/GoogleCloudPlatform/python-runtime#kubernetes-engine--other-docker-hosts"" rel=""noreferrer"">an example of a Dockerfile config for the python-runtime image on Google Cloud</a>, which is basically the same stuff as above.</p>

<blockquote>
  <p>Setting these environment variables are the same as running source /env/bin/activate.</p>
</blockquote>

<p><code>ENV VIRTUAL_ENV /env</code></p>

<p><code>ENV PATH /env/bin:$PATH</code></p>

<p>Additionally, what does <code>ENV VIRTUAL_ENV /env</code> mean and how it is used?</p>
","<p>I am trying to setup a python virtual environment on a docker image running a <strong>docker build</strong></p>
<p>The terminal output is ok when I run <strong>docker build ..</strong> but when I login into my container, no packages are installer in my virtual environment.</p>
<pre class=""lang-sh prettyprint-override""><code>#Dockerfile

RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
RUN python3.8 get-pip.py
RUN pip install virtualenv
RUN virtualenv venv
RUN /home/ubuntu/venv/bin/pip install -r auto/requirements.txt
...
</code></pre>
<pre class=""lang-sh prettyprint-override""><code># Docker build command
docker build --no-cache -t auto-server:1.0 .
</code></pre>
<pre class=""lang-sh prettyprint-override""><code># Terminal output

Step 27/27 : RUN /home/ubuntu/venv/bin/pip install -r auto/requirements.txt
 ---&gt; Running in d27dbb9a4c97
Collecting asgiref==3.2.10
  Downloading asgiref-3.2.10-py3-none-any.whl (19 kB)
Collecting beautifulsoup4==4.9.1
  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)
Collecting Django==3.1.1
  Downloading Django-3.1.1-py3-none-any.whl (7.8 MB)

...

Successfully installed Django-3.1.1 asgiref-3.2.10 beautifulsoup4-4.9.1 fake-useragent-0.1.11 joblib-0.16.0 numpy-1.19.2 pandas-1.1.2 python-dateutil-2.8.1 pytz-2020.1 scikit-learn-0.23.2 scipy-1.5.2 six-1.15.0 sklearn-0.0 soupsieve-2.0.1 sqlparse-0.3.1 threadpoolctl-2.1.0
</code></pre>
<p>Here is what I get when I list pakages in my virtual environment:</p>
<pre class=""lang-sh prettyprint-override""><code>$ docker exec -ti auto-server bash
root@9c1f914d1b7b:/home/ubuntu# source venv/bin/activate
(venv) root@9c1f914d1b7b:/home/ubuntu# pip list
Package    Version
---------- -------
pip        20.2.2
setuptools 49.6.0
wheel      0.35.1
WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.
You should consider upgrading via the '/home/ubuntu/venv/bin/python -m pip install --upgrade pip' command.

</code></pre>
<p>Is it the right way to do it? How to make sure packages will be installed?</p>
"
"45282608","How to directly mount NFS share/volume in container using docker compose v3","<docker><docker-compose><docker-swarm><docker-swarm-mode>","64138567","How to mount a folder in a machine to a docker container in another machine?","<docker><nfs><mounted-volumes>","<p>I have a compose file with v3 where there are 3 services sharing/using the same volume. While using swarm mode we need to create extra containers &amp; volumes to manage our services across the cluster. </p>

<p>I am planning to use NFS server so that single NFS share will get mounted directly on all the hosts within the cluster. </p>

<p>I have found below two ways of doing it but it needs extra steps to be performed on the docker host -</p>

<ul>
<li><p>Mount the NFS share using ""fstab"" or ""mount"" command on the host &amp; then use it as a host volume for docker services.</p></li>
<li><p>Use Netshare plugin - <a href=""https://github.com/ContainX/docker-volume-netshare"" rel=""noreferrer"">https://github.com/ContainX/docker-volume-netshare</a></p></li>
</ul>

<p>Is there a standard way where i can directly use/mount NFS share using docker compose v3 by performing only few/no steps(I understand that ""nfs-common"" package is required anyhow) on the docker host?</p>
","<p>I have a running docker container that uses volume mounting and NFS to store data in another machine.</p>
<p>Now I want to map this volume to a new folder in the filesystem of a new machine.</p>
<p>shall I do bind mounting in the container to the new VM location like this &lt;VM_IP&gt;:/path_to_folder is that possible?</p>
<p>or is there a way to make the existing volume always uses that new folder as a reference instead of the NFS?</p>
<p>Thank you!</p>
"
"48633244","Access Angular app running inside Docker container","<angular><docker><dockerfile><redhat><redhat-containers>","64248411","run an application live for dev purpose","<angular><docker>","<p>I've created a docker image to test an Angular app but I cannot connect from host to running app inside docker container.</p>

<p>The image was created using a Dockerfile with:
EXPOSE 4200 8080 80</p>

<p>I run the docker container with command:
docker run -ti -p 4200:4200 angulardev /bin/bash</p>

<p>Inside container I create the Angular application and start it using:
ng serve</p>

<p>From container, if I open localhost:4200 I see the application but I cannot access it from host OS (RHEL7)</p>

<p>What is wrong? The Angular app starts on port 4200 which is exposed and mapped to host 4200.</p>

<p>Thanks.</p>
","<p>I'am a beginner in Docker and I'am trying to run my source code in a Container with making my source-code folder as Volume to be able to modify source code in realtime.</p>
<p>My source code is a starter angular app</p>
<p>So my current repo contains :</p>
<pre><code>DockerFile
angular-app/
</code></pre>
<p>#DockerFile:</p>
<pre><code>FROM node:latest
LABEL author=&quot;Karim&quot;

RUN npm install -g @angular/cli

WORKDIR /var/www/angular-app

ENTRYPOINT [&quot;npm&quot;,&quot;start&quot;]
</code></pre>
<p>in this repo I run :</p>
<pre><code>&gt; docker build -t angular-image .
&gt; docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
angular-image       latest              fcbcf2d10c65        58 minutes ago      989MB

&gt; docker run -p 1000:4200 -v &quot;C:\Users\k.garali\Desktop\Docker\angular-app-image/angular-app&quot;:/var/www/angular-app angular-image


&gt; angular-app@0.0.0 start /var/www/angular-app
&gt; ng serve


chunk {main} main.js, main.js.map (main) 56.9 kB [initial] [rendered]
chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 141 kB [initial] [rendered]
chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
chunk {styles} styles.js, styles.js.map (styles) 12.5 kB [initial] [rendered]
chunk {vendor} vendor.js, vendor.js.map (vendor) 2.38 MB [initial] [rendered]
Date: 2020-10-07T16:27:10.277Z - Hash: 17160092220a366bdbcb - Time: 18247ms
** Angular Live Development Server is listening on localhost:4200, open your browser on http://localhost:4200/ **
: Compiled successfully.

&gt; docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS
    PORTS                    NAMES
7f9a2185636d        angular-image       &quot;npm start&quot;         2 seconds ago       Up 2 seconds
    0.0.0.0:1000-&gt;4200/tcp   fervent_morse
</code></pre>
<p>But Finally when I go in browser to localhost:1000, I get no reponse ERR_EMPTY_RESPONSE</p>
<p>any clarification ?</p>
"
"46166304","Docker Compose: volumes without colon (:)","<docker><docker-compose>","64316357","What is the differences between /aaa:/bbb /bbb ccc:/bbb in volumes of docker-compose.yml","<docker><docker-compose>","<p>I have a <code>docker-compose.yml</code> file with the following:</p>

<pre><code>volumes:
  - .:/usr/app/
  - /usr/app/node_modules
</code></pre>

<p>First option maps current host directory to <code>/usr/app</code>, but what does the second option do?</p>
","<p>I'd like to understand <code>volumes</code> in <code>docker-compose.yml</code></p>
<p>What is the difference between them.</p>
<p><code>/aaa:/bbb</code> <code>/bbb</code> <code>ccc:/bbb</code></p>
<p>for example here is sample of  <code>docker-compose.yml</code></p>
<pre><code>version: '3'

services:
  api-server:
    build: ./api
    links:
      - 'db'
    ports:
      - '3000:3000'
    volumes:
      - ./api:/src
      - ./src/node_modules
    tty: true
    container_name: api-server
</code></pre>
<p><code>Dockerfile</code> is following.</p>
<pre><code>FROM node:alpine

WORKDIR /src
COPY . .

RUN rm -rf /src/node_modules
RUN rm -rf /src/package-lock.json
RUN apk --no-cache add curl



RUN yarn install

CMD yarn start:dev
</code></pre>
<p>What the following mean ?</p>
<pre><code>    volumes:
      - ./api:/src
      - ./src/node_modules
</code></pre>
<p>Following is directory structure</p>
<pre><code>base) [api]
 $ ls
Dockerfile          dist                node_modules        package-lock.json   src                 tsconfig.build.json yarn.lock
README.md           nest-cli.json       ormconfig.js        package.json        test                tsconfig.json
</code></pre>
<p>If someone has opinion, please let me know.</p>
<p>Thanks</p>
"
"40465979","Change Docker native images location on Windows 10 Pro","<docker><windows-10>","64318985","How to save Docker files (images, container) in specific driver?","<docker><docker-for-windows>","<p><em>This is not a duplicate of</em> <a href=""https://stackoverflow.com/questions/33933107/change-docker-machine-location-windows"">Change Docker machine location - Windows</a></p>

<p>I'm using docker native, version 1.12.1-stable (build: 7135) on Windows 10 Pro with Hyper-V enabled.
So docker is <strong>not</strong> running with VirtualBox nor do I have the folder <em>C:\Users\username\.docker</em></p>

<p>I'd like to move docker's images, caches, ... to my secondary drive <em>D:\</em></p>

<p>I guess I should edit the Docker Daemon configuration.</p>

<p><img src=""https://i.imgur.com/nC9XaRu.jpg"" alt=""Docker Daemon Configuration""></p>

<p>I tried to add <code>""graph"": ""/D/docker""</code>. Docker started correctly but I couldn't pull any image because of an error</p>

<blockquote>
  <p>open /D/docker/tmp/GetImageBlob135686954: no such file or directory</p>
</blockquote>

<p>How to tell docker to use another path to store its images, etc ?</p>
","<p>I am using Docker for Windows 19.03.13 on Windows 10 x64 version 2004. (I seen this type of question 3 years ago, and at this time, many thing changed.)</p>
<p>How to save Docker files (images, container) in specific driver? My <code>C:\</code> is nearly full now.</p>
<p>This is my system information</p>
<pre><code>C:\Users\donhuvy&gt;docker version
Client: Docker Engine - Community
 Cloud integration  0.1.18
 Version:           19.03.13
 API version:       1.40
 Go version:        go1.13.15
 Git commit:        4484c46d9d
 Built:             Wed Sep 16 17:00:27 2020
 OS/Arch:           windows/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.13
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       4484c46d9d
  Built:            Wed Sep 16 17:07:04 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.3.7
  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175
 runc:
  Version:          1.0.0-rc10
  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683

C:\Users\donhuvy&gt;docker system df
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              9                   5                   2.95GB              1.245GB (42%)
Containers          14                  0                   39.01kB             39.01kB (100%)
Local Volumes       2                   2                   48.77MB             0B (0%)
Build Cache         12                  0                   32.56kB             32.56kB

C:\Users\donhuvy&gt;

</code></pre>
<p><a href=""https://i.stack.imgur.com/JwGmE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwGmE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/EbC3R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbC3R.png"" alt=""enter image description here"" /></a></p>
"
"36362150","Docker with a new nameserver","<docker><dockerfile>","64370210","How to Add nameserver as echo nameserver 8.8.8.8 >> /etc/resolv.conf using docker file","<python><docker>","<p>How I can add new nameserver in <code>/etc/resolv.conf</code> (dockerfile)?</p>

<p>On my dockerfile I use:</p>

<pre><code>FROM ubuntu:14.04

RUN echo ""nameserver 10.111.122.1"" &gt;&gt; /etc/resolv.conf
</code></pre>

<p>On my test I use:</p>

<pre><code>docker run --rm 746cb98d6c9b echo cat /etc/resolv.conf
</code></pre>

<p>I didn't get my change (the new nameserver)... So I try adding mannualy with </p>

<pre><code>docker run --rm 746cb98d6c9b echo ""nameserver 10.111.122.1"" &gt;&gt; /etc/resolv.conf 
</code></pre>

<p>and I get </p>

<pre><code>zsh: permission denied: /etc/resolv.conf
</code></pre>

<p>How I can change permission of this file OR use a root user OR use a chmod in docker files ? My real task is to add and dns server for my build of this dockerfile.</p>

<p>I'm using a linux mint.</p>

<p>I'm get a correct result with a ping test on docker run command (with <code>--dns</code>)</p>
","<p>I need to add nameserver as 8.8.8.8 in /etc/resolv.conf using docker file</p>
<p>I used below command
echo nameserver 8.8.8.8 &gt;&gt; /etc/resolv.conf in docker file but not working.
When I tried same command inside docker conatainer it is working.</p>
<p>Please help me on this issue to resolve using docker file</p>
"
"49293967","How to pass environment variable to docker-compose up","<docker><docker-compose><dockerfile>","64389243","Docker compose with environment variables in command","<docker><docker-compose>","<p>I am trying to run a container. I already have the image uploaded to private Docker registry. I want to write a compose file to download and deploy the image. But I want to pass the TAG name as a variable from the docker-compose run command.My compose file looks like below. How can I pass the value for KB_DB_TAG_VERSION as part of docker-compose up command?</p>

<pre><code>version: '3'
services:
   db:
    #build: k-db
    user: ""1000:50""
    volumes:
      - /data/mysql:/var/lib/mysql
    container_name: k-db
    environment:
      - MYSQL_ALLOW_EMPTY_PASSWORD=yes
    image:  XX:$KB_DB_TAG_VERSION
    image: k-db
    ports:
      - ""3307:3306""
</code></pre>
","<p>Is it possible to pass environment variables in command to Docker compose like:</p>
<pre><code>docker-compose up --build -d --env VAR1=123 VAR2=456
</code></pre>
<p>I know that it is possible in <code>docker</code> but for Docker compose it seems to work for me.</p>
"
"50057998","How can I setup an official maven docker image with my own global settings.xml?","<maven><docker>","64392367","Building docker image with maven and dependency in corporate nexus repo with authentication","<java><docker><maven><build><nexus>","<p>I use docker for the first time in connection with GitLab CI. I am happy that GitLab does most of the work for me. 
I am using the official maven docker image <code>maven:3-jdk-8</code> (<a href=""https://hub.docker.com/_/maven/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/maven/</a>)</p>

<p>Now I would like to set the global <code>settings.xml</code> on that docker image, which contains data to my nexus server for the deploy phase.</p>

<p>I tried to follow this guide: <a href=""https://www.thegeekdiary.com/how-to-update-add-a-file-in-the-docker-image/"" rel=""nofollow noreferrer"">Adding a directory and image in the docker image</a> , but unfortunally I cannot connect to the bash of the docker image.</p>

<pre><code>root@build:~# docker run maven:3-jdk-8 /bin/bash -it
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
root@7d62e8b066f7:/# exit
</code></pre>

<p>How can I add my own global <code>settings.xml</code> to my maven docker image?</p>
","<p>I'm trying to use a docker image with maven to build my sources.
The project needs a dependency which is stored on our corporate authenticated nexus.
I've defined the repository in pom.xml but since the Nexus repo is authenticated I don't know where to provide the credentials.
If the build was local i would use settings.xml but given I wanna do this in the image build I can find a way to do this.</p>
<p>The dockerfile is like this:</p>
<pre><code>FROM diamol/maven AS builder


WORKDIR D:/wsEmy/myproject/src/
COPY pom.xml .
RUN mvn -B dependency:go-offline

COPY . .
RUN mvn package

# app
FROM diamol/openjdk

WORKDIR /app
COPY --from=builder D:/wsEmy/myproject/target/myproject.jar .
COPY --from=builder D:/wsEmy/myproject/confExt/application.properties .

EXPOSE 80
ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app/myproject.jar --com.led.appconf=&quot;/app&quot;]
</code></pre>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","64558374","curl: (56) Recv failure: Connection reset by peer","<linux><docker><flask>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I have a flask application running on Linux machine (Ubuntu 18.04) and it is containerized inside a docker. The host and port on which the docker is running is on <code>0.0.0.0:3000</code> but I am not able to see the logs and moreover when I try to access the port via browser it is showing <code>curl: (56) Recv failure: Connection reset by peer</code>. I even tried to expose the port by <code>docker run -p 3000:3000 docker-app:6.0</code> But is giving error which reads:</p>
<pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint reverent_booth (46381f8b4b3e1c7853f1bb79c447f430de18ca81079ff4263e552d38fada675d): Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use.
ERRO[0001] error waiting for container: context canceled
</code></pre>
<p>Any suggestions would be really helpful.</p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","64650906","How to prevent passwordless login from docker exec?","<docker>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I have source code in docker images which I have to protect from the copy.</p>
<p>I have set up a password in the image but when I run command
<code>docker exec -it container-name /bin/bash </code> then it login without password.</p>
<p>How to set any protect so when anyone run above command then it will ask password for that so I can protect my source from the copy.</p>
"
"49638532","Docker copy file to host from within dockerfile","<docker>","64743398","How to get artifact out of docker build?","<docker><docker-build>","<p>I am writing a <code>Dockerfile</code> to build an application.  I know that you can copy files out of the container manually with </p>

<pre><code>docker cp &lt;containerId&gt;:/file/path/within/container /host/path/target
</code></pre>

<p>(see <a href=""https://stackoverflow.com/questions/22049212/copying-files-from-docker-container-to-host"">Copying files from Docker container to host</a>), but I would like to just run </p>

<pre><code>docker build
</code></pre>

<p>and have it dump the build artifacts created by my <code>Dockerfile</code> somewhere on my host file system.</p>

<p>Is there a command I can use within the Dockerfile to copy out of the container and into the host?  Like the opposite of <code>COPY</code>?</p>
","<p>Is there a way to get artifacts from <code>docker build</code> out to the host machine?</p>
<p>As an example, given the following <code>Dockerfile</code>, is there a way to get the file <code>log.log</code>?</p>
<pre><code>FROM alpine:3.7 as base
RUN mkdir /log/
RUN touch /log/log.log
</code></pre>
<hr />
<p>My attempt at using <code>COPY</code> seems to only work copying <em>from</em> host <em>to</em> docker image:</p>
<pre><code>FROM alpine:3.7 as base
RUN mkdir /log/
RUN touch /log/log.log
COPY ./foo.bar /log/
RUN ls -l /log/
COPY /log/log.log ./
</code></pre>
<pre><code>$ touch ./foo.bar &amp;&amp; docker build -t demo --no-cache .
Sending build context to Docker daemon  15.36kB
Step 1/6 : FROM alpine:3.7 as base
 ---&gt; 6d1ef012b567
Step 2/6 : RUN mkdir /log/
 ---&gt; Running in 4b6df3797ee3
Removing intermediate container 4b6df3797ee3
 ---&gt; 827e6001d34a
Step 3/6 : RUN touch /log/log.log
 ---&gt; Running in d93d50d61b69
Removing intermediate container d93d50d61b69
 ---&gt; c44620d4f9c4
Step 4/6 : COPY ./foo.bar /log/
 ---&gt; 6996718d44da
Step 5/6 : RUN ls -l /log/
 ---&gt; Running in 84e997af182b
total 0
-rw-r--r--    1 root     root             0 Nov  8 21:44 foo.bar
-rw-r--r--    1 root     root             0 Nov  8 21:44 log.log
Removing intermediate container 84e997af182b
 ---&gt; 5a440f258772
Step 6/6 : COPY /log/log.log ./
COPY failed: stat /var/lib/docker/tmp/docker-builder677155266/log/log.log: no such file or directory
</code></pre>
<hr />
<p>I'm aware of the <code>-v</code> (volume mount) argument to <code>docker run</code> - but that's not what I'm looking for: I'm looking to learn if there's a way to get artifacts from out of the <code>docker build</code> process specifically.</p>
<hr />
<p><strong>Update:</strong> RE: @ChristianFosli's suggestion to use <code>docker cp</code>: that solution requires a docker <em>container</em>. However, in my case, the reason I am looking to extract files specifically during the <code>docker build</code> process is that my <code>Dockerfile</code> runs an executable that fails, therefore I have no image that I can run as a container on which I can perform the <code>docker cp</code>. I.e. the file that I would like to extract from the <code>docker build</code> process is a metadata file that contains debugging information about the failure, that I'd like to inspect.</p>
"
"36756751","View logs for all docker containers simultaneously","<docker><docker-compose>","64783523","How do I view output of python script when running from docker on Amazon Linux","<python><linux><docker><logging><stdout>","<p>I currently use docker for my backend, and when I first start them up with</p>

<pre><code>docker-compose up
</code></pre>

<p>I get log outputs of all 4 dockers at once, so I can see how they are interacting with each other when a request comes in. Looking like this, one request going from nginx to couchdb</p>

<p><img src=""https://i.stack.imgur.com/S6Ldf.png"" alt=""http://i.imgur.com/E4GQ43F.png""></p>

<p>The issue is now that I am running on GCE with load balancing, when a new VM spins up, it auto starts the dockers and runs normally, I would like to be able to access a load balanced VM and view the live logs, but I can not get docker to allow me this style, when I use logs, it gives me normal all white font with no label of where it came from.</p>

<p>Using</p>

<pre><code>docker events  
</code></pre>

<p>does nothing, it won't return any info. </p>

<p>tldr; what is the best way to obtain a view, same as the log output you get when running ""docker-compose up""</p>
","<p>I'm currently trying to setup a flask application hosted by an nginx server through docker on a machine running Amazon Linux 2.</p>
<p>Basically I'm just trying to see the simple output of my python script when I trigger it through nginx on my running docker image.</p>
<p>I've previously worked on an Ubuntu machine and I could simply see the logs live with the command:</p>
<p>tail -f /var/log/syslog</p>
<p>but with Amazon linux or Linux distros in general I believe all logging is done through journalctl, and I can't seem to see any of my python script output when I search the main journalctl logs with the command:</p>
<p>sudo journalctl -f</p>
<p>Could anyone point me in the right direction on where my python script's output is going?</p>
"
"38485607","Mount host directory with a symbolic link inside in docker container","<docker>","64805889","How to properly include symlink in Docker volumes?","<wordpress><docker><docker-compose><symlink>","<p>I mounted the container with this parameter:</p>

<blockquote>
  <p>-v /home/test/:/home/test</p>
</blockquote>

<p>Inside /home/test in the host there is a symbolic link pointing to a /mnt/ folder.</p>

<p>But that link, although can be seen where is pointing to, seems broken inside the container:</p>

<pre><code>root@f93f72b45013:/var/www/html# cd /home/test/ 
root@f93f72b45013:/home/test# ls -lrt 
total 11956 
lrwxrwxrwx. 1 root root 40 Jul 20 15:55 file -&gt; /mnt/mountedfile/
root@f93f72b45013:/home/test# ls -lrt file/*
ls: cannot access file/*: No such file or directory
</code></pre>

<p>Is that even possible to be done in docker? I am not sure if is there a way to do it.</p>

<p>I know I can just directly mount where the symbolic link is pointing at but I was just wondering if this is possibe.</p>
","<p>I'm having an issue with symlinks in Docker volumes.</p>
<p>I'm new to Docker and for now, I tend to use it only for local WordPress plugin development.
My goal is to set up a WordPress container and create a symlink in plugins directory.</p>
<p>In docker-compose file I mapped wp volume to root dir.
Directory structure is like this:</p>
<pre><code>- some-wp-plugins
    - plugin-1
    - plugin-2
    - plugin-3

- some-wp-projects   
    - wp-project-1
        - docker-compose.yml
        - wp files...
        - wp-content
            - plugins
                - symlink to the plugin-1

    - wp-project-2
    - wp-project-3
</code></pre>
<p>The symlink is visible on the drive, but not inside a WordPress. If I copy plugin files instead of symlinking them, then everything is ok and I can activate plugin in wp.</p>
<p>Can symlinks be used in this scenario?</p>
<p>I have the latest Docker version (2.5.0.1) installed on mac machine.</p>
"
"48915458","windows run docker with --network=host and access with 127.0.0.1","<windows><docker><hyper-v>","64930840","How to connect to container from windows?","<windows><docker>","<p>I have windows 10 pro and I'm trying to run a docker with network mode host.</p>

<p>my issue is that I can't run a docker and access it using the host ip not 127.0.0.1 and not the ip (in linux it works differently).</p>

<p>looks like the hyper v has it's own network that not accessible using the host ip </p>

<pre><code>docker run -d --network=host nginx
</code></pre>

<p>output:</p>

<pre><code>CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS                          PORTS               NAMES
8edd86bf292b        nginx                                             ""nginx -g 'daemon of…""   3 seconds ago       Up 2 seconds                                        happy_curie
</code></pre>

<p>so there is no ports as expected but and no errors.
When I'm trying to open the browser using 127.0.0.1 I'm getting <code>ERR_CONNECTION_REFUSED</code> 
if I set ports to instead of network mode host it is working</p>

<pre><code>docker run -d -p 80:80   nginx
</code></pre>

<p>Hyper v  Ethernet adapter vEthernet (DockerNAT):</p>

<pre><code>   Connection-specific DNS Suffix  . :
   IPv4 Address. . . . . . . . . . . : 10.0.75.1
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . 
</code></pre>

<p>Remarks:</p>

<ul>
<li>changing in the hyper v virtual switch manager the network to be external - not helping

<ul>
<li>firewall is disabled</li>
</ul></li>
</ul>

<p>any idea how to work with network mode host in windows? </p>
","<p>I'm on Windows 10, Docker version 19.03.8
I have the following docker-compose.yml:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.2&quot;
services:
  mongo:
    image: mongo
    #restart: always
    network_mode: &quot;host&quot;
    ports:
      - 27017:27017
</code></pre>
<p>I start it with <strong>docker-compose up mongo</strong></p>
<p>Now I want to connect to it from my console by calling mongo.exe
I got error:</p>
<pre><code>Error: couldn't connect to server 127.0.0.1:27017, connection attempt failed:
</code></pre>
<p>Why can't I connect to my docker container? How do I connect to it?</p>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","64932302","docker: Error response from daemon: driver failed programming external connectivity on endpoint funny_golick","<sql-server><docker>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I am trying to run a docker to run Microsoft SQL Server 2019 in my ubuntu 20.04 OS.
For that reason I write my terminal:</p>
<pre><code>docker pull mcr.microsoft.com/mssql/server
</code></pre>
<p>then I write</p>
<pre><code>sudo docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=Zima1000' -p 1433:1433 -d mcr.microsoft.com/mssql/server:2019-latest
</code></pre>
<p>then it send this messege</p>
<pre><code>7faf65528d4d0fe12567afb6b0b39a07cd060c3974a0b77ac6c0e66f19c23e57
</code></pre>
<p>docker: Error response from daemon: driver failed programming external connectivity on endpoint funny_golick (8946bfd95b1e65fb700ffa4fe457d3de30c8d11a9c8a8f286ebebc70af096d19): Error starting userland proxy: listen tcp 0.0.0.0:1433: bind: address already in use.</p>
<p>plese help how I can fix it.</p>
"
"37132899","installing cPickle with python 3.5","<python><docker><pickle><python-3.5>","64940221","Pickle vs cPickle (?) in python3","<python><python-3.x><pickle>","<p>This might be silly but I am unable to install <code>cPickle</code> with python 3.5 docker image</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM python:3.5-onbuild
</code></pre>

<p><strong>requirements.txt</strong></p>

<pre><code>cpickle
</code></pre>

<p>When I try to build the image</p>

<pre><code>$ docker build -t sample .
Sending build context to Docker daemon 3.072 kB
Step 1 : FROM python:3.5-onbuild
# Executing 3 build triggers...
Step 1 : COPY requirements.txt /usr/src/app/
Step 1 : RUN pip install --no-cache-dir -r requirements.txt
 ---&gt; Running in 016c35a032ee
Collecting cpickle (from -r requirements.txt (line 1))
  Could not find a version that satisfies the requirement cpickle (from -r requirements.txt (line 1)) (from versions: )
No matching distribution found for cpickle (from -r requirements.txt (line 1))
You are using pip version 7.1.2, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
The command '/bin/sh -c pip install --no-cache-dir -r requirements.txt' returned a non-zero code: 1
</code></pre>
","<p>There used to be <a href=""https://docs.python.org/2/library/pickle.html#module-cPickle"" rel=""nofollow noreferrer""><code>cPickle</code></a> in python2.7. However, I don't see it anymore in <a href=""https://docs.python.org/3.8/library/pickle.html"" rel=""nofollow noreferrer""><code>python3 pickle</code></a>. What ever happened to that module, did it get merged into the regular <code>pickle</code> module?</p>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","64956328","Docker-Compose connect service to another network","<docker><networking><docker-compose>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have multiple <code>docker-compose.yml</code> which each defines a network.</p>
<p>I want to be able to link a service from one network to a service in another network.</p>
<p><strong>service1/docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.3'

services:
  service_1-mongo:
    image: mongo
    container_name: service_1-mongo
    restart: always
    environment:
      MONGO_INITDB_DATABASE: service_1-service
      MONGO_INITDB_ROOT_USERNAME: service_1
      MONGO_INITDB_ROOT_PASSWORD: service_1
    ports:
      - &quot;27017-27019:27017-27019&quot;
    networks:
      - service_1

  service_1-service:
    image: '...'
    container_name: service_1-service
    restart: always
    environment:
      MONGO_HOST: service_1-mongo
      MONGO_BASE: service_1-service
      MONGO_USER: service_1
      MONGO_PASS: service_1
    ports:
      - '4000:4000'
    networks:
      - network_1
    depends_on:
      - service_1-mongo
</code></pre>
<p><strong>service2/docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.3'

services:
  service_2-mongo:
    image: mongo
    container_name: service_2-mongo
    restart: always
    environment:
      MONGO_INITDB_DATABASE: service_2-service
      MONGO_INITDB_ROOT_USERNAME: service_2
      MONGO_INITDB_ROOT_PASSWORD: service_2
    ports:
      - &quot;27027-27029:27027-27029&quot;
    networks:
      - service_2

  service_2-service:
    image: '...'
    container_name: service_2-service
    restart: always
    environment:
      MONGO_HOST: service_2-mongo
      MONGO_BASE: service_2-service
      MONGO_USER: service_2
      MONGO_PASS: service_2
      SERVICE_1_URL: **SOMETHING**
    ports:
      - '4100:4000'
    networks:
      - network_2
    depends_on:
      - service_2-mongo
</code></pre>
<p>I need <code>network_1:service_1-service</code> to be able to connect to <code>network_2:service_2-service</code>.</p>
<p>I don't know how to do it.</p>
"
"37599263","What's the reason Docker Ubuntu official image would exit immediately when you run/start it?","<linux><bash><ubuntu><docker>","64970769","docker container does not seem to stay running","<docker>","<p>I understand that a container will exit when the main process exit. My question is about the reason behind it, not how to get it to work. I of course know that I could pass the parameter <code>-it</code> to start with interactive mode. </p>

<p>The Ubuntu image will run <code>/bin/bash</code> when it starts according to the image Dockerfile. Shouldn't bash process wait for user input commands and not exit? (just like when you run <code>/bin/bash</code> in the host machine, it would start an interactive shell and wait for user inputs and not exit) Why would the Docker Ubuntu's bash exit right away?</p>
","<p>Dockerfile:</p>
<pre><code>FROM ubuntu
CMD [&quot;bash&quot;]

</code></pre>
<p>I ran</p>
<pre><code>docker build -t myimage:mytag .

docker run myimage:mytag 

docker ps
</code></pre>
<p>No images are seen running. Why not?  How do I find more information on this, such as logs of why it exited?</p>
"
"38133849","Can't use yum inside Docker container running on CentOS","<docker><centos><containers><sudo><yum>","64990005","Dockerfile Permission Denied when using yum install","<docker><dockerfile><centos7><rpm><yum>","<p>I am unable to run any yum commands inside my Docker container without getting the following error:</p>

<pre><code>ovl: Error while doing RPMdb copy-up:
[Errno 13] Permission denied: '/var/lib/rpm/Group'
You need to be root to perform this command.
</code></pre>

<p>I'm confused because I'm pretty sure docker containers are run with the default user root.  Still, if I try putting sudo in front of a <code>yum install -y &lt;package&gt;</code> or <code>yum update -y</code> command I get:</p>

<pre><code>/bin/sh: sudo: command not found
</code></pre>

<p>I'm using the following base image so I can easily run a Java Wildfly application inside Docker: <a href=""https://hub.docker.com/r/jboss/wildfly/"" rel=""noreferrer"">https://hub.docker.com/r/jboss/wildfly/</a></p>

<p>The underlying distro is <code>CentOS Linux release 7.2.1511 (Core)</code></p>
","<p>I'm trying to create Dockerfile from this Image: <a href=""https://hub.docker.com/r/centos/python-27-centos7"" rel=""nofollow noreferrer"">https://hub.docker.com/r/centos/python-27-centos7</a></p>
<p>My Dockerfile:</p>
<pre><code>FROM centos/python-27-centos7:latest

RUN yum install rpm-build -y

WORKDIR /usr/src/app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt


CMD [&quot;bash&quot;]
</code></pre>
<p>But I got next error:</p>
<pre><code> =&gt; ERROR [2/9] RUN yum install rpm-build -y                                                                                                                                0.6s
------                                                                                                                                                                           
 &gt; [2/9] RUN yum install rpm-build -y:                                                                                                                                           
#5 0.480 Loaded plugins: fastestmirror, ovl                                                                                                                                      
#5 0.485 ovl: Error while doing RPMdb copy-up:
#5 0.485 [Errno 13] Permission denied: '/var/lib/rpm/Dirnames'
#5 0.579 You need to be root to perform this command.
------
</code></pre>
<p>I tried add sudo but still doesn't work.
What is wrong here?</p>
"
"40944479","Docker: How to use bash with an Alpine based docker image?","<bash><docker><dockerfile><alpine>","64998374","Docker run won't find shell script","<docker><sh>","<p>I created a docker image from openjdk:8-jdk-alpine but when I try to execute simple commands I get the following errors:</p>

<pre><code>RUN bash
/bin/sh: bash: not found

RUN ./gradlew build
env: can't execute 'bash': No such file or directory
</code></pre>
","<p>I've created a docker image, and attempted to run it like this:</p>
<pre><code>IMAGE=$(docker build --network=host -q .)

docker run \
        --network=host \
        --rm $IMAGE sh -c &quot;cd bamboo &amp;&amp; ./publish.sh&quot;
</code></pre>
<p>The output is:</p>
<pre><code>sh: ./publish.sh: not found
</code></pre>
<p>This variation of the command</p>
<pre><code>IMAGE=$(docker build --network=host -q .)

docker run \
        --network=host \
        --rm $IMAGE sh -c &quot;cd bamboo &amp;&amp; ls -l&quot;
</code></pre>
<p>shows:</p>
<pre><code>-rwxr-xr-x    1 node     node           646 Nov 25 01:40 publish.sh
</code></pre>
<p>Outside the container (whose content is copied into the container via the docker file), while in same parent folder as the above script, running</p>
<pre><code>cd bamboo &amp; ./publish.sh
</code></pre>
<p>works as expected.  Any ideas what I can be doing wrong?</p>
<p>I found this similar question:
<a href=""https://stackoverflow.com/questions/37419042/container-command-start-sh-not-found-or-does-not-exist-entrypoint-to-contain"">Container command &#39;/start.sh&#39; not found or does not exist, entrypoint to container is shell script</a></p>
<p>and it mentioned line ending on Windows (where I am running Docker) can be problematic.  I confirmed my line ending are LF rather than CRLF.</p>
<p>EDIT: The first few lines of the called script is this:</p>
<pre><code>#!/bin/bash

set -ex

SCRIPTDIR=&quot;$( cd &quot;$( dirname &quot;${BASH_SOURCE[0]}&quot; )&quot; &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )&quot;
</code></pre>
<p>and my dockerfile looks like this:</p>
<pre><code>FROM node:12.16.2-alpine3.11

WORKDIR /app

# node image creates node user for security purposes
RUN chown -R node:node /app

USER node

COPY --chown=node:node . .

RUN yarn install --no-progress --pure-lockfile

RUN yarn build
</code></pre>
"
"47303141","How to use the host network, and any other user-defined network together in Docker-Compose?","<docker><networking><service><docker-compose><ethernet>","65009367","Is it possible for a container with the host network to receive the network bridge to the network?","<docker>","<p>I want to connect two Docker containers, defined in a Docker-Compose file to each other (<code>app</code> and <code>db</code>). And one of them (<code>app</code>) should also be connected to the <code>host</code> network.</p>

<p>The containers should be connected to a common user-defined network (<code>appnet</code> or <code>default</code>) to use the <em>embedded DNS</em> capabilities from docker networking.</p>

<p><code>app</code> needs also to be directly connected to the host network to receive ethernet broadcasts (network layer 2) in the physical network of the docker host.</p>

<p>If I use both directives <code>network_mode: host</code> and <code>networks</code> in compose together, I get the following error.</p>

<p><code>ERROR: 'network_mode' and 'networks' cannot be combined</code></p>

<p>So I have to do this with <code>networks</code> only!?</p>

<pre><code>version: ""3.3""

services:

  app:
    build: .
    image: app
    container_name: app
    environment:
      - MONGODB_HOST=db
    depends_on:
      - db
    networks:
      - appnet
      - hostnet

  db:
    image: mongo:latest
    container_name: db
    networks:
      - appnet

networks:
  appnet:
  hostnet:
    external:
      name: host
</code></pre>

<p>The foregoing compose file produces a error.
<code>ERROR: for app  network-scoped alias is supported only for containers in user defined networks</code></p>

<p>If I use the network name <code>host</code> in the service without defining it in networks (because it already exists), it says.
<code>ERROR: Service ""app"" uses an undefined network ""host""</code></p>

<p>How to use the <code>host</code> network, and any other user-defined network (or the default) together in Docker-Compose?</p>

<p>I'm using <code>Docker version 17.11.0-ce-rc3, build 5b4af4f</code> and <code>docker-compose version 1.17.1, build 6d101fb</code></p>
","<p>Let's assume that I have docker compose as bellow</p>
<p>version: '2.1'</p>
<pre><code>services:

    example-first:
      image: example-first:latest
      container_name: example-first
      ports:
        - &quot;6161:6161&quot;   
      restart: always
      networks:
      - default


        
    example-second:
      image: example-second:latest
      container_name: example-second
      ports:
        - &quot;6767:6767&quot;  
      restart: always
      network_mode: host


networks:
  default:
      driver: bridge
</code></pre>
<p>How the second container can receive (send request) the first container? Is it possible?</p>
<p>Thanks in advance!</p>
"
"44635352","difference between localhost and postgres for host in docker","<django><postgresql><docker><docker-compose>","65010985","Database Postgres in Docker with Django?","<django><database><postgresql><docker>","<p>I am developing a django app and trying to run it inside docker. I have an issue that I could not understand so far. while running the app with <code>docker-compose</code>, it seems that the <code>web</code> app cannot connect to the database when i use these configurations:</p>

<pre><code>DATABASES = {
    'default': {
       'ENGINE': 'django.db.backends.postgresql_psycopg2',
       'NAME': 'my_db',
       'USER': 'my_user',
       'PASSWORD': '',
       'HOST': 'localhost',
       'PORT': '5432',
    }
</code></pre>

<p>but once I change the host to <code>postgres</code>, it works. like this</p>

<pre><code>DATABASES = {
    'default': {
       'ENGINE': 'django.db.backends.postgresql_psycopg2',
       'NAME': 'my_db',
       'USER': 'my_user',
       'PASSWORD': '',
       'HOST': 'postgres',
       'PORT': '5432',
    }
</code></pre>

<p>what is the difference between <code>postgres</code> and <code>localhost</code>. One is running without and issue inside docker and not in development environment in my mac and the other one is the opposite.</p>

<pre><code># docker-compose.yml    
version: '3'

    services:
      db:
        image: postgres
        expose: 
          - ""5432""
      web:
        build: .
        command: python3 manage.py runserver 0.0.0.0:8000
        volumes:
          - .:/code
        ports:
          - ""8000:8000""
        depends_on:
          - db
</code></pre>
","<p>For example I Use  defoult settings in Django</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'database',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': '127.0.0.1',
        'PORT': '5432',
    }
</code></pre>
<p>When dockerize my db with</p>
<pre><code>depends_on:
          - db
</code></pre>
<p>Should I change change my <code>'HOST'</code> parameter <code> '127.0.0.1'</code>  for <code>'db'</code></p>
<p>could you explain this process</p>
<p>thank you in advance</p>
"
"39711924","Merge two docker images","<docker><dockerfile>","65019512","How to merge Docker Compose in one image?","<docker><google-cloud-platform><docker-compose><dockerfile><google-cloud-run>","<p>I'm hoping to use docker to set up some bioinformatic analysis.</p>

<p>I have found two docker images that I would like to use:</p>

<ol>
<li>jupyter/datascience-notebook</li>
<li>bioconductor/devel_base</li>
</ol>

<p>I have been successful in running each of these images independently, however I don't know how to merge them together.</p>

<p>Is merging two docker containers possible? Or do you start with one, and then manually install the features of the other?</p>
","<p>I'm trying to host a Docker Compose file via Google Cloud Run but for that I can only choose one image. So my idea was to merge it into one image. I know it's not exactly the best way to do it but I don't have time to learn Kubernetes.
Thank you!</p>
"
"49718431","docker-compose.yml file naming convention","<docker><docker-compose>","65060070","How to run docker-compose with custom file name","<docker><docker-compose>","<p>What is the naming convention for the <code>docker-compose.yml</code> file? Should it always be named like this or am I free to rename it to, for instance, <code>docker-compose-jenkins.yml</code>?</p>
","<p>I have this file: <code>docker-compose.test.yml</code></p>
<p>But when I try to run <code>docker-compose</code>, this happens:</p>
<pre class=""lang-sh prettyprint-override""><code>$ docker-compose up
ERROR:
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml

</code></pre>
<p><strong>Is there some option to specify which file to use?</strong></p>
"
"41208870","Docker compose named volume: find volume on host machine","<docker><docker-compose><volume>","65084035","docker-compose named volume not showing on host","<docker><docker-compose>","<p>I have a docker-compose.yml:</p>

<pre><code>version: '2'

services:
   db:
     image: mysql:5.7
     volumes:
       - db_data:/var/lib/mysql
     restart: always
     environment:
       MYSQL_ROOT_PASSWORD: wordpress
       MYSQL_DATABASE: wordpress
       MYSQL_USER: wordpress
       MYSQL_PASSWORD: wordpress

   wordpress:
     depends_on:
       - db
     image: wordpress:latest
     ports:
       - ""8000:80""
     restart: always
     environment:
       WORDPRESS_DB_HOST: db:3306
       WORDPRESS_DB_PASSWORD: wordpress
volumes:
    db_data:
</code></pre>

<p>When I run this <code>docker-compose up -d</code> and then do <code>docker inspect -f '{{ (index .Mounts 0).Source }}' ef8be254a08b</code> for the running db container to get the Source which is specifying the volume location on the host, I always get ""No such file or directory"" if I <code>ls</code> the directory (<code>ls /var/lib/docker/volumes/test_db_data/_data: No such file or directory</code>).
How is that possible to get the real location for the volume?</p>
","<p>I am learning docker-compose, I have a simple docker-compose, which is working, e.g I can use mongo from my nodejs api:</p>
<pre><code>version: '3'    
services:
  main:
    build: ./api
    container_name: main
    command: npm run start
    restart: unless-stopped
    ports:
      - &quot;3001:3001&quot;
    environment:
      - PORT=3001
      - MONGO_URL=mongodb://api_db:27017/api
    depends_on:
      - api_db
    networks:
      - realworld-docker-network
  api_db:
      image: mongo:latest
      container_name: mongo
      volumes:
        - mongodb_auth:/data/db
      networks:
        - realworld-docker-network
networks:
  realworld-docker-network:
    driver: bridge
volumes:
  mongodb_auth:
    driver: local
</code></pre>
<p>What i can not understand is where mongodb is mapped to the host system directory, because there is no <code>/data/db</code> on my host ubuntu machine, when all containers are up.</p>
<p>I tried to search on net, but cant figure this out. Please help.</p>
"
"37242217","Access docker container from host using containers name","<docker><docker-compose>","65094480","How do I make a Docker Compose alias available to the local machine?","<docker><docker-compose>","<p>I am developing a service and using there docker compose to spin services like postgres, redis, elasticsearch. I have a web application that is based on RubyOnRails and writes and reads from all those services.</p>

<p>Here is my <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      - frontapp

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      - frontapp

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      - frontapp

networks:
  frontapp:
    driver: bridge
</code></pre>

<p>And i can ping containers within this network</p>

<pre><code>$ docker-compose run redis /bin/bash
root@777501e06c03:/data# ping postgres
PING postgres (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.346 ms
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.047 ms
...
</code></pre>

<p>So far so good. Now I want to run ruby on rails application on my host machine but be able to access postgres instance with url like <code>postgresql://username:password@postgres/database</code> currently that is not possible</p>

<pre><code>$ ping postgres
ping: unknown host postgres
</code></pre>

<p>I can see my network in docker</p>

<pre><code>$ docker network ls
NETWORK ID          NAME                DRIVER
ac394b85ce09        bridge              bridge              
0189d7e86b33        elephant_default    bridge              
7e00c70bde3b        elephant_frontapp   bridge              
a648554a72fa        host                host                
4ad9f0f41b36        none                null 
</code></pre>

<p>And I can see an interface to it</p>

<pre><code>$ ifconfig 
br-0189d7e86b33 Link encap:Ethernet  HWaddr 02:42:76:72:bb:c2  
          inet addr:172.18.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:76ff:fe72:bbc2/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:36 errors:0 dropped:0 overruns:0 frame:0
          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2000 (2.0 KB)  TX bytes:8792 (8.7 KB)

br-7e00c70bde3b Link encap:Ethernet  HWaddr 02:42:e7:d1:fe:29  
          inet addr:172.20.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e7ff:fed1:fe29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1584 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1597 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:407137 (407.1 KB)  TX bytes:292299 (292.2 KB)
...
</code></pre>

<p>But i am not sure what should I do next. I tried to play a bit with <code>/etc/resolv.conf</code>, mainly with <code>nameserver</code> directive, but that had no effect.</p>

<p>I would appreciate any help of suggestions how to configure this setup correctly.</p>

<p><strong>UPDATE</strong></p>

<p>After browsing through Internet resources I managed to assign static IP addresses to boxes. For now it is enough for me to continue development. Here is my current <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      frontapp:
        ipv4_address: 172.25.0.11

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      frontapp:
        ipv4_address: 172.25.0.12

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      frontapp:
        ipv4_address: 172.25.0.10

networks:
  frontapp:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1
</code></pre>
","<p>Apologies if this is a duplicate. It's very possible I'm not familiar enough with Docker to even know the right terminology to know what to look for.</p>
<p>For the sake of the question, suppose that I have a web app that has a frontend that's a single-page app written in Vue, and then a backend written in Python. I'd like to run some end-to-end tests with Docker Compose by making an environment with the frontend and backend plus a third container running TestCafé. I'd also like to be able to use the app locally to poke around (e.g., go to <code>localhost:3000</code> on my computer to get the frontend with all the data hooked up).</p>
<p>I have this partly working. The Docker Compose file runs all the services, and I've created a DNS record inside the Docker Compose universe so that the frontend is at <code>frontend.local</code> and the backend is at <code>backend.local</code>, so TestCafé can get the resources when the tests runs.</p>
<p>But, I have to specify a URL for the frontend code to access the backend. In this case, that's <code>backend.local</code>. Which works in the TestCafé container browser, but doesn't work when I use the frontend at <code>localhost:3000</code>. Is there a way to map <code>backend.local</code> and <code>frontend.local</code> to my computer's local network as well as in the Docker Compose environment? If so, how do I do that?</p>
<p>For reference, here's a rough sketch of what I'm doing with the <code>docker-compose.yml</code>:</p>
<pre><code>version: &quot;3.7&quot;
services:

  frontend:
    image: frontend:latest
    networks:
      default:
        aliases:
        - frontend.local
    build:
      context: .
      dockerfile: Dockerfile
    ports:
     - &quot;3000:3000&quot;
    depends_on:
      - &quot;backend&quot;

  testcafe:
    image: testcafe:latest
    networks:
      default:
    build:
      context: testcafe
      dockerfile: Dockerfile
    depends_on:
      - &quot;frontend&quot;
    volumes:
      - ./testcafe/artifacts:/artifacts

  backend:
    image: backend:latest
    ports:
     - &quot;8100:8100&quot;
     - &quot;8000:8000&quot;
    networks:
      default:
        aliases:
        - backend.local

networks:
  default:
</code></pre>
"
"39136601","Changing /proc/sys/kernel/core_pattern file inside docker container","<linux><docker><linux-kernel><dockerfile><privileges>","65101860","Change /proc/sys/kernel/core_pattern inside docker container, but also changed host machine value","<linux><docker>","<p>How can i change <code>/proc/sys/kernel/core_pattern</code> file inside the docker container with out privileged mode? Are there any flags to be passed to <code>docker daemon</code> or <code>docker run</code> or anything related to <code>Dockerfile</code>?</p>
","<p>I just add</p>
<pre><code>echo &quot;${POOL_RUNTIME_PATH}/core.%e.signal_%s.unix_time_%t&quot; &gt;/proc/sys/kernel/core_pattern
</code></pre>
<p>to my Dockerfile.<br />
But after I run Docker container, The host /proc/sys/kernel/core_pattern value also changed.<br />
Why?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65116462","Dockerfile fails on build - cannot connect to docker daemon","<docker><dockerfile>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I have this dockerfile:</p>
<pre><code>FROM ubuntu
RUN DEBIAN_FRONTEND=noninteractive apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common

#install freecad
RUN add-apt-repository ppa:freecad-maintainers/freecad-daily
RUN DEBIAN_FRONTEND=noninteractive apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y freecad-daily

#install utils
RUN DEBIAN_FRONTEND=noninteractive apt install -y vim
RUN DEBIAN_FRONTEND=noninteractive apt install -y nano
RUN DEBIAN_FRONTEND=noninteractive apt install -y git

#install protobuf
RUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/protocolbuffers/protobuf
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y autoconf autogen
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y --fix-missing autoconf automake pkg-config #missing: libgtk-3-dev
RUN DEBIAN_FRONTEND=noninteractive apt install -y build-essential #needed for g++?
RUN cd protobuf &amp;&amp; ./autogen.sh
RUN cd protobuf &amp;&amp; ./configure
RUN cd protobuf &amp;&amp; make
RUN cd protobuf &amp;&amp; make install

#install libarcus
#https://community.ultimaker.com/topic/20409-stuck-up-in-creating-a-docker-image-for-curaengine/
RUN DEBIAN_FRONTEND=noninteractive apt install -y cmake  #missing:  protobuf-compiler python3-dev python3-sip-dev libprotoc-dev libprotobuf-dev
RUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/Ultimaker/libArcus.git \
    &amp;&amp; cd libArcus \
    &amp;&amp; sed -i '/option(BUILD_PYTHON &quot;Build &quot; ON)/c\option(BUILD_PYTHON &quot;Build &quot; OFF)' ./CMakeLists.txt \
    &amp;&amp; mkdir build &amp;&amp; cd build \
    &amp;&amp; ls -la .. \
    &amp;&amp; cmake .. \
    &amp;&amp; make -j4 \
    &amp;&amp; make install

#install cura-engine and cura
RUN git clone https://github.com/Ultimaker/CuraEngine.git
RUN git clone https://github.com/Ultimaker/Cura.git
RUN cd CuraEngine &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make

#install node
RUN apt-get install git-core curl build-essential openssl libssl-dev \
 &amp;&amp; git clone https://github.com/nodejs/node.git \
 &amp;&amp; cd node \
 &amp;&amp; ./configure \
 &amp;&amp; make \
 &amp;&amp; sudo make install

CMD [&quot;node&quot;,&quot;server.js&quot;]
</code></pre>
<p>I open the terminal and navigate to the directory this is in and i type:</p>
<pre><code>docker build .
</code></pre>
<p>I get this error:</p>
<pre><code>ERRO[0000] failed to dial gRPC: cannot connect to the Docker daemon. Is 'docker daemon' running on this host?: dial unix /var/run/docker.sock: connect: permission denied 
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;session=y3on0l6lkezz43j468liw0907&amp;shmsize=0&amp;target=&amp;ulimits=null&amp;version=1: dial unix /var/run/docker.sock: connect: permission denied
</code></pre>
<p>I have no clue what is the problem.... I do everything by the book...</p>
"
"49019652","not able to connect to mysql docker from local","<mysql><docker>","65155723","Mysql Docker ERROR 1045 (28000): Plugin caching_sha2_password could not be loaded","<mysql><docker>","<p>I am trying to connect to mysql database from docker image. However it's throwing errors. </p>

<p>following is the docker image I am using. 
<a href=""https://hub.docker.com/_/mysql/"" rel=""noreferrer"">https://hub.docker.com/_/mysql/</a></p>

<p>And following is the command I have used to run the docker image. </p>

<pre><code>docker run -p 3306:3306 --name mysql_80 -e MYSQL_ROOT_PASSWORD=password -d mysql:8
</code></pre>

<p>Following is the output of <code>docker ps</code> command </p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                              NAMES
9f35d2e39476        mysql:8             ""docker-entrypoint.s…""   5 minutes ago       Up 5 minutes        0.0.0.0:3306-&gt;3306/tcp
</code></pre>

<p>if I check the IP using docker inspect and ping that IP, it shows IP is not reachable. </p>

<pre><code>docker inspect 9f35d2e39476 | grep -i ipaddress
</code></pre>

<p>And if i try to connect using <code>localhost</code> and <code>127.0.0.1</code> I am getting following error. </p>

<blockquote>
  <p>Unable to load authentication plugin 'caching_sha2_password'.</p>
</blockquote>
","<p>I have setup a mysql container in Docker and i start it up and when i want to connect via DBeaver of a CLI with mysql command i get this error :</p>
<pre><code>ERROR 1045 (28000): Plugin caching_sha2_password could not be loaded: /usr/lib/x86_64-linux-gnu/mariadb19/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory

</code></pre>
<p>I managed to solve this partially with an answer from this <a href=""https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded"">github post</a> like that:</p>
<blockquote>
<p>I loged in the container</p>
<p>docker exec -it CONTAINER_ID bash then log into mysql as root</p>
<p>mysql --user=root --password Enter the password for root (Default is
'root') Finally Run:</p>
<p>ALTER USER 'username' IDENTIFIED WITH mysql_native_password BY
'password';</p>
</blockquote>
<p>And i say partially because every time i drop the database and start it up again, it gives me the same error and i have again to change the password in order to connect successfully.
Take in mind that i just put the same password and not a new one. It's not a wrong password error.</p>
"
"41685603","Reset Docker Machine to run Docker commands on my local machine","<docker><docker-machine>","65188558","docker-machine - how to restore previous ""machine"" or default machine","<docker><docker-machine>","<p>I'm learning docker right now and I have run into something that's confusing. I started using <code>docker-machine</code>, which was great for creating a local VM and spinning up a server on Digital Ocean. However, I now want to return to a state where the docker commands execue on my local machine so that I can add features, build, and push my docker repo. </p>

<p>I know how to change to other machines using <code>docker-machine env</code>, but how do I go back to having no active machine? If I restart docker this happens, but I don't want to be restarting docker all of the time. It seems like there should be a command for this?</p>

<p>And just to clarify, I am not trying to connect to my <code>default</code> machine. I want the docker commands to execute on my machine like they did before I set up Docker Machine. </p>
","<p><strong>Problem Description</strong></p>
<p>I'm not sure how to switch back to my local docker desktop machine.</p>
<p><strong>Background</strong></p>
<p>I'm running on a mac... and I downloaded Docker desktop, created a few containers and everything was hunky dory.</p>
<p>Then I discovered docker-machine and so I:</p>
<ul>
<li>downloaded oracle virtual box.</li>
<li>created a new docker machine. &quot;docker-machine create virtualbox dev&quot; or something like that</li>
<li>successfully created a new container on this dev host.</li>
<li>tested to make sure my dummy app runs properly on this new host.</li>
</ul>
<p>admins-Mac-mini-3:dev admin$ <strong>docker-machine ls</strong>
NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
dev    *        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.12<br />
admins-Mac-mini-3:dev admin$</p>
<pre><code>admins-Mac-mini-3:dev admin$ **docker ps**
        CONTAINER ID   IMAGE            COMMAND             CREATED             STATUS              PORTS                  NAMES
        69892fee42e7       cbrewsapp   &quot;npm start&quot;         About an hour ago   Up About an hour    0.0.0.0:80-&gt;3000/tcp   vigorous_stonebraker
       admins-Mac-mini-3:dev admin$ 


admins-Mac-mini-3:dev admin$ **curl http://192.168.99.100**
        {&quot;msg&quot;:&quot;hello world&quot;}admins-Mac-mini-3:dev admin$ 
</code></pre>
<p>Now I need to know how to switch back to the original host i had going when i downloaded / set up Docker Desktop.</p>
"
"44938344","Cannot connect to MongoDB via node.js in Docker","<node.js><mongodb><docker><docker-compose><dockerfile>","65190197","Issue connecting a Node.js app to a MongoDB using docker-compose","<node.js><mongodb><docker><docker-compose>","<p>My node.js express app cannot connect to the MongoDB in a Docker. <em>I'm not that familiar with Docker.</em></p>

<p>node.js connection:</p>

<pre><code>import mongodb from 'mongodb';
...
mongodb.MongoClient.connect('mongodb://localhost:27017', ... );
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM node:argon
RUN mkdir /app
WORKDIR /app
COPY package.json /app
RUN npm install
COPY . /app
EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: “2”
 services:
  web:
   build: .
   volumes:
     — ./:/app
   ports:
   — “3000:3000”
   links:
    — mongo
   mongo:
    image: mongo
    ports:
      — “27017:27017”
</code></pre>

<p>Build command: <code>docker build -t NAME .</code></p>

<p>Run command: <code>docker run -ti -p 3000:3000 NAME</code></p>

<p>Connection error:</p>

<pre><code>[MongoError: failed to connect to server [localhost:27017] on first connect [MongoError: connect ECONNREFUSED 127.0.0.1:27017]]
  name: 'MongoError',
  message: 'failed to connect to server [localhost:27017] on first connect [MongoError: connect ECONNREFUSED 127.0.0.1:27017]'
</code></pre>
","<p>Hy, I am pretty new to docker and I am having some issue understanding how containers interact with each other in the docker world created using a docker compose</p>
<p>I have a docker-compose.yaml file that contain 2 services:</p>
<ol>
<li>A MongoDB service which is suppose to create a MongoDB database</li>
<li>A Node.js app service that is trying to connect to that MongoDB database</li>
</ol>
<p>When i try to run the docker-compose up command I get an error letting me know there was a issue connecting to my MongoDB instance.</p>
<pre><code>info: Listening on port 3000...
error: uncaughtException: failed to connect to server [localhost:27017] on first connect [Error: connect ECONNREFUSED 127.0.0.1:27017
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1146:16) {
  name: 'MongoNetworkError'
</code></pre>
<p>This is my docker-compose.yaml file</p>
<pre><code>version: '3.7'
services:
  mongodb:
    container_name: mongodb_container
    image: mongo:latest
    ports:
      - 27017:27017
    volumes:
      - mongodb_data_container:/data/db

  my_app: 
    container_name: my_app_container
    image: my_app:1.0
    ports: 
      - 3000:3000
</code></pre>
<p>Can anyone help me out with this issue?</p>
"
"50547724","How to resolve the error: SQL authentication method unknown in Laravel-MySql","<php><docker><docker-compose><laravel-5.5><mysql-8.0>","65242424","Problem on laravel when I execute the command : php artisan migrate","<php><laravel><macos>","<p>I using docker and I have a container of Laravel Framework 5.5.25 and other with mysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL). in my configuration of docker compose I have this: </p>

<pre><code>version: ""2""
services:
    mysql:
    image: mysql
        ports:
            - ""3307:3306""
        command: --sql_mode=""""
</code></pre>

<p>So, when Laravel try to connect to MySql I have this error: </p>

<blockquote>
  <p>SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from</p>
</blockquote>
","<p>I get the following error message when I run the command &quot;php artisan migration&quot;.</p>
<pre><code>Illuminate\Database\QueryException  SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: create table `migrations` (`id` int unsigned not null auto_increment primary key, `migration` varchar(255) not null, `batch` int not null) default character set utf8mb4 collate 'utf8mb4_unicode_ci')

at vendor/laravel/framework/src/Illuminate/Database/Connection.php:671
     667▕         // If an exception occurs when attempting to run a query, we'll format the error
     668▕         // message to include the bindings with SQL, which will make this exception a
     669▕         // lot more helpful to the developer instead of just the database's errors.
     670▕         catch (Exception $e) {
   ➜ 671▕             throw new QueryException(
     672▕                 $query, $this-&gt;prepareBindings($bindings), $e
     673▕             );
     674▕         }
     675▕ 
 
       +29 vendor frames 
   30  artisan:37
      Illuminate\Foundation\Console\Kernel::handle(Object(Symfony\Component\Console\Input\ArgvInput), Object(Symfony\Component\Console\Output\ConsoleOutput))
</code></pre>
<p>I changed the host database, localhost to 127.0.0.1. And I executed the command &quot;php artisan config:clear&quot;.</p>
<p>Here is my configuration in the .env file :</p>
<pre><code>DB_CONNECTION=mysql
DB_HOST=127.0.0.1.
DB_PORT=3306
DB_DATABASE=new_reservation
DB_USERNAME=root
DB_PASSWORD=
</code></pre>
<p>I use MAMP on my Mac.</p>
<p>I use PHP version 7.3.22.</p>
<p>Thank you in advance for your help.</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65256027","docker.sock: connect: permission denied","<docker><permissions>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I'm trying to start playing with docker but unfortunately I faced an issue with socket permission</p>
<pre><code>&gt;&gt;&gt; whoami
tomasz

&gt;&gt;&gt; grep tomasz /etc/group
docker:x:462:tomasz

&gt;&gt;&gt; docker run --rm hello-world
docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create&quot;: dial unix /var/run/docker.sock: connect: permission denied.

&gt;&gt;&gt; ls -l /var/run/docker.sock
srw-rw---- 1 root docker 0 Dec 11 18:20 /var/run/docker.sock=
</code></pre>
<p>I've tried restart docker several times but it does not help. As you can see I am in docker group but this fact did not change anything. :)
Do you have any idea why?</p>
<p>ps sorry for my english.</p>
"
"45788253","Mounting development docker container directory on host","<docker>","65322794","How create a reverse bind volume with docker-compose?","<docker><docker-compose><containers><devops>","<p>I am using docker for software development, as I can bundle all my dependencies (compilers, libraries, ...) within a nice contained environment, without polluting the host.</p>

<p>The way I usually do things (which I guess is pretty common): I have a directory on the host that only contains the source code, which is mounted into a development container using a docker volume, where my software gets built and executed. Thanks to volumes being in sync, any changes in the source is reflected within the container.</p>

<p>Here is the pitfall: when using a code editor, software dependencies are considered broken as they are not accessible from the host. Therefore linting, etc... does not work.</p>

<p>I would like to be able to mount, let's say <code>/usr/local/include</code> from the container onto the host so that, be correctly configuring my editor, I can fix all the warnings.</p>

<p>I guess docker volume is not the solution here, because it would override the contained file system...</p>

<p>Also, I'm using Windows (no choice here) therefore my flow is:</p>

<p><code>Windows &gt; Samba &gt; Linux Host &gt; Docker &gt; Container</code></p>

<p>and I'd prefer not switching IDE (VS Code).</p>

<p>Any ideas? Thank you!</p>
","<p>maybe it's a stupid question but, i would like to know how can i bring the data,folders and code FROM my container in MY desktop to work with my IDE with docker-compose, is it possible?</p>
<p>Thank you in advance to everyone!</p>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","65371364","Is Docker Platform Independent?","<docker><containers>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I've always this question and no where I found exact answer:</p>
<p><strong>Is Docker Platform Independent?</strong></p>
<p>I mean, when I generate image in Windows, can I run that image on Linux or MacOS? Or when I generate image on Ubuntu, can I run that image on Windows?</p>
<p>If yes, how?
If no, what's the point of saying, docker images can be ran anywhere without worrying about any dependencies?</p>
<p>Please help me with this question</p>
"
"42494853","standard_init_linux.go:178: exec user process caused ""exec format error""","<python><linux><bash><docker>","65391847","standard_init_linux.go:211: exec user process caused ""exec format error"" when build tiktokapi:latest","<python><linux><docker><api>","<p>docker started throwing this error:</p>

<blockquote>
  <p>standard_init_linux.go:178: exec user process caused ""exec format error""</p>
</blockquote>

<p>whenever I run a specific docker container with CMD or ENTRYPOINT, with no regard to any changes to the file other then removing CMD or ENTRYPOINT. here is the docker file I have been working with which worked perfectly until about an hour ago:</p>

<pre><code>FROM buildpack-deps:jessie

ENV PATH /usr/local/bin:$PATH

ENV LANG C.UTF-8

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        tcl \
        tk \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D
ENV PYTHON_VERSION 3.6.0

ENV PYTHON_PIP_VERSION 9.0.1

RUN set -ex \
    &amp;&amp; buildDeps=' \
        tcl-dev \
        tk-dev \
    ' \
    &amp;&amp; apt-get update &amp;&amp; apt-get install -y $buildDeps --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/* \
    \
    &amp;&amp; wget -O python.tar.xz ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz"" \
    &amp;&amp; wget -O python.tar.xz.asc ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc"" \
    &amp;&amp; export GNUPGHOME=""$(mktemp -d)"" \
    &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys ""$GPG_KEY"" \
    &amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \
    &amp;&amp; rm -r ""$GNUPGHOME"" python.tar.xz.asc \
    &amp;&amp; mkdir -p /usr/src/python \
    &amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \
    &amp;&amp; rm python.tar.xz \
    \
    &amp;&amp; cd /usr/src/python \
    &amp;&amp; ./configure \
        --enable-loadable-sqlite-extensions \
        --enable-shared \
    &amp;&amp; make -j$(nproc) \
    &amp;&amp; make install \
    &amp;&amp; ldconfig \
    \
    &amp;&amp; if [ ! -e /usr/local/bin/pip3 ]; then : \
        &amp;&amp; wget -O /tmp/get-pip.py 'https://bootstrap.pypa.io/get-pip.py' \
        &amp;&amp; python3 /tmp/get-pip.py ""pip==$PYTHON_PIP_VERSION"" \
        &amp;&amp; rm /tmp/get-pip.py \
    ; fi \
    &amp;&amp; pip3 install --no-cache-dir --upgrade --force-reinstall ""pip==$PYTHON_PIP_VERSION"" \
    &amp;&amp; [ ""$(pip list |tac|tac| awk -F '[ ()]+' '$1 == ""pip"" { print $2; exit }')"" = ""$PYTHON_PIP_VERSION"" ] \
    \
    &amp;&amp; find /usr/local -depth \
        \( \
            \( -type d -a -name test -o -name tests \) \
            -o \
            \( -type f -a -name '*.pyc' -o -name '*.pyo' \) \
        \) -exec rm -rf '{}' + \
    &amp;&amp; apt-get purge -y --auto-remove $buildDeps \
    &amp;&amp; rm -rf /usr/src/python ~/.cache

RUN cd /usr/local/bin \
    &amp;&amp; { [ -e easy_install ] || ln -s easy_install-* easy_install; } \
    &amp;&amp; ln -s idle3 idle \
    &amp;&amp; ln -s pydoc3 pydoc \
    &amp;&amp; ln -s python3 python \
    &amp;&amp; ln -s python3-config python-config

RUN pip install uwsgi

RUN mkdir /config

RUN mkdir /logs

ENV HOME /var/www

WORKDIR /config

ADD conf/requirements.txt /config

RUN pip install -r /config/requirements.txt

ADD conf/wsgi.py /config

ADD conf/wsgi.ini /config

ADD conf/__init__.py /config

ADD start.sh /bin/start.sh

RUN chmod +x /bin/start.sh

EXPOSE 8000

ENTRYPOINT [""start.sh"", ""uwsgi"", ""--ini"", ""wsgi.ini""]
</code></pre>
","<p>trying to build</p>
<blockquote>
<p>docker build . -t tiktokapi:latest and get this error message:</p>
</blockquote>
<blockquote>
<p>Step 2/4 : RUN apt-get update &amp;&amp; apt-get install -y python3-pip
---&gt; Running in e46c478aadab
standard_init_linux.go:211: exec user process caused &quot;exec format error&quot;</p>
</blockquote>
<p>I am new to doker so how can I fix this.</p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","65447118","How to map a directory from docker container to host?","<docker>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I have a docker container which has <code>/opt/boost/</code> directory. Is there any way that I can map it
to <code>/opt/boost</code> on my host machine and use its content?</p>
"
"35414479","Containerized Node server inaccessible with server.listen(port, '127.0.0.1')","<networking><docker><port>","65452006","""This page isn’t working localhost didn’t send any data."" when navigating to localhost port bound to docker container","<node.js><docker><docker-compose>","<p>I set up a simple Node server in Docker.</p>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:latest
RUN apt-get -y update
ADD example.js .
EXPOSE 1337   
CMD node example.js
</code></pre>
<p><strong>example.js</strong></p>
<pre class=""lang-js prettyprint-override""><code>var http = require('http');
http.createServer(function (req, res) {
  res.writeHead(200, {'Content-Type': 'text/plain'});
  res.end('Hello World\n'+new Date);
}).listen(1337, '127.0.0.1');
console.log('Server running at http://127.0.0.1:1337/');
</code></pre>
<p>Now build the image</p>
<pre><code>$ docker build -t node_server .
</code></pre>
<p>Now run in container</p>
<pre><code>$ docker run -p 1337:1337 -d node_server  
$ 5909e87302ab7520884060437e19ef543ffafc568419c04630abffe6ff731f70
</code></pre>
<p>Verify the container is running and ports are mapped:</p>
<pre><code>$ docker ps  

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
5909e87302ab        node_server         &quot;/bin/sh -c 'node exa&quot;   7 seconds ago       Up 6 seconds        0.0.0.0:1337-&gt;1337/tcp   grave_goldberg
</code></pre>
<p>Now let's attach to the container and verify the server is running inside:</p>
<pre><code>$ docker exec -it 5909e87302ab7520884060437e19ef543ffafc568419c04630abffe6ff731f70 /bin/bash 
</code></pre>
<p>And in the container command line type:</p>
<pre><code>root@5909e87302ab:/# curl http://localhost:1337
Hello World
Mon Feb 15 2016 16:28:38 GMT+0000 (UTC)
</code></pre>
<p>Looks good right?</p>
<p><strong>The problem</strong></p>
<p>When I execute the same curl command on the host (or navigate with my browser to http://localhost:1337) I see nothing.</p>
<p>Any idea why the port mapping between container and host doesn't work?</p>
<p>Things I already tried:</p>
<ul>
<li>Running with the <code>--expose 1337</code> flag</li>
</ul>
","<p>I am running docker desktop for windows version 3.0.0 and am attempting to setup a basic hello world example using docker and docker-compose.  when I run <code>docker-compose up</code> the containers build fine and log the appropriate <code>Server running at...</code> messages.</p>
<p>However when I navigate to <code>localhost:3000</code> I would expect to see 'Hello World' and am instead seeing a browser error reading &quot;This page isn’t working  localhost didn’t send any data.&quot;</p>
<p>when I run the server.js file directly in node, I get &quot;hello world&quot; at <code>localhost:3000</code> as expected.</p>
<p>Ideally I would be able to access all of my docker containers on local host ports 3000-3003.  relevant files are below.</p>
<p>Each of my dockerfiles contain the following</p>
<pre><code>FROM node:14
WORKDIR /usr/src/app
RUN npm install
COPY . .
EXPOSE 3000
CMD [ &quot;node&quot;, &quot;server.js&quot; ]
</code></pre>
<p>Below is the server.js file:</p>
<pre class=""lang-js prettyprint-override""><code>const http = require('http');

const hostname = '127.0.0.1';
const port = 3000;

const server = http.createServer((req, res) =&gt; {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () =&gt; {
  console.log(`Server running at http://${hostname}:${port}/`);
});
</code></pre>
<p>My docker-compose file is below:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.9&quot;
services:
  analyze:
    build: services/analyze/
    volumes:
      - ./services/analyze:/analyze
    ports:
      - &quot;3000:3000&quot;
  extract:
    build: services/extract/
    volumes:
      - ./services/extract:/extract
    ports:
      - &quot;3001:3000&quot;
  search:
    build: services/search/
    volumes:
      - ./services/search:/search
    ports:
      - &quot;3002:3000&quot;
  reply:
    build: services/reply/
    volumes:
      - ./services/reply:/reply
    ports:
      - &quot;3003:3000&quot;
</code></pre>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","65468602","Problem creating a Docker container for Flask web application","<python><python-3.x><postgresql><docker><psycopg2>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>I'm trying to build a Docker container for my Python web application that uses Flask and PostgreSQL.I work with Windows 10 and use Python 3.7.3, PostgreSQL 13 and PyCharm. Here is my Dockerfile:</p>
<pre><code>FROM python:3.7

WORKDIR /app

COPY requirements.txt ./
RUN pip install --trusted-host pypi.python.org -r requirements.txt

RUN export PYTHONPATH='${PYTHONPATH}:/app'

COPY . .

CMD [&quot;python&quot;, &quot;./run.py&quot;]
</code></pre>
<p>And requierements.txt file:</p>
<pre><code>click==7.1.2
Flask==1.1.2
Flask-SQLAlchemy==2.4.4
itsdangerous==1.1.0
Jinja2==2.11.2
MarkupSafe==1.1.1
psycopg2==2.8.6
SQLAlchemy==1.3.20
Werkzeug==1.0.1
</code></pre>
<p>All files look normal, but when I try to build a container, I get an error:</p>
<pre><code>D:\PyCharms\Module 6\app&gt;docker build -t doc .
[+] Building 91.6s (8/10)
 =&gt; [internal] load build definition from Dockerfile                                                                                                                                                             3.6s
 =&gt; =&gt; transferring dockerfile: 256B                                                                                                                                                                             1.6s
 =&gt; [internal] load .dockerignore                                                                                                                                                                                1.4s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                                                  0.1s
 =&gt; [internal] load metadata for docker.io/library/python:3.7                                                                                                                                                    9.4s
 =&gt; [1/6] FROM docker.io/library/python:3.7@sha256:bdd950df83006ce9e7f7f9cb878a2ca72e945f3cbd37faef07509c8510d22ba8                                                                                              0.3s
 =&gt; [internal] load build context                                                                                                                                                                                7.1s
 =&gt; =&gt; transferring context: 172.02kB                                                                                                                                                                            5.2s
 =&gt; CACHED [2/6] WORKDIR /app                                                                                                                                                                                    0.0s
 =&gt; CACHED [3/6] COPY requirements.txt ./                                                                                                                                                                        0.0s
 =&gt; ERROR [4/6] RUN pip install --trusted-host pypi.python.org -r requirements.txt                                                                                                                              70.7s
------
 &gt; [4/6] RUN pip install --trusted-host pypi.python.org -r requirements.txt:
#8 55.94 Collecting click==7.1.2
#8 56.54   Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
#8 56.90 Collecting Flask==1.1.2
#8 57.01   Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
#8 57.74 Collecting Flask-SQLAlchemy==2.4.4
#8 57.89   Downloading Flask_SQLAlchemy-2.4.4-py2.py3-none-any.whl (17 kB)
#8 58.55 Collecting itsdangerous==1.1.0
#8 58.66   Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
#8 58.96 Collecting Jinja2==2.11.2
#8 59.07   Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
#8 59.55 Collecting MarkupSafe==1.1.1
#8 59.65   Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)
#8 59.88 Collecting psycopg2==2.8.6
#8 60.00   Downloading psycopg2-2.8.6.tar.gz (383 kB)
#8 64.98     ERROR: Command errored out with exit status 1:
#8 64.98      command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-8yce09_u
#8 64.98          cwd: /tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/
#8 64.98     Complete output (7 lines):
#8 64.98     running egg_info
#8 64.98     creating /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info
#8 64.98     writing /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/PKG-INFO
#8 64.98     writing dependency_links to /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/dependency_links.txt
#8 64.98     writing top-level names to /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/top_level.txt
#8 64.98     writing manifest file '/tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/SOURCES.txt'
#8 64.98     Error: b'strict.pm did not return a true value at /usr/bin/pg_config line 12.\n'
#8 64.98     ----------------------------------------
#8 64.99 ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
------
executor failed running [/bin/sh -c pip install --trusted-host pypi.python.org -r requirements.txt]: exit code: 1
</code></pre>
<p>Looks like there is some problem with installing psycopg(concretely, some trouble with pg_config file),
but I didn't find any useful information about that kind of error. Only one tip I found is that &quot;strict.pm&quot; is some kind of Perl file, that should end with &quot;1;&quot; line or something like that.
So my questions are:</p>
<ol>
<li>What is &quot;strict.pm&quot; file and what is connection between it and pg_config and psycopg?</li>
<li>How I can get rid of this kind of error?</li>
</ol>
<p>(P.S. I'm complete newbie in Docker and Postgre stuff and this is my first question, so sorry for possible mistakes)</p>
"
"48927312","MountVolume.SetUp failed for volume ""mongo"" : hostPath type check failed: /mongo/data is not a directory","<docker><kubernetes><rancher>","65480178","Kubernetes - Mount Volume for Data Persistence","<kubernetes>","<p>I'm trying to configure a hostPath to launch Mongodb pod.</p>

<p>I have only one node of kubernetes v1.8.5 installed with rancher last stable version.</p>

<p>I have create folder <code>/mongo/data</code> and allow all permissions to all users.
<a href=""https://i.stack.imgur.com/US7WD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/US7WD.png"" alt=""enter image description here""></a></p>

<p>I'm able to run docker image perfectly with docker without sudo:</p>

<p><code>docker run --name some-mongo -v /mongo/data:/data/db mongo:3.2.1</code></p>

<p>But when I launch to kubernetes:</p>

<p><code>sudo kubectl create -f mongodb.yml</code></p>

<p>I get <code>MountVolume.SetUp failed for volume ""mongo"" : hostPath type check failed: /mongo/data is not a directory</code></p>

<p>This is the mongodb.yml:</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: mongo:3.2.1
    name: test-container
    volumeMounts:
    - mountPath: /data/db
      name: mongo
  volumes:
  - name: mongo
    hostPath:
      # directory location on host
      path: /mongo/data
      # this field is optional
      type: Directory
</code></pre>

<p>Any idea where I should looking for?</p>
","<p>For training purpose and learn how does volume works I'm trying to mount a volume to a mysql app for data Persistency. Here is my resource:</p>
<pre class=""lang-yaml prettyprint-override""><code>---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: mysql
  namespace: ex3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:5.6
          name: mysql
          env:
            - name: MYSQL_DATABASE
              valueFrom:
                configMapKeyRef:
                  name: &quot;wordpress-mysql&quot;
                  key: db_name
            - name: MYSQL_USER
              valueFrom:
                configMapKeyRef:
                  name: &quot;wordpress-mysql&quot;
                  key: db_user
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: &quot;mysql&quot;
                  key: db_password
            - name: MYSQL_RANDOM_ROOT_PASSWORD
              valueFrom:
                configMapKeyRef:
                  name: &quot;wordpress-mysql&quot;
                  key: db_random_root_password
          ports:
            - containerPort: 3306
              protocol: TCP
          volumeMounts:
            - mountPath: /var/lib/mysql
              name: data-volume
      volumes:
      - name: data-volume
        hostPath:
          path: /var/data
          type: Directory
</code></pre>
<p>Problem is, my pod is still in a Container-Creating state.<br />
<code>kubectl get pods -n ex3</code></p>
<pre><code>NAME                         READY   STATUS              RESTARTS   AGE
mysql-7f56bffdbb-l8hgq       0/1     ContainerCreating   0          56s
</code></pre>
<p><code>kubectl describe pod mysql-7f56bffdbb-l8hgq -n ex3</code></p>
<pre><code>Events:
  Type     Reason       Age               From               Message
  ----     ------       ----              ----               -------
  Normal   Scheduled    73s               default-scheduler  Successfully assigned ex3/mysql-7f56bffdbb-l8hgq to worker-1
  Warning  FailedMount  9s (x8 over 73s)  kubelet            MountVolume.SetUp failed for volume &quot;data-volume&quot; : hostPath type check failed: /var/data is not a directory
</code></pre>
<p>I have created a <code>var/data</code> folder in my one single node so I don't understand why I have this issue: <code>/var/data is not a directory</code></p>
"
"41093812","How to get Docker containers to talk to each other while running on my local host?","<docker>","65495914","Multiple containers in docker works on same IP","<docker><docker-compose>","<p>I have a Webapp running completely locally on my MacBook.</p>

<p>The Webapp has a Front End (Angular/Javascript) and a Back End (Python/Django) which implements a RESTful API.</p>

<p>I have Dockerized the Back End so that it is completely self-contained in a Docker Container and exposes port 8000. I map this port locally to 4026.</p>

<p>Now I need to Dockerize the Front End. But if I have these two docker containers running on my localhost, how can I get the FE to send HTTP requests to the BE? The FE container won't know anything that exists outside of it. Right?</p>

<p>This is how I run the FE:</p>

<pre><code>$ http-server
Starting up http-server, serving ./
Available on:
  http://127.0.0.1:8080
  http://192.168.1.16:8080
Hit CTRL-C to stop the server
</code></pre>

<p>Please provide references explaining how I can achieve this.</p>
","<p>I want my Docker containers to work on the same IP. Is it possible? I want them to have the same IP address so that they can link to each other through it.</p>
"
"43911793","Cannot connect to Go GRPC server running in local Docker container","<docker><go><grpc>","65539581","docker doesn't forward tcp connection to golang","<docker><go>","<p>I have a go grpc service. I'm developing on a mac, sierra. When running a grpc client against the service locally, all is well, but when running same client against same service in the docker container I get this error:</p>

<pre><code>transport: http2Client.notifyError got notified that the client transport was broken EOF.
FATA[0000] rpc error: code = Internal desc = transport is closing
</code></pre>

<p>this is my docker file:</p>

<pre><code>FROM golang:1.7.5

RUN mkdir -p /go/src/github.com/foo/bar
WORKDIR /go/src/github.com/foo/bar

COPY . /go/src/github.com/foo/bar
# ONBUILD RUN go-wrapper download
RUN go install

ENTRYPOINT /go/bin/bar

EXPOSE 51672
</code></pre>

<p>my command to build the image:</p>

<pre><code>docker build -t bar .
</code></pre>

<p>my command to launch the docker container:</p>

<pre><code>docker run -p 51672:51672 --name bar-container bar
</code></pre>

<h2>Other info:</h2>

<ul>
<li>client program runs fine from within the docker container</li>
<li>connecting to a regular rest endpoint works fine (http2, grpc related?)</li>
<li><p>running the <code>lsof</code> command in OS X yields these results</p>

<pre><code>$lsof -i | grep 51672
com.docke   984 oldDave   21u  IPv4 0x72779547e3a32c89      0t0  TCP *:51672 (LISTEN)
com.docke   984 oldDave   22u  IPv6 0x72779547cc0fd161      0t0  TCP localhost:51672 (LISTEN)
</code></pre></li>
<li><p>here's a snippet of my server code:</p>

<pre><code>server := &amp;Server{}
endpoint := ""localhost:51672""
lis, err := net.Listen(""tcp"", endpoint)
if err != nil {
    log.Fatalf(""failed to listen: %v"", err)
}

s := grpc.NewServer(grpc.Creds(creds))

pb.RegisterExpServiceServer(s, server)

// Register reflection service on gRPC server.
reflection.Register(s)

log.Info(""Starting Exp server: "", endpoint)

if err := s.Serve(lis); err != nil {
    log.Fatalf(""failed to serve: %v"", err)
}
</code></pre></li>
</ul>
","<p>I am new to both golang and docker.</p>
<p>I have this golang program.</p>
<pre class=""lang-golang prettyprint-override""><code>package main

import (
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;net&quot;
)

const (
    CONN_HOST = &quot;localhost&quot;
    CONN_PORT = &quot;3333&quot;
    CONN_TYPE = &quot;tcp&quot;
)

func main() {
    l, err := net.Listen(CONN_TYPE, CONN_HOST+ &quot;:&quot;+CONN_PORT)
    if err != nil {
        fmt.Println(&quot;Error listening: &quot;, err.Error())
        os.Exit(1)
    }
    defer l.Close()
    fmt.Println(&quot;Listening on &quot; + CONN_HOST + &quot;：&quot; + CONN_PORT)
    for {
        fmt.Println(&quot;about to accpet.&quot;)
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&quot;Error accepting: &quot;, err.Error())
            os.Exit(1)
        }
        fmt.Println(&quot;Accepted connection.&quot;)
        go handleRequest(conn)
    }
}

func handleRequest(conn net.Conn) {
    buf := make([]byte, 1024)
    _, err := conn.Read(buf)
    if err != nil {
        fmt.Println(&quot;Error reading: &quot;, err.Error())
    }
    fmt.Println(&quot;received &quot; + string(buf))
    conn.Write([]byte(&quot;echo from server: &quot; + os.Getenv(&quot;ServerName&quot;)))
    conn.Close()
}
</code></pre>
<p>It's a standard echo server. I tested it locally.</p>
<pre class=""lang-sh prettyprint-override""><code>echo &quot;test&quot; | netcat -v localhost 3333
</code></pre>
<p>It works fine. By fine I mean, it receives the echo from server, and server prints message out in the console.</p>
<p>Then I dockerized the golang program. My dockerfile goes like this</p>
<pre><code>FROM golang:alpine
ENV ServerName=test
WORKDIR /build
COPY . .
RUN go mod download
RUN go build -o main .
WORKDIR /dist
RUN cp /build/main .
EXPOSE 3333/tcp
CMD [&quot;/dist/main&quot;]
</code></pre>
<p>I built the image and run it with the following command.</p>
<pre class=""lang-sh prettyprint-override""><code>docker run -p 3333:3333/tcp --env ServerName=test echo-server
</code></pre>
<p>It starts up fine. But somehow, when I run the same commands</p>
<pre class=""lang-sh prettyprint-override""><code>echo &quot;test&quot; | netcat -v localhost 3333
</code></pre>
<p>It doesn't work. It shows</p>
<pre><code>Connection to localhost 3333 port [tcp/*] succeeded!
</code></pre>
<p>But that's it, no echo from the server, the server doesn't print message either.</p>
<p>To make sure the server runs ok, I went into the container and run the same commands. Again, it shows the echo message, and prints the message.</p>
<p>It seems the docker doesn't forward tcp connection into the container. I can't seem to figure out why this happens. Is it because I didn't write the dockefile correctly? Or is it because something is wrong with the environment?</p>
<p>P.S. My system is windows 10. Docker is the latest version of docker desktop, with WSL2 integration enabled. Please help me, thanks.</p>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","65564520","Communication between 2 Docker Composes using Network results in ""SequelizeHostNotFoundError""","<javascript><postgresql><docker><docker-compose><dockerfile>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have 2 docker composes , one is Infrastructures and the second is Appplications.</p>
<p>I need a direct line from <strong>Appplications =&gt; Infrastructures</strong> , to run migrations on a Postresql container in <code>Appplications</code> docker ,and it (Infrastructures) must be listening before make requests from <code>Appplications</code>.</p>
<p><strong>Infrastructures :</strong></p>
<pre><code>version: &quot;3&quot;

networks:
  shared_network:
    driver: bridge

services:

  # PostgreSQL
  my_postgres:
    image: postgres:latest
    environment:
      .... // ENVS
    ports:
      - &quot;5432:5432&quot;
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:5432&quot;]
      interval: 30s
      timeout: 10s
      retries: 15
    networks:
      - shared_network
      
      
      ... more infrastructures
</code></pre>
<p><strong>Applications :</strong></p>
<pre><code>version: &quot;3&quot;

networks:
  shared_network:
    driver: bridge

services:
  # Employees Service
  emps:    
    build:
      context: ./emps_service
      dockerfile: ./Dockerfile
    ports:
      - &quot;9000:9000&quot;
    networks:
      - shared_network
      
      
      
      
      
      ... More apps
</code></pre>
<p>I'm using <strong>default.yaml</strong> for Sequelize :</p>
<pre><code>... more definitions 


sequelize:
  database: some_random_db
  host: my_postgres
  user: xxxx
  password: yyyy
  dialect: postgres  
  seederStorage: &quot;sequelize&quot;
</code></pre>
<p>and when I run <code>docker-compose up</code> , twice , first for the <strong>Infrastructures</strong> , and only once it's up another <code>docker-compose up</code> for the <strong>Applications</strong> , I get :</p>
<pre><code>**SequelizeHostNotFoundError: getaddrinfo ENOTFOUND my_postgres**
</code></pre>
<p>How can we fix this ?</p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","65658723","localhost rejects requests to a simple Docker-run node app","<node.js><docker><express><dockerfile><docker-for-windows>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I am experimenting with Docker (actually Docker Toolbox on Windows 8.1) and a very simple node/express app, and I am getting refused requests from localhost.</p>
<p><strong>server.js</strong></p>
<pre><code>const express = require('express');
const app = express();

app.get('/', (req, res) =&gt; {
  res.send('hello!');
});

app.listen(3500, () =&gt; console.log('Running on 3500'))
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:latest
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3500
CMD [&quot;node&quot;, &quot;server.js&quot;]
</code></pre>
<p>I followed <a href=""https://docs.docker.com/get-started/nodejs/run-containers/"" rel=""nofollow noreferrer"">Docker's guide</a> here.</p>
<p>I am then running the following commands:</p>
<ul>
<li><code>docker build -t dockertest .</code></li>
<li><code>docker run dockertest -d -p 3500:3500</code></li>
</ul>
<p>And try to access http://localhost:3500 in the browser - <strong>connection refused!</strong></p>
<p>Trying <code>curl --request GET --url http://localhost:3500</code> - <strong>connection refused!</strong></p>
<p>The same app run directly un-Dockered works just fine.
What can be the reason for such a simple project to fail?
How can I troubleshoot it further?</p>
"
"43099116","Error ""The input device is not a TTY""","<docker><jenkins><jenkins-pipeline>","65678122","The input device is not a TTY when I'm using docker?","<docker><dockerfile><bitbucket-pipelines><tty>","<p>I am running the following command from my <code>Jenkinsfile</code>. However, I get the error <em>""The input device is not a TTY""</em>.</p>

<pre><code>docker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh
</code></pre>

<p>Is there a way to run the script from the <code>Jenkinsfile</code> without doing interactive mode?</p>

<p>I basically have a file called <code>script.sh</code> that I would like to run inside the Docker container.</p>
","<p>I'm currently running the following command:</p>
<pre><code>docker run -it -v $PWD:/e2e -w /e2e cypress/included:6.2.1 

</code></pre>
<p>Error Message:</p>
<pre><code>+ docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1
the input device is not a TTY
</code></pre>
<p>I'm pulling the cypress container from the <a href=""https://github.com/cypress-io/cypress-docker-images/tree/master/included/6.2.1"" rel=""nofollow noreferrer"">Cypress Github Account</a></p>
<p>My bitbucket-pipelines.yaml file:</p>
<pre><code>
image: atlassian/default-image:2

pipelines:
  default:
    - step:
        services:
          - docker
        script:
          - docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1

</code></pre>
<p>My dockerfile:</p>
<pre><code>
FROM cypress/browsers:node12.18.3-chrome87-ff82


ENV CI=1


ENV QT_X11_NO_MITSHM=1
ENV _X11_NO_MITSHM=1
ENV _MITSHM=0

# should be root user
RUN echo &quot;whoami: $(whoami)&quot;
RUN npm config -g set user $(whoami)

# command &quot;id&quot; should print:
# uid=0(root) gid=0(root) groups=0(root)
# which means the current user is root
RUN id

# point Cypress at the /root/cache no matter what user account is used
# see https://on.cypress.io/caching
ENV CYPRESS_CACHE_FOLDER=/root/.cache/Cypress
RUN npm install -g &quot;cypress@6.2.1&quot;
RUN cypress verify

# Cypress cache and installed version
# should be in the root user's home folder
RUN cypress cache path
RUN cypress cache list
RUN cypress info
RUN cypress version

# give every user read access to the &quot;/root&quot; folder where the binary is cached
# we really only need to worry about the top folder, fortunately
RUN ls -la /root
RUN chmod 755 /root

# always grab the latest NPM and Yarn
# otherwise the base image might have old versions
RUN npm i -g yarn@latest npm@latest

# should print Cypress version
# plus Electron and bundled Node versions
RUN cypress version
RUN echo  &quot; node version:    $(node -v) \n&quot; \
  &quot;npm version:     $(npm -v) \n&quot; \
  &quot;yarn version:    $(yarn -v) \n&quot; \
  &quot;debian version:  $(cat /etc/debian_version) \n&quot; \
  &quot;user:            $(whoami) \n&quot; \
  &quot;chrome:          $(google-chrome --version || true) \n&quot; \
  &quot;firefox:         $(firefox --version || true) \n&quot;

ENTRYPOINT [&quot;cypress&quot;, &quot;run&quot;]

</code></pre>
<p>If I run this command:</p>
<pre><code>docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1
</code></pre>
<p>It will tell me that it cannot find my son file, and I don't why. Could someone help me a little bit?</p>
<p>What should I have to do? &amp; Why I'm getting this issue, I guess I don't understand what is TTY means?</p>
"
"40216311","Reading in environment variables from an environment file","<python><docker>","65693810","os.getenv('TOKEN') returns None with TOKEN defined in .env","<python>","<p>I'd like to run in a local environment a Python script which is normally run in a Docker container. The <code>docker-compose.yml</code> specifies an <a href=""https://docs.docker.com/compose/env-file/"" rel=""noreferrer"">env_file</a> which looks (partially) like the following:</p>

<pre><code>DB_ADDR=rethinkdb
DB_PORT=28015
DB_NAME=ipercron
</code></pre>

<p>In order to run this locally, I would like these lines to be converted to</p>

<pre><code>os.environ['DB_ADDR'] = 'rethinkdb'
os.environ['DB_PORT'] = '28015'
os.environ['DB_NAME'] = 'ipercron'
</code></pre>

<p>I could write my parser, but I was wondering if there are any existing modules/tools to read in environment variables from configuration files?</p>
","<p>In the same folder, I have an <code>.env</code> file and a <code>main.py</code> file.</p>
<p>In <code>.env</code> I have set</p>
<pre><code>TOKEN = 123456
</code></pre>
<p>And in <code>main.py</code> I have</p>
<pre class=""lang-py prettyprint-override""><code>import os
print(os.getenv('TOKEN'))
</code></pre>
<p>When I run the Python script, I get <code>None</code> in the standard output. What am I doing wrong?</p>
<p>I have looked at all the similar questions but it seems like I'm not missing any steps. I'm using MacOS if that affects anything.</p>
<p>Thanks in advance!</p>
"
"51518087","RUN inside a conditional statement in Dockerfile","<docker>","65703289","Include If else statement in docker file","<docker><if-statement><conditional-statements>","<p>I have a <code>Dockerfile</code> which currently uses the following:</p>

<pre><code>COPY ./compose/local/django/start.sh /start.sh
RUN sed -i 's/\r//' /start.sh
RUN chmod +x /start.sh
</code></pre>

<p>As you can see it copies the file and then makes it executable. I now need to change this depending on a argument provided when I build the image. I have tried using this:</p>

<pre><code>RUN if [ ""$arg"" = ""True"" ]; then \
    COPY ./compose/local/django/new.sh /new.sh \
    RUN sed -i 's/\r//' /new.sh \
    RUN chmod +x /new.sh; else \
    COPY ./compose/local/django/start.sh /start.sh \
    RUN sed -i 's/\r//' /start.sh \
    RUN chmod +x /start.sh; fi
</code></pre>

<p>But this fails as it appears that I can't run <code>COPY</code> or <code>RUN</code> commands inside a conditional statement. It always fails with:</p>

<pre><code>/bin/sh: 1: COPY: not found
</code></pre>

<p>or</p>

<pre><code>/bin/sh: 1: RUN: not found
</code></pre>

<p>So, I think that the best course of action is to create a separate bash file which does the copies, something like:</p>

<pre><code>#!/usr/bin/env bash

if [ ""${arg}"" = ""True"" ]; then
    echo ""arg = ${arg}""
    cp ./compose/local/django/new.sh /new.sh
elif [ ""${arg}"" = ""False"" ]; then
    echo ""arg = ${arg}""
    cp ./compose/local/django/start.sh /start.sh
fi
</code></pre>

<p>But I am struggling with how to make the copied bash files executable. I know I can do it by running <code>chmod +x</code> in the command line, but as I am not the only person who is going to be building this, I was hoping that there would be a better solution, something like I was doing previously in the original <code>Dockerfile</code> script.</p>

<p>Can anyone help me with this? Any help would be much appreciated.</p>
","<p><strong>I want to create docker file with conditional statements.</strong> i get parameters from outside ( BUILD_TOOL) .
This is the code but i got error while building docker image.</p>
<p>i got this error =  <em>dockerfile parse error line 23: unknown instruction: ELSE</em></p>
<pre><code>RUN if [ &quot;$BUILD_TOOL&quot; = &quot;maven&quot; ] ; then 
    RUN mvn clean install;

#if build tool is gradle
else 
    RUN gradle clean;
fi
</code></pre>
"
"40513545","How to prevent docker from starting a container automatically on system startup?","<docker><startup>","65746709","How do I disable all docker-compose automatic restarts on boot","<docker><docker-compose>","<p>Docker starts a container on every system startup (debian) but I didn't create a service to do so. How can I prevent docker from doing that?</p>
","<p>I have an old laptop that has a few docker-compose projects with restart=unless-stopped. One of them is possibly causing the laptop to crash on start. The laptop then crashes before I have a chance to stop that container.</p>
<p>I can mount the underlying linux system by booting from a USB and so can access the docker-compose.yml files.</p>
<p>How can I disable the restart policy (possibly once off) so when I boot the laptop, it doesn't start those docker-compose containers?</p>
<p>Can this be done without editting the docker-compose.yml files?</p>
"
"40454470","How can I use a variable inside a Dockerfile CMD?","<docker><dockerfile>","65796030","CMD doesn't run in Dockerfile","<docker><dockerfile>","<p>Inside my Dockerfile:</p>

<pre><code>ENV PROJECTNAME mytestwebsite
CMD [""django-admin"", ""startproject"", ""$PROJECTNAME""]
</code></pre>

<p>Error:</p>

<pre><code>CommandError: '$PROJECTNAME' is not a valid project name
</code></pre>

<p>What is the quickest workaround here?  Does Docker have any plan to ""fix"" or introduce this functionality in later versions of Docker?</p>

<p>NOTE:  If I remove the CMD line from the Docker file and then run the Docker container, I am able to manually run Django-admin startproject $PROJECTNAME from inside the container and it will create the project...</p>
","<p>My Dockerfile contains the following..</p>
<pre><code>ENV APP_HOME=/app/foo-service
CMD [$APP_HOME/start-foo-service.sh]
</code></pre>
<p>What is wrong in the above syntax, since the following runs fine</p>
<pre><code>CMD [&quot;/app/foo-service/start-foo-service.sh&quot;]
</code></pre>
"
"41593135","Does putting ARG at top of Dockerfile prevent layer re-use?","<docker><dockerfile>","65813457","Use env with arg in Dockerfile cause cache layer invalidate","<docker><dockerfile>","<p>If an ARG that is declared at the top of a Dockerfile gets changed, but its value is only used for a RUN command near the end of the Dockerfile, does Docker rebuild the whole image from scratch or is it able to re-use the intermediate image from right before the relevant RUN command?</p>

<p>To better utilize layering, should I place my ARG declarations at the top of the Dockerfile, or just before the section that uses them?</p>

<p>I guess part of my question is whether or not an ARG directive generates an intermediate layer.</p>
","<p><strong>Why the cache layer of the <code>RUN</code> command is invalidate?</strong></p>
<pre><code>FROM python:3.7-slim
ARG VERSION
RUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt
ENV VERSION_2 2
ENV VERSION $VERSION
CMD [&quot;env&quot;]
</code></pre>
<p>As shown in the dockerfile above, using build arg to dynamically set environment variables will cause the cache layer of the <code>RUN</code> instruction to become invalid, because build-arg will change every time it is built, and the history of the image is as follows</p>
<pre><code>IMAGE          CREATED         CREATED BY                                      SIZE      COMMENT
13d8c1c91aed   2 minutes ago   CMD [&quot;env&quot;]                                     0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ENV VERSION=1                                   0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ENV VERSION_2=2                                 0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   RUN |1 VERSION=1 /bin/sh -c pip install --up…   5.28MB    buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ARG VERSION                                     0B        buildkit.dockerfile.v0
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  CMD [&quot;python3&quot;]              0B        
&lt;missing&gt;      7 days ago      /bin/sh -c set -ex;   savedAptMark=&quot;$(apt-ma…   8.49MB    
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_SHA256…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_URL=ht…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_PIP_VERSION=20…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c cd /usr/local/bin  &amp;&amp; ln -s idle3…   32B       
&lt;missing&gt;      7 days ago      /bin/sh -c set -ex   &amp;&amp; savedAptMark=&quot;$(apt-…   27.1MB    
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_VERSION=3.7.9     0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV GPG_KEY=0D96DF4D4110E…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c set -eux;  apt-get update;  apt-g…   7.03MB    
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  ENV PATH=/usr/local/bin:/…   0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  CMD [&quot;bash&quot;]                 0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop) ADD file:422aca8901ae3d869…   69.2MB
</code></pre>
"
"51247609","What is the difference between docker run and docker container run","<docker>","65845677","Why docker provides two commands 'docker image load' and 'docker load' with same functionality?","<docker>","<p>can anyone help me in the understanding difference between <strong>docker run</strong> &amp; <strong>docker container run</strong>?</p>

<p>when i do <strong>docker run --help</strong> &amp; <strong>docker container run --help</strong> from docker cmd line. I see the following </p>

<p><strong>Run a command in a new container</strong>.</p>

<p>Is there any difference in how they run the container internally or both are same doing same work?</p>

<p>As per <a href=""https://forums.docker.com/t/docker-run-and-docker-container-run/30526"" rel=""noreferrer"">https://forums.docker.com/t/docker-run-and-docker-container-run/30526</a>. <strong>docker run</strong> is still the old one, which will be deprecated soon but same is not confirmed.</p>
","<p>Why does docker provides two commands <a href=""https://docs.docker.com/engine/reference/commandline/load/"" rel=""nofollow noreferrer"">docker load</a> and <a href=""https://docs.docker.com/engine/reference/commandline/image_load/"" rel=""nofollow noreferrer"">docker image load</a> doing the exact same thing? What is the history behind these commands?</p>
"
"42345235","How to specify Memory & CPU limit in docker compose version 3","<docker><docker-compose>","65850918","Add cpu limit via docker-compose","<docker><docker-compose>","<p>I am unable to specify CPU &amp; memory for services specified in version 3 .</p>

<p>With version 2 it works fine with ""mem_limit"" &amp; ""cpu_shares"" parameters under the services . But it fails while using version 3 , putting them under deploy section doesn't seem worthy unless i am using swarm mode .</p>

<p>Can somebody help ?</p>

<pre><code>version: ""3""
services:
  node:
    build:
     context: .
      dockerfile: ./docker-build/Dockerfile.node
    restart: always
    environment:
      - VIRTUAL_HOST=localhost
    volumes:
      - logs:/app/out/
    expose:
      - 8083
    command: [""npm"",""start""]
    cap_drop:
      - NET_ADMIN
      - SYS_ADMIN
</code></pre>
","<p>I found several posts and blogs on this but cannot get it to work as expected in my case.</p>
<p>I have a docker compose and I tried to add cpu limits per <a href=""https://www.baeldung.com/ops/docker-memory-limit"" rel=""nofollow noreferrer"">this</a> blog. Tried:</p>
<pre><code>version: &quot;3&quot;
services:
  shell:
    image: someaddress/executor-blah/main:latest
    cpus: 4
    entrypoint:
      - /bin/bash
    working_dir: /workspace
    volumes:
      - .:/workspace
</code></pre>
<p>However when I tried to run this container I got a compose error:</p>
<blockquote>
<p>The Compose file './docker-compose.yaml' is invalid because:
Unsupported config option for services.shell: 'cpus'</p>
</blockquote>
<p>Tried also:</p>
<pre><code>version: &quot;3&quot;
services:
  shell:
    image: someaddress/executor-blah/main:latest
    cpus: 4
    entrypoint:
      - /bin/bash
    working_dir: /workspace
    volumes:
      - .:/workspace
    deploy:
      resources:
        limits:
          cpus: '4'
</code></pre>
<p>This time the container composed but when I check on a async loop in my hosts terminal using <code>top &gt; 1</code>, all 8 of my laptops cores are used up by this process.</p>
<p>How can I limit a container to, in this case, 4 cores?</p>
"
"43099116","Error ""The input device is not a TTY""","<docker><jenkins><jenkins-pipeline>","65876075","Why run Docker container with -t?","<docker><terminal>","<p>I am running the following command from my <code>Jenkinsfile</code>. However, I get the error <em>""The input device is not a TTY""</em>.</p>

<pre><code>docker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh
</code></pre>

<p>Is there a way to run the script from the <code>Jenkinsfile</code> without doing interactive mode?</p>

<p>I basically have a file called <code>script.sh</code> that I would like to run inside the Docker container.</p>
","<p>The Docker Run Reference says that running a container with <code>-t</code></p>
<blockquote>
<p>-t              : Allocate a pseudo-tty</p>
</blockquote>
<p>But only running it with <code>-i</code> allows the user to interact with the containerized process through the terminal. So I wonder, what is the meaning of &quot;Allocating a pseudo-tty&quot;, since even when running without <code>-t</code>, content written to <code>STDOUT</code> by the process will be passed to the terminal (The process will have a pipe as stdout instead of a tty) ?</p>
<p>I read <a href=""https://stackoverflow.com/questions/43099116/error-the-input-device-is-not-a-tty"">this answer</a> which says that you may run <code>docker run -t</code> to have &quot;Terminal support&quot;, such as text coloring etc. Well I already done the following experiment:</p>
<pre><code>// Dockerfile

FROM ubuntu:latest

CMD [&quot;echo&quot;, &quot;-e&quot;, &quot;\u001b[31mHello World&quot;]
</code></pre>
<p>And ran this image with no <code>-t</code>. Since I'm running it from a terminal (<code>docker run</code> will always run from some terminal won't it?) I can see a red &quot;Hello World&quot;. So I still don't understand why running with <code>-t</code> alone...</p>
"
"41935435","Understanding ""VOLUME"" instruction in DockerFile","<docker><dockerfile>","65885555","Directory is not mounted","<docker><docker-compose><dockerfile>","<p>Below is the content of my ""Dockerfile"" </p>

<pre><code>FROM node:boron

# Create app directory
RUN mkdir -p /usr/src/app

# change working dir to /usr/src/app
WORKDIR /usr/src/app

VOLUME . /usr/src/app

RUN npm install

EXPOSE 8080

CMD [""node"" , ""server"" ]
</code></pre>

<p>In this file I am expecting ""VOLUME . /usr/src/app"" instruction to mount contents of present working directory in host to be mounted on /usr/src/app folder of container.</p>

<p>Please let me know if this is the correct way ?</p>
","<pre><code>-- project
----&gt; configs
...
------&gt; nodejs
-------- Dockerfile
-- databases
-- production // (PHP + VUE)
</code></pre>
<p><strong>Dockerfile</strong>:</p>
<pre><code>FROM node:lts-alpine
VOLUME ../../frontend /var/www/project/production/frontend
WORKDIR /var/www/finex_project
RUN ls -la
</code></pre>
<p>Why the directory(frontend) is not mounted?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65905718","AWS EC2 Failed to execute script docker-compose","<python><docker><ubuntu><amazon-ec2><docker-compose>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I can run my app with <code>sudo docker-compose up</code>  but when I just do <code>docker-compose up</code> I get</p>
<pre><code>$ docker-compose up
Traceback (most recent call last):
  File &quot;urllib3/connectionpool.py&quot;, line 670, in urlopen
  File &quot;urllib3/connectionpool.py&quot;, line 392, in _make_request
  File &quot;http/client.py&quot;, line 1255, in request
  File &quot;http/client.py&quot;, line 1301, in _send_request
  File &quot;http/client.py&quot;, line 1250, in endheaders
  File &quot;http/client.py&quot;, line 1010, in _send_output
  File &quot;http/client.py&quot;, line 950, in send
  File &quot;docker/transport/unixconn.py&quot;, line 43, in connect
PermissionError: [Errno 13] Permission denied

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;requests/adapters.py&quot;, line 439, in send
  File &quot;urllib3/connectionpool.py&quot;, line 726, in urlopen
  File &quot;urllib3/util/retry.py&quot;, line 410, in increment
  File &quot;urllib3/packages/six.py&quot;, line 734, in reraise
  File &quot;urllib3/connectionpool.py&quot;, line 670, in urlopen
  File &quot;urllib3/connectionpool.py&quot;, line 392, in _make_request
  File &quot;http/client.py&quot;, line 1255, in request
  File &quot;http/client.py&quot;, line 1301, in _send_request
  File &quot;http/client.py&quot;, line 1250, in endheaders
  File &quot;http/client.py&quot;, line 1010, in _send_output
  File &quot;http/client.py&quot;, line 950, in send
  File &quot;docker/transport/unixconn.py&quot;, line 43, in connect
urllib3.exceptions.ProtocolError: ('Connection aborted.', PermissionError(13, 'Permission denied'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;docker/api/client.py&quot;, line 214, in _retrieve_server_version
  File &quot;docker/api/daemon.py&quot;, line 181, in version
  File &quot;docker/utils/decorators.py&quot;, line 46, in inner
  File &quot;docker/api/client.py&quot;, line 237, in _get
  File &quot;requests/sessions.py&quot;, line 543, in get
  File &quot;requests/sessions.py&quot;, line 530, in request
  File &quot;requests/sessions.py&quot;, line 643, in send
  File &quot;requests/adapters.py&quot;, line 498, in send
requests.exceptions.ConnectionError: ('Connection aborted.', PermissionError(13, 'Permission denied'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;docker-compose&quot;, line 3, in &lt;module&gt;
  File &quot;compose/cli/main.py&quot;, line 80, in main
  File &quot;compose/cli/main.py&quot;, line 189, in perform_command
  File &quot;compose/cli/command.py&quot;, line 60, in project_from_options
  File &quot;compose/cli/command.py&quot;, line 152, in get_project
  File &quot;compose/cli/docker_client.py&quot;, line 41, in get_client
  File &quot;compose/cli/docker_client.py&quot;, line 170, in docker_client
  File &quot;docker/api/client.py&quot;, line 197, in __init__
  File &quot;docker/api/client.py&quot;, line 221, in _retrieve_server_version
docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))
[48420] Failed to execute script docker-compose
</code></pre>
<p>This is my Docker file</p>
<pre><code>FROM golang:alpine AS builder

ENV GO111MODULE=on
RUN mkdir /app
ADD . /app/
WORKDIR /app
COPY ./data.go .
COPY ./handlers.go .
COPY ./main.go .
COPY ./static /static
COPY ./ssl /ssl
COPY ./views /views
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build $(ls -1 *.go)
RUN useradd -u ubuntu
USER ubuntu
EXPOSE 3000
CMD [&quot;go&quot;, &quot;run&quot;, &quot;.&quot;]
</code></pre>
"
"36243559","How to connect Docker web app container to Docker PostgreSQL container?","<postgresql><go><docker><docker-compose>","65921326","Connecting to Postgresql in a docker container","<postgresql><docker><remote-connection>","<p>I'm practicing making a Golang web app that interacts with a PostgreSQL database, each running on their own container.</p>

<p>I'm running the containers with <code>docker-compose up</code></p>

<p>But I seem to be failing on getting the postgres container properly set up.</p>

<p>For brevity, links to <code>Dockerfile</code>s and other settings files are <a href=""https://gist.github.com/andradei/922e11f469aec277ab6e"" rel=""nofollow"">on this gist</a> (let me know if you want it here instead).</p>

<pre><code>version: '2'
services:
  web_app:
    build: dockerfiles/web_app
    ports:
      - ""9000:9000""
    volumes:
      - .:/go/src/gitlab.com/repo/web_app
    # links might be replaced by depends_on.
    # links:
    #   - db
    depends_on:
      - db
    # tty and stdin_open cause docker-compose to disconnect from docker-machine after 60sec.
    # A fix is on the way.
    # tty: true
    # stdin_open: true
  db:
    build: dockerfiles/db
    volumes:
      - data:/var/lib/postgresql/data
volumes:
  data: {}
</code></pre>

<p><code>docker-compose up</code> works fine. But when the application tries to open a database connection with:</p>

<pre><code>var pgConf string = ""user=web_app dbname=web_app sslmode=verify-full password=password""

db, err := sql.Open(""postgres"", pgConf)
</code></pre>

<p>I get the following error from docker compose:</p>

<pre><code>Error creating new user:  dial tcp [::1]:5432: getsockopt: connection refused
</code></pre>

<p>What can I do to make both containers talk to each other?</p>

<p>Thank you in advance.</p>
","<p>I have a GO project, inside which I am implementing work with the Postgresql database (P.s. I use the pgx driver, if it matters). I have uploaded the whole project to a docker container, and when I run the project image, I get an error that it cannot connect to the database. I understand that the image is looking for a database in itself. Question: How do I connect to the database?</p>
<p>In main func I have this code:</p>
<pre><code>addr := flag.String(&quot;addr&quot;, &quot;:4000&quot;, &quot;HTTP network address&quot;)
flag.Parse()

infoLog := log.New(os.Stdout, &quot;INFO\t&quot;, log.Ldate|log.Ltime)
errorLog := log.New(os.Stderr, &quot;ERROR\t&quot;, log.Ldate|log.Ltime|log.Lshortfile)

pool, err := pgxpool.Connect(context.Background(), &quot;user=postgres password=root host=localhost port=5432 dbname=snippetbox sslmode=disable pool_max_conns=10&quot;)
if err != nil {
    log.Fatalf(&quot;Unable to connection to database&quot;)
}
defer pool.Close()
</code></pre>
<p><a href=""https://i.stack.imgur.com/cXXwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cXXwI.png"" alt=""enter image description here"" /></a></p>
"
"46517241","""docker build"" requires exactly 1 argument(s)","<docker><dockerfile>","65932455","docker: specifying a Dockerfile - ""docker build"" requires exactly 1 argument","<docker>","<p>I am trying to build an image from a specific Dockerfile, and tag it at the same time; I am following the <a href=""https://docs.docker.com/engine/reference/commandline/build/#tag-an-image--t"" rel=""noreferrer"">online instructions</a> for<code>docker build</code>, but I get the following error:</p>
<blockquote>
<p>&quot;docker build&quot; requires exactly 1 argument(s)</p>
</blockquote>
<h3>My directory structure:</h3>
<pre><code>project/
    foo/
    MyDockerfile
</code></pre>
<p>This is the command I run:</p>
<p><code>docker build -f /full/path/to/MyDockerfile -t proj:myapp</code></p>
<p>I have tried various combinations of the above command, but the results are always the error message given above. Why is this happening - as I am following what the documentation says?</p>
","<p>The docs say:</p>
<pre><code>docker build [OPTIONS] PATH | URL | -
</code></pre>
<p>and</p>
<pre><code>--file , -f     Name of the Dockerfile (Default is 'PATH/Dockerfile')
</code></pre>
<p>so I'm using</p>
<pre><code>docker build -t &lt;my tag&gt; -f /path/to/my/Dockerfile&gt;
</code></pre>
<p>but I get:</p>
<pre><code>&quot;docker build&quot; requires exactly 1 argument.
See 'docker build --help'.

Usage:  docker build [OPTIONS] PATH | URL | -
</code></pre>
<p>How do the docs need correcting?</p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","65939775","Where is docker volume and images created on local windows 10","<docker><docker-desktop>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I ran command <code>docker volume create --name data-postgresql --driver local</code>, and I am unable to view this volume before I could use it. I am naïve to docker.</p>
"
"48568172","docker.sock permission denied","<linux><docker>","65955193","How to run docker on ubuntu without the sudo","<docker><ubuntu>","<p>When I try to run simple docker commands like:</p>

<pre><code>$ docker ps -a
</code></pre>

<p>I get an error message:</p>

<blockquote>
  <p>Got permission denied ... /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<p>When I check permissions with</p>

<pre><code>$ ls -al /var/run/
</code></pre>

<p>I see this line:</p>

<pre><code>srw-rw---- root docker docker.sock
</code></pre>

<p>So, I follow an advice from many forums and add local user to docker group:</p>

<pre><code>$ sudo usermod -aG docker $USER
</code></pre>

<p>But it does not help. I still get the very same error message. How can I fix it?</p>
","<p>I want to run my docker by running this command:</p>
<pre><code>docker build -t getting-started .
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=getting-started&amp;target=&amp;ulimits=null&amp;version=1: dial unix /var/run/docker.sock: connect: permission denied</p>
</blockquote>
<p>My question is: how can I run docker on ubuntu without adding &quot;sudo&quot;?</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","65966577","docker compose access shell script from other container","<docker><docker-compose>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>I have a docker-compose file with 2 containers.</p>
<p>On container A I am running a shell script which should execute a shell script that is in Container B. I have tried a shared_volume but then the needed files to start container B are missing.</p>
<p>So in my file, I would like to execute the init.sh (which is workig so far) but it should execute a file from the service &quot;keycloak&quot;</p>
<pre><code>keycloak:
    build:
      context: .
      dockerfile: Dockerfile_2
    volumes: 
      - ./imports/cache_reload/disable-theme-cache.cli:/opt/jboss/startup-scripts/disable-theme-cache.cli
      - ./imports/themes/custom/:/opt/jboss/keycloak/themes/custom-theme/
      - ./imports/realm/realm-export.json:/root/opt/jboss/keycloak/bin/custom-import.json
    environment:
      DB_VENDOR: MYSQL
      DB_ADDR: mysql
      DB_DATABASE: keycloak
      DB_USER: keycloak
      DB_PASSWORD: password
        #KEYCLOAK_USER: admin
        #KEYCLOAK_PASSWORD: Pa55w0rd
    ports:
      - 8080:8080
    depends_on:
      - mysql
  keycloak_installer:
    build:
      context: .
      dockerfile: Dockerfile_1
    volumes:
      - ./imports/scripts/import-realm.sh:/docker-entrypoint-initdb.d/init.sh
    ports:
      - 8090:8080
    depends_on: 
      - keycloak
</code></pre>
"
"39564861","Mount ""named volume"" as non-root in Docker","<docker><mount><mount-point>","65982541","Docker volumes as non root user (Create and docker-compose)","<docker><docker-compose>","<p>Is there any way to mount a named volume as a non-root user? I am trying to avoid having to run a <code>chown</code> in each Dockerfile but I need the mount to be writable by a non-root user to be able to write the artifacts created by a build in the image</p>

<p>This is what I'm trying</p>

<pre><code>docker run --rm -it -v /home/bob/dev/:/src/dev -v builds:/mnt/build --name build hilikus/build /bin/bash
</code></pre>

<p>but for the second mount I get</p>

<pre><code>[user@42f237282128 ~]$ ll /mnt
total 4
drwxr-xr-x 2 root root 4096 Sep 18 19:29 build
</code></pre>

<p>My other mount (<code>/src/dev/</code>) is owned by <em>user</em>, not by root so it gives what I need; however, I haven't been able to do the same with the named volume.</p>
","<p>I use a docker-compose file to create some containers. Everything works fine except the volumes that are being created by docker-compose are created with <code>root:root</code>. That makes it very hard to move around as non root and I don't want to have to use sudo all the time to change, copy, or read files in that file tree.</p>
<p>I can run docker as the main user &quot;jack&quot; without a problem--docker-compose for example is being run as the local user &quot;jack&quot;. <code>sudo chown -R jack:docker /opt/docker/volumes/</code> changes permissions without breaking the function of the container but that would require to run chown periodically which I don't want to do.</p>
<p>My question is: How can I make docker-compose create the volumes as anybody else than root? Ideally, all volumes would be created with the ownership set to docker:docker or jack:docker.</p>
<p>The compose file looks like this:</p>
<pre><code>bitwarden:
    image: bitwardenrs/server:latest
    container_name: bitwarden
    environment:
      - PUID=${PUID} #ie. 1000
      - PGID=${PGID} #ie. 1000
      - TZ=${TZ}
      - [...]
    volumes:
      - bitwarden-vol-config:/config
      - bitwarden-vol-data:/data1
    restart: unless-stopped

[...]
volumes:
  bitwarden-vol-config:
  bitwarden-vol-data:
</code></pre>
"
"39023640","How to cache package manager downloads for docker builds?","<caching><docker><composer-php><docker-volume><diskcache>","66042906","Shared volume across multiple docker-compose projects","<performance><docker><docker-compose><composer-php><yarnpkg>","<p>If I run <code>composer install</code> from my host, I hit my local composer cache:</p>

<pre><code>  - Installing deft/iso3166-utility (1.0.0)
    Loading from cache
</code></pre>

<p>Yet when building a container having in its Dockerfile:</p>

<pre><code>RUN composer install -n -o --no-dev
</code></pre>

<p>I download all the things, e.g.:</p>

<pre><code>  - Installing deft/iso3166-utility (1.0.0)
    Downloading: 100%         
</code></pre>

<p>It's expected, yet I like to avoid it. As even on a rebuilt, it would also download everything again.</p>

<p>I would like to have a universal cache for composer that I could also reshare for other docker projects.</p>

<p>I looked into this and found the approach to <a href=""https://git.framasoft.org/Chill-project/docker-ci-image/commit/002aeaf8ae5845c71eb8191821a98aa7a8565f0e"" rel=""noreferrer"">define a volume in the Dockerfile</a>:</p>

<pre><code>ENV COMPOSER_HOME=/var/composer
VOLUME /var/composer
</code></pre>

<p>I added that to my <code>Dockerfile</code>, and expected to only download the files once, and hit the cache afterwards.</p>

<p>Yet when I modify my <code>composer</code>, e.g. remove the <code>-o</code> flag, and rerun <code>docker build .</code>, I expected to hit the cache on build, yet I still download the vendors again.</p>

<p>How are volumes supposed to work to have a data cache inside a docker container?</p>
","<p>I'm using <code>docker-compose</code> to orchestrate containers for multiple separate projects. Each of these projects has their own set of containers and do not relate to other projects.</p>
<p>For example:</p>
<pre><code>/my-projects/project-1/docker-compose.yml
/my-projects/project-2/docker-compose.yml
/my-projects/project-3/docker-compose.yml
</code></pre>
<p>These projects are, however, similar in that they are all PHP projects and use webpack for front-end assets, thus share the same package managers: <code>composer</code> and <code>yarn</code>.</p>
<p>I was wondering, in the interest of performance, if it would be possible to mount a shared volume outside the directory root of all the projects for package manager caches?</p>
<p>For example:</p>
<pre><code>/my-projects/caches/composer
/my-projects/caches/npm
/my-projects/project-1/docker-compose.yml
/my-projects/project-2/docker-compose.yml
/my-projects/project-3/docker-compose.yml
</code></pre>
<p>Where <code>/my-projects/caches/composer</code> and <code>/my-projects/caches/npm</code> get mounted inside the relevant containers within each project. In case it's not clear, only one project would be spun up at a time.</p>
<p>At the moment, if two projects share the same deps then each downloads and caches it individually. A more performant (in terms of build times) would be to mount a common volume and point the package manager's caches there so that when &quot;Project A&quot; downloads an update to a dip, &quot;Project B&quot; can load it from cache.</p>
"
"51011552","MongoDB on with Docker ""failed to connect to server [localhost:27017] on first connect ""","<node.js><mongodb><docker><docker-compose>","66076158","Connecting database (MongoDB) and backend (nodejs) running in docker containers","<node.js><mongodb><docker>","<p>I am using mongoDB with and NodeJS backend. The Problem is I am getting the following error</p>

<blockquote>
  <p>node:16) UnhandledPromiseRejectionWarning: MongoNetworkError: failed
  to connect to server [localhost:27017] on first connect
  [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]</p>
</blockquote>

<p>This is my docker-compose</p>

<pre><code>version: '3.4'

services:
  db:
    image: mongo:latest
    ports:
      - '27017:27017'

  rest-api-node:
    build: .
    ports:
      - '5000:5000'
    links:
      - db
    restart: on-failure
</code></pre>

<p>I have tried with <code>depends_on</code> as well , was not working.</p>

<p>On backend I am mongoose as a middleware to communicate with DB. this is the part of my <code>index.js</code></p>

<pre><code>mongoose.Promise = global.Promise
mongoose.connect('mongodb://localhost/demo')
app.listen(port, () =&gt; console.log(""live""))
</code></pre>

<p>I have tried using promise as well , no change though. Please Help me out.
Thanks in advance </p>

<p>complete error log</p>

<blockquote>
  <p>at Pool.
  (/app/node_modules/mongodb-core/lib/topologies/server.js:505:11)
  rest-api-node_1  |     at Pool.emit (events.js:180:13) rest-api-node_1
  |     at Connection.
  (/app/node_modules/mongodb-core/lib/connection/pool.js:329:12)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Connection.emit (events.js:180:13)
  rest-api-node_1  |     at Socket.
  (/app/node_modules/mongodb-core/lib/connection/connection.js:245:50)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Socket.emit (events.js:180:13)
  rest-api-node_1  |     at emitErrorNT
  (internal/streams/destroy.js:64:8) rest-api-node_1  |     at
  process._tickCallback (internal/process/next_tick.js:178:19)
  rest-api-node_1  |   name: 'MongoNetworkError', rest-api-node_1  |<br>
  message: 'failed to connect to server [localhost:27017] on first
  connect [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]' }</p>
</blockquote>
","<p>First, I looked through several discussion with similar problems and it still did not work.</p>
<p>I have a mongodb docker container running, I did port forwarding with -p command
to be exact this is the command I ran:</p>
<pre><code>sudo docker run -t -d -p 27017:27017 --name mongo mongo-0000
</code></pre>
<p>docker ps shows container running</p>
<pre><code>9d9040a7bd66   mongo-0000   &quot;docker-entrypoint.s…&quot;   4 minutes ago   Up 4 minutes   0.0.0.0:27017-&gt;27017/tcp   mongo
</code></pre>
<p>as in another thread it was suggested to change mongodb bindip from 127.0.0.1 to 0.0.0.0 , which I also did (I tried both ways).</p>
<p>then I am trying to start up a backend app container with nodejs express backend app, I have had them working fine together on a VM, not on docker jet.</p>
<p>And i get following error</p>
<pre><code>    sudo docker run conduit-backend
Listening on port 3000

/ConduitReactApp/src/node_modules/mongodb/lib/server.js:261
        process.nextTick(function() { throw err; })
                                      ^
Error [MongoError]: failed to connect to server [localhost:27017] on first connect
    at Pool.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/topologies/server.js:313:35)
    at Pool.emit (node:events:378:20)
    at Connection.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/connection/pool.js:260:12)
    at Object.onceWrapper (node:events:485:26)
    at Connection.emit (node:events:378:20)
    at Socket.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/connection/connection.js:162:49)
    at Object.onceWrapper (node:events:485:26)
    at Socket.emit (node:events:378:20)
    at emitErrorNT (node:internal/streams/destroy:188:8)
    at emitErrorCloseNT (node:internal/streams/destroy:153:3)
    at processTicksAndRejections (node:internal/process/task_queues:81:21)
Emitted 'error' event on NativeConnection instance at:
    at /ConduitReactApp/src/node_modules/mongoose/lib/connection.js:288:17
    at NativeConnection.Connection.error (/ConduitReactApp/src/node_modules/mongoose/lib/connection.js:489:12)
    at /ConduitReactApp/src/node_modules/mongoose/lib/connection.js:520:15
    at /ConduitReactApp/src/node_modules/mongoose/lib/drivers/node-mongodb-native/connection.js:69:21
    at /ConduitReactApp/src/node_modules/mongodb/lib/db.js:229:14
    at Server.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb/lib/server.js:259:9)
    at Object.onceWrapper (node:events:485:26)
    at Server.emit (node:events:378:20)
    at Pool.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/topologies/server.js:313:21)
    at Pool.emit (node:events:378:20)
    [... lines matching original stack trace ...]
    at Socket.emit (node:events:378:20)
</code></pre>
<p>Also, inside app.js (in the backend app) for connecting to mongoDB it reads so</p>
<pre><code>if(isProduction){
  mongoose.connect(process.env.MONGODB_URI);
} else {
  mongoose.connect('mongodb://localhost/conduit');
  mongoose.set('debug', true);
}
</code></pre>
<p>What is still wrong here ?</p>
"
"47979270","django cannot connect mysql in docker-compose","<mysql><django><docker><docker-compose><mariadb>","66092813","Is the server running on host ""localhost"" (::1) and accepting TCP/IP connections on port 5432? in Django","<python><django><postgresql><docker>","<p>I'm very new for docker, now I am trying to run django with mariadb in docker through docker-compose, but I always get this error:</p>

<p>I use <code>Docker version 17.09.1-ce, build 19e2cf6</code>, <code>docker-compose version 1.18.0, build 8dd22a9</code></p>

<blockquote>
  <p>django.db.utils.OperationalError: (2003, 'Can\'t connect to MySQL
  server on \'mariadb55\' (111 ""Connection refused"")')</p>
</blockquote>

<p>I can connect db correctly after run <code>docker-compose up db</code> in local or remote, and I even can run <code>python manage.py runserver 0.0.0.0:6001</code> correctly in <em>anaconda virtual environment</em> to connect <code>db</code> service in docker by setting parameters of <strong>settings.py</strong> file like below:</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'test',
        'USER': 'belter',
        # 'HOST': 'mariadb55',
        'HOST': '127.0.0.1',
        'PORT': '3302',
        'PASSWORD': 'belter_2017',
        'default-character-set': 'utf8',
        'OPTIONS': {
            'sql_mode': 'traditional',
        }
    }
}
</code></pre>

<p>This is my <strong>docker-compose.yml</strong> file</p>

<pre><code>version: '3'

services:
  db:
    image: mariadb:5.5
    restart: always
    environment:
      - MYSQL_HOST=localhost
      - MYSQL_PORT=3306
      - MYSQL_ROOT_HOST=%
      - MYSQL_DATABASE=test
      - MYSQL_USER=belter
      - MYSQL_PASSWORD=belter_2017
      - MYSQL_ROOT_PASSWORD=123456_abc
    volumes:
      - /home/belter/mdbdata/mdb55:/var/lib/mysql
    ports:
      - ""3302:3306""
  web:
    image: onlybelter/django_py35
    command: python3 manage.py runserver 0.0.0.0:6001
    volumes:
      - /mnt/data/www/mysite:/djcode
    ports:
      - ""6001:6001""
    depends_on:
      - db
    links:
      - db:mariadb55
</code></pre>

<p>I almost tried everything I can find, but still cannot figure it out, any help would be nice!</p>

<p>What I have tried:</p>

<p><a href=""https://stackoverflow.com/questions/42106613/docker-compose-mysql-connection-failing"">Docker compose mysql connection failing</a></p>

<p><a href=""https://stackoverflow.com/questions/31035887/linking-django-and-mysql-containers-using-docker-compose?rq=1"">Linking django and mysql containers using docker-compose</a></p>

<p><a href=""https://stackoverflow.com/questions/42811727/django-connection-to-postgres-by-docker-compose"">Django connection to postgres by docker-compose</a></p>
","<p>I'm getting this error while trying to run:</p>
<pre><code>docker-compose run app python manage.py migrate
</code></pre>
<p>with Django.</p>
<p>this is the docker-compose.yml:</p>
<pre><code>version: '3'

volumes:
  database: { }

services:
  app:
    build:
      context: .
      dockerfile: ./docker/app/dev/Dockerfile
    depends_on:
      - postgres
    volumes:
      - ./app:/app:z
    env_file:
      - ./.envs/.dev/.app
      - ./.envs/.dev/.postgres
    ports:
      - &quot;8000:8000&quot;
    command: &quot;python manage.py runserver 0.0.0.0:8000&quot;

  postgres:
    image: &quot;postgres:13.1&quot;
    volumes:
      - database:/var/lib/postgresql/data:Z
    env_file:
      - ./.envs/.dev/.postgres
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
    ports:
      - &quot;5432:5432&quot;
</code></pre>
<p>I don't know why I'm getting this error. PostgreSQL is running correctly and all the settings are appropiately configured.</p>
<p>This is the full traceback error:.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/app/manage.py&quot;, line 7, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/commands/migrate.py&quot;, line 92, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/executor.py&quot;, line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 53, in __init__
    self.build_graph()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 216, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 77, in applied_migrations
    if self.has_table():
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 55, in has_table
    with self.connection.cursor() as cursor:
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 259, in cursor
    return self._cursor()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 235, in _cursor
    self.ensure_connection()
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
    self.connect()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/utils.py&quot;, line 90, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
    self.connect()
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
    self.connection = self.get_new_connection(conn_params)
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/postgresql/base.py&quot;, line 187, in get_new_connection
    connection = Database.connect(**conn_params)
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: could not connect to server: Connection refused
        Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
        Is the server running on host &quot;localhost&quot; (::1) and accepting
        TCP/IP connections on port 5432?
</code></pre>
<p>I've searched for multiple answers and tried multiple solutions but they are not working.</p>
<p>They have asked me to put the .postgres note so here it is:</p>
<pre><code>POSTGRES_USER=app
POSTGRES_DB=app
POSTGRES_PASSWORD=&lt;set_me&gt;
</code></pre>
<p>And also the database settings:</p>
<pre><code>DATABASES = {
    'default': {
        &quot;ENGINE&quot;: &quot;django.db.backends.postgresql&quot;,
        &quot;NAME&quot;: 'DjangoCRM',
        &quot;USER&quot;: 'postgres',
        &quot;PASSWORD&quot;: 'xxxx',
        &quot;HOST&quot;: 'localhost',
        &quot;PORT&quot;: '5432',
    }
}
</code></pre>
"
"43308319","How can I run bash in a new container of a docker image?","<bash><docker>","66114411","docker run exited right away?","<docker>","<p>I am able to run arbitrary shell commands in a container created from docker/whalesay image.</p>

<pre><code>$ docker run docker/whalesay ls -l
total 56
-rw-r--r-- 1 root root  931 May 25  2015 ChangeLog
-rw-r--r-- 1 root root  385 May 25  2015 INSTALL
-rw-r--r-- 1 root root 1116 May 25  2015 LICENSE
-rw-r--r-- 1 root root  445 May 25  2015 MANIFEST
-rw-r--r-- 1 root root 1610 May 25  2015 README
-rw-r--r-- 1 root root  879 May 25  2015 Wrap.pm.diff
drwxr-xr-x 2 root root 4096 May 25  2015 cows
-rwxr-xr-x 1 root root 4129 May 25  2015 cowsay
-rw-r--r-- 1 root root 4690 May 25  2015 cowsay.1
-rw-r--r-- 1 root root   54 May 25  2015 install.pl
-rwxr-xr-x 1 root root 2046 May 25  2015 install.sh
-rw-r--r-- 1 root root  631 May 25  2015 pgp_public_key.txt
$ docker run docker/whalesay lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.2 LTS
Release:    14.04
Codename:   trusty
</code></pre>

<p>However, I am unable to run a shell in a container created from this image.</p>

<pre><code>$ docker run docker/whalesay bash
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS               NAMES
7ce600cc9904        docker/whalesay     ""bash""                   5 seconds ago       Exited (0) 3 seconds ago                           loving_mayer
</code></pre>

<p>Why did it not work? How can I make it work?</p>
","<p>I have the following Dockerfile so I can login the container to check the image Some_Linux_Img.</p>
<pre><code>FROM Some_Linux_Img
ENTRYPOINT [&quot;/bin/bash&quot;]
</code></pre>
<p>And I built it</p>
<pre><code>docker build -t test:v2 .
Sending build context to Docker daemon 2.048 kB
Step 1/2 : FROM abc.com/shared/miniconda
 ---&gt; ec1a66fb9030
Step 2/2 : ENTRYPOINT /bin/bash
 ---&gt; Running in ea14b4ce6c6e
 ---&gt; 21ebe99c7ef1
Removing intermediate container ea14b4ce6c6e
Successfully built 21ebe99c7ef1
</code></pre>
<p>However, it exited right after <code>docker run</code>?</p>
<p>$ docker run --name test test:v2
$ docker ps -a</p>
<p>Status showed &quot;Exited (0) x seconds ago&quot;. I wanted to run some bash commands inside the container to check the Linux image <code>Some_Linux_Img</code>. (<code>docker exec -it test /bin/bash</code>)</p>
"
"47580528","Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 192.168.65.1:53: no such host","<docker><docker-machine><docker-registry>","66117136","Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.171.30.4:53: no such host","<docker><build><gitlab><yaml>","<p>I am new to dockers. When I am running the docker pull sonarqube I am getting the following error.</p>

<p><strong>Error response from daemon: Get <a href=""https://registry-1.docker.io/v2/"" rel=""noreferrer"">https://registry-1.docker.io/v2/</a>: dial TCP: lookup registry-1.docker.io on 192.168.65.1:53: no such host</strong></p>

<p>Can you please let me know why I am getting the error and how can I rectify this.</p>
","<p>Docker image from a Dockerfile.
Part&quot; and potentially the &quot;test-job&quot;</p>
<p>variables:</p>
<h3>EDIT PART</h3>
<p><a href=""https://hub.docker.com/r/library/docker/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/library/docker/</a>)
DOCKER_VERSION: &quot;20.10&quot;</p>
<p>(<a href=""https://docs.docker.com/engine/reference/builder/#healthcheck"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/builder/#healthcheck</a>)
DOCKER_HEALTHCHECK_TIMEOUT: &quot;60&quot;</p>
<p>REGISTRY_USER: gitlab-ci-token
REGISTRY_PASSWORD: $CI_JOB_TOKEN
REGISTRY_TEST_URL: $CI_REGISTRY
REGISTRY_RELEASE_URL: $CI_REGISTRY
CONTAINER_TEST_IMAGE: $CI_REGISTRY_IMAGE:latest
K8S_SERVER: <a href=""https://caas-cnp-sys.com.intraorange"" rel=""nofollow noreferrer"">https://caas-cnp-sys.com.intraorange</a>
K8S_GITLAB_USER: massive-migration</p>
<p>stages:</p>
<ul>
<li>build-docker</li>
<li>deploy</li>
</ul>
<p>.job_template_docker: &amp;job_definition_docker
image: dockerproxy-iva.si.francetelecom.fr/docker:$DOCKER_VERSION
services:
- name: dockerproxy-iva.si.francetelecom.fr/docker:$DOCKER_VERSION-dind
alias: docker
tags:
- rsc
- docker-privileged
- shared
before_script:
- env | grep ^DOCKER_
- env | grep ^CI_
- docker info
- '[ -z &quot;$REGISTRY_PASSWORD&quot; ] &amp;&amp; echo &quot;Registry Password is not set or empty (or protected) in Secret Variables&quot; &amp;&amp; exit 1'
- '[ -z &quot;$REGISTRY_USER&quot; ] &amp;&amp; echo &quot;Registry User is not set or empty (or protected) in Secret Variables&quot; &amp;&amp; exit 1'
- echo $REGISTRY_PASSWORD | docker login $REGISTRY_TEST_URL --username $REGISTRY_USER --password-stdin</p>
<p>docker-build-job:
&lt;&lt;: *job_definition_docker
stage: build-docker
environment:
name: prod
script:
- nslookup 10.171.30.4
- echo $CONTAINER_TEST_IMAGE
- docker build --pull --build-arg &quot;http_proxy=http://devwatt-proxy.si.fr.intraorange:8080&quot; --build-arg &quot;https_proxy=http://devwatt-proxy.si.fr.intraorange:8080&quot; -t $CONTAINER_TEST_IMAGE .
- docker push $CONTAINER_TEST_IMAGE
- docker history $CONTAINER_TEST_IMAGE
- docker images $CONTAINER_TEST_IMAGE</p>
<p>deploy-job:
tags:
- rsc
- docker
image: dockerfactory-iva.si.francetelecom.fr/kubectl-client:1.8.11
stage: deploy
environment:
name: prod
before_script:
- echo &quot;$K8S_CA_CRT&quot; &gt; ca.crt
- kubectl config set-cluster OBSIT-cluster --server=$K8S_SERVER --certificate-authority=ca.crt
- kubectl config set-credentials $K8S_GITLAB_USER --certificate-authority=ca.crt --token=$K8S_GITLAB_USER_TOKEN
- kubectl config set-context OBSIT-context --cluster=OBSIT-cluster --user=$K8S_GITLAB_USER
- kubectl config use-context OBSIT-context
script:
- kubectl apply -f deployment.yaml -n sandbox-obsit --validate=false
- kubectl delete pods -l app=massive-migration-portal -n sandbox-obsit</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","66156962","Spring boot and Apache Kafka via Docker compose throw LEADER_NOT_AVAILABLE of topic and Send failed error","<spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I just wanted to implement Spring boot and Apache Kafka via docker-compose.
Firstly I installed the Apache Kafka Docker image via this link: <a href=""https://hub.docker.com/r/ches/kafka/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/ches/kafka/</a> and then install jplock/zookeeper via this command shown below.</p>
<p><code>docker run -d — name zookeeper — publish 2181:2181 jplock/zookeeper:latest</code></p>
<p>Then I run this command (<code>docker ps</code>) to determine if all images work. their status of all these images is up. That's why they work flawlessly.</p>
<p><img src=""https://i.stack.imgur.com/F6YaP.png"" alt=""Images"" /></p>
<p>Then I run the app and sent a message via Postman. The post url is <code>localhost:8080/kafkamessage</code>.</p>
<p>JSON object is here</p>
<pre><code>{
    &quot;message&quot; : &quot;Hello World&quot;
}
</code></pre>
<p>When I sent a request, it throws an error shown below.</p>
<pre><code>&quot;message&quot;: &quot;Send failed; nested exception is org.apache.kafka.common.errors.TimeoutException: Topic k-topic not present in metadata after 60000 ms.&quot;
</code></pre>
<p>Expect for this, I defined an auto-created topic but the consumer and producer couldn't produce the topic in the console</p>
<pre><code>[Consumer clientId=consumer-k-group-1, groupId=k-group] Error while fetching metadata with correlation id 728 : {k-topic=LEADER_NOT_AVAILABLE}
[Producer clientId=producer-1] Error while fetching metadata with correlation id 681 : {k-topic=LEADER_NOT_AVAILABLE}
</code></pre>
<p>Here is my error</p>
<p><img src=""https://i.stack.imgur.com/yU6Es.png"" alt=""enter image description here"" /></p>
<p>Here is the link: <a href=""https://github.com/Rapter1990/springbootkafka"" rel=""nofollow noreferrer"">https://github.com/Rapter1990/springbootkafka</a></p>
<p>How can I fix the issue?</p>
<p><img src=""https://i.stack.imgur.com/NcIWS.png"" alt=""enter image description here"" /></p>
"
"39632038","Cannot run webpack-dev-server inside docker","<docker><webpack><webpack-dev-server>","66173709","Working with Docker, but site isn't showing on localhost","<reactjs><typescript><docker><docker-compose><dockerfile>","<p>I have created a docker image which serves a simple react app using webpack from inside the container, but I get nothing in the browser.</p>

<p>Here are my config files</p>

<p><code>package.json</code></p>

<pre><code>{
  ""name"": ""invas_client"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""start"": ""webpack --inline --content-base .""
  },
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""react"": ""^0.14.7"",
    ""react-dom"": ""^0.14.7"",
    ""react-router"": ""^2.0.0""
  },
  ""devDependencies"": {
    ""babel-core"": ""^6.5.1"",
    ""babel-loader"": ""^6.2.2"",
    ""babel-preset-es2015"": ""^6.5.0"",
    ""babel-preset-react"": ""^6.5.0"",
    ""http-server"": ""^0.8.5"",
    ""webpack"": ""^1.12.13"",
    ""webpack-dev-server"": ""^1.14.1""
  }
}
</code></pre>

<p><code>webpack.config.js</code></p>

<pre><code>module.exports = {
    entry: './index.js',

    output: {
        filename: 'bundle.js',
        publicPath: ''
    },

    module: {
        loaders: [
            { test: /\.js$/, exclude: /node_modules/, loader: 'babel-loader?presets[]=es2015&amp;presets[]=react' }
        ]
    }
}
</code></pre>

<p><code>Dockerfile</code></p>

<pre><code># Use starter image
FROM node:argon

# Create app directory
RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

# Install app dependencies
COPY package.json /usr/src/app/
RUN npm install

# Bundle app source
COPY . /usr/src/app

# Expose port
EXPOSE 8080

# Default command to run
CMD [""npm"", ""start""]
</code></pre>

<h2>What's working fine</h2>

<p>When I run <code>npm start</code>, the <code>webpack-dev-server</code> runs normally, and when I go to <a href=""http://localhost:8080"" rel=""noreferrer"">http://localhost:8080</a>, I see my page.</p>

<h2>What isn't working</h2>

<p>When I run my server using docker, with the following command:</p>

<p><code>docker build -t anubhav756/app . &amp;&amp; docker run -p 80:8080 anubhav756/app</code> </p>

<p>the logs show everything working normally from inside the container, but when I point my browser to <a href=""http://localhost"" rel=""noreferrer"">http://localhost</a>, I get <code>ERR_CONNECTION_RESET</code></p>

<p>Sample code's over <strong><a href=""https://github.com/anubhav756/webpack-docker"" rel=""noreferrer"">here</a></strong></p>
","<p>I'm a bit new to Docker and was able to get one site up using create-react-app, but at the moment, I'm trying to get one working from scratch using Webpack.  The site compiles with warnings, but I'm not seeing any result in the browser.</p>
<p>This is the output I'm getting when compiling, but I'm not seeing it in the browser.</p>
<pre><code>Successfully built 190902aff07e
Successfully tagged climbtrack_web:latest
Recreating climbtrack ... done
Attaching to climbtrack
climbtrack | 
climbtrack | &gt; climbtrack@1.0.0 start
climbtrack | &gt; webpack serve --open
climbtrack | 
climbtrack | ℹ ｢wds｣: Project is running at http://localhost:3100/
climbtrack | ℹ ｢wds｣: webpack output is served from /
climbtrack | ℹ ｢wds｣: Content not from webpack is served from ./dist
climbtrack | ℹ ｢wds｣: 404s will fallback to /index.html
climbtrack | ℹ ｢wdm｣: Compiled with warnings.
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:15.5.0

WORKDIR /app

COPY package*.json ./

RUN npm install

COPY . .

EXPOSE 3100

CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: &quot;3.8&quot;

services:
  web:
    container_name: climbtrack
    restart: always
    build: .
    volumes:
      - .:/app
    ports:
      - &quot;3100:3100&quot;
</code></pre>
<p><strong>webpack.config.js</strong></p>
<pre><code>const path = require(&quot;path&quot;);
const HtmlWebPackPlugin = require(&quot;html-webpack-plugin&quot;);
const { CleanWebpackPlugin } = require(&quot;clean-webpack-plugin&quot;);
module.exports = {
  resolve: {
    extensions: [&quot;.js&quot;, &quot;.jsx&quot;, &quot;.ts&quot;, &quot;.tsx&quot;],
  },
  entry: &quot;./src/index.tsx&quot;,
  module: {
    rules: [
      {
        test: /\.(ts|tsx)$/,
        exclude: /node_modules/,
        use: &quot;ts-loader&quot;,
      },
      {
        test: /\.(js|jsx)$/,
        exclude: /node_modules/,
        use: &quot;babel-loader&quot;,
      },
      {
        test: /\.scss$/,
        use: [&quot;style-loader&quot;, &quot;css-loader&quot;, &quot;sass-loader&quot;],
      },
      {
        test: /\.(jpg|png|svg)$/,
        use: &quot;file-loader&quot;,
      },
    ],
  },
  plugins: [
    new CleanWebpackPlugin({ cleanStaleWebpackAssets: false }),
    new HtmlWebPackPlugin({
      template: &quot;./public/index.html&quot;,
    }),
  ],
  output: {
    filename: &quot;[name].bundle.js&quot;,
    path: path.resolve(__dirname, &quot;dist&quot;)
  },
  devServer: {
    contentBase: &quot;./dist&quot;,
    historyApiFallback: true,
    port: 3100
  },
  stats: &quot;errors-only&quot;,
};
</code></pre>
<p><strong>package.json</strong></p>
<pre><code>{
  &quot;name&quot;: &quot;climbtrack&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;description&quot;: &quot;Track your every climb&quot;,
  &quot;main&quot;: &quot;index.js&quot;,
  &quot;scripts&quot;: {
    &quot;watch&quot;: &quot;webpack --watch&quot;,
    &quot;start&quot;: &quot;webpack serve --open&quot;,
    &quot;test&quot;: &quot;npm test&quot;
  },
  &quot;author&quot;: &quot;Brannon Glover&quot;,
  &quot;license&quot;: &quot;ISC&quot;,
  &quot;devDependencies&quot;: {
    &quot;@types/styled-components&quot;: &quot;^5.1.7&quot;,
    &quot;babel-loader&quot;: &quot;^8.2.2&quot;,
    &quot;clean-webpack-plugin&quot;: &quot;^3.0.0&quot;,
    &quot;css-loader&quot;: &quot;^5.0.2&quot;,
    &quot;file-loader&quot;: &quot;^6.2.0&quot;,
    &quot;html-webpack-plugin&quot;: &quot;^5.0.0&quot;,
    &quot;sass-loader&quot;: &quot;^11.0.1&quot;,
    &quot;style-loader&quot;: &quot;^2.0.0&quot;,
    &quot;styled-components&quot;: &quot;^5.2.1&quot;,
    &quot;ts-loader&quot;: &quot;^8.0.17&quot;,
    &quot;typescript&quot;: &quot;^4.1.5&quot;,
    &quot;webpack&quot;: &quot;^5.21.2&quot;,
    &quot;webpack-cli&quot;: &quot;^4.5.0&quot;,
    &quot;webpack-dev-server&quot;: &quot;^3.11.2&quot;
  },
  &quot;dependencies&quot;: {
    &quot;@types/html-webpack-plugin&quot;: &quot;^3.2.4&quot;,
    &quot;@types/react&quot;: &quot;^17.0.1&quot;,
    &quot;@types/react-dom&quot;: &quot;^17.0.0&quot;,
    &quot;react&quot;: &quot;^17.0.1&quot;,
    &quot;react-dom&quot;: &quot;^17.0.1&quot;
  }
}
</code></pre>
"
"44876778","How can I use a local file on container?","<docker><dockerfile><boot2docker><docker-machine>","66187372","Airflow doesnt see my local file: ' FileNotFoundError: [Errno 2] No such file or directory: '","<python><airflow>","<p>I'm trying create a container to run a program. I'm using a pre configurate image and now I need run the program. However, it's a machine learning program and I need a dataset from my computer to run. </p>

<p>The file is too large to be copied to the container. It would be best if the program running in the container searched the dataset in a local directory of my computer, but I don't know how I can do this.</p>

<p>Is there any way to do this reference with some docker command? Or using Dockerfile?</p>
","<p>I'm working on airflow with docker. My python file has json path like:</p>
<blockquote>
<p>'/home/user_name/airflow2-local_setup/credentials/my_file.json'</p>
</blockquote>
<p>I'm using it my python file in dags directory. In web ui got an error</p>
<pre><code>    Broken DAG ...  
FileNotFoundError: [Errno 2] No such file or directory: '/home/user_name/airflow2-local_setup/credentials/my_file.json'
</code></pre>
<p>How can i give file path to airflow?</p>
"
"49612412","Kubenetes: Is it possible to hit multiple pods with a single request in Kubernetes cluster","<docker><kubernetes><kubernetes-ingress>","66221740","Kubernetes - Send messages from service to all pods at once (broadcast)","<amazon-web-services><kubernetes><kubernetes-pod><kubernetes-service>","<p>I want to clear cache in all the pods in my Kubernetes namespace. I want to send one request to the end-point which will then send a HTTP call to all the pods in the namespace to clear cache. Currently, I can hit only one pod using Kubernetes and I do not have control over which pod would get hit.</p>

<p>Even though the load-balancer is set to RR, continuously hitting the pods(n number of times, where n is the total number of pods) doesn't help as some other requests can creep in.</p>

<p>The same issue was discussed here, but I couldn't find a solution for the implementation:
<a href=""https://github.com/kubernetes/kubernetes/issues/18755"" rel=""noreferrer"">https://github.com/kubernetes/kubernetes/issues/18755</a></p>

<p>I'm trying to implement the clearing cache part using Hazelcast, wherein I will store all the cache and Hazelcast automatically takes care of the cache update.</p>

<p>If there is an alternative approach for this problem, or a way to configure kubernetes to hit all end-points for some specific requests, sharing here would be a great help.</p>
","<p>Is it possible to send information from a single service to <strong>all pods</strong> in the same cluster <strong>at once</strong>?</p>
<p>Currently I have a service that transfers messages one by one (Round-Robin). I need a way for it to send all in parallel, multicast.</p>
<p>Thanks.</p>
"
"39404280","Make container accessible only from localhost","<docker><portforwarding>","66232627","How to secure my docker containers on a server?","<docker><nginx><docker-compose><devops><iptables>","<p>I have Docker engine installed on Debian Jessie and I am running there container with nginx in it. My ""run"" command looks like this:</p>

<pre><code>docker run -p 1234:80 -d -v /var/www/:/usr/share/nginx/html nginx:1.9
</code></pre>

<p>It works fine, problem is that now content of this container is accessible via <code>http://{server_ip}:1234</code>. I want to run multiple containers (domains) on this server so I want to setup reverse proxies for them. </p>

<p>How can I make sure that container will be only accessible via reverse proxy and not directly from <code>IP:port</code>? Eg.:</p>

<pre><code>http://{server_ip}:1234  # not found, connection refused, etc...
http://localhost:1234  # works fine
</code></pre>

<p><strong>//EDIT:</strong> Just to be clear - I am not asking how to setup reverse proxy, but how to run Docker container to be accessible only from localhost.</p>
","<p>Hello everyone :) I am working on a DigitalOcean server, on which I am running docker containers.
My docker containers have their port exposed to the outside world and I would like to only expose the port 3001 to the outside world, and expose others port only on my server.</p>
<p>Indeed I have a postgres container on port 5432 and a nodeJS container on port 3000, and they are accessible from &lt;server_ip&gt;:5432 and &lt;server_ip&gt;:3000 so it's really not safe!
I would like to do like on this :
<a href=""https://i.stack.imgur.com/UaGem.png"" rel=""nofollow noreferrer"">scheme</a></p>
<p>I tried with adding iptables rules in the chain DOCKER-USER but I didn't success.... I saw that  could use a reverse proxy like NGINX to change IP of requests but I didn't well understood and I am really not sure it's gonna work.</p>
<p>I don't really know how to secure my application, so if you know some ways to help me to solve my problem ? Thanks by advance !</p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","66235533","How to prevent others from seeing code inside my container?","<docker><security><containers>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I don't want the other party to see the code in my container when the other party pulls the image of my container registered on the docker hub.</p>
<p>Or is there a way to make my container inaccessible with the command of <code>docker exec</code>??</p>
<p>Is there any way to prevent this?</p>
"
"43296019","Docker build gives ""unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile""","<docker><dockerfile>","66239870","i want build docker file","<java><spring-boot><docker>","<p>I have a Dockerfile that is supposed to build an Ubuntu image. But whenever I run</p>
<pre><code>docker build -t ubuntu-test:latest ./Dockerfile
</code></pre>
<p>it shows the following error on the console</p>
<blockquote>
<p>unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile</p>
</blockquote>
<p>I'm on Mac OsX. I tried to <code>sudo</code> as well. Nothing works.</p>
","<p>when run this command</p>
<pre><code>sudo docker build -t jhipster /home/abed/project/microServies/StudentApp/src/main/docker/dockerfile1

sudo docker build -t jhipster /home/abed/project/microServies/StudentApp/src/main/docker/dockerfile2
</code></pre>
<p>see this error:</p>
<pre><code>unable to prepare context: context must be a directory: /home/abed/project/microServies/StudentApp/src/main/docker/dokcerfile1

unable to prepare context: context must be a directory: /home/abed/project/microServies/StudentApp/src/main/docker/dokcerfile2
</code></pre>
<p>i don't know this message what is do you meaning</p>
<p>and i have many docker file i must build and run but i cant run any file</p>
"
"37439887","How to access the metadata of a docker container from a script running inside the container?","<bash><docker><dockerfile>","66256352","How to get docker image labels in runtime","<docker>","<p>I am trying to understand whether it is possible to read the metadata (Labels, in particular) properties of a container using a bash script.</p>

<p>For instance, if there is a Dockerfile like:</p>

<pre><code>FROM busybox
LABEL abc = abc_value1
</code></pre>

<p>And, if I build and run an image based on the file above, like so:</p>

<pre><code>docker build . -t image1
docker run -ti image1 /bin/bash
</code></pre>

<p>Is there any way to access the value of the ""abc"" label inside the bash shell?  If so, how?</p>
","<p>How to get docker image labels in runtime?</p>
<p>Or get <code>docker inspect image</code> info when the image instantiation is running .</p>
<p>thanks!</p>
"
"40905761","How do I mount a host directory as a volume in docker compose","<docker><docker-compose><docker-volume>","66321985","debugging, constant reload","<python><docker><flask>","<p>I have a development environment I'm dockerizing and I would like the ability to livereload my changes without having to rebuild docker images. I'm using docker compose because redis is one of my app's dependencies and I like being able to link a redis container</p>
<p>I have two containers defined in my <code>docker-compose.yml</code>:</p>
<pre class=""lang-yml prettyprint-override""><code>node:
  build: ./node
  links:
    - redis
  ports:
    - &quot;8080&quot;
  env_file:
    - node-app.env

redis:
  image: redis
  ports:
    - &quot;6379&quot;
</code></pre>
<p>I've gotten to the point in my <code>node</code> app's dockerfile where I add a volume, but how do I mount the the host's directory in the volume so that all my live edits to the code are reflected in the container?</p>
<p>Here's my current Dockerfile:</p>
<pre class=""lang-dockerfile prettyprint-override""><code># Set the base image to Ubuntu
FROM    node:boron

# File Author / Maintainer
MAINTAINER Amin Shah Gilani &lt;amin@gilani.me&gt;

# Install nodemon
RUN npm install -g nodemon

# Add a /app volume
VOLUME [&quot;/app&quot;]

# TODO: link the current . to /app

# Define working directory
WORKDIR /app

# Run npm install
RUN npm install

# Expose port
EXPOSE  8080

# Run app using nodemon
CMD [&quot;nodemon&quot;, &quot;/app/app.js&quot;]
</code></pre>
<p>My project looks like this:</p>
<pre><code>/
- docker-compose.yml
- node-app.env
- node/
  - app.js
  - Dockerfile.js
</code></pre>
","<p>hello i would like to use debug inside container but i cannot add working &quot;volumes&quot; into docker comopse.xml</p>
<p>file structure in the piscture:</p>
<p><a href=""https://i.stack.imgur.com/q70LS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q70LS.png"" alt=""enter image description here"" /></a></p>
<pre><code>FROM python:3.9-alpine
WORKDIR /planner
ENV FLASK_APP=app
ENV FLASK_RUN_HOST=0.0.0.0
RUN apk add --no-cache gcc musl-dev linux-headers
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt
EXPOSE 5000
COPY . .
CMD [&quot;flask&quot;, &quot;run&quot;]
</code></pre>
<p>compose:</p>
<pre><code>   version: &quot;3.7&quot;
    services:
      web:
        build: .
        restart: always
        ports:
          - &quot;5000:5000&quot;
        volumes:
          - .:/planner
    
      rq-dashboard:
        image: jaredv/rq-docker:0.0.2
        command: rq-dashboard -H rq-server
        ports:
          - 9181:9181
    
      rq-worker:
        image: jaredv/rq-docker:0.0.2
        command: rq worker -u redis://rq-server:6379 high normal low
        deploy:
          replicas: 3
    
      rq-server:
        image: redis:alpine
        ports:
          - 6379:6379
</code></pre>
<p>problem is with:</p>
<pre><code>volumes:
  - .:/planner
</code></pre>
<p><a href=""https://i.stack.imgur.com/FXtWj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXtWj.png"" alt=""enter image description here"" /></a></p>
<p>could you help me with it?</p>
"
"39468841","Is it possible to start a stopped container from another container","<docker><docker-container>","66324634","Deploy django app that uses docker run command","<python><django><docker><heroku><docker-compose>","<p>There are two containers A and B. Once container A starts, one process will be executed, then the container will stop. Container B is just an web application (say expressjs). Is it possible to kickstart A from container B ?</p>
","<p>I have a Django app that I want to deploy on Heroku. The application calls the command <code>docker run hello-world</code> in views.py when the button is pressed. As far as I know, on Heroku you can set up a stack on Ubuntu or Docker.</p>
<ol>
<li>When I use Ubuntu, I install docker using <code>heroku-buildpack-apt</code>. Unfortunately I then get the following error:</li>
</ol>
<pre><code>docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
See 'docker run --help'.
</code></pre>
<p>I tried using <code>docker run -v /var/run/docker.sock:/var/run/docker.sock hello-world</code> but to no effect.</p>
<p>Aptfile</p>
<pre><code>uidmap
iptables
daemon
apt-transport-https
ca-certificates
curl
gnupg-agent
software-properties-common
http://security.ubuntu.com/ubuntu/pool/universe/d/docker.io/docker.io_19.03.8-0ubuntu1.20.04.1_amd64.deb
</code></pre>
<ol start=""2"">
<li>When I try to dockerize the django app (and apply docker in docker), I have the same problem. I tested it locally. After typing <code>django-compose up</code> and clicking button, I get</li>
</ol>
<pre><code>docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
See 'docker run --help'.
</code></pre>
<p>Dockerfile</p>
<pre><code>FROM python:3.8-buster

ENV PATH=&quot;/scripts:${PATH}&quot;
ENV LD_LIBRARY_PATH=/usr/local/lib

COPY ./requirements.txt /requirements.txt

RUN apt-get update
RUN pip install --upgrade pip
RUN pip install -r /requirements.txt

# install Docker
RUN apt-get remove docker docker-engine docker.io runc
RUN apt-get install -yq --no-install-recommends apt-utils
RUN apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common 

RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -

RUN add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/debian \
   $(lsb_release -cs) \
   stable&quot;

RUN apt-get update
RUN apt-get install -y docker-ce docker-ce-cli containerd.io

RUN usermod -aG docker $(whoami)


RUN mkdir /app
COPY ./app /app
WORKDIR /app
COPY ./scripts /scripts

RUN chmod +x /scripts/*

CMD [&quot;entrypoint.sh&quot;]
</code></pre>
<p>entrypoint.sh</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh

set -e

python manage.py collectstatic --noinput

uwsgi --socket :8000 --master --enable-threads --module app.wsgi
</code></pre>
<p>docker-compose.yml</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.7'

services:
  app:
    build:
      context: .
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - ./app:/app
    command: sh -c &quot;python manage.py runserver 0.0.0.0:8000&quot;
    environment:
      - DEBUG=1
</code></pre>
<p>Is it even possible to deploy such an app? In both cases docker is installed, because I can type <code>docker --help</code>.</p>
"
"44769315","How to see docker image contents","<docker>","66332689","Explore content of files of nginx container on my host machine","<docker><docker-compose><docker-container>","<p>I did a docker pull and can list the image that's downloaded. I want to see the contents of this image. Did a search on the net but no straight answer.</p>
","<p>How can i see content of files of all my nginx container on my host machine ?
I want to see all the configurations files for exemple.</p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","66369850","I have created volumes with Docker Desktop, I see the volumes using ""docker volume ls"" but the folder in windows are empy","<docker><docker-compose><docker-volume><docker-desktop>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I have got a docker compose file that uses a volume, then I check that the volume is created while my container is running.</p>
<p><a href=""https://i.stack.imgur.com/q46kG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q46kG.png"" alt=""enter image description here"" /></a></p>
<p>Then I go to the folder where the volumes are stored in docker desktop, but the folder is empty.</p>
<p><a href=""https://i.stack.imgur.com/G3XBW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G3XBW.png"" alt=""enter image description here"" /></a></p>
<p>this is my docker-compose.yml</p>
<pre><code>version: &quot;3.3&quot;

services:

  backnode:
    restart: always
    build:
      context: .
    image: foo-backnode:1.0.0
    env_file: docker-compose.env
    volumes: 
      - html:/var/log/foo
    networks:
      - internal
    ports: 
      - &quot;3001:3001&quot;

networks:
  internal:
    internal: false

volumes: 
  html:
    external: false
</code></pre>
<p>I expect to see the log files produced by my application.</p>
<p><strong>Edit:</strong></p>
<p>All this works properly under linux enviroment (ubuntu).</p>
<p><a href=""https://i.stack.imgur.com/ApzxG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ApzxG.png"" alt=""enter image description here"" /></a></p>
"
"36354423","What is the best way to pass AWS credentials to a Docker container?","<amazon-web-services><docker><docker-compose>","66376115","Pass AWS Credentials FILE to Python Docker (debian based)","<python><amazon-web-services><docker><debian>","<p>I am running docker-container on Amazon EC2. Currently I have added AWS Credentials to Dockerfile. Could you please let me know the best way to do this?</p>
","<p>I know there are several similar questions but none of them have helped me. I want to run a python script (<em>migrations.py</em>) in a Docker container that needs the AWS credentials. The Dockerfile is the following:</p>
<pre><code>FROM python:3.8

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY migrations.py ./
COPY aws_credentials ~/.aws/credentials

CMD [ &quot;python3&quot;, &quot;./migrations.py&quot; ]
</code></pre>
<p>When I do:</p>
<pre><code>docker build -t yoyo . &amp;&amp; docker run yoyo
</code></pre>
<p>It shows the error</p>
<pre><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>All of the files (Dockerfile, requirements.txt, migrations.py and aws_credentials) are in the same place (the root of a folder).</p>
<p>I have tried the copy line like:</p>
<pre><code>COPY aws_credentials $HOME$/.aws/credentials
COPY aws_credentials ./.aws/credentials
</code></pre>
<p>I need AWS to access the credentials using the file credentials. How could I achieve it? Thank you very much</p>
"
"50136694","What is the simplest way to share a multi-container app on docker hub?","<docker><docker-compose><docker-swarm><dockerhub>","66387086","Upload docker-compose.yml file to docker hub","<docker><docker-compose><dockerhub>","<p>I have a fully dockerised app with multiple services: frontend, backend, user-service, multiple databases etc...</p>

<p>I orchestrate everything using a <code>docker-compose.yml</code> file. I am now trying to share this app with someone who has no CS/coding knowledge so that all they will need to do is pull this app from docker hub and enter the command <code>docker-compose up</code> or something similar. </p>

<p>I'm looking into the best way to do this and I'm getting very lost with the docker lexicon of swarms, bundles, stacks, services etc...</p>

<p>What is the best practice for sharing a collection of images started with a docker-compose.yml file?</p>
","<p>I am trying to sign and upload a <strong>docker-compose.yml</strong> to <strong>docker-hub</strong>, I have been looking and only images can be uploaded so I have searched for how to create images from a docker-compose.yml but I can't find anything, I have tried the command: <code>docker-compose build</code> but it doesn't create a full image for me just from the php part</p>
<p>Here is the docker-compose.yml file just in case it doesn't work due to a bug</p>
<pre><code>version: '3'

networks:
    LEMP:

services:
    nginx:
        image: nginx:stable-alpine
        container_name: LEMP_nginx
        ports:
            - &quot;80:80&quot;
        volumes:
            - ./app:/app
            - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
        depends_on:
            - php
        networks:
            - LEMP

    php:
        # image: php:7-fpm-alpine
        build: ./php
        container_name: LEMP_php
        volumes:
            - ./app:/app
        ports:
            - &quot;9000:9000&quot;
        networks:
            - LEMP


    mysql:
        image: mysql:latest
        container_name: LEMP_mysql
#        command: --default-authentication-plugin=mysql_native_password
        volumes:
            - ./database:/docker-entrypoint-initdb.d
        ports:
            - &quot;3307:3306&quot;
        depends_on:
            - nginx
        environment:
            - MYSQL_ROOT_PASSWORD=admin123
        networks:
            - LEMP

    phpmyadmin:
        image: phpmyadmin/phpmyadmin
        container_name: LEMP_phpmyadmin
        ports:
            - &quot;8000:80&quot;
        environment:
            PMA_ARBITRARY: 1
        depends_on:
            - mysql
        networks:
            - LEMP
</code></pre>
<p>And this is my Dockerfile for PHP</p>
<pre><code>FROM php:7-fpm-alpine
RUN docker-php-ext-install mysqli pdo pdo_mysql
</code></pre>
"
"46597290","Can't change the permissions of files/folders on a volume with Docker windows","<windows><docker><docker-for-windows>","66388158","Changing file permissions from a mounted file from inside the docker container (Windows 10)","<docker><file-permissions>","<p>I tried to change the permissions of files/folders on a volume with Docker windows.  But the permissions are not changed, unexpectedly.</p>

<p>Environment:
Host: Windows 10 Pro
Docker version 17.09.0-ce, build afdb6d4</p>

<p>Step to reproduce:</p>

<ol>
<li>Build a image with the Dockerfile below.</li>
<li>Run a container with a volume.</li>
<li>Change the permissions of files/folders.</li>
</ol>

<p>Dockerfile:</p>

<pre><code>FROM microsoft/windowsservercore
CMD [ ""powershell"" ]
</code></pre>

<p>Outputs:</p>

<pre><code>D:\data\docker\sample&gt;docker build -t sample .
Sending build context to Docker daemon  1.272GB
Step 1/2 : FROM microsoft/windowsservercore
 ---&gt; 2cddde20d95d
Step 2/2 : CMD powershell
 ---&gt; Running in dd207fe8b262
 ---&gt; e0203df155cd
Removing intermediate container dd207fe8b262
Successfully built e0203df155cd
Successfully tagged sample:latest

D:\data\docker\sample&gt;docker run -d --name sample --mount type=volume,source=sample_volume,target=C:/data sample ping -t localhost
5a21f41d63de321e912ec3b99010a062d2e04d5f99145c6cd8bf649d3fbbebf1

D:\data\docker\sample&gt;docker exec -i sample cmd
Microsoft Windows [Version 10.0.14393]
(c) 2016 Microsoft Corporation. All rights reserved.

C:\&gt;cd c:\data
cd c:\data

c:\data&gt;MKDIR foo
MKDIR foo

c:\data&gt;ECHO hoge &gt; foo\hoge.txt
ECHO hoge &gt; foo\hoge.txt

c:\data&gt;cacls foo
cacls foo
c:\data\foo NT AUTHORITY\SYSTEM:(OI)(CI)(ID)F
            BUILTIN\Administrators:(OI)(CI)(ID)F
            CREATOR OWNER:(OI)(CI)(IO)(ID)F
            BUILTIN\Users:(OI)(CI)(ID)R
            BUILTIN\Users:(CI)(ID)(special access:)
                                  FILE_WRITE_DATA
                                  FILE_APPEND_DATA
                                  FILE_WRITE_EA
                                  FILE_WRITE_ATTRIBUTES



c:\data&gt;cacls foo\hoge.txt
cacls foo\hoge.txt
c:\data\foo\hoge.txt NT AUTHORITY\SYSTEM:(ID)F
                     BUILTIN\Administrators:(ID)F
                     BUILTIN\Users:(ID)R


c:\data&gt;cacls foo /T /E /G everyone:F
cacls foo /T /E /G everyone:F
processed dir: c:\data\foo
processed file: c:\data\foo\hoge.txt

c:\data&gt;cacls foo
cacls foo
c:\data\foo NT AUTHORITY\SYSTEM:(OI)(CI)(ID)F
            BUILTIN\Administrators:(OI)(CI)(ID)F
            CREATOR OWNER:(OI)(CI)(IO)(ID)F
            BUILTIN\Users:(OI)(CI)(ID)R
            BUILTIN\Users:(CI)(ID)(special access:)
                                  FILE_WRITE_DATA
                                  FILE_APPEND_DATA
                                  FILE_WRITE_EA
                                  FILE_WRITE_ATTRIBUTES



c:\data&gt;cacls foo\hoge.txt
cacls foo\hoge.txt
c:\data\foo\hoge.txt NT AUTHORITY\SYSTEM:(ID)F
                     BUILTIN\Administrators:(ID)F
                     BUILTIN\Users:(ID)R
</code></pre>

<p>I found a document which says that the permissions of files/folders on volumes cannot be changed on Linux containers.  But I could not found documentation about Windows containers.  Does Windows containers support the permission changes of file/folders on a volume <strong>on Windows containers</strong>?</p>

<p>Link:</p>

<ul>
<li>Logs and troubleshooting | Docker Documentation <a href=""https://docs.docker.com/docker-for-windows/troubleshoot/#permissions-errors-on-data-directories-for-shared-volumes"" rel=""nofollow noreferrer"">https://docs.docker.com/docker-for-windows/troubleshoot/#permissions-errors-on-data-directories-for-shared-volumes</a></li>
</ul>
","<p>Is it possible to change the file permissions from a mounted file from inside the docker container?
And if yes how do I do that?
If I try it with chmod nothing happens.
I'm using a Debian container on windows 10.</p>
<p>Here's a picture of what I've tried.</p>
<p><a href=""https://i.stack.imgur.com/Uxi0V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uxi0V.png"" alt=""What I've tried"" /></a></p>
<p>Thanks for the help. :)</p>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","66421681","How can I run windows docker containers on linux?","<docker><virtual-machine><docker-machine>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I have a docker container that was built using another windows container that requires the host OS be windows.</p>
<p>Is there a way I can get this to run on linux?</p>
"
"45173574","Docker-compose.yml file that builds a base image, then children based on it?","<docker><docker-compose><dockerfile><docker-stack>","66444428","Service to be only use for docker-compose build, but not up","<docker><docker-compose>","<p>For clarification, when I say base image, I mean the parent image that has all the common configurations, so that the children based on it don't need to download the dependencies individually.</p>

<p>From my understanding, docker-compose.yml files are the run-time configurations, while Dockerfiles are the build-time configurations. However, there is a <code>build</code> option using docker-compose, and I was wondering how I could use this to build a base image.</p>

<p>As of right now, I use a shellscript that runs other shellscripts. One builds all my images, from a base image that it also creates. The other runs them as containers with the necessary configurations. However, the base image is never ran as a container.</p>

<p>Currently, the shellscript I hope to change into a docker-compose file, looks like so:</p>

<pre><code>echo ""Creating docker network net1""
docker network create net1

echo ""Running api as a container with port 5000 exposed on net1""
docker run --name api_cntr --net net1 -d -p 5000:5000 api_img

echo ""Running redis service with port 6379 exposed on net1""
docker run --name message_service --net net1 -p 6379:6379 -d redis

echo ""Running celery worker on net1""
docker run --name celery_worker1 --net net1 -d celery_worker_img

echo ""Running flower HUD on net1 with port 5555 exposed""
docker run --name flower_hud --net net1 -d -p 5555:5555 flower_hud_img
</code></pre>

<p>The shellscript that makes the images, is as follows:</p>

<pre><code>echo ""Building Base Image""
docker build -t base ../base-image

echo ""Building api image from Dockerfile""
docker build -t api_img  ../api

echo ""Building celery worker image""
docker build -t celery_worker_img ../celery-worker

echo ""Building celery worker HUD""
docker build -t flower_hud_img ../flower-hud
</code></pre>

<p>My questions comes down to one thing, can I create this Base image without ever running it in a container with docker-compose. (All the Dockerfiles start with <code>FROM base:latest</code> other than the base itself). I'm looking to make it as easy as possible for other people, so that they only have to run a single command.</p>

<p>EDIT: I am using version 3, and acording to the docs, <code>build:</code> is ignored, and docker-compose only accepts pre-built images.</p>
","<p>Is there a way to specify one service which would not be initialised when <code>docker-compose up</code> is called, but will be built when <code>docker-compose build</code> is run.</p>
<p>This might be a weird thing I am trying to do, but the goal is to use that built image as a base for other multiple services.</p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","66445929","Best way to inherit from multiple Docker images","<docker><dockerfile>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p>My OS is Ubuntu, but i want to grab very large components + dependencies from other, let's say 3-4 base images.</p>
<p>All images that i need to derive from have, as i've mentioned, very large components + some dependencies.</p>
<p>Is there a way to incrementally build on an Ubuntu image, by copying stuff from other downloaded images ?</p>
<p>The most stupid think i can thing at the moment is to do it like this:</p>
<pre><code>FROM my_base_1 AS base_1
FROM my_base_2 AS base_2
FROM ubuntu:18:04
COPY --from=base_1 relevant relevant_1
COPY --from=base_2 relevant relevant_2

apt-get install relevant-dependencies

etc
```
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","66476303","java with kafka docker image Broker may not be available.Error","<spring><spring-boot><docker><spring-mvc><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i want run kafka with java spring boot application
but when run my application kafka cannot connection with my application</p>
<p>there error when run application is :</p>
<pre><code> 2021-03-04 15:32:12.787  WARN 14381 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group_id-1, groupId=group_id] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2021-03-04 15:32:12.787  WARN 14381 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group_id-1, groupId=group_id] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected
</code></pre>
<p>my application.yml is:</p>
<pre><code> spring:
  kafka:
    consumer:
      bootstrap-servers: localhost:9092
      group-id: group_id
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    producer:
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
</code></pre>
<p>the spring cannot see kafka container
in kafka execute i can add new topic but from spring i dont send message</p>
"
"47854463","Docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock","<docker><jenkins><jenkins-pipeline>","66497967","Jenkins error with Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock","<docker><jenkins>","<p>I am new to docker. I just tried to use docker in my local machine(Ubuntu 16.04) with Jenkins. </p>

<p>I configured a new job with below pipeline script.</p>

<pre class=""lang-groovy prettyprint-override""><code>node {
    stage('Build') {
      docker.image('maven:3.3.3').inside {
        sh 'mvn --version'
      }
    }
}
</code></pre>

<p>But it fails with below error.</p>

<p><a href=""https://i.stack.imgur.com/nz6Ig.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nz6Ig.png"" alt=""enter image description here""></a></p>
","<p>I have a jenkins job that does a docker ps command. It is a pipeline script as follows:</p>
<pre><code>node {
    wrap([$class: 'BuildUser']) {
        sh 'echo &quot;The build user: &quot;'
        sh 'echo &quot;${BUILD_USER}&quot;'
    }
    stage 'run ps command'
    sh &quot;\$(docker ps)&quot;
}
</code></pre>
<p>It prints:</p>
<pre><code>+ echo 'The build user: '
The build user: 
[Pipeline] sh
+ echo 'Anonymous'
Anonymous
[Pipeline] }
[Pipeline] // wrap
[Pipeline] stage (run ps command)
Using the ‘stage’ step without a block argument is deprecated
Entering stage run ps command
Proceeding
[Pipeline] sh
++ docker ps
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json: dial unix /var/run/docker.sock: connect: permission denied
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
ERROR: script returned exit code 1
Finished: FAILURE
</code></pre>
<p>It is my understanding that jenkins job will always run as the same OS user, so in thise case &quot;<strong>jenkinsuser</strong>&quot;. I have already added this user to docker group and restarted jenkins multiple times.</p>
<pre><code>$&gt; grep docker /etc/group
docker:x:497:jenkinsuser

$&gt; ps -ef | grep jenkins
root     11177   978  0 15:48 ?        00:00:00 sshd: jenkinsuser [priv]
jenkinsuser 11180 11177  0 15:48 ?        00:00:00 sshd: jenkinsuser@pts/0
jenkinsuser 13276     1  0  2020 ?        1-19:07:35 java -jar jenkins.war
jenkinsuser 20929 11181  0 20:04 pts/0    00:00:00 grep --color=auto jenkins
</code></pre>
<p>Is there something I am missing?</p>
<p>When i am on the linux box, I can successfully do docker ps:</p>
<pre><code>$&gt; whoami
jenkinsuser

$&gt; docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
</code></pre>
<p>but this does not work when i do it as part of a jenkins pipeline build job!! Why?!?</p>
"
"42385977","accessing a docker container from another container","<docker>","66502654","Docker container talks to docker container in the same local host?","<docker>","<p>i created two docker containers based on two different images. one of db and another for webserver. both containers are running on my mac osx. </p>

<p>i can access db container from host machine and same way can access webserver from host machine. </p>

<p>however, how do i access db connection from webserver? </p>

<p>the way i started db container is</p>

<pre><code>docker run --name oracle-db -p 1521:1521 -p 5501:5500 oracle/database:12.1.0.2-ee
</code></pre>

<p>I started wls container as</p>

<pre><code>docker run --name oracle-wls -p 7001:7001 wls-image:latest
</code></pre>

<p>I can access db on host by connecting to </p>

<pre><code>sqlplus scott/welcome1@//localhost:1521/ORCLCDB
</code></pre>

<p>I can access wls on host as</p>

<pre><code>http://localhost:7001/console
</code></pre>
","<p>I am in a confusion right now. I try many things I can find on the web, but, none solved it. I have Win10 and Docker desktop installed using WSL 2 to host Linux containers. I use the following command to start the Jenkins website.</p>
<pre><code>docker run --name jenkins-master-c -d -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkins/jenkins:2.282-alpine
</code></pre>
<p>This works fine. I can access the website using <code>http://localhost:8080/</code></p>
<p>The problem is, I try to curl http://localhost:8080 from another alpine docker container, but, I am not getting the web page back, it said connection refused. I tried my own tiny web service on my Windows machine without docker. Same thing. I can access the web service using web browser on Windows 10. However, if I get inside a container, I couldn't access the web service on the localhost.</p>
<p>I know I am missing some thing really basic, because the web doesn't seem to have this topic. I am just on my own computer without anything fancy. Thus, I just want to use localhost. The web said the default is supposed to use bridge which the container should talk to each other easily, but, it is not working for me. What am I missing. Maybe I shouldn't type <code>localhost</code>? But, what else should I do?</p>
<p>thank you</p>
<p>Edit: just want to explain what I did to get my problem solved. The creating network <code>--network my-network-name</code> was what I originally did, which failed because the way I curl the webpage is wrong. I did <code>--name jenkins-master-c</code> only to make it easy locate my container on the <code>docker ps</code>. But, as pointed out in my question, I suspected the <code>localhost</code> is wrong, which is confirmed by the solution. Instead of using localhost, I do <code>curl http://jenkins-master-c:8080</code> which worked. Thanks</p>
"
"56340537","Docker, Springboot and Mysql : com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure","<mysql><spring><spring-boot><docker><jdbc>","59602209","docker-compose spring boot cannot connect to mysql","<java><mysql><spring-boot><docker><docker-compose>","<p>I try to run a Springboot app with mysql connection on Docker. Without docker, it works well. But when I try to deploy on a container, I have <code>com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</code>.
I use this docker-compose:</p>

<pre><code>version: '3'
services:
  nginx:
   container_name: nginx
   image: nginx:latest
   restart: always
   ports:
   - 80:80
   - 443:443
   volumes:
   - ./nginx/conf.d:/etc/nginx/conf.d
   depends_on:
   - spring-boot-app


  mysql:
   container_name: mysql
   image: mysql:latest
   environment:
    MYSQL_ROOT_PASSWORD: ""rootpass""
    MYSQL_DATABASE: ""db""
    MYSQL_USER: ""user""
    MYSQL_PASSWORD: ""pass""
   ports:
   - ""1306:3306""
   volumes:
   - ./data/:/var/lib/mysql
   networks: 
   - mysql-db
   restart: always

  spring-boot-app:
    container_name: spring-boot-app
    image: spring-boot-app
    build:
      context: ./spring-boot-app
      dockerfile: Dockerfile
    ports:
      - ""8080:8080""
    depends_on:
      - mysql
    networks: 
      - mysql-db
    restart: always

networks: 
  mysql-db:
      driver: bridge
</code></pre>

<p>Here is the Dockerfile for springboot app:</p>

<pre><code>FROM openjdk:11
COPY target/spring-boot-app.jar /spring-boot-app/spring-boot-app.jar
ENTRYPOINT [ ""java"", ""-jar"", ""-Dspring.profiles.active=prod"",""/spring-boot-app/spring-boot-app.jar"" ]
</code></pre>

<p>And finally, here is application.properties:</p>

<pre><code>spring.datasource.url = jdbc:mysql://mysql:1306/db
spring.datasource.username = user
spring.datasource.password = pass

# Show or not log for each sql query
spring.jpa.show-sql = true

# Hibernate ddl auto (create, create-drop, update)
spring.jpa.hibernate.ddl-auto = update

# Use spring.jpa.properties.* for Hibernate native properties (the prefix is
# stripped before adding them to the entity manager)

# The SQL dialect makes Hibernate generate better SQL for the chosen database
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5Dialect

server.port=8080
</code></pre>

<p>I don't think this is a port problem because i tried to set mysql to default port (3306), and it still doesn't work.</p>

<p>If it can help, here is <code>docker ps</code> :</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                  PORTS                                      NAMES
08ea16361c61        nginx:latest        ""nginx -g 'daemon of…""   1 second ago        Up Less than a second   0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp   nginx
52ef8d5c2127        spring-boot-app     ""java -jar -Dspring.…""   3 seconds ago       Up 1 second             0.0.0.0:8080-&gt;8080/tcp                     spring-boot-app
7bbcdc1fae3c        mysql:latest        ""docker-entrypoint.s…""   4 seconds ago       Up 3 seconds            33060/tcp, 0.0.0.0:1306-&gt;3306/tcp          mysql
</code></pre>

<p>Edit : full stacktrace :</p>

<pre><code>ubuntu@ubuntu:~/Documents/SpringBoot$ docker-compose up
Creating network ""springboot_mysql-db"" with driver ""bridge""
Creating mysql ... done
Creating spring-boot-app ... done
Attaching to mysql, spring-boot-app
mysql              | 2019-05-27T21:50:19.645920Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
mysql              | 2019-05-27T21:50:19.645986Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.16) starting as process 1
mysql              | 2019-05-27T21:50:20.451523Z 0 [System] [MY-010229] [Server] Starting crash recovery...
mysql              | 2019-05-27T21:50:20.466040Z 0 [System] [MY-010232] [Server] Crash recovery finished.
mysql              | 2019-05-27T21:50:20.544487Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
mysql              | 2019-05-27T21:50:20.551894Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
mysql              | 2019-05-27T21:50:20.584210Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.16'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
mysql              | 2019-05-27T21:50:20.682338Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: '/var/run/mysqld/mysqlx.sock' bind-address: '::' port: 33060
spring-boot-app    | 
spring-boot-app    |   .   ____          _            __ _ _
spring-boot-app    |  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
spring-boot-app    | ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
spring-boot-app    |  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
spring-boot-app    |   '  |____| .__|_| |_|_| |_\__, | / / / /
spring-boot-app    |  =========|_|==============|___/=/_/_/_/
spring-boot-app    |  :: Spring Boot ::        (v2.1.5.RELEASE)
spring-boot-app    | 
spring-boot-app    | 2019-05-27 21:50:22.884  INFO 1 --- [           main] com.jv.demo.DemoApplication              : Starting DemoApplication v0.0.1-SNAPSHOT on c69469fd7461 with PID 1 (/spring-boot-app/spring-boot-app.jar started by root in /)
spring-boot-app    | 2019-05-27 21:50:22.903  INFO 1 --- [           main] com.jv.demo.DemoApplication              : The following profiles are active: prod
spring-boot-app    | 2019-05-27 21:50:25.152  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data repositories in DEFAULT mode.
spring-boot-app    | 2019-05-27 21:50:25.378  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 205ms. Found 1 repository interfaces.
spring-boot-app    | 2019-05-27 21:50:26.313  INFO 1 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$e43b97d] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
spring-boot-app    | 2019-05-27 21:50:27.043  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
spring-boot-app    | 2019-05-27 21:50:27.195  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
spring-boot-app    | 2019-05-27 21:50:27.196  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.19]
spring-boot-app    | 2019-05-27 21:50:27.472  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
spring-boot-app    | 2019-05-27 21:50:27.472  INFO 1 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 4316 ms
spring-boot-app    | 2019-05-27 21:50:27.928  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
spring-boot-app    | 2019-05-27 21:50:29.226 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
spring-boot-app    | 
spring-boot-app    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
spring-boot-app    | 
spring-boot-app    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
spring-boot-app    |    at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:835) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:455) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:240) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.fetchConnection(DataSourceUtils.java:157) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:115) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:78) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:319) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:356) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.DatabaseLookup.getDatabase(DatabaseLookup.java:73) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.JpaProperties.determineDatabase(JpaProperties.java:142) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.JpaBaseConfiguration.jpaVendorAdapter(JpaBaseConfiguration.java:113) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec.CGLIB$jpaVendorAdapter$5(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec$$FastClassBySpringCGLIB$$cb57d9da.invoke(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:244) ~[spring-core-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:363) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec.jpaVendorAdapter(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
spring-boot-app    |    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:622) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:456) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:277) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1248) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:857) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:760) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:277) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1248) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:857) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:760) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at com.jv.demo.DemoApplication.main(DemoApplication.java:10) ~[classes!/:0.0.1-SNAPSHOT]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
spring-boot-app    |    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
spring-boot-app    | 
spring-boot-app    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[na:na]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.NativeSession.connect(NativeSession.java:152) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:955) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:825) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    ... 84 common frames omitted
spring-boot-app    | Caused by: java.net.ConnectException: Connection refused (Connection refused)
spring-boot-app    |    at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
spring-boot-app    |    at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
spring-boot-app    |    at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
spring-boot-app    |    at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    ... 87 common frames omitted
spring-boot-app    | 
spring-boot-app    | 2019-05-27 21:50:29.234  WARN 1 --- [           main] o.s.b.a.orm.jpa.DatabaseLookup           : Unable to determine jdbc url from datasource
spring-boot-app    | 
spring-boot-app    | org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta-data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Failed to obtain JDBC Connection; nested exception is com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
</code></pre>
","<p>When I make the docker-compose file only contains MySQL  and I run the application from Intellij or from the .jar file it works fine, but when i'm trying to get it running using docker compose I get this error
<code>com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure</code></p>

<p>My dockerfile</p>

<pre><code>FROM adoptopenjdk/openjdk13
ADD ./build/libs/AppointmentsSystem-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar
</code></pre>

<p>my docker-compose.yml file</p>

<pre><code>version: ""3""
services:
  app:
    build:
      dockerfile: Dockerfile
      context: .
    ports:
      - 8081:8081
    depends_on:
      - mysql

  mysql:
    image: mysql:5.7
    volumes:
      - ./mysql_data:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: appointmentSystem
    ports:
      - 3308:3306
</code></pre>

<p>build.gradle</p>

<pre><code>plugins {
    id 'org.springframework.boot' version '2.2.1.RELEASE'
    id 'io.spring.dependency-management' version '1.0.8.RELEASE'
    id 'java'
}

group = 'com.sulimanLab'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '13'

configurations {

    compileOnly {
        extendsFrom annotationProcessor
    }
}

repositories {
    mavenCentral()
}

wrapper {
    gradleVersion = ""6.0""
}

dependencies {
    compile group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa'
    compile group: 'mysql', name: 'mysql-connector-java'
    compileOnly 'org.projectlombok:lombok'
    annotationProcessor 'org.projectlombok:lombok'
    testImplementation('org.springframework.boot:spring-boot-starter-test') {
        exclude group: 'org.junit.vintage', module: 'junit-vintage-engine'
    }
    testImplementation 'io.projectreactor:reactor-test'
}

test {
    useJUnitPlatform()
}

</code></pre>

<p>And application.yml</p>

<pre><code>spring:
  datasource:
    url: jdbc:mysql://localhost:3308/appointmentSystem
    username: root
    password: rootpassword
  jpa:
    database-platform: org.hibernate.dialect.MySQL5Dialect
    database: MYSQL
    hibernate:
      ddl-auto: update
server:
  port: 8081

</code></pre>

<p>The error </p>

<pre><code>PS C:\Users\user\Desktop\Github\AppointmentsSystem - Copy&gt; docker-compose.exe up
Creating network ""appointmentssystem-copy_default"" with the default driver
Creating appointmentssystem-copy_mysql_1 ... done                                                                                                                                                                                                                              Creating appointmentssystem-copy_app_1   ... done                                                                                                                                                                                                                              Attaching to appointmentssystem-copy_mysql_1, appointmentssystem-copy_app_1
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.28-1debian9 started.
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.28-1debian9 started.
mysql_1  | 2020-01-05T17:05:11.307944Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
mysql_1  | 2020-01-05T17:05:11.313597Z 0 [Note] mysqld (mysqld 5.7.28) starting as process 1 ...
mysql_1  | 2020-01-05T17:05:11.321895Z 0 [Note] InnoDB: PUNCH HOLE support available
mysql_1  | 2020-01-05T17:05:11.321911Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mysql_1  | 2020-01-05T17:05:11.321914Z 0 [Note] InnoDB: Uses event mutexes
mysql_1  | 2020-01-05T17:05:11.321916Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
mysql_1  | 2020-01-05T17:05:11.321919Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
mysql_1  | 2020-01-05T17:05:11.321921Z 0 [Note] InnoDB: Using Linux native AIO
mysql_1  | 2020-01-05T17:05:11.322089Z 0 [Note] InnoDB: Number of pools: 1
mysql_1  | 2020-01-05T17:05:11.322160Z 0 [Note] InnoDB: Using CPU crc32 instructions
mysql_1  | 2020-01-05T17:05:11.323705Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
mysql_1  | 2020-01-05T17:05:11.349344Z 0 [Note] InnoDB: Completed initialization of buffer pool
mysql_1  | 2020-01-05T17:05:11.351768Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
mysql_1  | 2020-01-05T17:05:11.420065Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
mysql_1  | 2020-01-05T17:05:11.525183Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
mysql_1  | 2020-01-05T17:05:11.527734Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
mysql_1  | 2020-01-05T17:05:11.735100Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
mysql_1  | 2020-01-05T17:05:11.744572Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
mysql_1  | 2020-01-05T17:05:11.744744Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
mysql_1  | 2020-01-05T17:05:11.745330Z 0 [Note] InnoDB: Waiting for purge to start
mysql_1  | 2020-01-05T17:05:11.796296Z 0 [Note] InnoDB: 5.7.28 started; log sequence number 12442260
mysql_1  | 2020-01-05T17:05:11.796835Z 0 [Note] Plugin 'FEDERATED' is disabled.
mysql_1  | 2020-01-05T17:05:11.797225Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
mysql_1  | 2020-01-05T17:05:11.878505Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
mysql_1  | 2020-01-05T17:05:11.878712Z 0 [Note] Skipping generation of SSL certificates as certificate files are present in data directory.
mysql_1  | 2020-01-05T17:05:11.898316Z 0 [Warning] CA certificate ca.pem is self signed.
mysql_1  | 2020-01-05T17:05:11.900205Z 0 [Note] Skipping generation of RSA key pair as key files are present in data directory.
mysql_1  | 2020-01-05T17:05:11.910728Z 0 [Note] Server hostname (bind-address): '*'; port: 3306
mysql_1  | 2020-01-05T17:05:11.911146Z 0 [Note] IPv6 is available.
mysql_1  | 2020-01-05T17:05:11.911674Z 0 [Note]   - '::' resolves to '::';
mysql_1  | 2020-01-05T17:05:11.912095Z 0 [Note] Server socket created on IP: '::'.
mysql_1  | 2020-01-05T17:05:11.917607Z 0 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
mysql_1  | 2020-01-05T17:05:11.947376Z 0 [Note] InnoDB: Buffer pool(s) load completed at 200105 17:05:11
mysql_1  | 2020-01-05T17:05:12.631614Z 0 [Note] Event Scheduler: Loaded 0 events
mysql_1  | 2020-01-05T17:05:12.632095Z 0 [Note] mysqld: ready for connections.
mysql_1  | Version: '5.7.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
app_1    |
app_1    |   .   ____          _            __ _ _
app_1    |  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
app_1    | ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
app_1    |  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
app_1    |   '  |____| .__|_| |_|_| |_\__, | / / / /
app_1    |  =========|_|==============|___/=/_/_/_/
app_1    |  :: Spring Boot ::        (v2.2.1.RELEASE)
app_1    |
app_1    | 2020-01-05 17:05:13.641  INFO 1 --- [           main] c.s.A.AppointmentsSystemApplication      : Starting AppointmentsSystemApplication on 185f7c998f26 with PID 1 (/app.jar started by root in /)
app_1    | 2020-01-05 17:05:13.660  INFO 1 --- [           main] c.s.A.AppointmentsSystemApplication      : No active profile set, falling back to default profiles: default
app_1    | 2020-01-05 17:05:14.985  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data repositories in DEFAULT mode.
app_1    | 2020-01-05 17:05:15.132  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 131ms. Found 5 repository interfaces.
app_1    | 2020-01-05 17:05:15.844  INFO 1 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
app_1    | 2020-01-05 17:05:16.390  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8081 (http)
app_1    | 2020-01-05 17:05:16.416  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
app_1    | 2020-01-05 17:05:16.417  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.27]
app_1    | 2020-01-05 17:05:16.533  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
app_1    | 2020-01-05 17:05:16.533  INFO 1 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2786 ms
app_1    | 2020-01-05 17:05:17.203  INFO 1 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
app_1    | 2020-01-05 17:05:17.317  INFO 1 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.4.8.Final}
app_1    | 2020-01-05 17:05:17.565  INFO 1 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.1.0.Final}
app_1    | 2020-01-05 17:05:17.807  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
app_1    | 2020-01-05 17:05:18.033 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
app_1    |
app_1    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    |      at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:456) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:175) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:118) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1245) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
app_1    |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Method.invoke(Method.java:567) ~[na:na]
app_1    |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
app_1    | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    |      at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[na:na]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.NativeSession.connect(NativeSession.java:144) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      ... 57 common frames omitted
app_1    | Caused by: java.net.ConnectException: Connection refused
app_1    |      at java.base/sun.nio.ch.Net.pollConnect(Native Method) ~[na:na]
app_1    |      at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:579) ~[na:na]
app_1    |      at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) ~[na:na]
app_1    |      at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
app_1    |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:339) ~[na:na]
app_1    |      at java.base/java.net.Socket.connect(Socket.java:585) ~[na:na]
app_1    |      at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      ... 60 common frames omitted
app_1    |
app_1    | 2020-01-05 17:05:18.035  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    | 2020-01-05 17:05:18.068  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
app_1    | 2020-01-05 17:05:19.454  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
app_1    | 2020-01-05 17:05:19.462 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
app_1    |
app_1    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
</code></pre>
"
"53429062","Docker: Springboot container can not connect to PostgreSql Container Connection error","<postgresql><spring-boot><docker><docker-compose>","59611011","docker-compose spring boot can't connect to Postgres","<postgresql><spring-boot><docker><docker-compose>","<p>I am building my first Springboot 2.0 application. I am trying to put my Springboot application into one docker container and my PostgresDB into another container.  </p>

<p><strong>My Dockerfile</strong></p>

<pre><code>    FROM frolvlad/alpine-oraclejdk8:slim
    VOLUME /tmp
    ADD springboot-api-demo-0.1*.jar app.jar
    RUN sh -c 'touch /app.jar'
    EXPOSE 9443
    ENTRYPOINT [ ""sh"", ""-c"", ""java $JAVA_OPTS -Djava.security.egd=file:/dev/urandom -jar /app.jar"" ]
</code></pre>

<p><strong>My docker-compose.yml file</strong></p>

<pre><code>version: ""2.1""

services:
  springboot-api-demo:
    image: ""fw/springboot-api-demo""
    mem_limit: 1024m
    ports:
      - ""8080:8080""
    environment:
      - SPRING_PROFILES_ACTIVE=local
      - AWS_REGION=local
      - ENVIRONMENT=local
      - AUTH_ENABLED=false
  postgres:
    container_name: pgdb
    image: postgres:9.6-alpine
    environment:
    - 'POSTGRES_ROOT_PASSWORD=postgres'
    - 'POSTGRES_USER=postgres'
    - 'POSTGRES_PASSWORD=postgres'
    ports:
    - ""54321:5432""
</code></pre>

<p>I am using Springboot JPA Data 2.0 with below config data in my <strong>application.properties</strong></p>

<pre><code>spring.datasource.url= jdbc:postgresql://localhost:54321/java_learning
spring.datasource.username=postgres
spring.datasource.password=postgres
</code></pre>

<p>I can test that Both of the Images are up. Also from docker log and docker events, I see that postgres  Container is running fine, even I can access it and also created a DB too.
But springboot container started but i died because it could not connect to postgress and throwing error below. </p>

<blockquote>
  <p>Unable to obtain connection from database: The connection attempt
  failed</p>
</blockquote>

<p>Note that my host machine already has Postgres on port 5432 thats why I did a port mapping ofr 54321:5432 on my postgres container. Here is Proof :) -</p>

<pre><code>➜  springboot-api-demo git:(master) ✗ lsof -i:54321              
COMMAND     PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 44345 shailendra.singh   18u  IPv4 0xf62897fbdd69e31d      0t0  TCP *:54321 (LISTEN)
com.docke 44345 shailendra.singh   21u  IPv6 0xf62897fbdd119975      0t0  TCP localhost:54321 (LISTEN)

➜  springboot-api-demo git:(master) ✗ lsof -i:5432 
COMMAND  PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 715 shailendra.singh    5u  IPv6 0xf62897fbb43e03b5      0t0  TCP localhost:postgresql (LISTEN)
postgres 715 shailendra.singh    6u  IPv4 0xf62897fbbaeea9bd      0t0  TCP localhost:postgresql (LISTEN)
</code></pre>

<p>I am not sure what is the problem. But my Springboot application is not able to connect my postgres container which is running fine with proper creadentials. </p>
","<p>I am facing the weirdest issue ever, I have an spring boot application with postgres,</p>

<p>I deployed Postgres on docker and I have connect my spring application to it and everything went fine.</p>

<p>but when I try to docker-compose my application with Postgres it refuses to connect with this stack error</p>

<pre><code>springbootapp_1  | 2020-01-06 10:53:54.419  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
springbootapp_1  | 2020-01-06 10:53:54.484 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
springbootapp_1  | 
springbootapp_1  | org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.makeConnection(Driver.java:458) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.connect(Driver.java:260) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:175) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:118) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1245) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
springbootapp_1  |      at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
springbootapp_1  |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
springbootapp_1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
springbootapp_1  |      at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
springbootapp_1  |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
springbootapp_1  |      at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
springbootapp_1  |      at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:75) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      ... 58 common frames omitted
springbootapp_1  | 
springbootapp_1  | 2020-01-06 10:53:54.485  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  | 2020-01-06 10:53:54.502  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQLDialect
springbootapp_1  | 2020-01-06 10:53:55.307  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
springbootapp_1  | 2020-01-06 10:53:55.309 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
springbootapp_1  | 
springbootapp_1  | org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.makeConnection(Driver.java:458) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.connect(Driver.java:260) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:60) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.SequenceInformationExtractorLegacyImpl.extractMetadata(SequenceInformationExtractorLegacyImpl.java:40) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.initializeSequences(DatabaseInformationImpl.java:65) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.&lt;init&gt;(DatabaseInformationImpl.java:59) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.Helper.buildDatabaseInformation(Helper.java:155) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:96) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:184) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:73) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:320) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:462) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1249) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
springbootapp_1  |      at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
springbootapp_1  |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
springbootapp_1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
springbootapp_1  |      at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
springbootapp_1  |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
springbootapp_1  |      at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
springbootapp_1  |      at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:75) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      ... 56 common frames omitted
springbootapp_1  | 
springbootapp_1  | 2020-01-06 10:53:55.310  WARN 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 08001
springbootapp_1  | 2020-01-06 10:53:55.310 ERROR 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  | 2020-01-06 10:53:55.319  WARN 1 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
springbootapp_1  | 2020-01-06 10:53:55.322  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
springbootapp_1  | 2020-01-06 10:53:55.336  INFO 1 --- [           main] ConditionEvaluationReportLoggingListener : 
springbootapp_1  | 
springbootapp_1  | Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
springbootapp_1  | 2020-01-06 10:53:55.339 ERROR 1 --- [           main] o.s.boot.SpringApplication               : Application run failed
springbootapp_1  | 
springbootapp_1  | org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution

appointmentssystem_springbootapp_1 exited with code 1

</code></pre>

<p>I will share my Dockerfile </p>

<pre><code>FROM adoptopenjdk/openjdk11
ADD ./build/libs/AppointmentsSystem-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar
</code></pre>

<p>and this is my build.gradle</p>

<pre><code>plugins {
    id 'org.springframework.boot' version '2.2.1.RELEASE'
    id 'io.spring.dependency-management' version '1.0.8.RELEASE'
    id 'java'
}

group = 'com.sulimanLab'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '11'

configurations {

    compileOnly {
        extendsFrom annotationProcessor
    }
}

repositories {
    mavenCentral()
}

wrapper {
    gradleVersion = ""5.0""
}

dependencies {

    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa'
    implementation group: 'org.postgresql', name: 'postgresql'


    compileOnly 'org.projectlombok:lombok'
    annotationProcessor 'org.projectlombok:lombok'
    testImplementation('org.springframework.boot:spring-boot-starter-test') {
        exclude group: 'org.junit.vintage', module: 'junit-vintage-engine'
    }
    testImplementation 'io.projectreactor:reactor-test'
}

test {
    useJUnitPlatform()
}

</code></pre>

<p>and this is my docker-compose file</p>

<pre><code>version: ""3.7""
services:
  postgres:
    image: postgres:latest
    container_name: postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - 5432:5432
    environment:
      - 'POSTGRES_ROOT_PASSWORD=admin'
      - 'POSTGRES_USER=postgres'
      - 'POSTGRES_PASSWORD=admin'
      - 'POSTGRES_DB=appointmentSystem'

  springbootapp:
    build: .
    environment:
      - 'spring.datasource.url=jdbc:postgresql://postgres:5432/appointmentSystem'
    ports:
      - 8080:8080
    depends_on:
      - postgres
volumes:
  postgres-data:

</code></pre>

<p>lastly this is my application.yml</p>

<pre><code>spring:
  datasource:
    driver-class-name: org.postgresql.Driver
    url: jdbc:postgresql://localhost:5432/appointmentSystem
    username: postgres
    password: admin
  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    database: postgresql
    hibernate:
      ddl-auto: update
    properties:
      dialect: org.hibernate.dialect.PostgreSQLDialect

</code></pre>

<p><strong>------------------</strong></p>

<p><strong>NOTE</strong></p>

<p>when I build the jar file and run it alone it works, so I started to think that the problem came from dockerfile but I don't know, Please help.</p>
"
"59472638","Docker-compose depends on not waiting until depended on service isn't fully started","<postgresql><docker><docker-compose>","59648655","dependency is not respected in docker","<docker><docker-compose>","<pre><code>version: '3'
services: 
server:
  container_name: hotel-server
  build: 
    dockerfile: Dockerfiles/server/Dockerfile
    context: .
  environment:
    dbhost: db
  links: 
    - db
  depends_on:
    - db
  restart: always
  ports: 
    - ""3456:3456""

db:
  image: ""custom-postgis:latest""
  container_name: hotelsdb
  environment:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: alliswell
    # POSTGRES_DB: hotels

  ports: 
    - ""5437:5432""
  volumes: 
    - hoteldata:/var/lib/postgresql/data #persistence
volumes: 
hoteldata: {}
</code></pre>

<p>The way I've set up the custom-postgis, its a custom postgres database container that has some initialization scripts which I have to wait until it starts. 
The problem is the server service starts before the db service fully starts.</p>

<p>Is there any workaround to that?</p>
","<p>I am dealing with a docker composer project. Here is the compose file :</p>

<pre><code>version: '3.3'
services:
  tomcatserver:
    build: ./mavenServer
    depends_on:
      - db
    ports:
      - ""8010:8080""
  db:
    image: mariadb
    environment:
      MYSQL_ALLOW_EMPTY_PASSWORD: ""true""
      MYSQL_ROOT_PASSWORD: ""root""
      MYSQL_DATABASE: ""tpformation""
      MYSQL_USER: ""tomcat""
      MYSQL_PASSWORD: ""tomcat"" 
    expose:
      - ""3306""
</code></pre>

<p>when I start my stack with docker-composer up, it is always the maven containers that starts first. However, it should be the db one. Could you help please.</p>
"
"56066235","How to pull from multiple private registries with docker-compose?","<docker><docker-compose><gitlab-ci>","59666283","How to pull docker image from private docker repository in docker-compose.yml?","<docker><docker-compose><dockerfile><docker-registry><docker-repository>","<p>I've attempted to pull two images from two different projects registries (gitlab container registry). All that in a <code>docker-compose.yml</code> file. </p>

<p>How can I configure my <code>gitlab-ci.yml</code> or configure variables (whatever works) in order to pull my images properly without any access problems ?</p>

<p>I have found a solution using <code>docker login</code> with a <em>deploy token</em> to have read-only access to my project registry. The problem is that works if I had only one image to pull : <a href=""https://stackoverflow.com/questions/50683869/how-to-build-push-and-pull-multiple-docker-containers-with-gitlab-ci/50684269?noredirect=1#comment88403258_50684269"">How to build, push and pull multiple docker containers with gitlab ci?</a></p>
","<p>How can I add the docker hub credentials in <code>docker-compose.yml</code> and pull the private image?</p>

<p>I want to pull 2 images from 2 different private repositories?</p>
"
"55241474","Why docker-compose creates directories/files with user:group 999:999?","<docker><docker-compose><docker-volume><file-ownership>","59747054","What does chown 999 do?","<docker><chown>","<p>I used <code>docker-compose up</code> with the following <code>docker-compose.yml</code></p>

<pre><code>version: '3.5'
services:
 mysql-server:
  image: mysql:5.7
  environment:
    - MYSQL_ROOT_PASSWORD=root_pwd
  volumes:
   - ./var/lib/mysql:/var/lib/mysql:rw
</code></pre>

<p>The directory <code>./var/lib/mysql</code> does not exist initially. </p>

<p>After running <code>docker-compose up</code> .. </p>

<p><code>ls -al ./var/lib/mysql</code> command shows all the files with <code>user:group</code> <code>999:999</code>. </p>

<p>I cannot find user or group called <code>999</code> in my system. </p>

<p><strong>Why</strong> <code>docker-compose</code> chooses to <strong>create files</strong> with a <strong>non-existing uid:gid</strong> ?</p>

<p>In my case, I cannot commit the specific directory unless I change ownership. But even when I do, on a next run, <code>docker-compose up</code> updates the ownership again to <code>999:999</code>.</p>

<p>What is the solution to the above problem? e.g. Is there a way to instruct docker-compose to map files in host machine with a specific uid:gid pair?</p>

<p>Host: ubuntu-18.04<br>
docker-compose: 1.22.0</p>
","<p>I know what <code>chmod</code> (change mode) and what <code>chown</code> (change owner) do.</p>

<p>I also understand what the numbers mean after <code>chmod</code> do.</p>

<p>I was going through a <a href=""https://youtu.be/-XzMfd4XQak?t=410"" rel=""nofollow noreferrer"">docker tutorial</a> and I saw <code>chown 999</code> being used.</p>

<p>What does it mean and do?</p>
"
"51462200","Docker Compose connect ECONNREFUSED 172.18.0.4:3306","<mysql><docker><docker-compose><sequelize.js>","59765806","Docker: NodeJs crashing due to Mysql connections","<mysql><docker><docker-compose>","<p>When I build the container of the project with this command:</p>

<pre><code>sudo docker build -t PROJECT_NAME .
</code></pre>

<p>And then I download the mysql's image through this Docker-Compose config:</p>

<pre><code>  db:
    image: mysql
    restart: always
    ports:
      - ""8999:3306""
    networks:
      - webnet
    environment:
      MYSQL_DATABASE: slack
      MYSQL_ROOT_PASSWORD: admin
</code></pre>

<p>And then, the project will connect with MySQL through the <a href=""http://docs.sequelizejs.com/"" rel=""nofollow noreferrer"">Sequelize ORM</a></p>

<p>I have this error:</p>

<pre><code>Unhandled rejection SequelizeConnectionRefusedError: connect ECONNREFUSED 172.18.0.4:3306
</code></pre>

<p><a href=""https://i.stack.imgur.com/s4LGb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s4LGb.png"" alt=""enter image description here""></a></p>

<p>How can I resolve this?</p>

<p>The Sequelize config is this:</p>

<pre><code>const sequelize = new Sequelize(process.env.TEST_DB || 'slack', 'root', 'admin', {
  host: process.env.DB_HOST || 'localhost',
  operatorsAliases: Sequelize.Op,
  dialect: 'mysql',
  define: {
    underscored: true
  }
});
</code></pre>

<p>The config of the web is this:</p>

<pre><code>web:
    image: slack-clone-server
    ports:
      - ""8080:8080""
    networks:
      - webnet
    environment:
      DB_HOST: db
      REDIS_HOST: redis
    command: [""./wait-for-it.sh"", ""db:3306"", ""--"", ""npm"", ""run"", ""prod""]
</code></pre>

<p>The script called ""wait for it"" is <a href=""https://github.com/vishnubob/wait-for-it/blob/master/wait-for-it.sh"" rel=""nofollow noreferrer"">this</a>.</p>

<p>If someone needs the complete code, here you go:</p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/src/models/index.js"" rel=""nofollow noreferrer"">Sequelize config</a></p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/docker-compose.yml"" rel=""nofollow noreferrer"">Docker Compose config</a></p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile config</a></p>
","<p>Using <code>Docker-compose version 3</code>,  Both container is running fine at 3306(mysql), 5002(app), but nodejs not connecting to <code>mysql</code> service</p>

<p><strong>Docker containers list</strong>:</p>

<pre><code>8e475b1fd110        app           ""npm start""              15 seconds ago      Up 10 seconds       0.0.0.0:5002-&gt;5002/tcp             
d5fe68309cd4        mysql:5.7     ""docker-entrypoint.s…""   19 seconds ago      Up 15 seconds       0.0.0.0:3306-&gt;3306/tcp, 33060/tcp
</code></pre>

<p>docker logs for both <code>app</code> and <code>mysql</code>.</p>

<p><strong>App Docker Logs:</strong></p>

<pre><code> docker logs --details 8e475b1fd110

 &gt; rest@1.0.0 start /home/node/src/server
 &gt; nodemon --exec babel-node src/server.js

 [nodemon] 2.0.2
 [nodemon] to restart at any time, enter `rs`
 [nodemon] watching dir(s): *.*
 [nodemon] watching extensions: js,mjs,json
 [nodemon] starting `babel-node src/server.js`
 Server started at port 5002
 /home/node/src/server/src/config/db.js:26
   if (err) throw err;
            ^

 Error: connect ECONNREFUSED 172.22.0.2:3306
     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1097:14)
     --------------------
     at Protocol._enqueue (/home/node/src/server/node_modules/mysql/lib/protocol/Protocol.js:144:48)
     at Protocol.handshake (/home/node/src/server/node_modules/mysql/lib/protocol/Protocol.js:51:23)
     at Connection.connect (/home/node/src/server/node_modules/mysql/lib/Connection.js:119:18)
     at Object.&lt;anonymous&gt; (/home/node/src/server/src/config/db.js:12:12)
 [nodemon] app crashed - waiting for file changes before starting...
</code></pre>

<p><strong>Mysql Docker Logs:</strong></p>

<pre><code> 2020-01-16T08:20:29.252232Z 0 [Note] mysqld (mysqld 5.7.28) starting as process 1 ...
 2020-01-16T08:20:29.273098Z 0 [Note] InnoDB: PUNCH HOLE support available
 2020-01-16T08:20:29.273139Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
......
 2020-01-16T08:20:29.652300Z 0 [Note] IPv6 is available.
 2020-01-16T08:20:29.652315Z 0 [Note]   - '::' resolves to '::';
 2020-01-16T08:20:29.652337Z 0 [Note] Server socket created on IP: '::'.
 2020-01-16T08:20:29.689338Z 0 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
 2020-01-16T08:20:29.708091Z 0 [Note] Event Scheduler: Loaded 0 events
 2020-01-16T08:20:29.708416Z 0 [Note] mysqld: ready for connections.
 Version: '5.7.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM node:10.15.3-jessie
ENV APP_HOME=/home/node/src/server
WORKDIR ${APP_HOME}
COPY package.json .
RUN npm install -g npm
COPY . .
USER node
EXPOSE 5002
CMD [""npm"", ""start""]
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3'
services:
  db:
    container_name: db_mysql
    image: mysql:5.7
    ports:
      - '3306:3306'
    environment:
      - MYSQL_ROOT_USER=root
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=sampledb
    networks:
      - dbnet
  app:
    container_name: app_server
    restart: always
    build: .
    ports:
      - '5002:5002'
    depends_on: 
      - db
    environment:
      - PORT=5002
      - NODE_ENV=development
      - DB_HOST=db_mysql
      - DB_PORT=3306
      - DB_PASSWORD=root
      - DB_USER=root
      - DB_DATABASE=sampledb
    networks:
      - dbnet
networks:
  dbnet:
    external: true
</code></pre>

<p>Created network by run <code>docker network create dbnet</code></p>

<p><strong>database.js</strong></p>

<pre><code>import mysql from ""mysql"";

const connection = mysql.createConnection({
  host: ""db_mysql"",            // using DB container name
  user: ""root"",
  password: ""root"",
  database: ""sampledb""
});

// connect to database
connection.connect(function(err) {
  if (err) throw err
  console.log('You are now connected with mysql database...')
});


export default connection
</code></pre>
"
"58991516","Kubernetes - services without selector","<amazon-web-services><docker><nginx><kubernetes><kops>","59769072","Can't access Service via Kubernetes Service with specified endpoints","<networking><kubernetes><service><tcp><connection-refused>","<p>I'm struggling with Kubernetes' service without a selector. The cluster is installed on AWS with the kops. I have a deployment with 3 nginx pods exposing port 80:</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
 name: ngix-dpl                 # Name of the deployment object
 labels:
   app: nginx                     
spec:
 replicas: 3                    # Number of instances in the deployment
 selector:                      # Selector identifies pods to be
     matchLabels:               #     part of the deployment 
        app: nginx              #     by matching of the label ""app"" 
 template:                      # Templates describes pods of the deployment
   metadata:
     labels:                    # Defines key-value map
       app: nginx               # Label to be recognized by other objects
   spec:                        #     as deployment or service
     containers:                # Lists all containers in the pod
     - name: nginx-pod          # container name
       image: nginx:1.17.4      # container docker image
       ports:
       - containerPort: 80      # port exposed by container
</code></pre>

<p>After creation of the deployment, I noted the IP addresses:</p>

<pre><code>$ kubectl get pods -o wide | awk {'print $1"" "" $3"" "" $6'} | column -t
                                                                           NAME                       STATUS   IP
curl                       Running  100.96.6.40
ngix-dpl-7d6b8c8944-8zsgk  Running  100.96.8.53
ngix-dpl-7d6b8c8944-l4gwk  Running  100.96.6.43
ngix-dpl-7d6b8c8944-pffsg  Running  100.96.8.54
</code></pre>

<p>and created a service that should serve the IP addresses:</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: dummy-svc
  labels:
    app: nginx
spec:
 ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
apiVersion: v1
kind: Endpoints
metadata:
  name: dummy-svc 
subsets: 
  - addresses:
    - ip: 100.96.8.53
    - ip: 100.96.6.43
    - ip: 100.96.8.54
    ports:
    - port: 80
      name: http
</code></pre>

<p>The service is successfully created:</p>

<pre><code>$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
dummy-svc    ClusterIP   100.64.222.220   &lt;none&gt;        80/TCP    32m
kubernetes   ClusterIP   100.64.0.1       &lt;none&gt;        443/TCP   5d14h
</code></pre>

<p>Unfortunately, my attempt to connect to the nginx through the service from another pod of the same namespace  fails:</p>

<pre><code>$ curl 100.64.222.220
curl: (7) Failed to connect to 100.64.222.220 port 80: Connection refused
</code></pre>

<p>I can successfully connect to the nginx pods directly:</p>

<pre><code>$ curl 100.96.8.53
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
....
</code></pre>

<p>I noticed that my service does not have any endpoints. But I'm not sure that the manual endpoints should be shown there:</p>

<pre><code>$ kubectl get svc/dummy-svc -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |      
       {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""labels"":{""app"":""nginx""},""name"":""dummy-svc"",""namespace"":""default""},""spec"":{""ports"":[{""port"":80,""protocol"":""TCP"",""targetPort"":80}]}}
  creationTimestamp: ""2019-11-22T08:41:29Z""
  labels:
    app: nginx
  name: dummy-svc
  namespace: default
  resourceVersion: ""4406151""
  selfLink: /api/v1/namespaces/default/services/dummy-svc
  uid: e0aa9d01-0d03-11ea-a19c-0a7942f17bf8
spec:
  clusterIP: 100.64.222.220
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
</code></pre>

<p>I understand that it is not a proper use case for services and using of a pod selector will bring it to work. But I want to understend why this configuration does not work. I don't know where to look for the solution. Any hint will be appreciated.</p>
","<p>I've created a Kubernetes <strong>Service</strong> whose backend nodes aren't part of the Cluster but a fixed set of nodes (having fixed IPs), so I've also created an <strong>Endpoints</strong> resource with the same name:</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: hive
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 10002
---
apiVersion: v1
kind: Endpoints
metadata:
  name: hive
subsets:
  - addresses:
      - ip: 10.52.7.28
      - ip: 10.52.7.29
    ports:
      - port: 10002
</code></pre>

<p>Description of Service and Endpoints:</p>

<pre><code>$ kubectl describe svc/hive
Name:              hive
Namespace:         default
Labels:            &lt;none&gt;
Annotations:       &lt;none&gt;
Selector:          &lt;none&gt;
Type:              ClusterIP
IP:                10.0.192.103
Port:              http  80/TCP
TargetPort:        10002/TCP
Endpoints:
Session Affinity:  None
Events:            &lt;none&gt;
$ 
$ kubectl describe ep/hive
Name:         hive
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Subsets:
  Addresses:          10.52.7.28,10.52.7.29
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port   Protocol
    ----     ----   --------
    &lt;unset&gt;  10002  TCP

Events:  &lt;none&gt;
</code></pre>

<p>If I exec into one of the pods and telnet directly to Endpoint subset addresses, I am able to connect but If I access it via Service, I am getting connection refused. Just for completeness, Service and the pod are in same namespace:</p>

<pre><code># telnet 10.52.7.28 10002
Trying 10.52.7.28...
Connected to 10.52.7.28.
Escape character is '^]'.
^CConnection closed by foreign host.
#
# telnet 10.52.7.29 10002
Trying 10.52.7.29...
Connected to 10.52.7.29.
Escape character is '^]'.
^CConnection closed by foreign host.
#
# telnet hive 80
Trying 10.0.192.103...
telnet: Unable to connect to remote host: Connection refused
#
</code></pre>

<p>Any idea why I can directly connect to the IP but can't go via Kubernetes Service? I believe this isn't because of Firewall rules because then it should've blocked the direct requests as well.</p>

<p><strong>Edit:</strong> I suspect its something to do with <code>Endpoints</code> being empty when I run <code>kubectl describe svc/hive</code> but I can see in the dashboard that Endpoints (under Service page) shows those Endpoints.</p>
"
"48561981","Activate python virtualenv in Dockerfile","<python><docker><virtualenv><dockerfile>","59777926","Dockerized flask app builds and runs locally but wont work when deployed on Azure","<python><azure><docker><flask><dockerfile>","<p>I have a Dockerfile where I try to activate python virtualenv after what, it should install all dependencies within this env. However, everything still gets installed globally. I used different approaches and non of them worked. I also do not get any errors. Where is a problem?</p>

<p>1.
<code>ENV PATH $PATH:env/bin</code></p>

<p>2.
<code>ENV PATH $PATH:env/bin/activate</code></p>

<p>3.
<code>RUN . env/bin/activate</code></p>

<p>I also followed <a href=""https://github.com/GoogleCloudPlatform/python-runtime#kubernetes-engine--other-docker-hosts"" rel=""noreferrer"">an example of a Dockerfile config for the python-runtime image on Google Cloud</a>, which is basically the same stuff as above.</p>

<blockquote>
  <p>Setting these environment variables are the same as running source /env/bin/activate.</p>
</blockquote>

<p><code>ENV VIRTUAL_ENV /env</code></p>

<p><code>ENV PATH /env/bin:$PATH</code></p>

<p>Additionally, what does <code>ENV VIRTUAL_ENV /env</code> mean and how it is used?</p>
","<p>I'm attempting to deploy a flask web app using a docker container. I'm dockerizing the app because I have a package (RDKit) which cannot be pip installed and must be done using conda.</p>

<p>When using 
<code>docker run my_image</code></p>

<p>I get an import error for flask - the first library that my python script imports. However, I'm able to deploy the app locally when I specify the port using the command: <code>docker run -p 80:80 my_image</code>. I have a feeling that my Dockerfile might be the problem:</p>

<pre><code>FROM cameroncruz/flask-nginx-uwsgi-miniconda:python3.6

# Create conda env (base image is built on python 3.6 and will have to migrate to 3.7 later)
RUN conda create --name myenv python=3.6

# Install the required dependencies
RUN /bin/bash -c "". activate myenv &amp;&amp; \
    conda config --add channels conda-forge &amp;&amp; \
    conda install -y \
        scikit-learn \
        numpy \
        scipy \
    rdkit \
    wtforms \
    ipython \
        flask \
        uwsgi""

# Copy custom supervisor config
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# Copy application files

# The application to be served
COPY server/main.py /app/main.py

# Folder containing the styling
COPY server/static/ /app/static

# Folder containing the html
COPY server/templates/ /app/templates

# The trained machine learning model
copy server/model.pkl /app/model.pkl

# EXPOSE PORT 80
EXPOSE 80

# Run app.py when the container launches
CMD [""python"", ""main.py""]
</code></pre>

<p>The beginning of my python code:</p>

<pre><code>from flask import Flask, jsonify, request, json
</code></pre>

<p>The end of my python code:</p>

<pre><code>    app.run(debug=False, host='0.0.0.0', port=80)
</code></pre>

<p>Here is the supervisord.conf file:</p>

<pre><code>nodaemon=true

[program:uwsgi]
environment=PATH='/opt/conda/envs/myenv/bin'
command=/opt/conda/envs/myenv/bin/uwsgi --ini /etc/uwsgi/uwsgi.ini --die-on-term --need-app

stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0

[program:nginx]
command=/usr/sbin/nginx
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0
# Graceful stop, see http://nginx.org/en/docs/control.html
stopsignal=QUIT
</code></pre>

<p>I have deployed apps on Azure before but this is my first time with a container. I'm also using Ubuntu 18.04, if that matters. Any and all help is appreciated, thank you!</p>
"
"50294165","What is the best practice to use kubernetes for local development?","<docker><kubernetes><google-cloud-platform>","59783602","Kubernetes best way 4 locally","<docker><kubernetes><docker-compose><containers><google-kubernetes-engine>","<p>I am using docker containers and have docker-compose files for both local development and production environment. I want to try Google Cloud Platform for my new app and specifically Google Kubernetes Engine. My tools is Docker for Mac with Kubernetes on local machine.</p>

<p>It is super important for developers to be able to change code and to see changes live for local development.</p>

<p>Use cases:</p>

<ol>
<li><p>Backend developer make changes to basic Flask API (or whatever you use) and should see changes on reloaded app immediately.</p></li>
<li><p>Frontend developer make changes to HTML layout and should see changes on web page immediately.</p></li>
</ol>

<p>At the moment i am using docker-compose files to mount source code to local containers. But Kubernetes does not support relative paths to mount the source code.</p>

<p>Ideally i should be able to set the variable </p>

<blockquote>
  <p>Deployment.spec.templates.spec.containers.volumes.hostPath</p>
</blockquote>

<p>as relative path to my repo. For example, in our team developers clone repo to this folders:</p>

<blockquote>
  <p>/User/BACKEND_developer/code/project_repo</p>
  
  <p>/User/FRONTEND_developer/code/project_repo</p>
</blockquote>

<p>Obviously you can't commit and build the image after every little change to the source code.</p>

<p>So what is the best practice for local development with Kubernetes? Do i need some additional tools to modify .yaml files for every developer?</p>
","<p>Im new at kubernetes and would like to manage docker on my local servers.
Which ist he best way to use kubernetes locally?
Minikube just support 1 node. There are better ways?</p>
"
"53988649","Docker - SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306","<node.js><docker><express><sequelize.js>","59889986","Cant connect Postgres with NodeJS (Docker Compose)","<node.js><postgresql><docker><docker-compose><sequelize.js>","<p>I'm trying to get my nodejs application up and running using a docker container. I have no clue what might be wrong. The credentials seems to be passed correctly when I debug the credentials with the console. Also firing up sequel pro and connecting directly with the same username and password seems to work. When node starts in the container I get the error message: </p>

<blockquote>
  <p>SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306</p>
</blockquote>

<p>The application itself is loading correctly on port 3000, however no data is retrieved from the database. If have also tried adding the environment variables directly to the docker compose file, but this also doesn't seem to work.</p>

<p>My project code is hosted over here: <a href=""https://github.com/pietheinstrengholt/rssmonster"" rel=""noreferrer"">https://github.com/pietheinstrengholt/rssmonster</a></p>

<p>The following database.js configuration is used. When I add console.log(config) the correct credentials from the .env file are displayed.</p>

<pre><code>require('dotenv').load();

const Sequelize = require('sequelize');
const fs = require('fs');
const path = require('path');
const env = process.env.NODE_ENV || 'development';
const config = require(path.join(__dirname + '/../config/config.js'))[env];

if (config.use_env_variable) {
  var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
  var sequelize = new Sequelize(config.database, config.username, config.password, config);
}

module.exports = sequelize;
</code></pre>

<p>When I do a console.log(config) inside the database.js I get the following output:</p>

<pre><code>{
username: 'rssmonster',
password: 'password',
database: 'rssmonster',
host: 'localhost',
dialect: 'mysql'
}
</code></pre>

<p>Following .env:</p>

<pre><code>DB_HOSTNAME=localhost
DB_PORT=3306
DB_DATABASE=rssmonster
DB_USERNAME=rssmonster
DB_PASSWORD=password
</code></pre>

<p>And the following docker-compose.yml:</p>

<pre><code>version: '2.3'
services:

  app:
    depends_on:
      mysql:
        condition: service_healthy
    build:
      context: ./
      dockerfile: app.dockerfile
    image: rssmonster/app
    ports:
      - 3000:3000
    environment:
      NODE_ENV: development
      PORT: 3000
      DB_USERNAME: rssmonster
      DB_PASSWORD: password
      DB_DATABASE: rssmonster
      DB_HOSTNAME: localhost
    working_dir: /usr/local/rssmonster/server
    env_file:
      - ./server/.env
    links:
      - mysql:mysql

  mysql:
    container_name: mysqldb
    image: mysql:5.7
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_RANDOM_ROOT_PASSWORD: ""yes""
      MYSQL_DATABASE: ""rssmonster""
      MYSQL_USER: ""rssmonster""
      MYSQL_PASSWORD: ""password""
    ports:
      - ""3307:3306""
    volumes:
      - /var/lib/mysql
    restart: unless-stopped
    healthcheck:
      test: [""CMD"", ""mysqladmin"" ,""ping"", ""-h"", ""localhost""]
      timeout: 5s
      retries: 10

volumes:
  dbdata:
</code></pre>

<p>Error output:</p>

<pre><code>{ SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306
app_1    |     at Promise.tap.then.catch.err (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:128:19)
app_1    | From previous event:
app_1    |     at ConnectionManager.connect (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:125:13)
app_1    |     at sequelize.runHooks.then (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:306:50)
app_1    | From previous event:
app_1    |     at ConnectionManager._connect (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:306:8)
app_1    |     at ConnectionManager.getConnection (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:247:46)
app_1    |     at Promise.try (/usr/local/rssmonster/server/node_modules/sequelize/lib/sequelize.js:564:34)
app_1    | From previous event:
app_1    |     at Promise.resolve.retryParameters (/usr/local/rssmonster/server/node_modules/sequelize/lib/sequelize.js:464:64)
app_1    |     at /usr/local/rssmonster/server/node_modules/retry-as-promised/index.js:60:21
app_1    |     at new Promise (&lt;anonymous&gt;)
</code></pre>
","<p>I have the next docker-compose file:</p>

<pre><code>version: ""3""

services:
  proxy:
    build:
      context: ./client
      dockerfile: proxy.Dockerfile
    ports:
      - ""80:80""
  server:
    build:
      context: .
      dockerfile: server.Dockerfile
    ports:
      - ""3000:3000""
    depends_on: [db]
  db:
    image: postgres
    environment:
      POSTGRES_USER: lautidamo
      POSTGRES_PASSWORD: la12papa
      POSTGRES_DB: product-manager
    ports:
      - ""5432:5432""
</code></pre>

<p>And i have this database.ts:</p>

<pre><code>import { Sequelize } from ""sequelize"";

export const sequelize = new Sequelize(
  ""product-manager"",
  ""lautidamo"",
  ""la12papa"",
  {
    dialect: ""postgres"",
    host: ""localhost"",
    port: 5432
  }
);

</code></pre>

<p>The proxy and the server services works fine. But when I make a request to a route where have to connect to the database, i have this error connect ECONNREFUSED 127.0.0.1:5432.</p>
"
"56870478","Cannot start docker container In docker CE on oracle linux","<docker>","59891824","Docker build error OCI runtime create failed","<docker><docker-compose>","<p>Recently I installed Docker CE on my Oracle Linux.<br>
Unfortunately, when I want to start my first container with:</p>

<pre><code>docker run hello-world
</code></pre>

<p>I get this error message:</p>

<blockquote>
  <p>docker: Error response from daemon: OCI runtime create failed:
  container_linux.go:345: starting container process caused
  ""process_linux.go:430: container init caused \""write
  /proc/self/attr/keycreate: permission denied\"""": unknown. ERRO[0000]
  error waiting for container: context canceled</p>
</blockquote>
","<p>I am trying to build an agent on the linux machine using docker. Everything was working fine and i was able to create the agent. But suddenly it was showing offline. When i tried to recreate the agent it was throwing below error:-</p>

<pre><code>ERROR: for build-agent_dl-build-agent_1  Cannot start service dl-build-agent: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown

ERROR: for dl-build-agent  Cannot start service dl-build-agent: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown
ERROR: Encountered errors while bringing up the project.
</code></pre>

<p>even if i am trying to run a simple docker command  'docker run hello-world' i am getting below error,</p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown.
ERRO[0000] error waiting for container: context cancelled
</code></pre>

<p>Any suggestions?</p>
"
"53045112","Explore Docker's image files if container exits immediately?","<docker><dockerfile>","59968631","Inspect docker container at container startup","<docker><containers><inspect>","<p>I have a Docker image that I pulled from Docker hub.</p>

<p>When I run <code>docker run image_name</code>, container exits immediately.</p>

<p>I don't have access to the source code of the Docker image, including Dockerfile. All I have is an image I pulled from <code>hub.docker.com</code>.</p>

<p>I need to debug/see what is inside the image (e.g. see and explore image's filesystem), without running it as a container.</p>

<p>Is it possible?</p>
","<p>I downloaded an image which immediately stops. How can I inspect it (or any container spawned from it)?</p>

<p>I cannot use anything like <code>docker exec -it CONTAINER_ID bash</code> since I don't have time to get the <code>CONTAINER_ID</code>.</p>

<p>(<code>docker run -it 5413e661e579 bash</code> does not help, it starts the container and stops immediatly.)</p>

<p>I don't know how the image was built, I don't have the Dockerfile ; the only thing I know is the entrypoint: <code>[""python"" ""app.py""]</code> but it does not output anything useful.</p>

<p><strong>Answer <a href=""https://stackoverflow.com/a/53045181/719276"">from duplicate question</a>:</strong></p>

<p><code>docker run -it --entrypoint ""/bin/bash"" image_name</code></p>
"
"56046148","Is there a way to enforce inter-module dependencies/initialization order?","<docker><azure-iot-edge><docker-engine>","60034001","Is it possible to start the various iot-edge modules one by one sequentially? If so, how to do it?","<azure><azure-iot-hub><azure-iot-edge><azure-iot-sdk>","<p>Using Azure IoT Edge, I have not found any way to guarantee the initialization order of containers/modules in a deployment. Suppose for example, I have 2 modules, A and B. A is a server and B is a client that depends on A. As far as I know, there's no way to guarantee that A starts up before B does.</p>

<p>The Azure IoT Edge deployment templates conform to the Docker Engine API, and I could not find any way to enforce dependencies through that API. As a workaround, I make no assumptions about which containers are running in each container's code. This works, although the overhead of additional code is not ideal, especially considering a tool like docker-compose would make enforcing initialization order rather trivial.</p>

<p>I want to do something like this (src: <a href=""https://docs.docker.com/compose/compose-file/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/</a>):</p>

<pre><code>version: ""3.7""
services:
  web:
    build: .
    depends_on:
      - db
      - redis
  redis:
    image: redis
  db:
    image: postgres
</code></pre>

<p>As a workaround, and following the example above, in the <code>web</code> container I've been doing things like the following to ensure <code>postgres</code> is up and running before <code>web</code> performs <code>postgres</code> dependent actions:</p>

<pre><code>postgresIsUp = False
while not postgresIsUp:
    try:
        pingPostgres()
        postgresIsUp = True
    except PingError:
        print(""postgres is not yet running"")
</code></pre>

<p>This is, of course, a contrived example with obvious flaws, but it demonstrates the gist of the workaround.</p>
","<p>Is it possible to start the various iot-edge modules one by one sequentially? If so, how to do it?</p>
"
"53591417","Kubernetes: kubectl apply does not update pods when using ""latest"" tag","<docker><kubernetes>","60045265","How to make my kubernetes to take my latest docker image?","<kubernetes>","<p>I'm using <code>kubectl apply</code> to update my Kubernetes pods:</p>

<pre><code>kubectl apply -f /my-app/service.yaml
kubectl apply -f /my-app/deployment.yaml
</code></pre>

<p>Below is my service.yaml:</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: my-app
  labels:
    run: my-app
spec:
  type: NodePort
  selector:
    run: my-app 
  ports:
  - protocol: TCP
    port: 9000
    nodePort: 30769
</code></pre>

<p>Below is my deployment.yaml:</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:  
  selector:
    matchLabels:
      run: my-app
  replicas: 2
  template:
    metadata:
      labels:
        run: my-app
    spec:
      containers:
      - name: my-app
        image: dockerhubaccount/my-app-img:latest
        ports:
        - containerPort: 9000
          protocol: TCP
      imagePullSecrets:
      - name: my-app-img-credentials
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
</code></pre>

<p>This works fine the first time, but on subsequent runs, my pods are not getting updated.</p>

<p>I have read the suggested workaround at <a href=""https://github.com/kubernetes/kubernetes/issues/33664"" rel=""noreferrer"">https://github.com/kubernetes/kubernetes/issues/33664</a> which is:</p>

<pre><code>kubectl patch deployment my-app -p ""{\""spec\"":{\""template\"":{\""metadata\"":{\""labels\"":{\""date\"":\""`date +'%s'`\""}}}}}""
</code></pre>

<p>I was able to run the above command, but it did not resolve the issue for me.</p>

<p>I know that I can trigger pod updates by manually changing the image tag from ""latest"" to another tag, but I want to make sure I get the latest image without having to check Docker Hub.</p>

<p>Any help would be greatly appreciated.</p>
","<p>Problem</p>

<p>A frequent question that comes up on Slack and Stack Overflow is how to trigger an update to a Deployment/RS/RC when the image tag hasn't changed but the underlying image has.</p>

<p>Consider:</p>

<ol>
<li><p>There is an existing Deployment with image foo: latest</p></li>
<li><p>The user builds a new image foo: latest</p></li>
<li><p>The user pushes foo: latest to their registry</p></li>
<li><p>The user wants to do something here to tell the Deployment to pull the new image and do a rolling-update of existing pods</p></li>
</ol>

<p>For Example: </p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata: 
  name: worker-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: worker
  template:
    metadata:
      labels:
        component: worker
    spec:
      containers:
        - name: worker
          image: stephengrider/multi-worker
          env:
            - name: REDIS_HOST
              value: redis-cluster-ip-service
            - name: REDIS_PORT
              value: ""6379""
</code></pre>

<p>And now I have made changes in my multiWorker image</p>

<p>and redeploys the deployment, but it doesn't fetch the latest image?</p>
"
"58880998","Communications link failure , Spring Boot + MySql +Docker + Hibernate","<mysql><spring><spring-boot><docker><docker-compose>","60109913","Docker compose error on mysql db connect for spring boot app Caused by: java.net.UnknownHostException:","<spring-boot><docker><docker-compose>","<p>I'm working with spring boot, hibernate &amp; MySql. While running the application it is running well as per expectation . But while making the docker-compose file and running the app docker image with mysql docker image it gives this error.</p>

<blockquote>
  <p><strong>Error</strong>
  com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
  java.net.ConnectException: Connection refused.</p>
</blockquote>

<pre><code>private Connection createConnection() throws SQLException 
{
        DriverManager.registerDriver(new com.mysql.jdbc.Driver());
        String mysqlUrl = ""jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false"";
        Connection connection = DriverManager.getConnection(mysqlUrl, ""root"", ""root"");
        return connection;
}
</code></pre>

<blockquote>
  <p><strong>Application.properties</strong></p>
  
  <p>spring.datasource.url=jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false
  spring.datasource.username=root </p>
  
  <p>spring.datasource.password=root</p>
</blockquote>

<p>Please guide me how to tackle this.</p>

<pre><code>**docker-compose.yml**

version: '3'

services:
  docker-mysql:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=database
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
    ports:
      - 3307:3306

  app:
    image: app:latest
    ports:
       - 8091:8091
    depends_on:
       - docker-mysql
</code></pre>
","<p>I have spring boot app</p>

<p>My Dockerfile is</p>

<pre><code>FROM openjdk:8-jdk-alpine
EXPOSE 8080
ARG JAR_FILE=target/demo-0.0.1-SNAPSHOT.jar
ADD ${JAR_FILE} demo.jar
ENTRYPOINT [""java"",""-jar"",""/demo.jar""]


My docker compose file
# Docker Compose file Reference (https://docs.docker.com/compose/compose-file/)

version: '3.7'

# Define services
services:
  # App backend service
  app-server:
    # Configuration for building the docker image for the backend service
    build:
      context: . # Use an image built from the specified dockerfile in the `polling-app-server` directory.
      dockerfile: ./Dockerfile
    container_name: empserver
    ports:
      - ""3000:3000"" # Forward the exposed port 8080 on the container to port 8080 on the host machine
    restart: always
    depends_on: 
      - db # This service depends on mysql. Start that first.
    environment: # Pass environment variables to the service
      SPRING_DATASOURCE_URL: jdbc:mysql://db:3306/employee_entries?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_PASSWORD: root     

  # Database Service (Mysql)
  db:
    image: mysql:5.7
    ports:
      - ""3306:3306""
    restart: always
    environment:
      MYSQL_DATABASE: employee_entries
      MYSQL_USER: root
      MYSQL_PASSWORD: root
      MYSQL_ROOT_PASSWORD: root
</code></pre>

<p>My docker net works</p>

<pre><code>NETWORK ID          NAME                DRIVER              SCOPE
b95e3d99b266        Default Switch      ics                 local
7fff4f9713f8        demo_default        nat                 local
fe8883b77d1d        emp-mysql           ics                 local
f464aab9064a        nat                 nat                 local
a5bd5e8efe61        none                null                local
</code></pre>

<p>App is successfully running using java -jar target\demo-0.0.1-SNAPSHOT.jar</p>

<p>but when I am doing docker-compose up</p>

<p>I got below error</p>

<pre><code>app-server_1  | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
app-server_1  |
app-server_1  | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app-server_1  |         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_212]
app-server_1  |         at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_212]
app-server_1  |         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_212]
app-server_1  |         at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_212]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.NativeSession.connect(NativeSession.java:144) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         ... 56 common frames omitted
app-server_1  | Caused by: java.net.UnknownHostException: db
app-server_1  |         at java.net.InetAddress.getAllByName0(InetAddress.java:1281) ~[na:1.8.0_212]
app-server_1  |         at java.net.InetAddress.getAllByName(InetAddress.java:1193) ~[na:1.8.0_212]
app-server_1  |         at java.net.InetAddress.getAllByName(InetAddress.java:1127) ~[na:1.8.0_212]
app-server_1  |         at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:132) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         ... 59 common frames omitted
</code></pre>

<p>I am able to access mysql database and tables but from docker compose it was not</p>

<p>any suggestion would really be helpful</p>
"
"60126842","Docker: The command returned a non-zero code: 137","<python><django><docker><pip><dockerfile>","60117479","How to run a python script based on pip installation in docker?","<python><django><docker><dockerfile>","<p>My docker file is as follows:</p>
<pre><code>#Use python 3.6 image
FROM python:3.6
ENV PYTHONUNBUFFERED 1

#install required packages
RUN apt-get update
RUN apt-get install libsasl2-dev libldap2-dev libssl-dev python3-dev psmisc -y

#install a pip package
#Note: This pip package has a completely configured django project in it
RUN pip install &lt;pip-package&gt;

#Run a script
#Note: Here appmanage.py is a file inside the pip installed location(site-packages), but it will be accessible directly without cd to the folder
RUN appmanage.py appconfig appadd.json

#The &lt;pip-packge&gt; installed comes with a built in django package, so running it with following CMD
#Note: Here manage.py is present inside the pip package folder but it is accesible directly
CMD [&quot;manage.py&quot;,&quot;runserver&quot;,&quot;0.0.0.0:8000&quot;]
</code></pre>
<p>When i run :</p>
<pre><code>sudo docker build -t test-app .
</code></pre>
<p>The steps in dockerfile till: <code>RUN appmanage.py appconfig </code> runs sucessfully as expected but after that i get the error:</p>
<pre><code>The command '/bin/sh -c appmanage.py appconfig ' returned a non-zero code: 137
</code></pre>
<p>When i google for the error i get suggestions like memory is not sufficient. But i have verified, the system(centos) is having enough memory.</p>
<h1>Additional info</h1>
<p>The commandline output during the execution of <code>RUN appmanage.py appconfig</code> is :</p>
<pre><code>Step 7/8 : RUN appmanage.py appconfig
 ---&gt; Running in 23cffaacc81f

======================================================================================
configuring katana apps...
 Please do not quit (or) kill the server manually, wait until the server closes itself...!
======================================================================================
Performing system checks...

System check identified no issues (0 silenced).
February 08, 2020 - 12:01:45
Django version 2.1.2, using settings 'katana.wui.settings'
Starting development server at http://127.0.0.1:9999/
Quit the server with CONTROL-C.
9999/tcp:
    20Killed
</code></pre>
","<p>My docker file design is as follows:</p>
<pre><code>#Use python 3.6 image
FROM python:3.6
ENV PYTHONUNBUFFERED 1

#install required packages
RUN apt-get update
RUN apt-get install libsasl2-dev libldap2-dev libssl-dev python3-dev psmisc -y

#install a pip package
#Note: This pip package has a completely configured django project in it
RUN pip install &lt;pip-packge&gt;

#add a configuration file required for setup
ADD appAdd.json /

#Run a script
#Note: Here appmanage.py is a file inside the pip installed location, but it will be accesible directly without cd to the folder
RUN appmanage.py appAdd.json

#The &lt;pip-packge&gt; installed comes with a built in django package, so running it with following CMD
#Note: Here manage.py is present inside the pip package folder but it is accesible directly
CMD [&quot;manage.py&quot;,&quot;runserver&quot;,&quot;0.0.0.0:8000&quot;]
</code></pre>
<p>When i run :</p>
<pre><code>sudo docker build -t test-app .
</code></pre>
<p>The python script running part will be successful in terms of functionality but the image is not getting created because at this point it gets exited with following error:</p>
<pre><code>The command '/bin/sh -c appmanage.py appAdd.json' returned a non-zero code: 137
</code></pre>
<p>Is it treating it as shell script rather than python script. How can i overcome this and run the django project sucessfully?</p>
<p>Note: In local environment I could execute the steps in my machine and successfully setup.So no issues with the code of the django project which comes with pip package</p>
<h1>Update</h1>
<p>The script appmanage.py runs the django project in a port 9999 and performs some tests and kills the port 9999. Is the kill operation in the script causing the error(137) as mentioned above?</p>
"
"51140055","Cannot access web server using docker","<docker>","60147804","Nuxt and Docker: specifying server in nuxt.config.js - even to default - prevents access on localhost?","<node.js><docker><nuxt.js>","<p>I used the following command to run the container :</p>

<pre><code>docker run -p 3333:3333 -d maill/node-web-app
</code></pre>

<p>Here is the result of docker ps :</p>

<pre><code>CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS                    NAMES
f26270107bfa        maill/node-web-app   ""npm run dev""       49 seconds ago      Up 46 seconds       0.0.0.0:3000-&gt;3000/tcp   musing_fermi
</code></pre>

<p>However when I try to access webserver on host using localhost:3333  it doesn't work.</p>

<p>I am using windows 10 pro.</p>

<p><code>docker logs musing_fermi</code> shows:</p>

<pre><code>DONE Compiled successfully in 3541ms16:04:50 | OPEN localhost:3000
</code></pre>

<p>Dockerfile :</p>

<pre><code>FROM node:8
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm i
COPY . .
EXPOSE 3333
CMD [ ""npm"", ""run"", ""dev"" ]
</code></pre>

<p>package.json :</p>

<pre><code>{
  ""name"": ""webapp-pst-horizon"",
  ""version"": ""1.0.0"",
  ""description"": ""Webapp pour les formations enedis"",
  ""author"": ""Léo Coletta"",
  ""private"": true,
  ""scripts"": {
    ""dev"": ""cross-env HOST=0.0.0.0 PORT=3333 nuxt"",
    ""build"": ""nuxt build"",
    ""start"": ""nuxt start"",
    ""generate"": ""nuxt generate"",
    ""lint"": ""eslint --ext .js,.vue --ignore-path .gitignore ."",
    ""precommit"": ""npm run lint""
  },
  ""dependencies"": {
    ""@nuxtjs/axios"": ""^5.3.1"",
    ""@nuxtjs/proxy"": ""^1.2.4"",
    ""axios"": ""^0.18.0"",
    ""babel-polyfill"": ""^6.26.0"",
    ""cookie"": ""^0.3.1"",
    ""js-cookie"": ""^2.2.0"",
    ""nuxt"": ""^1.4.1"",
    ""vuetify"": ""^1.0.19"",
    ""webpack"": ""^3.1.0""
  },
  ""devDependencies"": {
    ""babel-eslint"": ""^8.2.3"",
    ""cross-env"": ""^5.2.0"",
    ""eslint"": ""^4.9.0"",
    ""eslint-config-airbnb-base"": ""^12.1.0"",
    ""eslint-loader"": ""^2.0.0"",
    ""eslint-plugin-import"": ""^2.7.0"",
    ""eslint-plugin-vue"": ""^4.5.0"",
    ""stylus"": ""^0.54.5"",
    ""stylus-loader"": ""^3.0.2""
  }
}
</code></pre>
","<p>In essence the project was setup as follows:</p>

<pre><code>mkdir project
cd project
npx create-nuxt-app frontend
cd frontend
touch Dockerfile.development
cd ..
touch docker-compose.web.development.yml
</code></pre>

<p>In <code>frontend/nuxt.config.js</code> the following was updated:</p>

<pre class=""lang-js prettyprint-override""><code>const isDevelopment = process.env.NODE_ENV !== 'PRODUCTION'

export default {
  // ... 
  server: {
    host: isDevelopment ? 'localhost' : '0.0.0.0', // default: localhost
    port: isDevelopment ? 3000 : 80, // default: 3000
  },
  // ...
}
</code></pre>

<p>In <code>frontend/Dockerfile.development</code>:</p>

<pre><code>FROM node:10.15
ENV APP_ROOT /src

RUN mkdir ${APP_ROOT}
WORKDIR ${APP_ROOT}
COPY ./frontend/package.json .
RUN npm install
</code></pre>

<p>and in <code>./docker-compose.web.development.yml</code></p>

<pre><code>version: '3'

services:
  nuxt:
    image: frontend
    container_name: nuxt_fe
    build:
      context: .
      dockerfile: ./frontend/Dockerfile.development
    restart: always
    environment:
      - HOST
    ports:
      - ""3000:3000""
    command: ""npm run dev""
    volumes:
      - ./frontend/assets:/src/assests
      # and then the rest of the nuxt directories
      # if I just map ./frontend:/src it will overwrite node_modules installed during build
</code></pre>

<p>If I in <code>nuxt.config.js</code> comment out the <code>server</code> configuration (even though it is set to default values), when I go to <code>localhost:3000</code> I see the site. If I uncomment <code>server</code> in <code>nuxt.config.js</code>, I see nothing.</p>

<p>Thought?</p>
"
"60326148","How to create table postgresql when start by docker compose","<postgresql><docker><docker-compose>","60326200","How to create table postgresql when start by docker compose (given answer doesn't work)","<postgresql><docker><docker-compose>","<p>I know this question is already asked and also answer given. But it is not working for me. I follow the same. My postgres container is running fine. I checked inside the container that /docker-entrypoint-initdb.d/init.sql exist.I used the following docker-compose.yml.</p>

<pre><code>version: ""3""
services:
  postgres:
    image: postgres:latest
    network_mode: bridge
    container_name: postgres
    expose:
    - 5432
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
         - POSTGRES_PASSWORD=admin
         - POSTGRES_USER=postgres
         - POSTGRES_DB=dev
    restart: unless-stopped

# APP
  profile_editor2:
    build:
      context: .
      dockerfile: ./Dockerfile
    network_mode: bridge
    container_name: profile_editor2
    volumes:
      - ./image:/app/image
    expose:
      - 8080
    ports:
      - 8080:8080
    restart: unless-stopped
    depends_on:
      - postgres
    links:
      - postgres
volumes:
  postgres-data:
</code></pre>

<p>init.sql:-</p>

<pre><code>  create table sometable(id int);   
</code></pre>

<p>No table created. I can see only the database is created. How do I create a table and also if possible insert some data into the table?       </p>
","<p>I know this question is already asked <a href=""https://stackoverflow.com/questions/33309121/using-docker-compose-to-create-tables-in-postgresql-database"">here</a> and also answer given. But it is not working for me. I follow the same. My postgres container is running fine. I checked inside the container that /docker-entrypoint-initdb.d/init.sql exist.I used the following docker-compose.yml.</p>

<pre><code>version: ""3""
services:
  postgres:
    image: postgres:latest
    network_mode: bridge
    container_name: postgres
    expose:
    - 5432
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
         - POSTGRES_PASSWORD=admin
         - POSTGRES_USER=postgres
         - POSTGRES_DB=dev
    restart: unless-stopped

# APP
  profile_editor2:
    build:
      context: .
      dockerfile: ./Dockerfile
    network_mode: bridge
    container_name: profile_editor2
    volumes:
      - ./image:/app/image
    expose:
      - 8080
    ports:
      - 8080:8080
    restart: unless-stopped
    depends_on:
      - postgres
    links:
      - postgres
volumes:
  postgres-data:
</code></pre>

<p>init.sql:-</p>

<pre><code>  create table sometable(id int);   
</code></pre>

<p>No table created. I can see only the database is created. How do I create a table and also if possible insert some data into the table?       </p>
"
"53460628","Adding a file to docker image results in ""permission denied"" error","<docker>","60339290","OCI runtime create failed: container_linux.go:345","<docker><go><github><google-kubernetes-engine><github-actions>","<p>I am creating a docker golang image, but my golang app needs to read a config.yaml on start. I tried to add the file as shown in the dockerfile below:</p>

<pre><code>FROM golang:alpine as builder
# Install git + SSL ca certificates
RUN apk update &amp;&amp; apk add git &amp;&amp; apk add ca-certificates
# Create appuser
COPY . $GOPATH/src/github.com/user/app/
WORKDIR $GOPATH/src/github.com/user/app/
#get dependancies
RUN go get -d -v
#build the binary
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o /go/bin/app
# STEP 2 build a small image
# start from scratch
FROM scratch
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
ADD ./config.yaml /go/bin/app/
# Copy our static executable
COPY --from=builder /go/bin/app /go/bin/app
EXPOSE 3000
ENTRYPOINT [""/go/bin/app""]
</code></pre>

<p>But I get the following error:</p>

<blockquote>
  <p>docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""/go/bin/app\"": permission denied"": unknown.</p>
</blockquote>
","<p>Currently we are migrating from GitLab to GitHub and we've decided to move the CI/CD process to GitHub actions. The pipeline process works like a charm but when GKE tries to spin up the newly pushed image it gives back this error:</p>

<p><code>'OCI runtime create failed: container_linux.go:345: starting container process caused ""exec: \""/socket-server\"": permission denied"": unknown'</code></p>

<p>It's important to note here that this whole process was working on GitLab. Anyway, the GitHub workflow yaml file looks like this:</p>

<pre><code>name: Build and deploy
on:
  - push
  - pull_request

jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    steps:
      - uses: fusion-engineering/setup-git-credentials@v2
        with:
          credentials: https://${{ secrets.MACHINE_ACCOUNT_ACCESS_TOKEN }}:x-oauth-basic@github.com/

      - name: Setup environment
        shell: bash
        run: |
          echo ""::set-env name=GOPATH::${{ github.workspace }}/go""
          echo ""::add-path::${{ github.workspace }}/go/bin""

      - name: Install Go
        uses: actions/setup-go@v1
        with:
          go-version: 1.12.4

      - name: Checkout code
        uses: actions/checkout@v2
        with:
          path: go/src/github.com/${{ github.repository }}

      - name: Prepare environment
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make prepare

      - name: Format code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make fmt &amp;&amp; git diff --exit-code

      - name: Lint code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make lint

      - name: Vet code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make vet

      - name: Test code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make cover

      - name: Build code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make build

      - name: Upload artifact
        uses: actions/upload-artifact@v1
        with:
          name: socket-server
          path: go/src/github.com/${{ github.repository }}/socket-server

  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [build]
    if: contains(github.ref, 'refs/tags')

    steps:
      - name: Set release version
        run: echo ::set-env name=CI_COMMIT_TAG::${GITHUB_REF/refs\/tags\//}

      - name: Checkout code
        uses: actions/checkout@v2

      - name: Get artifact from build step
        uses: actions/download-artifact@v1
        with:
          name: socket-server

      - name: Set ci auth
        run: echo ::set-env name=CI_AUTH::$(cat ci_auth.json | base64)

      - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master
        with:
          service_account_key: ${{ env.CI_AUTH }}

      - name: Configure gcloud docker authentication
        run: |
          gcloud config set project foo
          gcloud auth configure-docker

      - name: Build, push and deploy container
        run: |
          bash deploy.sh
</code></pre>

<p>The deploy.sh file looks like this:</p>

<pre><code>#!/bin/bash

if [[ -z ""${CI_COMMIT_TAG}"" ]]; then
    echo ""CI_COMMIT_TAG is empty, this stage should not run""
    exit 0
fi

export ENV=""stage""

if [[ ""$CI_COMMIT_TAG"" != ""${CI_COMMIT_TAG%-release}"" ]]; then
    export ENV=""prod""
fi

echo ""Current environment: $ENV""

make deploy
</code></pre>

<p>The deploy step in the Makefile looks like this:</p>

<pre><code>deploy:
    ( echo ""cat &lt;&lt;EOF"" ; cat k8s.yml.template; ) | sh &gt; k8s-${ENV}.yml
    docker build --no-cache \
    --build-arg RELEASE=${CI_COMMIT_TAG} \
    --build-arg ENV=${ENV} \
    -t gcr.io/foo/socket-server:${CI_COMMIT_TAG} .
    docker push gcr.io/foo/socket-server:${CI_COMMIT_TAG}
    gcloud container clusters get-credentials api-${ENV} --zone=europe-west1-b
    kubectl apply -f k8s-${ENV}.yml
</code></pre>

<p>And the Dockerfile looks like this:</p>

<pre><code>FROM alpine:latest as certs
RUN apk --update add ca-certificates

FROM scratch
COPY --from=certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt

ARG RELEASE
ARG ENV

ADD ./socket-server /socket-server
ADD ./config.yml /config.yml
ADD ./dbconfig.yml /dbconfig.yml
ADD ./migrations /migrations

ENV SOCKET_SERVER_SENTRY_DSN https://foo@sentry.io/bar
ENV SOCKET_SERVER_SENTRY_RELEASE $RELEASE
ENV SOCKET_SERVER_SENTRY_ENVIRONMENT $ENV

CMD [""/socket-server"", ""--port"", ""9345"", ""--host"", """"]
</code></pre>

<p>I have already tried to <code>chmod +x socket-server</code> on the pipeline and also in the Dockerfile (<a href=""https://stackoverflow.com/questions/53460628/adding-a-file-to-docker-image-results-in-permission-denied-error"">as suggested here</a>). When I do it in the Dockerfile it fails with the following error:</p>

<pre><code>Step 14/15 : RUN chmod +x socket-server
 ---&gt; Running in 9c66aef0c35b
OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""/bin/sh\"": stat /bin/sh: no such file or directory"": unknown
</code></pre>

<p>Am I missing something on the GitHub workflow or is there anybody who is seeing something that I don't? Some help is appreciated!</p>
"
"50278632","What does localhost means inside a Docker container?","<linux><docker>","60362455","Why two containers on the same docker machine can't communicate through localhost?","<windows><docker><docker-for-windows><docker-toolbox>","<p>Say, if I use this command inside a docker container.</p>

<pre><code>/opt/lampp/bin/mysql -h localhost -u root -pThePassword
</code></pre>

<p>What would the localhost here refer to? The host machine's IP or the docker container's own IP? </p>
","<p>Getting started with Docker I do not understand why two containers on the same docker machine can't communicate through <strong>localhost</strong>.</p>

<p>I read about this SO post already : <a href=""https://stackoverflow.com/questions/29143903/how-to-connect-two-docker-containers-through-localhost"">how-to-connect-two-docker-containers-through-localhost</a> but could not get a clear answer.</p>

<p>My use case, I am on Windows Home using Docker Toolbox (virtualbox) having the current docker machine :</p>

<pre><code>NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER     ERRORS
default   *        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.5
</code></pre>

<p>I am launching 3 services, one <strong>react</strong> application, one <strong>spring boot</strong> application, and one <strong>postgreSQL</strong> database. </p>

<p>In my understanding each one will be hosted on <strong>192.168.99.100</strong> on the port I specified (respectively 3000, 8080, 5432). So each one in their context should have <strong>localhost</strong> refering to <strong>192.168.99.100</strong>. </p>

<p>I know it is not working like that because of <strong>CORS</strong> issues that I only get when launching the app environment with docker.</p>

<p>Why is that ? Can someone explain ?</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","60459990","phantom jvm memory usage, native allocations, and sizing k8s pods","<java><jvm><kubernetes-pod>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I have a Spring application (<code>openjdk 11.0.6</code>) running on a linux docker image (<code>openjdk:11-jre-slim</code>) deployed to k8s. I started investigating the app's memory usage so that I can size memory <code>request</code> and <code>limit</code> values for the pod, and noticed some strange memory usage. Highlights are as follows:</p>

<ul>
<li>4GB node</li>
<li>1GB pod memory limit</li>
</ul>

<pre><code>ps -o pid,user,%mem,command ax | sort -b -k3 -r
    1 root     20.4 java ...
 2565 root      0.0 sort -b -k3 -r
 2564 root      0.0 ps -o pid,user,%mem,command ax
 2388 root      0.0 bash
  102 root      0.0 bash
  PID USER     %MEM COMMAND
</code></pre>

<pre><code>jcmd 1 VM.native_memory
Native Memory Tracking:

Total: reserved=1933717KB, committed=565453KB
-                 Java Heap (reserved=245760KB, committed=223124KB)
                            (mmap: reserved=245760KB, committed=223124KB) 

-                     Class (reserved=1169540KB, committed=136324KB)
                            (classes #25495)
                            (  instance classes #23876, array classes #1619)
                            (malloc=4228KB #78404) 
                            (mmap: reserved=1165312KB, committed=132096KB) 
                            (  Metadata:   )
                            (    reserved=116736KB, committed=115200KB)
                            (    used=112833KB)
                            (    free=2367KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=16896KB)
                            (    used=15552KB)
                            (    free=1344KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=166301KB, committed=25601KB)
                            (thread #161)
                            (stack: reserved=165500KB, committed=24800KB)
                            (malloc=579KB #809) 
                            (arena=222KB #321)

-                      Code (reserved=254614KB, committed=82974KB)
                            (malloc=6926KB #21050) 
                            (mmap: reserved=247688KB, committed=76048KB) 

-                        GC (reserved=1830KB, committed=1758KB)
                            (malloc=1022KB #3092) 
                            (mmap: reserved=808KB, committed=736KB) 

-                  Compiler (reserved=2121KB, committed=2121KB)
                            (malloc=1991KB #2720) 
                            (arena=131KB #5)

-                  Internal (reserved=33603KB, committed=33603KB)
                            (malloc=33563KB #122858) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=855KB, committed=855KB)
                            (malloc=855KB #109) 

-                    Symbol (reserved=25799KB, committed=25799KB)
                            (malloc=23073KB #310022) 
                            (arena=2725KB #1)

-    Native Memory Tracking (reserved=9062KB, committed=9062KB)
                            (malloc=447KB #6338) 
                            (tracking overhead=8616KB)

-        Shared class space (reserved=17084KB, committed=17084KB)
                            (mmap: reserved=17084KB, committed=17084KB) 

-               Arena Chunk (reserved=6353KB, committed=6353KB)
                            (malloc=6353KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #184) 

-                 Arguments (reserved=18KB, committed=18KB)
                            (malloc=18KB #481) 

-                    Module (reserved=773KB, committed=773KB)
                            (malloc=773KB #4712) 
</code></pre>

<pre><code>kubectl top pod app
NAME                               CPU(cores)   MEMORY(bytes)   
app-XXXXXXX                        661m         827Mi    
</code></pre>

<p><strong>To sum up</strong>:</p>

<ul>
<li>k8s reports pod memory usage of <code>827Mi</code></li>
<li><code>jcmd</code> reports total memory usage of ~ <code>560MB</code></li>
<li><code>ps</code> reports 20.4% of system. As I understand, that's a percent of the node's memory so ~ <code>796MB</code></li>
</ul>

<p>So I have over 200MB of memory usage reported by <code>ps</code> which <code>jcmd</code> doesn't account for. What am I missing? Does the JVM allocate memory even <code>jcmd</code> can't track? As mentioned, my goal is to determine what limits to set on my pods (I'd rather not have them killed under normal usage).</p>

<p>Thanks for taking the time!</p>
"
"52193937","pip install mysqlclient fails on Ubuntu with python 2.7 on docker","<python><docker><pip>","60542809","Getting error messages installing packages using pip and pipenv on Debian Buster","<pip><python-3.7><pipenv><psutil><debian-buster>","<p>When I ran the docker file code below</p>

<pre><code>RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        ca-certificates \
        vim \
        git \
        python \
        python-pip \
        curl \
        wget \
        lsof \
        libmysqlclient-dev \
        python-dev \
        &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

#PYTHON requirements

ENV HTTPS_PROXY ""https://gec-proxy-svr.homeoffice.wal-mart.com:8080/""

RUN python --version
RUN pip install pip --upgrade
RUN pip install  setuptools
RUN pip install setuptools --upgrade
RUN pip install git+https://github.com/PyMySQL/mysqlclient-python.git
RUN pip install -r requirements.txt
</code></pre>

<p>I got this error    </p>

<pre><code>Running setup.py install for mysqlclient: finished with status 'error'
        Complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-req-build-LIEfN7/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-OFwDUp/install-record.txt --single-version-externally-managed --compile:
        running install
        running build
        running build_py
        creating build
        creating build/lib.linux-x86_64-2.7
        copying _mysql_exceptions.py -&gt; build/lib.linux-x86_64-2.7
        creating build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/__init__.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/compat.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/connections.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/converters.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/cursors.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/release.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        copying MySQLdb/times.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb
        creating build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/__init__.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/CLIENT.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/CR.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/ER.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/FLAG.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        copying MySQLdb/constants/REFRESH.py -&gt; build/lib.linux-x86_64-2.7/MySQLdb/constants
        running build_ext
        building '_mysql' extension
        creating build/temp.linux-x86_64-2.7
        x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 -I/usr/include/mysql -I/usr/include/python2.7 -c _mysql.c -o build/temp.linux-x86_64-2.7/_mysql.o
        unable to execute 'x86_64-linux-gnu-gcc': No such file or directory
        error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

        ----------------------------------------
    Command ""/usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-req-build-LIEfN7/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-OFwDUp/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-req-build-LIEfN7/
</code></pre>

<p>I tried installing  libmysqlclient-dev and python-dev through apt-get update but it does not seem to address the issue. </p>
","<p>I am trying to install gphotos-sync via pipenv. Unfortunately I am getting errors when pipenv tries to install dependencies</p>

<pre><code>Installing gphotos-sync…
✔ Installation Succeeded
Installing dependencies from Pipfile.lock (f27b09)…
An error occurred while installing psutil==5.7.0 --hash=sha256:1413f4158eb50e110777c4f15d7c759521703bd6beb58926f1d562da40180058 --hash=sha256:298af2f14b635c3c7118fd9183843f4e73e681bb6f01e12284d4d70d48a60953 --hash=sha256:60b86f327c198561f101a92be1995f9ae0399736b6eced8f24af41ec64fb88d4 --hash=sha256:685ec16ca14d079455892f25bd124df26ff9137664af445563c1bd36629b5e0e --hash=sha256:73f35ab66c6c7a9ce82ba44b1e9b1050be2a80cd4dcc3352cc108656b115c74f --hash=sha256:75e22717d4dbc7ca529ec5063000b2b294fc9a367f9c9ede1f65846c7955fd38 --hash=sha256:a02f4ac50d4a23253b68233b07e7cdb567bd025b982d5cf0ee78296990c22d9e --hash=sha256:d008ddc00c6906ec80040d26dc2d3e3962109e40ad07fd8a12d0284ce5e0e4f8 --hash=sha256:d84029b190c8a66a946e28b4d3934d2ca1528ec94764b180f7d6ea57b0e75e26 --hash=sha256:e2d0c5b07c6fe5a87fa27b7855017edb0d52ee73b71e6ee368fae268605cc3f5 --hash=sha256:f344ca230dd8e8d5eee16827596f1c22ec0876127c28e800d7ae20ed44c4b310! Will try again.
  🐍   ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 14/14 — 00:00:15
Installing initially failed dependencies…
[pipenv.exceptions.InstallError]:   File ""/usr/local/lib/python3.7/dist-packages/pipenv/core.py"", line 1992, in do_install
[pipenv.exceptions.InstallError]:       skip_lock=skip_lock,
[pipenv.exceptions.InstallError]:   File ""/usr/local/lib/python3.7/dist-packages/pipenv/core.py"", line 1253, in do_init
[pipenv.exceptions.InstallError]:       pypi_mirror=pypi_mirror,
[pipenv.exceptions.InstallError]:   File ""/usr/local/lib/python3.7/dist-packages/pipenv/core.py"", line 859, in do_install_dependencies
[pipenv.exceptions.InstallError]:       retry_list, procs, failed_deps_queue, requirements_dir, **install_kwargs
[pipenv.exceptions.InstallError]:   File ""/usr/local/lib/python3.7/dist-packages/pipenv/core.py"", line 763, in batch_install
[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)
[pipenv.exceptions.InstallError]:   File ""/usr/local/lib/python3.7/dist-packages/pipenv/core.py"", line 681, in _cleanup_procs
[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)
[pipenv.exceptions.InstallError]: ['Collecting psutil==5.7.0', '  Using cached psutil-5.7.0.tar.gz (449 kB)', 'Building wheels for collected packages: psutil', '  Building wheel for psutil (setup.py): started', ""  Building wheel for psutil (setup.py): finished with status 'error'"", '  Running setup.py clean for psutil', 'Failed to build psutil', 'Installing collected packages: psutil', '    Running setup.py install for psutil: started', ""    Running setup.py install for psutil: finished with status 'error'""]
[pipenv.exceptions.InstallError]: ['ERROR: Command errored out with exit status 1
</code></pre>

<p>the error message continues for multiple pages. I remember having error messages installing pipenv, but I get none when reinstalling via</p>

<pre><code>python3 -m pip install pipenv --force-reinstall
</code></pre>

<p>everything seems to run smoothly. Since pipenv needs psutil and it is mentioned in the error message above, I tried to force-reinstall that too. This leads to another error message:</p>

<pre><code>~/gphotos-sync# python3 -m pip install psutil --force-reinstall
Collecting psutil
  Using cached psutil-5.7.0.tar.gz (449 kB)
Building wheels for collected packages: psutil
  Building wheel for psutil (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-vmk57m02/psutil/setup.py'""'""'; __file__='""'""'/tmp/pip-install-vmk57m02/psutil/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-gttht4x_
       cwd: /tmp/pip-install-vmk57m02/psutil/
  Complete output (112 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.7
  creating build/lib.linux-x86_64-3.7/psutil
  copying psutil/_psosx.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_pssunos.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_pslinux.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_psaix.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/__init__.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_psposix.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_common.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_compat.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_psbsd.py -&gt; build/lib.linux-x86_64-3.7/psutil
  copying psutil/_pswindows.py -&gt; build/lib.linux-x86_64-3.7/psutil
  creating build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/runner.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_contracts.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/__main__.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_linux.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_process.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_osx.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_memory_leaks.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/__init__.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_sunos.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_misc.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_windows.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_posix.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_unicode.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_connections.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_bsd.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_system.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  copying psutil/tests/test_aix.py -&gt; build/lib.linux-x86_64-3.7/psutil/tests
  running build_ext
  building 'psutil._psutil_linux' extension
  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/psutil
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=570 -DPSUTIL_LINUX=1 -DPSUTIL_ETHTOOL_MISSING_TYPES=1 -I/usr/include/python3.7m -c psutil/_psutil_common.c -o build/temp.linux-x86_64-3.7/psutil/_psutil_common.o
  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory
  Traceback (most recent call last):
    File ""/usr/lib/python3.7/distutils/unixccompiler.py"", line 118, in _compile
      extra_postargs)
    File ""/usr/lib/python3.7/distutils/ccompiler.py"", line 909, in spawn
      spawn(cmd, dry_run=self.dry_run)
    File ""/usr/lib/python3.7/distutils/spawn.py"", line 36, in spawn
      _spawn_posix(cmd, search_path, dry_run=dry_run)
    File ""/usr/lib/python3.7/distutils/spawn.py"", line 159, in _spawn_posix
      % (cmd, exit_status))
  distutils.errors.DistutilsExecError: command 'x86_64-linux-gnu-gcc' failed with exit status 1

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
      self.run_command(cmd)
    File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/usr/local/lib/python3.7/dist-packages/wheel/bdist_wheel.py"", line 223, in run
      self.run_command('build')
    File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/usr/local/lib/python3.7/dist-packages/setuptools/command/build_ext.py"", line 87, in run
      _build_ext.run(self)
    File ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 340, in run
      self.build_extensions()
    File ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 449, in build_extensions
      self._build_extensions_serial()
    File ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 474, in _build_extensions_serial
      self.build_extension(ext)
    File ""/usr/local/lib/python3.7/dist-packages/setuptools/command/build_ext.py"", line 208, in build_extension
      _build_ext.build_extension(self, ext)
    File ""/usr/lib/python3.7/distutils/command/build_ext.py"", line 534, in build_extension
      depends=ext.depends)
    File ""/usr/lib/python3.7/distutils/ccompiler.py"", line 574, in compile
      self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)
    File ""/usr/lib/python3.7/distutils/unixccompiler.py"", line 120, in _compile
      raise CompileError(msg)
  distutils.errors.CompileError: command 'x86_64-linux-gnu-gcc' failed with exit status 1

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File ""/tmp/pip-install-vmk57m02/psutil/setup.py"", line 393, in main
      setup(**kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/setuptools/__init__.py"", line 144, in setup
      return distutils.core.setup(**attrs)
    File ""/usr/lib/python3.7/distutils/core.py"", line 163, in setup
      raise SystemExit(""error: "" + str(msg))
  SystemExit: error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""/tmp/pip-install-vmk57m02/psutil/setup.py"", line 426, in &lt;module&gt;
      main()
    File ""/tmp/pip-install-vmk57m02/psutil/setup.py"", line 400, in main
      missdeps(""sudo apt-get install gcc python%s-dev"" % py3)
    File ""/tmp/pip-install-vmk57m02/psutil/setup.py"", line 116, in missdeps
      s = hilite(""C compiler or Python headers are not installed "", ok=False)
  TypeError: hilite() got an unexpected keyword argument 'ok'
  ----------------------------------------
  ERROR: Failed building wheel for psutil
  Running setup.py clean for psutil
Failed to build psutil
Installing collected packages: psutil
  Attempting uninstall: psutil
    Found existing installation: psutil 5.5.1
ERROR: Cannot uninstall 'psutil'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
</code></pre>

<p>I also tried reinstalling setuptools and wheel which went fine.</p>

<p>I am using Python 3.7.3</p>

<p>Anything else that I could try?</p>
"
"50956952","Is there any way by which we can list out all the dependencies or libraries installed in running docker container?","<docker>","60600339","View installed packages in a docker container","<docker>","<p>Is there any way by which we can list out all the dependencies or libraries installed in running <strong>docker container</strong>?</p>
","<p>There are containers raised in Docker. Containers run applications such as nginx, apache, etc.</p>

<p>How can I list the installed versions of those applications in each container?</p>
"
"48633244","Access Angular app running inside Docker container","<angular><docker><dockerfile><redhat><redhat-containers>","60606780","Can't open Angular application in the browser, that is running from docker container","<angular><docker><dockerfile><docker-for-windows>","<p>I've created a docker image to test an Angular app but I cannot connect from host to running app inside docker container.</p>

<p>The image was created using a Dockerfile with:
EXPOSE 4200 8080 80</p>

<p>I run the docker container with command:
docker run -ti -p 4200:4200 angulardev /bin/bash</p>

<p>Inside container I create the Angular application and start it using:
ng serve</p>

<p>From container, if I open localhost:4200 I see the application but I cannot access it from host OS (RHEL7)</p>

<p>What is wrong? The Angular app starts on port 4200 which is exposed and mapped to host 4200.</p>

<p>Thanks.</p>
","<p>I'm pretty new to Docker and I'm trying to create a docker image that clones a github repo with a very simple Angular application and runs it in the container. Then I want to access this running Angular application in my browser.</p>

<p>This is my Dockerfile:</p>

<pre><code>FROM node:latest

RUN apt-get update &amp;&amp; apt-get install -y git

WORKDIR /usr/src/app

RUN git clone https://github.com/path-to-repo.git

WORKDIR /usr/src/app/docker-angular

RUN npm install -g @angular/cli

RUN npm install

CMD ng serve --port 5000
</code></pre>

<p>I build the image and I'm running the container with the following command</p>

<pre><code>docker container run -d -p 5000:5000 --name [container-name] [image-name]
</code></pre>

<p>When i check the logs i see that the application is running successfully in the container, but when i try to access it in my browser on localhost:5000 it's not there.
This is the output when I run docker container ls:</p>

<pre><code>806a71aa5972        angular-app         ""docker-entrypoint.s…""   24 minutes ago      Up 24 minutes       0.0.0.0:5000-&gt;5000/tcp   sample-angular 
</code></pre>

<p>Can I access the application that is running in the container from the browser and how should I do that?</p>
"
"60235353","Docker is not running on Colab","<docker><google-colaboratory>","60614614","Unable to start docker in Google-Colab","<docker><google-colaboratory>","<p>I have tried  to install Docker on google Colab through the following ways:</p>

<p>(1)<a href=""https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04</a> </p>

<p>(2)<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04</a></p>

<p>(3)<a href=""https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP"" rel=""noreferrer"">https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP</a></p>

<p>I started the docker service and saw the status, but it showed 'Docker is not running'. Maybe the docker can not work on the Colab.
<a href=""https://i.stack.imgur.com/Xxs7O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xxs7O.png"" alt=""enter image description here""></a></p>

<p>I feel confused and want to know the reason.</p>

<p>Thanks</p>
","<p>I'm trying to build and run my docker image in Google Colab. I have installed docker in colab and I get the following error message ""docker: Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?"" even when I try to run the default hello-world container using <code>docker run hello-world</code></p>
"
"60677656","Listing the contents of a directory in a shared volume with 200K files from within a ubuntu container hosted in windows gets stuck","<docker><windows-subsystem-for-linux><docker-volume>","60644956","""ls"" in a container within a volume shared with Windows host gets stuck when there are many files in a directory","<java><spring-boot><docker><docker-desktop><volumes>","<h1>Problem</h1>

<p>Listing the contents of a directory in a shared volume with 200K files from within a ubuntu container hosted in windows gets stuck. </p>

<h1>How to reproduce</h1>

<ol>
<li>Install Docker Desktop 2.2.0.3 (42716) stable on Windows 10 </li>
<li>Configure docker to work in WSL (Ubuntu 16 distribution)

<ul>
<li>Expose daemon on <code>tcp://localhost:2375 without TLS</code></li>
<li>In WSL, define environment variable <code>DOCKER_HOST=localhost:2375</code></li>
<li>In WSL, in <code>/etc/wsl.conf</code> in the <code>[automount]</code> section, define <code>root = /</code></li>
</ul></li>
<li>Create a folder on the Windows host that contains 200,000 files (even small ones). e.g. <code>c:/temp</code></li>
<li>Run a docker container, mapping c:/temp on the host to /opt in the container as follows:
<code>docker run -it -v /c/temp:/opt ubuntu bash</code></li>
<li>In the bash prompt of the container run <code>cd /opt</code> and then <code>ls</code></li>
</ol>

<h1>Expected Result</h1>

<p>The <code>ls</code> command will start to list all files in the c:/temp folder.<br>
This can be expected to take some time, but it will begin almost immediately.</p>

<h1>Actual Result</h1>

<p>Nothing is printed and the shell does not respond.<br>
Moreover, CTR C does not interrupt the <code>ls</code> command.<br>
The only way I found to stop the container is to restart Docker Desktop</p>
","<h1>Background</h1>

<p>I am running a Spring Boot application in a docker container (Ubuntu image).
The code is written in Kotlin and it walks through a directory on disk that contains 300,000 files.<br>
I run the following code:  </p>

<pre><code>  File(dir)
    .walk()
    .forEach{logger.info(""{}"", it.name)}
</code></pre>

<p>and this code blocks for at least 10 minutes.  </p>

<p>I would expect that lines should start being printed very shortly after invoking walk.<br>
Indeed the code works as expected when running it from Intellij - that is, not in a container.</p>

<h1>Question:</h1>

<p>Why is this happening and how can I fix it?</p>

<hr>

<h2>What I Have Tried</h2>

<h2>First Trial</h2>

<p>I have tried just calling File.listfiles and logging the number of files like so:  </p>

<pre><code>   val count = File(dir).listFiles().size
   logger.info(""{}"", count)
</code></pre>

<p>This also blocked for a very long time and eventually logged the value 0.</p>

<h2>Second Trial</h2>

<p>I changed the settings for Docker Desktop.
I increased the RAM to 20 GB and the swap file to to 1 GB<br>
This had no effect on the result.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60647636","KafkaStreamApi in Docker worked, but don't consume record","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i use a kafka-wurstmeister-image on docker with a bridge-network, it works fine, i can use local producer/consumer to in-&amp;output data from kafka-topic. 
then i try to push my local producer/consumer/streamapi to docker-repository. After running i've got:<br>
<code>org.apache.kafka.common.errors.TimeoutException: Topic source-validator-topic not present in metadata after 60000 ms.</code><br>
i think there must be sth wrong with my configuration.</p>

<p>following is my step:
1. output java producer from eclipse.<br>
2. generate pom.xml for maven-build, and build.<br>
3. generate Dockerfile to create image, and create.<br>
3. push image to docker-repository.<br>
4. generate docker-compose.yml.<br>
5. run in CLi: docker-compose up  </p>

<p>here is my pom.xml file:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;StreamForBC&lt;/groupId&gt;
  &lt;artifactId&gt;StreamForBC&lt;/artifactId&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;blockchain-docker&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;
  &lt;properties&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;
  &lt;dependencies&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;3.8.1&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;


   &lt;dependency&gt;
       &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
       &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
       &lt;version&gt;1.7.5&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
       &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
       &lt;version&gt;1.7.5&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;log4j&lt;/groupId&gt;
       &lt;artifactId&gt;log4j&lt;/artifactId&gt;
       &lt;version&gt;1.2.17&lt;/version&gt;
   &lt;/dependency&gt;



    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka_2.13&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-log4j-appender&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;commons-io&lt;/groupId&gt;
        &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
        &lt;version&gt;2.6&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;


    &lt;build&gt;
        &lt;finalName&gt;blockchain-software&lt;/finalName&gt;
        &lt;plugins&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;mainClass&gt;StreamForBC.ProducerTest&lt;/mainClass&gt;
                            &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;!-- copy dependencies / jars to ${project.build.directory}/lib/ --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;outputDirectory&gt;
                                ${project.build.directory}/lib/
                            &lt;/outputDirectory&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>here is my Dockerfile:</p>

<pre><code>FROM openjdk
ARG JAR_FILE=blockchain-software.jar
ARG JAR_LIB_FILE=lib/
RUN mkdir app
WORKDIR /app
COPY blockchain-software.jar .
ADD ${JAR_LIB_FILE} lib/
EXPOSE 9092:9092
CMD [""java"",""-cp"",""blockchain-software.jar"",""StreamForBC.ProducerTest""]
</code></pre>

<p>here is my docker-compose.yml:</p>

<pre><code>version: '2'
services:
  validate:
    image: billkuku/bc-validate:v1
    ports:
      - ""4880:4880""
      - ""4822:22""
    volumes:
      - './data:/home/blockchain/validate/data'

  testproducer:
    image: billkuku/producer:v1
    ports:
      - ""9093:9093""
      - ""4823:23""
    volumes:
      - './data:/home/blockchain/producer/data'


  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""

  kafka:
    image: wurstmeister/kafka
    ports:
      - ""9092:9092""
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_MESSAGE_MAX_BYTES: 2000000000
      KAFKA_CREATE_TOPICS: ""source-topic:1:1,source-validator-topic:1:1,block-validator-topic:1:1,block-verifier-topic:1:1""
    depends_on:
      - zookeeper
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>
"
"57842718","Gunicorn Flask application in Docker Container not getting Exposed","<python><docker><flask><kubernetes><gunicorn>","60705576","Docker Container (Django Server) not accessible from local machine","<python><docker>","<p>The application inside the container is inaccessible from the outside i.e if I exec into the docker container and do </p>

<pre class=""lang-sh prettyprint-override""><code>curl localhost:5000 
</code></pre>

<p>it works correctly but not on the browser in my computer i get error : This site cant be reached</p>

<p>My Dockerfile:</p>

<pre><code># Use an official Python runtime as a parent image
FROM python:3.7-slim

# Set the working directory to /app
WORKDIR /web-engine

# Copy the current directory contents into the container at /app
COPY . /web-engine

# Install Gunicorn3
RUN apt-get update &amp;&amp; apt-get install default-libmysqlclient-dev gcc -y

# Install any needed packages specified in requirements.txt
RUN pip3 install --trusted-host pypi.python.org -r requirements.txt

# Make port 5000 available to the world outside this container
EXPOSE 5000

# Define environment variable
ENV username root

# Run app.py when the container launches
CMD gunicorn --workers 4 --bind 127.0.0.1:5000 application:app --threads 1

</code></pre>

<p>UPON executing docker in this way:</p>

<pre class=""lang-sh prettyprint-override""><code>sudo docker run -e password=$password -p 5000:5000 $reg/web-engine:ve0.0.2
</code></pre>

<p>I get the following output:</p>

<pre><code>[2019-09-08 11:53:36 +0000] [6] [INFO] Starting gunicorn 19.9.0
[2019-09-08 11:53:36 +0000] [6] [INFO] Listening at: http://127.0.0.1:5000 (6)
[2019-09-08 11:53:36 +0000] [6] [INFO] Using worker: sync
[2019-09-08 11:53:36 +0000] [9] [INFO] Booting worker with pid: 9
[2019-09-08 11:53:36 +0000] [10] [INFO] Booting worker with pid: 10
[2019-09-08 11:53:36 +0000] [11] [INFO] Booting worker with pid: 11
[2019-09-08 11:53:36 +0000] [12] [INFO] Booting worker with pid: 12
</code></pre>

<p>So as you can see I'm mapping port 5000 of the container to port 5000 of my computer but localhost:5000 is not working</p>

<p>Therefore i tried everthing the same but with the development server of Flask
with the following change in My dockerfile </p>

<p>FRom</p>

<pre class=""lang-sh prettyprint-override""><code>CMD gunicorn --workers 4 --bind 127.0.0.1:5000 application:app --threads 1

</code></pre>

<p>TO </p>

<pre class=""lang-sh prettyprint-override""><code>CMD python3.7 application.py
</code></pre>

<p>and IT WORKED; I goto localhost:5000 and see the application is working</p>

<p>There is nothing wrong with the application. I suppose there's an error in gunicorn server</p>

<p>the requirements.txt file :</p>

<pre><code>Flask
Flask-SQLAlchemy
mysqlclient
gunicorn
bs4
html5lib
</code></pre>

<p>Please help me out</p>

<p>I also tried different forms of gunicorn and docker run command combinations like</p>

<pre class=""lang-sh prettyprint-override""><code>CMD gunicorn -application:app &amp;&amp; sudo docker run -e password=$password -p 5000:8000 $reg/web-engine:ve0.0.2
</code></pre>

<p>It didnt work
<a href=""https://i.stack.imgur.com/Z6ors.png"" rel=""nofollow noreferrer"">terminal image of container working with development server</a></p>

<p>I would appreciate a solution involving nothing outside whats mentioned here like nginx, supervisor etc
 SOmeone please helllppp meeeeee.......😢</p>
","<p>I'm trying to run a simple container from a custom Dockerfile, but cannot access it when I go to my localhost. Anyone know why this could be? Here is the Dockerfile:</p>

<pre><code>FROM python:3.7.7-buster

RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
RUN echo deb ""https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list
RUN apt-get update
RUN yes | apt-get install npm
RUN yes | apt-get install yarn

COPY . .
RUN ./build.sh

EXPOSE 8000
CMD gunicorn -b 127.0.0.1:8000 project_config.wsgi.prod:application
</code></pre>

<p>And I use the following command to start the server:</p>

<pre><code>docker run -p 8000:8000 -e SECRET_KEY=supersecretkey mycontainer
</code></pre>

<p>I can see the server is running fine but when I got to localhost port 8000, I get nothing. Any help appreciated!</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","60738092","docker-compose up takes lot of time for mac but runs fine on windows","<node.js><reactjs><docker><docker-compose>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I have three services starting up from my docker-compose.yml file. A React frontend, a node js backend and a mongodb service. It starts up and run absolutely fine on windows but it takes lots of time on mac that most of the time it throws http timeout error.</p>

<p>My docker-compose file -</p>

<pre><code>version: ""2""
services:
  client:
    build:
      context: ./react-ui
      args:
        NPM_TOKEN: $NPM_TOKEN
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NPM_TOKEN=${NPM_TOKEN}
    ports:
      - ""3001:3001""
    volumes:
      - /app/node_modules
      - ./react-ui:/app
  server:
    command: yarn dev -- -L
    build:
      context: ./server
    restart: always
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NODE_ENV=development
      - MONGODB_URI=mongodb://mongodb:27017/react-node-project
    ports:
      - ""3000:3000""
    volumes:
      - /app/node_modules
      - ./server:/app
    depends_on:
      - mongodb
  mongodb:
    image: mongo
    ports:
      - ""27017:27017""
</code></pre>

<p>My react dockerfile -</p>

<pre><code>FROM node:alpine

ARG NPM_TOKEN

ENV NPM_TOKEN=${NPM_TOKEN}

WORKDIR ""/app""
COPY ./package.json ./
COPY ./.npmrc . 
RUN  yarn install
COPY . .
CMD [""yarn"", ""start""]
</code></pre>

<p>My server dockerfile - </p>

<pre><code>FROM node:8.11

EXPOSE 3000

ARG NODE_ENV
ENV NODE_ENV $NODE_ENV

WORKDIR /app
COPY ./package.json .
RUN yarn install
COPY . .

CMD [""yarn"", ""docker:start""]
</code></pre>

<p>docker:start script does <code>nodemon ./src/index.js</code></p>

<p>I have made sure to add node_modules in my .dockerignore file. Also the client service specifically is taking lots of time for mac. Maybe I have written something incorrectly there which is taking so much time.</p>

<p>I have tried on 2-3 mac machines, the behaviour is same.</p>
"
"56554029","What's the difference between docker restart_policy on-failure vs any","<docker><docker-swarm>","60753972","docker swarm restart 'any' meaning","<docker-compose><docker-swarm>","<p>In the docker swarm v3 docs, there are 3 different restart policy conditions that can be used. It's obvious what the <code>none</code> condition does, but I was wondering what the difference between <code>on-failure</code> and <code>any</code> is specifically.</p>

<p>Here's a picture from the <a href=""https://docs.docker.com/compose/compose-file/#restart_policy"" rel=""nofollow noreferrer"">docs</a>:
<a href=""https://i.stack.imgur.com/yvOMM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yvOMM.png"" alt=""enter image description here""></a></p>
","<p>The default restart policy in docker swarm is 'any' but its meaning is not documented in <a href=""https://docs.docker.com/compose/compose-file/#restart_policy"" rel=""nofollow noreferrer"">documentation</a>. Can someone please clarify the behaviour of 'any'?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60755094","Running clickhouse and Kafka in docker","<docker><apache-kafka><clickhouse>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am trying to integrate Kafka with clickhouse. I want to send in some data from my REST API to kafka, and pass it to clickhouse which would be my DB. So far, I was able to get Kafka to working indivually and pass data from one end to other. But when I tried to use it in docker, I am having some issues. Well, I can see the data being consumed at the kafka end, using a kafka tool. But my clickhouse apparently is not getting connected. This is the error, I keep getting when I run my docker:</p>

<pre><code>clickhouse-server_1  | %3|1584611439.583|FAIL|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 1ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 1ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: 1/1 brokers are down
clickhouse-server_1  | %3|1584611439.583|FAIL|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: 1/1 brokers are down
</code></pre>

<p>I have also added the log file below:</p>

<pre><code>2020.03.19 10:16:11.637331 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:8123 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.637658 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:9000 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.637854 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:9009 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.678748 [ 74 ] {} &lt;Error&gt; void DB::DDLWorker::runMainThread(): Code: 999, e.displayText() = Coordination::Exception: All connection tries failed while connecting to ZooKeeper. Addresses: 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
 (Connection loss), Stack trace (when copying this message, always include the lines below):

0. Poco::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0x102d352c in /usr/bin/clickhouse
1. DB::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0x8f2d989 in /usr/bin/clickhouse
2. Coordination::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int) @ 0xdd300e4 in /usr/bin/clickhouse
3. Coordination::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0xdd3070e in /usr/bin/clickhouse
4. ? @ 0xdd638b2 in /usr/bin/clickhouse
5. Coordination::ZooKeeper::ZooKeeper(std::__1::vector&lt;Poco::Net::SocketAddress, std::__1::allocator&lt;Poco::Net::SocketAddress&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, Poco::Timespan, Poco::Timespan, Poco::Timespan) @ 0xdd6099b in /usr/bin/clickhouse
6. zkutil::ZooKeeper::init(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) @ 0xdd3fd53 in /usr/bin/clickhouse
7. zkutil::ZooKeeper::ZooKeeper(Poco::Util::AbstractConfiguration const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) @ 0xdd405bd in /usr/bin/clickhouse
8. DB::Context::getZooKeeper() const @ 0xce5fb97 in /usr/bin/clickhouse
9. DB::DDLWorker::getAndSetZooKeeper() @ 0xce84c53 in /usr/bin/clickhouse
10. DB::DDLWorker::runMainThread() @ 0xce8eae3 in /usr/bin/clickhouse
11. ThreadFromGlobalPool::ThreadFromGlobalPool&lt;void (DB::DDLWorker::*)(), DB::DDLWorker*&gt;(void (DB::DDLWorker::*&amp;&amp;)(), DB::DDLWorker*&amp;&amp;)::'lambda'()::operator()() const @ 0xce96031 in /usr/bin/clickhouse
12. ThreadPoolImpl&lt;std::__1::thread&gt;::worker(std::__1::__list_iterator&lt;std::__1::thread, void*&gt;) @ 0x8f50b07 in /usr/bin/clickhouse
13. ? @ 0x8f4f00f in /usr/bin/clickhouse
14. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so
15. __clone @ 0x12188f in /lib/x86_64-linux-gnu/libc-2.27.so
 (version 20.3.3.6 (official build))
</code></pre>

<p>Here are some of the configuration I am using:</p>

<pre><code>CREATE TABLE IF NOT EXISTS filters (
  name String,
  value String
) ENGINE = Kafka SETTINGS
            kafka_broker_list = 'kafka://localhost:9092',
            kafka_topic_list = 'testtopic',
            kafka_group_name = 'test',
            kafka_format = 'JSONEachRow',
            kafka_num_consumers = 2
</code></pre>

<p>My kafka bootstrapping part:</p>

<pre><code>producer = KafkaProducer(bootstrap_servers=['kafka://localhost:9092'], value_serializer=lambda m: json.dumps(m).encode('ascii'))
</code></pre>

<p>And my docker config part:</p>

<pre><code>version: ""3.6""

services:
  clickhouse-server:
    image: yandex/clickhouse-server
    volumes:
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse/zookeeper-servers.xml:/etc/clickhouse-server/conf.d/zookeeper-servers.xml
      - ./shared/ch-data/clickhouse:/var/lib/clickhouse
    depends_on:
      - kafka
    links:
      - kafka
    ports:
      - 9000:9000

  clickhouse-client:
    image: yandex/clickhouse-client
    entrypoint:
      - /bin/sleep
    command:
      - infinity

  kafka:
    image: wurstmeister/kafka:2.11-1.0.2
    volumes:
      - ./shared/ch-data/kafka:/data
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: 192.168.1.86
    links:
     - zookeeper
    ports:
     - 9092:9092
     - 9094:9094

  zookeeper:
    image: zookeeper
    volumes:
      - ./shared/ch-data/zookeeper:/data
    ports:
      - 2181:2181
</code></pre>

<p>I am particularly confused on the bootstrap_Servers and KAFKA_ADVERTISED_LISTENERS part. So far as I know, I have provided my host IP as the KAFKA_ADVERTISED_LISTENER, from what I understood from <a href=""https://www.confluent.io/blog/kafka-listeners-explained/"" rel=""nofollow noreferrer"">here</a> </p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","60776495","Why is the java process user more memory than -XX:MaxRAM?","<java><memory><jvm>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I've set the options for a java process to use 80% of 1g max ram. But when I use 'ps -o vsz', I see it is using 3.5g (starting from 2.5g). This causes a lot of swap and thus freezing the device. Why is the discrepancy? </p>

<p>UPDATE: The options to the JVM are now: <code>-Xmx256m -Xshare:off -XX:+UseSerialGC -XX:NativeMemoryTracking=summary -XX:MaxRAM=768m -XX:MaxRAMPercentage=60</code>. They don't seem to change anything. The process starts at 2.4g and grows to 3.5g</p>

<p>UPDATE 2: </p>

<pre><code>openjdk version ""14"" 2020-03-16
OpenJDK Runtime Environment (build 14+36)
OpenJDK 64-Bit Server VM (build 14+36, mixed mode)
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60791807","cant read\write from kafka topic","<python><docker><apache-kafka><kafka-producer-api><kafka-python>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have Kafka running on a container on my desktop.
I can connect to it just fine using a tool called ""Kafka tool"" where I can see my topics for example.</p>

<p>I'm having issues reading and writing to/from a Kafka topic. 
what's annoying me is that it won't give me an error message, it's behaving like if the topic doesn't have any messages on it, but it does, I can see using the tool, even added two messages manually.</p>

<p>the topic exists and has two messages on it (which I added manually using this UI)</p>

<p>problem:
the code that sends messages to the topic runs fine, but the messages don't make it to Kafka
the code that reads messages from the topic doesn't read anything. It sits there like if there are no messages to be read.
Also, I can use the same consume to list the topics (which indicates the connection was successful) </p>

<p>The kafka version is 2.4. 
Any idea what the problem may be?
I have tried ""bootstrap_servers=['localhost:9092', 'kafka-server:9092']"" but it also didnt work</p>

<p>Thanks</p>

<p><a href=""https://i.stack.imgur.com/e5gZP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e5gZP.png"" alt=""enter image description here""></a></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60792143","Kafka failing to connect broker on docker","<node.js><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have set up a Kafka cluster with docker-compose file where i specify the brokers like this:</p>

<pre><code>kafka1:
    image: confluentinc/cp-kafka:latest
    hostname: kafka1
    ports:
      - ""19092:19092""
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:12181,zookeeper-2:12181,zookeeper-3:12181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:19092
</code></pre>

<p>Now when i try to create topic with js script and i refer to the brokers like follows(for the sake of this post i only attach code about 1 broker):</p>

<pre><code>try
    {
        const kafka = new Kafka({
            clientId: 'myapp',
            brokers: ['kafka1:19092','kafka2:29092','kafka3:39092']
        })
</code></pre>

<p>I get this error:</p>

<pre><code>{""level"":""ERROR"",""timestamp"":""2020-03-21T19:06:13.653Z"",""logger"":""kafkajs"",""message"":""[Connection] Connection error: getaddrinfo ENOTFOUND kafka1"",""broker"":""kafka1:19092"",""clientId"":""myapp"",""stack"":""Error: getaddrinfo ENOTFOUND kafka1\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:66:26)""}
</code></pre>

<p>I feel like i have exhausted internet trying to troubleshoot this. Anyone have any suggestions how to connect to the broker in the docker?</p>
"
"51247609","What is the difference between docker run and docker container run","<docker>","60798379","Difference between docker container commit and docker commit command","<docker><docker-container><docker-command>","<p>can anyone help me in the understanding difference between <strong>docker run</strong> &amp; <strong>docker container run</strong>?</p>

<p>when i do <strong>docker run --help</strong> &amp; <strong>docker container run --help</strong> from docker cmd line. I see the following </p>

<p><strong>Run a command in a new container</strong>.</p>

<p>Is there any difference in how they run the container internally or both are same doing same work?</p>

<p>As per <a href=""https://forums.docker.com/t/docker-run-and-docker-container-run/30526"" rel=""noreferrer"">https://forums.docker.com/t/docker-run-and-docker-container-run/30526</a>. <strong>docker run</strong> is still the old one, which will be deprecated soon but same is not confirmed.</p>
","<p>What is the difference between the following two commands:</p>

<pre><code>docker container commit 

docker commit
</code></pre>

<p>I read the documentation for both the commands but couldn't found any difference between the two commands(Also both the commands contain the same options).</p>

<p><a href=""https://docs.docker.com/engine/reference/commandline/commit/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/commandline/commit/</a>
<a href=""https://docs.docker.com/engine/reference/commandline/container_commit/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/commandline/container_commit/</a></p>

<p>The help page of both the commands show the same output on Docker 19.03</p>

<pre><code>Usage:  docker container commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]

Create a new image from a container's changes

Options:
  -a, --author string    Author (e.g., ""John Hannibal Smith &lt;hannibal@a-team.com&gt;"")
  -c, --change list      Apply Dockerfile instruction to the created image
  -m, --message string   Commit message
  -p, --pause            Pause container during commit (default true)
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60816086","KafkaProducer not reading messages","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have a dockerized Spark app for simple streaming. The listener generates random numbers and sends them to Kafka using this code:</p>

<pre><code>producer = KafkaProducer(bootstrap_servers=kafka_brokers, api_version=(0, 10, 1))
while True:
    data = //generate a json with a single number
    producer.send(topic_name, str.encode(json.dumps(data)))
</code></pre>

<p>Then I try to read this data using the consumer as such:</p>

<pre><code>consumer = KafkaConsumer(topic_name, bootstrap_servers=['192.168.99.100:9092'])
for message in consumer:
    record = json.loads(message.value)
    list.append(record['field'])
</code></pre>

<p>When I run the code it never gets past the 'for message in consumer' part. I checked within Kafka and the messages are all there but I cannot access them through Python.</p>

<p>Edit: I am using the <a href=""https://github.com/bitnami/bitnami-docker-spark"" rel=""nofollow noreferrer"">bitnami spark</a> containers and <a href=""https://github.com/simplesteph/kafka-stack-docker-compose/blob/master/zk-single-kafka-single.yml"" rel=""nofollow noreferrer"">this</a> settings for kafka and zookeeper.</p>

<p>I just have two separate files, one for the producer and one for the consumer. I run the producer file which sends to Kafka then I spark-submit the consumer file which should just print a list of the numbers received. For this I simply do spark-submit --master spark://spark:7077 consumer.py</p>
"
"50768317","""docker pull"" certificate signed by unknown authority","<docker><ssl><curl>","60837639","certificate signed by unknown authority when docker pull","<docker><ssl><docker-compose>","<p>I was trying to pull a docker image from a docker registry but hit the following issue:</p>

<pre><code>$ docker pull &lt;docker registry&gt;/&lt;image name&gt;/&lt;tag&gt; 
Error response from daemon: Get &lt;docker registry&gt;/v1/_ping: x509: certificate signed by unknown authority
</code></pre>

<p>I tried with ""curl"" and get a similar error message:</p>

<pre><code> curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
</code></pre>

<p>So I downloaded the CA certificate and imported to the server (RedHat Linux 7) with the following commands:</p>

<pre><code>cp root_cert.cer /etc/pki/ca-trust/source/anchors/
update-ca-trust
</code></pre>

<p>After the root cert is imported, I can see <code>curl</code> is working fine as it won't complain the cert error, however if I use <code>docker pull</code> I still have the same issue. Is <code>docker</code> using different ca-cert location than <code>curl</code>? How do I fix the issue with <code>docker pull</code> in this situation?</p>
","<p>I set the docker private registry with self cirtification</p>

<p>For now, it works with <code>curl</code></p>

<pre><code>curl https://docker.mysite.jp:5000/v2/ --cacert /etc/docker/certs.d/docker.mysite.jp\:5000/ca.crt
</code></pre>

<p>then I am testing docker pull.</p>

<p>It shows error.</p>

<p><code>certificate signed by unknown authority</code></p>

<pre><code>$docker image push docker.mysite.jp:5000/myapp/nginx:latest
The push refers to repository [docker.mysite.jp:5000/myapp/nginx]
Get https://docker.mysite.jp:5000/v2/: x509: certificate signed by unknown authority
</code></pre>

<p>I think it means docker repository dosen't accept self-certificate ?? Is there any work around? or the problem of my localhost(mac OSX)??</p>
"
"47528696","How can I copy files to a docker container when error happened:Nosuch container?","<docker><centos>","60849934","Docker cannot find file in image when it is there?","<docker>","<p>I am new about docker, so ,if any wrong thoughts come from me ,please point out it.Thanks~</p>

<p>I aim at running a web server that was developed by me ,or a team I belong to,in the docker.</p>

<p>So, I thought out three steps:</p>

<p>Have a image ,copy the web files into it,and run the  container.so,I do the step below:</p>

<p>1- get a docker image.</p>

<p>I try like this : <code>docker pull centos</code>, so that I can get a image based on centos.Here, I did not care about the version of centos,of course, it's version is 6.7 or ,just taged:latest.</p>

<p>Here,I check the image by <code>docker images</code>,and I can see it like this:</p>

<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
docker.io/centos    latest              d123f4e55e12        3 weeks ago         196.6 MB
</code></pre>

<p>So,I think this step successed.</p>

<p>2- try copying the files in local system to the container.</p>

<p>I stop at the path: /tornado,which had a folder named fordocker .The </p>

<p>fordocker contains the web-server files.</p>

<p>I try commonds like this(based on the guide):</p>

<pre><code>docker cp fordocker/ d123f4e55e12:/web
</code></pre>

<p>But! Here comes the error:</p>

<pre><code>Error response from daemon: No such container: d123f4e55e12
</code></pre>

<p>3- if I copy the files successfully,I could try like this:<code>docker run -d centos xxx python web.py</code>.</p>

<p>This step will come error?I don't know yet.</p>

<p>I searched a lot ,but do not explain the phenomenon.</p>

<p>It seemed that everyone,beside me ,use the commond would succes.</p>

<p>So,here comes the questions:</p>

<p>1- Is the method I thought out feasible? Must I create a images through profile?</p>

<p>2- Where comes the error if the method is feasible? Did the commond cp based on otherthings that I had not done?</p>

<p>3- What should I do if the method is not feasible?Create a image myself?</p>

<p>Have a good day~</p>
","<p>Pretty new to Docker but I am trying to use it to automate building of a layer for a Lambda function from Windows 10.</p>

<p>Currently my Dockerfile looks like this:</p>

<pre><code>FROM lambci/lambda:build-python3.7
ADD . /media/output
RUN pip install dos2unix
RUN python -m dos2unix /media/output/build_layer_docker.sh /root/build_layer_docker_cleaned.sh
RUN chmod +x /root/build_layer_docker_cleaned.sh
RUN /root/build_layer_docker_cleaned.sh
</code></pre>

<p>And I have this Batch file:</p>

<pre><code>docker build -t pyodbcbuild .

docker images -q pyodbcbuild &gt; containerId.txt
set /p containerId=&lt;containerId.txt

docker cp %containerId%:/root/pyodbc-layer.zip ./pyodbc-layer.zip
REM docker rm %containerId%
</code></pre>

<p>When I run the batch file the container builds fine and does all of the script correctly and creates a file at /root/pyodbc-layer.zip.</p>

<p>However at the cp command it always fails with this:</p>

<pre><code>docker cp 0e56835088ec:/root/pyodbc-layer.zip ./pyodbc-layer.zip Error: No such container:path: 0e56835088ec:/root/pyodbc-layer.zip
</code></pre>

<p>When I run the container I can clearly see the file is there?
<a href=""https://i.stack.imgur.com/MhzgX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MhzgX.png"" alt=""enter image description here""></a></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60894615","""kafka.errors.NoBrokersAvailable: NoBrokersAvailable"" problem","<python><docker><apache-kafka><spring-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i investigate too much topic in stackoverflow but i couldn't find solution for me. I have a docker-compose file and i am trying to set true parameters for configuration. Other than that i have a django api that manages web requests. </p>

<p>My goal is when a user visits someone elses ""post"" i want to send a message to kafka ""x user visited y users 'z' titled post"". Finished all my configurations. In endpoint i configured a producer that sends message to kafka. After request to endpoint it gives me that error in title. How can i solve that ? </p>

<p>django view.py;</p>

<pre><code> def get(self,request,post_id,format=None):
    """"""
    Gets a post
    """"""
    post = self.get_post(post_id)
    serializer = PostSerializer(post)

    send_data(post.owner, request.user, post, datetime.now().strftime(""%d/%m/%Y %H:%M:%S"") )

    return Response(serializer.data)
</code></pre>

<p>send data func;</p>

<pre><code>def send_data(visitor_user:User, owner_user:User, visitedPost:Post, timestamp:date):
  from kafka.errors import KafkaError
  from kafka import KafkaProducer
  producer = KafkaProducer( bootstrap_servers=['kafka:9092'] )


  visitor = visitor_user.first_name + "" "" + visitor_user.last_name + ""(username:"" + visitor_user.username + "") ""
  owner = owner_user.first_name + owner_user.last_name + ""(username:"" + owner_user.username + "") ""
  post = ""'"" + visitedPost.title + ""' titled post.""
  message = timestamp + ""-&gt;"" + visitor + ""read"" + owner + "" 's "" + post

  result = producer.send( 'visitation-log', message.encode() )
</code></pre>

<p>docker-compose.yml;</p>

<pre><code>version : ""3""

services:
    db:
       image: postgres
       environment:
        - ""POSTGRES_HOST_AUTH_METHOD=trust""
    dj: 
       container_name: dja 
       build: django # django's Dockerfile path
       command: python manage.py runserver 0.0.0.0:80
       volumes:
       - ./django:/code

       ports:
        - ""80:80""
       depends_on:
        - db

    ng:
        container_name: ngtrip
        build: angular
        ports:
            - ""8080:80""

    zookeeper:
        image: wurstmeister/zookeeper
        container_name: zookeeper
        restart: always
        ports:
            - 2181:2181

    kafka:
        image: wurstmeister/kafka
        container_name: kafka
        hostname: localhost
        restart: always
        ports:
            - 9092:9092
        depends_on:
            - zookeeper
        links:
            - zookeeper:zookeeper
        environment:
            KAFKA_CREATE_TOPICS: ""visitation-log:1:3,Topic2:1:1:compact""
            KAFKA_LISTENERS: PLAINTEXT://localhost:9092
            KAFKA_ADVERTISED_HOST: localhost
            KAFKA_ADVERTISED_POST: 9092
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
</code></pre>

<p><strong>UPDATE</strong></p>

<p>I organized my kafka environment configs like belove thanks to help of cricket and it is now producing messages from django with ""kafka:9092"" . </p>

<pre><code>kafka:
    image: wurstmeister/kafka
    command: [start-kafka.sh]
    container_name: kafka
    restart: always
    ports:
        - ""29092:9092""
    environment:
        KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
        KAFKA_ADVERTISED_LISTENERS: INSIDE://${container_ip}:9092,OUTSIDE://${outside_host_ip}:29092
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: ""true""
</code></pre>

<p>${} fields are empty string for now because i didn't attach anything to them. But i must ask another thing. My consumer is located outside of docker. Its in my local. When i set ConsumerConfig bootstrap.server config to ""kafka:29092"" or anything else it doesn't matter when i run the app it stills checking localhost:9092. How can i solve that?</p>

<p><strong>FINAL UPDATE</strong></p>

<p>It seems it is a total rookie mistake. I changed image to confluent kafka image and about that packaging problem i used spotify's docker maven plugin and created a image. Final docker-compose file;</p>

<pre><code>zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
        ZOOKEEPER_CLIENT_PORT: 2181
        ZOOKEEPER_TICK_TIME: 2000

kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
        - zookeeper
    ports:
        - 9092:9092
    environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: ""true""
consumer:
    image: kafkaconsumer:latest
    ports:
        - ""8070:8070""
    depends_on:
      - kafka
    environment:
      SPRING_KAFKA_BOOTSTRAPSERVERS: kafka:29092
    restart: always
</code></pre>

<p>i can access to kafka in docker with kafka:29092 and outside the kafka i use localhost:9092 with these configuration. Consumer image is builted by docker-maven plugin.</p>
"
"48066994","Docker: ""no matching manifest for windows/amd64 in the manifest list entries""","<docker>","60992936","Docker build in command prompt","<python><rest><api><docker>","<p>I use Docker on Windows, and when I tried to pull a PHP image with this command</p>

<pre><code>$ docker pull php
</code></pre>

<p>I got this message:</p>

<pre><code>Using default tag: latest
latest: Pulling from library/php no matching manifest for windows/amd64 
        in the manifest list entries
</code></pre>

<p>How can I fix this problem?</p>
","<p>Why do I keep getting this when I initial docker build? In cmd ""no matching manifest for windows/amd64 10.0.18362 in the manifest list entries"".</p>

<p>My Dockerfile:</p>

<pre class=""lang-none prettyprint-override""><code>FROM python:3.7-alpine
MAINTAINER Charles
ENV PYTHONUNBUFFERED 1
COPY ./requirements.txt /requirements.txt
RUN pip install -r /requirements.txt
RUN mkdir /app
WORKDIR /app
COPY ./app /app
RUN adduser -D user
USER user
</code></pre>
"
"48066994","Docker: ""no matching manifest for windows/amd64 in the manifest list entries""","<docker>","61002470","Docker Installation Error on Step 1/11 ./bin/acore-docker-build","<docker><azerothcore>","<p>I use Docker on Windows, and when I tried to pull a PHP image with this command</p>

<pre><code>$ docker pull php
</code></pre>

<p>I got this message:</p>

<pre><code>Using default tag: latest
latest: Pulling from library/php no matching manifest for windows/amd64 
        in the manifest list entries
</code></pre>

<p>How can I fix this problem?</p>
","<p>Having failed to install azerothcore normally, I have now tried to install it using docker.</p>

<p>But I am already running into an issue, probably because I am not very experienced with docker.</p>

<p>When I input <code>./bin/acore-docker-build</code>, it fails at step one, outputting this message:</p>

<pre><code>Sending build context to Docker daemon  862.4MB
Step 1/11 : FROM ubuntu:bionic
bionic: Pulling from library/ubuntu
no matching manifest for windows/amd64 10.0.18362 in the manifest list entries
Unable to find image 'acbuild:latest' locally
C:\Program Files\Docker\Docker\resources\bin\docker.exe: Error response from daemon: pull access denied for acbuild, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.
See 'C:\Program Files\Docker\Docker\resources\bin\docker.exe run --help'.
</code></pre>

<p>I am not sure what I am supposed to do about this. I am logged into a basic docker account. I went to the docker.exe referenced in the above message and changed permissions for the account running docker to have full permissions. No change. I ran git as administrator. No change.</p>

<p>What should I be doing differently?</p>

<p>Thanks for reading. Willing to clarify anything.</p>

<p>Also, I am using windows 10.</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","61070393","What is affecting the JVM memory allocation to go beyond -Xmx?","<java><jvm><heap>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>We're configuring a JVM to deploy a server application with Java 11. Turns out our VM has a limit of 1.3G of RAM and we assigned the JVM the following config:</p>

<pre><code>-Xms512m
-Xmx1024m
</code></pre>

<p>Turns out that when we start the VM it says that there is not enough memory for the application to start. (Due to security reasons I cannot place the error message here -_-).</p>

<p>We tried to run the app on a local machine and found out it is consuming more memory than intended with the help of <a href=""https://visualvm.github.io/"" rel=""nofollow noreferrer"">VisualVM</a>.
<a href=""https://i.stack.imgur.com/PQCLJ.png"" rel=""nofollow noreferrer"">Check the Max Value, is beyond 1GB</a></p>

<p>We found that the JVM is capable of adding more memory than intended, our question is: Which memories affect this ""extra"" allocation of memory? We tried with Perm (deprecated) of course but we don't have an idea about which specific flags might affect the difference </p>
"
"46907584","Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error","<python><docker><dockerfile><docker-image>","61195945","ERROR: Service 'web' failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder561233561/bin/start.sh: no such file or directory","<linux><bash><docker><nginx><docker-compose>","<p>I want to create a docker image. This is my work directory:
Dockerfile.in   test.json   test.py</p>

<p>And this is my Dockerfile:</p>

<pre><code>COPY ./test.json /home/test.json
COPY ./test.py /home/test.py

RUN python test.py
</code></pre>

<p>When i launch this command: 
<code>docker build -f Dockerfile.in -t 637268723/test:1.0 .</code></p>

<p>It gives me this error: </p>

<pre><code>`Step 1/5 : COPY ./test.json /home/test.json
 ---&gt; Using cache
 ---&gt; 6774cd225d60
 Step 2/5 : COPY ./test.py /home/test.py
 COPY failed: stat /var/lib/docker/tmp/docker-builder428014112/test.py: 
 no such file or directory`
</code></pre>

<p>Can anyone help me?</p>
","<p>I'm using centos7 in virtual box</p>

<p>I was starting to create a service</p>

<p>After running  COPY ./bin/start.sh /start.sh, its throwing the below error</p>

<pre><code>ERROR: Service 'web' failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder561233561/bin/start.sh: no such file or directory
</code></pre>

<p>Image shows the folder structure</p>

<p>DockerFile</p>

<pre><code>FROM remote-host

COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo

RUN                                                                          \
  yum -y install nginx-1.12.2 openssl --enablerepo=nginx                  &amp;&amp; \
  yum -y install https://centos7.iuscommunity.org/ius-release.rpm         &amp;&amp; \
  yum -y install                                                             \
    php72u-fpm                                                               \
    php72u-cli                                                               \
    php72u-mysqlnd                                                           \
    php72u-soap                                                              \
    php72u-xml                                                               \
    php72u-zip                                                               \
    php72u-json                                                              \
    php72u-mcrypt                                                            \
    php72u-mbstring                                                          \
    php72u-zip                                                               \
    php72u-gd                                                                \
     --enablerepo=ius &amp;&amp; yum clean all

EXPOSE 80 443

VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm

COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf

COPY ./bin/start.sh /start.sh

RUN chmod +x /start.sh

CMD /start.sh
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: '3'
services:
  jenkins:
    container_name: jenkins
    image: jenkins-ansible
    build:
      context: jenkins-ansible
    ports:
      - ""8080:8080""
    volumes:
      - $PWD/jenkins_home:/var/jenkins_home
    networks:
      - net
  remote_host:
    container_name: remote-host
    image: remote-host
    build:
      context: centos7
    volumes:
      - $PWD/script.sh:/tmp/newScript.sh
    networks:
      - net
  db_host:
    container_name: db
    image: mysql:5.7
    environment:
      - ""MYSQL_ROOT_PASSWORD=1234""
    volumes:
      - $PWD/db_data:/var/lib/mysql
    networks:
      - net
  web:
    container_name: web
    image: ansible-web
    build:
      context: jenkins-ansible/web
    ports:
      - ""80:80""
    networks:
      - net

networks:
  net:
</code></pre>

<p><a href=""https://i.stack.imgur.com/lcDzY.png"" rel=""nofollow noreferrer"">enter image description here</a>
The image shows the project structure
Please help me out to remove this error</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61211684","kafka docker works well on localhost but not on a remote server","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have an issue with kafka docker image. I am using also kafkamanager and I am running those images from .yml file. The thing is that on localhost everything works fine but on remote server it doesn't.</p>

<p>I am using this .yml file:</p>

<pre><code>version: ""2""

services:
  kafkaserver:
    image: ""spotify/kafka:latest""
    container_name: kafka
    hostname: kafkaserver
    networks:
     - kafkanet
    ports:
      - 2181:2181
      - 9092:9092
    environment:
      ADVERTISED_HOST: kafkaserver
      ADVERTISED_PORT: 9092
  kafka_manager:
    image: ""mzagar/kafka-manager-docker:1.3.3.4""
    container_name: kafkamanager
    networks:
      - kafkanet
    ports:
      - 9000:9000
    links:
      - kafkaserver
    environment:
      ZK_HOSTS: ""kafkaserver:2181""

networks:
  kafkanet:
    driver: bridge
</code></pre>

<p>I am developing in python using kafka-python. WHen i start my program, program stops at the point when producer should start sending messages. But he doesn't. </p>

<p>After a minute program will give me this error message: </p>

<blockquote>
  <p>kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs. </p>
</blockquote>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61223033","How do i connect my containerised project to kafka running on localhost?","<java><spring-boot><docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I've been trying to connect my containerised spring-boot project with a of kafka and zookeeper running on my localhost but i seem to be getting an error when  i run the docker images.</p>

<p><a href=""https://i.stack.imgur.com/p2sv9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p2sv9.png"" alt=""enter image description here""></a></p>

<p>does anyone know what could be causing this  error and if so , what the best way to go about fixing it?</p>

<p>i alreadyhave ports 9092 and 2021 exposed </p>

<p><strong>edit:</strong>
i was asked to post the text:</p>

<pre><code>2020-04-15 06:55:34,872 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG common.network.Selector.pollSelectionKeys - [Consumer clientId=consumer-1, groupId=message] Connection with /172.17.59.17 disconnected
java.net.NoRouteToHostException: No route to host
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
        at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:216)
        at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:531)
        at org.apache.kafka.common.network.Selector.poll(Selector.java:483)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:539)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:249)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:326)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1251)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1201)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:993)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:949)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:901)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:748)
2020-04-15 06:55:34,874 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG kafka.clients.NetworkClient.handleDisconnections - [Consumer clientId=consumer-1, groupId=message] Node -1 disconnected.
</code></pre>
"
"61225112","Combine two docker images to prevent docker inside docker","<javascript><node.js><docker><server>","61273960","Run a service on a call from another one docker-compose","<docker><server><docker-compose>","<p>I've got a nodejs server inside docker:</p>

<pre><code>FROM node:12

WORKDIR /app

COPY package.json /app

RUN yarn install

COPY . /app

EXPOSE 8080
CMD [ ""yarn"", ""start"" ]
</code></pre>

<p>And then I've got a dockerfile that is used for converting 3D models:</p>

<pre><code>FROM leon/usd:latest

WORKDIR /usr/src/ufg

# Configuration
ARG UFG_RELEASE=""3bf441e0eb5b6cfbe487bbf1e2b42b7447c43d02""
ARG UFG_SRC=""/usr/src/ufg""
ARG UFG_INSTALL=""/usr/local/ufg""
ENV USD_DIR=""/usr/local/usd""
ENV LD_LIBRARY_PATH=""${USD_DIR}/lib:${UFG_SRC}/lib""
ENV PATH=""${PATH}:${UFG_INSTALL}/bin""
ENV PYTHONPATH=""${PYTHONPATH}:${UFG_INSTALL}/python""

# Build + install usd_from_gltf
RUN git init &amp;&amp; \
    git remote add origin https://github.com/google/usd_from_gltf.git &amp;&amp; \
    git fetch --depth 1 origin ""${UFG_RELEASE}"" &amp;&amp; \
    git checkout FETCH_HEAD &amp;&amp; \
    python ""${UFG_SRC}/tools/ufginstall/ufginstall.py"" -v ""${UFG_INSTALL}"" ""${USD_DIR}"" &amp;&amp; \
    cp -r ""${UFG_SRC}/tools/ufgbatch"" ""${UFG_INSTALL}/python"" &amp;&amp; \
    rm -rf ""${UFG_SRC}"" ""${UFG_INSTALL}/build"" ""${UFG_INSTALL}/src""

RUN mkdir /usr/app
WORKDIR /usr/app
# Start the service

ENTRYPOINT [""usd_from_gltf""]
CMD [""usd_from_gltf""]
</code></pre>

<p>The image works like so: When run, the 3d model passed as an argument is converted and then the image stops running</p>

<p>I want to have my node.js server running all the time, and when there's a request for conversion, the second image converts the file. I don't really care whether the second image runs on request, or runs all the time.</p>

<p>How can I do this?</p>
","<p>I've got a docker-compose file that is supposed to do the following:</p>

<ul>
<li>Start up a node.js server that listens on an endpoint</li>
<li>When the endpoint is requested (and on every subsequent visit/reload of the endpoint), the second service runs. Also the <code>command</code> will also be modified</li>
</ul>

<p>Is it possible to achieve that? Alternatively, I'd need the binaries that are built in the <code>converter</code> container accessible in my <code>nodejs</code> container. How could I do that? </p>

<p>My compose file:</p>

<pre><code>version: ""3""
services:
  nodejs:
    image: node:12
    command: sh -c ""yarn install &amp;&amp; yarn start""
    working_dir: /app
    volumes:
      - ./node:/app
    ports:
      - 6969:3000

  converter:
    build: ./usd-from-gltf
    command: sh -c ""usd_from_gltf INPUT_FILE.glb OUTPUT_FILE.usdz""
    ports:
      - 6968:3000
    volumes:
      - ./usd-from-gltf:/usr/app
</code></pre>
"
"47135390","Getting exit code out of docker-compose up","<docker><docker-compose><exit><exit-code><terminate>","61291536","How to stop docker-compose after process termination","<docker><docker-compose><exit><exit-code><terminate>","<p>I have an issue getting an exit code out of <code>docker-compose up</code>.<br />
I have the simplest container that runs a script that always exits with 1:</p>
<pre><code>#!/usr/bin/env sh
exit 1
</code></pre>
<p>My Dockerfile:</p>
<pre><code>FROM mhart/alpine-node:6
RUN mkdir /app
WORKDIR /app
</code></pre>
<p>And my docker-compose.yml:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:
  test_container:
    container_name: test_container
    build: .
    volumes:
      - ${PWD}/run.sh:/app/run.sh
    entrypoint: [&quot;/app/run.sh&quot;]
</code></pre>
<p>When I run it with:</p>
<pre class=""lang-sh prettyprint-override""><code>docker-compose -f docker-compose.yml up --force-recreate test_container
</code></pre>
<p>I can see in logs:</p>
<pre><code>Recreating test_container ... 
Recreating test_container ... done
Attaching to test_container
test_container exited with code 1
</code></pre>
<p>But when I <code>echo $?</code>, I get <code>0</code>.<br />
Docker version 17.09.0-ce, build afdb6d4. Running on OSX 10.12.6.<br />
Am I doing something wrong? Is that a known issue (I couldn't find anything out there)?</p>
","<p>By default <code>docker-compose up</code> keeps running even if some of the containers terminate and <code>restart</code> is set to <code>no</code>. </p>

<p>Is there a configuration to tell it to terminate (and stop all containers) when one of my containers finishes (the process in CMD terminates)?</p>

<p>Bonus points if it gets the exit code from the process of that container</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61304223","Can't correctly send to kafka-docker from network device","<docker><networking><apache-kafka><bitnami>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to setup a Kafka / Docker Setup on my host maschine and connect to it from a network device. I'm using bitnami/kafka. Here is my docker-compose file:</p>

<pre><code>version: '2'

services:
  zookeeper:
    image: 'bitnami/zookeeper:3'
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper_data:/bitnami'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: 'bitnami/kafka:2'
    ports:
      - '9092:9092'
      - '29092:29092'
    volumes:
      - 'kafka_data:/bitnami'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
</code></pre>

<p>When I run my python script on my host maschine ( not within docker ) and mercury being the name of my host it works just fine:</p>

<pre><code>from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers=['mercury:29092'])
print(""Connected"")
producer.send('topic', b'It works!')
print('Theoretically send')
producer.close()
print('Closed')
</code></pre>

<p>When I try to run the same script from another network device it doesn't work. I don't even get an error. The script also runs just fine, so there is no delay when trying to connect or sending. That only happens when I f.e. use a wrong port or a topic that doesn't exist. Especially the last part let's me believe that the script can connect but doesn't send the message correctly. I'm not sure why that's the case. Is my port setup correct or do I need some extra kafka environment settings?</p>

<p>Thanks in advance for the help</p>
"
"60713384","Why git --version statement does not get recognize?","<git><docker>","61306915","Multistage build deletes directories of previous stages?","<node.js><docker>","<p>I have the following Dockerfile with content: </p>

<pre><code>FROM ubuntu:bionic AS os
RUN apt-get update
RUN apt-get install -y git
RUN git --version

FROM node:13.10.1-buster-slim

FROM python:3.7.7-slim-stretch as test
RUN pip install --user pipenv
RUN git --version
RUN git clone git@gitlab.com:silentdata/cdtc-identity-service.git

WORKDIR cdtc-identity-service

RUN pipenv install

CMD python service_test.py
</code></pre>

<p>Building the image, I've got the following output: </p>

<pre><code>Sending build context to Docker daemon  43.59MB
Step 1/12 : FROM ubuntu:bionic AS os
 ---&gt; 72300a873c2c
Step 2/12 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 42013f860b31
Step 3/12 : RUN apt-get install -y git
 ---&gt; Using cache
 ---&gt; 8f27d95fcb6e
Step 4/12 : RUN git --version
 ---&gt; Using cache
 ---&gt; ae49a9465233
Step 5/12 : FROM node:13.10.1-buster-slim
 ---&gt; 500c5a190476
Step 6/12 : FROM python:3.7.7-slim-stretch as test
 ---&gt; c9ec5ac0f580
Step 7/12 : RUN pip install --user pipenv
 ---&gt; Using cache
 ---&gt; 3a9358e72deb
Step 8/12 : RUN git --version
 ---&gt; Running in 545659570a84
/bin/sh: 1: git: not found
The command '/bin/sh -c git --version' returned a non-zero code: 127
</code></pre>

<p>Why the <code>git</code> command could not be found at the second time?</p>
","<p>I've got this docker file, that's supposed to install usd_from_gltf (the first stage) and then run the nodejs server with access to the usd_from_gltf command. But when I run this, it seems the container directories get all deleted and overwritten by the other stage. How do I prevent this?</p>

<pre><code>FROM leon/usd:latest

WORKDIR /usr/src/ufg

# Configuration
ARG UFG_RELEASE=""3bf441e0eb5b6cfbe487bbf1e2b42b7447c43d02""
ARG UFG_SRC=""/usr/src/ufg""
ARG UFG_INSTALL=""/usr/local/ufg""
ENV USD_DIR=""/usr/local/usd""
ENV LD_LIBRARY_PATH=""${USD_DIR}/lib:${UFG_SRC}/lib""
ENV PATH=""${PATH}:${UFG_INSTALL}/bin""
ENV PYTHONPATH=""${PYTHONPATH}:${UFG_INSTALL}/python""

# Build + install usd_from_gltf
RUN git init &amp;&amp; \
    git remote add origin https://github.com/google/usd_from_gltf.git &amp;&amp; \
    git fetch --depth 1 origin ""${UFG_RELEASE}"" &amp;&amp; \
    git checkout FETCH_HEAD &amp;&amp; \
    python ""${UFG_SRC}/tools/ufginstall/ufginstall.py"" -v ""${UFG_INSTALL}"" ""${USD_DIR}"" &amp;&amp; \
    cp -r ""${UFG_SRC}/tools/ufgbatch"" ""${UFG_INSTALL}/python"" &amp;&amp; \
    rm -rf ""${UFG_SRC}"" ""${UFG_INSTALL}/build"" ""${UFG_INSTALL}/src""


FROM node:12

WORKDIR /usr/src/app

COPY package.json ./

RUN yarn install

COPY . .
CMD [""yarn"", ""start""]
</code></pre>
"
"54062327","Running Docker inside Docker container: Cannot connect to the Docker daemon","<docker><ubuntu-16.04>","61321848","Docker within Docker Container","<docker>","<p>I created a Dockerfile to run Docker inside Docker:</p>

<pre><code>    FROM ubuntu:16.04
RUN apt-get update &amp;&amp; \
    apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common &amp;&amp; \
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp;\
    apt-key fingerprint 0EBFCD88

RUN add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"" &amp;&amp; \
   apt-get update &amp;&amp; \
   apt-get install -y docker-ce &amp;&amp; \
   systemctl enable docker
</code></pre>

<p>After i launched my container and run docker ps i got:
""Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?""</p>

<p>i executed the command dockerd inside my container resulted: </p>

<p>Error starting daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables -t nat -N DOCKER: iptables v1.6.0: can't initialize iptables table `nat': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
 (exit status 3)</p>

<p>Please advise</p>
","<p>Im entering a docker container via:
<code>docker run -it ubuntu bash</code></p>

<p>When im within the container, i am installing Docker:
<code>curl -sSL https://get.docker.com/ | sh</code></p>

<p>Yet when I fire <code>dockerd</code> im getting:</p>

<pre><code>INFO[2020-04-20T11:49:01.220025800Z] stopping event stream following graceful shutdown  error=""context canceled"" module=libcontainerd namespace=plugins.moby
failed to start daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables -t nat -N DOCKER: iptables v1.6.1: can't initialize iptables table `nat': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
 (exit status 3)
</code></pre>

<p>Any ideas how to fix this?</p>
"
"53429062","Docker: Springboot container can not connect to PostgreSql Container Connection error","<postgresql><spring-boot><docker><docker-compose>","61329370","Docker postgres on local machine: connection to localhost:5432 refused. Check that the hostname and port are correct","<java><postgresql><spring-boot><docker><dockerfile>","<p>I am building my first Springboot 2.0 application. I am trying to put my Springboot application into one docker container and my PostgresDB into another container.  </p>

<p><strong>My Dockerfile</strong></p>

<pre><code>    FROM frolvlad/alpine-oraclejdk8:slim
    VOLUME /tmp
    ADD springboot-api-demo-0.1*.jar app.jar
    RUN sh -c 'touch /app.jar'
    EXPOSE 9443
    ENTRYPOINT [ ""sh"", ""-c"", ""java $JAVA_OPTS -Djava.security.egd=file:/dev/urandom -jar /app.jar"" ]
</code></pre>

<p><strong>My docker-compose.yml file</strong></p>

<pre><code>version: ""2.1""

services:
  springboot-api-demo:
    image: ""fw/springboot-api-demo""
    mem_limit: 1024m
    ports:
      - ""8080:8080""
    environment:
      - SPRING_PROFILES_ACTIVE=local
      - AWS_REGION=local
      - ENVIRONMENT=local
      - AUTH_ENABLED=false
  postgres:
    container_name: pgdb
    image: postgres:9.6-alpine
    environment:
    - 'POSTGRES_ROOT_PASSWORD=postgres'
    - 'POSTGRES_USER=postgres'
    - 'POSTGRES_PASSWORD=postgres'
    ports:
    - ""54321:5432""
</code></pre>

<p>I am using Springboot JPA Data 2.0 with below config data in my <strong>application.properties</strong></p>

<pre><code>spring.datasource.url= jdbc:postgresql://localhost:54321/java_learning
spring.datasource.username=postgres
spring.datasource.password=postgres
</code></pre>

<p>I can test that Both of the Images are up. Also from docker log and docker events, I see that postgres  Container is running fine, even I can access it and also created a DB too.
But springboot container started but i died because it could not connect to postgress and throwing error below. </p>

<blockquote>
  <p>Unable to obtain connection from database: The connection attempt
  failed</p>
</blockquote>

<p>Note that my host machine already has Postgres on port 5432 thats why I did a port mapping ofr 54321:5432 on my postgres container. Here is Proof :) -</p>

<pre><code>➜  springboot-api-demo git:(master) ✗ lsof -i:54321              
COMMAND     PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 44345 shailendra.singh   18u  IPv4 0xf62897fbdd69e31d      0t0  TCP *:54321 (LISTEN)
com.docke 44345 shailendra.singh   21u  IPv6 0xf62897fbdd119975      0t0  TCP localhost:54321 (LISTEN)

➜  springboot-api-demo git:(master) ✗ lsof -i:5432 
COMMAND  PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 715 shailendra.singh    5u  IPv6 0xf62897fbb43e03b5      0t0  TCP localhost:postgresql (LISTEN)
postgres 715 shailendra.singh    6u  IPv4 0xf62897fbbaeea9bd      0t0  TCP localhost:postgresql (LISTEN)
</code></pre>

<p>I am not sure what is the problem. But my Springboot application is not able to connect my postgres container which is running fine with proper creadentials. </p>
","<p>I have a basic spring-boot application that does CRUD operations on a a postgres instance that is running on docker on my local machine. I am using Intellij to run my spring-boot application. When I run from the IDE, the application works fine with no errors. </p>

<p>But when I build a docker image out of the .jar file of my spring-boot app, I get the below error when I try to do a docker run.</p>

<p>Googled for a day before I posted the question here, help in any direction would help me!</p>

<p><strong>Docker file contents</strong></p>

<pre><code>FROM openjdk:11-jre-slim

COPY  target/demo-*.jar /demo.jar

CMD [""java"", ""-jar"", ""/demo.jar""]
</code></pre>

<p><strong>Error stack trace</strong></p>

<pre><code>Error Code : 0
Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

    at org.flywaydb.core.internal.jdbc.JdbcUtils.openConnection(JdbcUtils.java:60) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.internal.jdbc.JdbcConnectionFactory.&lt;init&gt;(JdbcConnectionFactory.java:80) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.Flyway.execute(Flyway.java:438) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.Flyway.migrate(Flyway.java:149) ~[flyway-core-6.0.8.jar!/:na]
    at org.springframework.boot.autoconfigure.flyway.FlywayMigrationInitializer.afterPropertiesSet(FlywayMigrationInitializer.java:65) ~[spring-boot-autoconfigure-2.2.6.RELEASE.jar!/:2.2.6.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1855) ~[spring-beans-5.2.5.RELEASE.jar!/:5.2.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1792) ~[spring-beans-5.2.5.RELEASE.jar!/:5.2.5.RELEASE]
    ... 54 common frames omitted
Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:285) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:211) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.Driver.makeConnection(Driver.java:459) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.2.11.jar!/:42.2.11]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:354) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:554) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.2.jar!/:na]
    at org.flywaydb.core.internal.jdbc.JdbcUtils.openConnection(JdbcUtils.java:56) ~[flyway-core-6.0.8.jar!/:na]
    ... 60 common frames omitted
Caused by: java.net.ConnectException: Connection refused (Connection refused)
    at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.SocksSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.Socket.connect(Unknown Source) ~[na:na]
    at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:81) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:93) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:197) ~[postgresql-42.2.11.jar!/:42.2.11]
    ... 72 common frames omitted


</code></pre>
"
"52081448","What is the benefit of putting multiple containers in a pod?","<docker><openshift><openshift-enterprise>","61430693","In what case is recommended to use one pod for many containers","<kubernetes><kubernetes-pod>","<p>What's the benefit of having multiple containers in a pod versus having standalone containers?</p>
","<p>In what case is recommended to use one pod for many containers? Is it a good practice?</p>
"
"48376928","On Windows Setup, how can I get docker image from local machine","<docker><kubernetes><minikube>","61454180","minikube's docker engine in powershell","<docker><hyper-v><minikube>","<p>I could understand, different ways to access docker image from local machine to Minikube VM.</p>

<p><a href=""https://stackoverflow.com/questions/46065342/kubernetes-minikube-cant-get-docker-image-from-local-registry"">(Kubernetes + Minikube) can&#39;t get docker image from local registry</a></p>

<p>All these examples are for Mac/Linux user.</p>

<p>I'm looking for an equivalent suggestion for Windows user. </p>

<p>What's windows equivalent to -> eval $(minikube docker-env) </p>
","<p>I've installed minikube on hyper-v windows. everything looks fine but there is a problem in switching to minikube docker engine. I always use <code>eval $(minikube docker-env)</code> in docker toolbox terminal , is there any equivalent for this in PowerShell?</p>
"
"61491853","Docker: Can't access binary copied to certain images","<docker>","61514445","How to use multi-stage build to eliminate /go/pkg files","<linux><docker><alpine><docker-build>","<p>I am trying to access a binary <code>COPY</code>ed from the <code>migrate</code> container.   When I <code>COPY</code> to <code>python:3.7-alpine</code> it works, but when I <code>COPY</code> to <code>debian:buster-slim</code> it can't be found.</p>

<p>Minimum steps to reproduce:</p>

<p>1.Create Dockerfile.test</p>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM python:3.7-alpine
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""2"">
<li>Build and run. This works.</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>Usage: migrate OPTIONS COMMAND [arg...]
       migrate [ -version | -help ]
</code></pre>

<ol start=""3"">
<li>Stop and remove container</li>
</ol>

<pre><code>docker stop migrate_test;docker rm migrate_test;
</code></pre>

<ol start=""4"">
<li>Change image in Dockerfile.test</li>
</ol>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM debian:buster-slim
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""5"">
<li>Build and run. This doesn't work</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>/bin/sh: 1: /migrate: not found
</code></pre>
","<p>I have this dockerfile:</p>

<pre><code>FROM golang:1.13 as cm_base

ADD ./netrc /root/.netrc

ENV GO111MODULE='on'
ENV GOPROXY='direct'
ENV GOSUMDB='off'

RUN mkdir -p /tmp/cm-go-api
WORKDIR /tmp/cm-go-api

ENV github_token='&lt;some-access-token&gt;'
ADD ""https://raw.githubusercontent.com/ChannelMeter/cm-go-api/master/go.mod?token=$github_token""  
RUN go mod download 

ENV PROJECT_DIR '/go/src/github.com/channelmeter/cm-go-api'
WORKDIR ""$PROJECT_DIR""
RUN git init
RUN git remote add origin 'https://github.com/ChannelMeter/cm-go-api.git'
ARG commit_id
RUN git fetch --depth 1 origin ""$commit_id""
RUN git checkout ""$commit_id""

RUN go install -v

FROM alpine:3.11.5

COPY --from=cm_base /go/bin /go/bin

EXPOSE 1992 

ENTRYPOINT [""/go/bin/cm-go-api""]
</code></pre>

<p>the above breaks tho - the binary executable doesn't seem to work, but if I remove the multi-stage part, it works. For example, I remove these 3 lines:</p>

<pre><code>    # FROM alpine:3.11.5
    # COPY --from=cm_base /go/bin /go/bin
</code></pre>

<p>does anyone know why the binary file produced by the original go build command would not work with alpine?</p>

<p>If I use the original base image, it will work:</p>

<pre><code>    FROM golang:1.13
    COPY --from=cm_base /go/bin /go/bin
</code></pre>

<p>The <strong>error</strong> when using Alpine as the 2nd stage is:</p>

<blockquote>
  <p>standard_init_linux.go:211: exec user process caused ""no such file or
  directory""</p>
</blockquote>
"
"54976184","Docker base image includes a volume. How can I stop mounting it in my derived image","<docker>","61595224","Undo Dockerfile VOLUME directive from a base image","<docker><docker-volume>","<p>I have a docker base image that uses a volume and the base image is out of my control</p>

<p>For example: <code>VOLUME [""/my/path""]</code></p>

<p>I'm trying to build a derived image that doesn't define a volume. I want the data in /my/path to be transient and never persisted.</p>

<p>Is there a way I can disable a volume that came from a parent container? </p>
","<p>I have an image derived from the Postgres official image, whose <a href=""https://github.com/docker-library/postgres/blob/master/11/alpine/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile</a> includes the following:</p>

<pre><code>VOLUME /var/lib/postgresql/data
</code></pre>

<p>I'd like to create my own image based on this official image, but I don't want it to reference any volume.  I'd like the Postgres data to be inside my image.</p>

<p>Any ideas please?</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","61671668","K8s Pod memory is higher than JVM heap size","<java><kubernetes><jvm>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>We have a pod running only a JVM process. The JVM memory occupies less memory compared to the memory occupied by Pod. </p>

<p>Below is the output of <code>jmap -heap &lt;PID&gt;</code></p>

<pre><code>Attaching to process ID 8, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.252-b14

using thread-local object allocation.
Garbage-First (G1) GC with 1 thread(s)

Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 536870912 (512.0MB)
   NewSize                  = 1363144 (1.2999954223632812MB)
   MaxNewSize               = 318767104 (304.0MB)
   OldSize                  = 5452592 (5.1999969482421875MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 100663296 (96.0MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 16777216 (16.0MB)

Heap Usage:
G1 Heap:
   regions  = 32
   capacity = 536870912 (512.0MB)
   used     = 138141888 (131.74237060546875MB)
   free     = 398729024 (380.25762939453125MB)
   25.730931758880615% used
G1 Young Generation:
Eden Space:
   regions  = 7
   capacity = 318767104 (304.0MB)
   used     = 117440512 (112.0MB)
   free     = 201326592 (192.0MB)
   36.8421052631579% used
Survivor Space:
   regions  = 1
   capacity = 16777216 (16.0MB)
   used     = 16777216 (16.0MB)
   free     = 0 (0.0MB)
   100.0% used
G1 Old Generation:
   regions  = 2
   capacity = 201326592 (192.0MB)
   used     = 3924160 (3.74237060546875MB)
   free     = 197402432 (188.25762939453125MB)
   1.9491513570149739% used

8145 interned Strings occupying 771960 bytes.
</code></pre>

<p>As you can see the JVM heap uses only 131MB but the pod memory consumption is around 450MB.</p>

<p>Below is the JVM args</p>

<blockquote>
  <p>java -Xmx512m -Xms128m -XX:MetaspaceSize=96m -XX:+UseG1GC
  -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -jar app.jar</p>
</blockquote>

<p>The Java version that we are using is container aware. 
What could be the reason for such a gap in memory consumptions?</p>

<p><strong>Update 1:</strong>
Added pod yaml</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2020-04-30T18:58:03Z""
  labels:
    pod-template-hash: 5c85b85966
    release: app-365b18d80ba58451c00b554a3429c9af-local
  name: pod-name
  namespace: default
  resourceVersion: ""26701471""
spec:
  containers:
  - env:
    - name: LOG_LEVEL
      value: INFO
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: repo/name:tag
    imagePullPolicy: Always
    name: container-name
    resources:
      limits:
        memory: 640Mi
      requests:
        cpu: 100m
        memory: 512Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-bwksb
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: pull-secret
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-bwksb
    secret:
      defaultMode: 420
      secretName: default-token-bwksb
</code></pre>

<p>The base image is <code>azul/zulu-openjdk:8</code></p>

<p>Update 2: Added DockerFile</p>

<pre><code>FROM azul/zulu-openjdk:8

RUN apt update &amp;&amp; apt dist-upgrade -y

WORKDIR /code
COPY ./build/libs/APP.jar ./
COPY ./docker/config config


CMD java -Xmx512m -Xms128m -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -jar APP.jar
</code></pre>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","61807252","Docker Compose : Unable to start react app server","<reactjs><docker><docker-compose>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I am trying to launch a react-springboot application using docker on my mac machine. I am able to individually launch the server and client urls using docker run commands but when docker compose start up fails in the react application with the following error logs:</p>

<p>app-client_1        | ℹ ｢wds｣: Project is running at <a href=""http://172.20.0.2/"" rel=""nofollow noreferrer"">http://172.20.0.2/</a>
app-client_1        | ℹ ｢wds｣: webpack output is served from 
app-client_1        | ℹ ｢wds｣: Content not from webpack is served from /app/public
app-client_1        | ℹ ｢wds｣: 404s will fallback to /
app-client_1        | Starting the development server...
app-client_1        | 
react-proj_app-client_1 exited with code 0
app-client_1        | 
app-client_1        | > my-app@0.1.0 start /app
app-client_1        | > react-scripts start
app-client_1        | </p>

<p>The following is docker-compose.yml file:</p>

<pre><code>version: '3.7'
services:
  # App backend service
  app-server:
    # Configuration for building the docker image for the backend service
    build:
      context: demo
      dockerfile: Dockerfile
    ports:
      - ""8080:8080"" # Forward the exposed port 8080 on the container to port 8080 on the host machine
    restart: always
    depends_on:
      - mysql-standalone # This service depends on mysql. Start that first.
    environment: # Pass environment variables to the service
      SPRING_DATASOURCE_URL: jdbc:mysql://mysql-standalone:3306/my_db
      SPRING_DATASOURCE_USERNAME: sa
      SPRING_DATASOURCE_PASSWORD: password
    networks:
      - backend
      - frontend
  # Frontend Service
  app-client:
    build:
      context: my-app
      dockerfile: Dockerfile
      args:
          REACT_APP_API_BASE_URL: http://localhost:8080
    ports:
      - 3000:3000
    restart: always
    depends_on:
      - app-server
    networks:
      - frontend
    command: [""npm"", ""start""]
  # Database Service (Mysql)
  mysql-standalone:
    image: mysql:5.6
    ports:
      - ""3306:3306""
    restart: always
    environment:
      MYSQL_DATABASE: my_db
      MYSQL_USER: sa
      MYSQL_PASSWORD: password
      MYSQL_ROOT_PASSWORD: some_password
    networks:
      - backend


  # Volumes
volumes:
  db-data:

  # Networks to be created to facilitate communication between containers
networks:
  backend:
  frontend:

</code></pre>

<p>What might be the issue here?</p>
"
"60235353","Docker is not running on Colab","<docker><google-colaboratory>","61822982","Create docker on Google colab","<docker><google-colaboratory>","<p>I have tried  to install Docker on google Colab through the following ways:</p>

<p>(1)<a href=""https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04</a> </p>

<p>(2)<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04</a></p>

<p>(3)<a href=""https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP"" rel=""noreferrer"">https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP</a></p>

<p>I started the docker service and saw the status, but it showed 'Docker is not running'. Maybe the docker can not work on the Colab.
<a href=""https://i.stack.imgur.com/Xxs7O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xxs7O.png"" alt=""enter image description here""></a></p>

<p>I feel confused and want to know the reason.</p>

<p>Thanks</p>
","<p>Is their any way to actually create a docker container on google colab?
I currently have a CPU based docker container on my local and I want to create a GPU based docker and since i developed my code on colab, I am thinking if I can directly create a docker on colab and leverage that.</p>

<p>I found a way that if we clone our Git repository on Google(colab), then we can initialize the docker but my docker size is 3 GB and git doesn't allow that.</p>

<p>Is it possible to do that?</p>
"
"51034120","Docker Could not resolve 'deb.debian.org'","<ubuntu><docker><debian>","61893153","Docker build in Jenkins problem with apt-get install","<docker><jenkins><jenkins-pipeline>","<p>I was trying to build a container in Docker for some experiments. Here is my <code>Dockerfile</code>.</p>

<pre><code>FROM debian

RUN mkdir -p /var/run/sshd

RUN apt-get update
RUN apt-get install -y openssh-server

RUN apt-get install -y sudo

RUN echo AddressFamily inet &gt;&gt; /etc/ssh/sshd_config

ARG username=Rivers
ARG userpasswd=perfectXJ2017

RUN useradd -ms /bin/bash $username &amp;&amp; (echo $username:$userpasswd | chpasswd)

RUN adduser $username sudo

CMD /usr/sbin/sshd -D
</code></pre>

<p>I tried to build my image with the command <code>sudo docker build -t ics-image .</code>.</p>

<p>But then I got some error messages and the whole process stopped. Here is the error messages.</p>

<pre><code>Sending build context to Docker daemon  2.048kB
Step 1/11 : FROM debian
 ---&gt; 8626492fecd3
Step 2/11 : RUN mkdir -p /var/run/sshd
 ---&gt; Running in 1e1f2dbbe5ca
Removing intermediate container 1e1f2dbbe5ca
 ---&gt; dd4bec2f81d4
Step 3/11 : RUN apt-get update
 ---&gt; Running in 022301215bfb
Get:1 http://security.debian.org/debian-security stretch/updates InRelease [94.3 kB]
Get:2 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [450 kB]
Err:3 http://deb.debian.org/debian stretch InRelease
  Could not resolve 'deb.debian.org'
Err:4 http://deb.debian.org/debian stretch-updates InRelease
  Could not resolve 'deb.debian.org'
Fetched 544 kB in 20s (26.9 kB/s)
Reading package lists...
W: Failed to fetch http://deb.debian.org/debian/dists/stretch/InRelease  Could not resolve 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/stretch-updates/InRelease  Could not resolve 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
Removing intermediate container 022301215bfb
 ---&gt; 054d32710b62
Step 4/11 : RUN apt-get install -y openssh-server
 ---&gt; Running in 802d2fa37b8d
Reading package lists...
Building dependency tree...
Reading state information...
Package openssh-server is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Package 'openssh-server' has no installation candidate
The command '/bin/sh -c apt-get install -y openssh-server' returned a non-zero code: 100
</code></pre>

<p>I did all this on Ubuntu 18.04. I cannot understand why this happened and how to solve this problem. Can anyone help?</p>
","<p>i am trying to build a docker image inside of jenkins with a JENKINSFILE.
It works fine when i build it local, but on jenkins i get fellow ouput</p>

<p>Local i use the same command but it works.</p>

<p>Has Anyone a clue why it doesn't works on jenkins?</p>

<p>UPDATE: </p>

<pre><code>+ docker build --no-cache -t app:build .
Sending build context to Docker daemon  11.87MB

Step 1/17 : FROM golang:1.14.3-stretch
 ---&gt; cc1f903b6fe7
Step 2/17 : RUN mkdir -p  /app
 ---&gt; Running in b8a0f9f5c73f
Removing intermediate container b8a0f9f5c73f
 ---&gt; b555e33481ca
Step 3/17 : RUN echo nameserver 8.8.8.8 &gt;&gt; /etc/resolv.conf &amp;&amp; cat /etc/resolv.conf
 ---&gt; Running in fb7779fc731a
# This file is managed by man:systemd-resolved(8). Do not edit.
#
# This is a dynamic resolv.conf file for connecting local clients directly to
# all known uplink DNS servers. This file lists all configured search domains.
#
# Third party programs must not access this file directly, but only through the
# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,
# replace this symlink by a static file or a different symlink.
#
# See man:systemd-resolved.service(8) for details about the supported modes of
# operation for /etc/resolv.conf.

nameserver 8.8.8.8
Removing intermediate container fb7779fc731a
 ---&gt; 8445167d56f7
Step 4/17 : RUN chmod 777 /etc/resolv.conf
 ---&gt; Running in d910167101cf
Removing intermediate container d910167101cf
 ---&gt; eeb553e7fb83
Step 5/17 : RUN apt-get update &amp;&amp; yes | apt-get install libxml2
 ---&gt; Running in 3c09069d085c
Err:1 http://security.debian.org/debian-security stretch/updates InRelease
  Could not resolve 'security.debian.org'
Err:2 http://deb.debian.org/debian stretch InRelease
  Could not resolve 'deb.debian.org'
Err:3 http://deb.debian.org/debian stretch-updates InRelease
  Could not resolve 'deb.debian.org'
Reading package lists...
[91mW: Failed to fetch http://deb.debian.org/debian/dists/stretch/InRelease  Could not resolve 'deb.debian.org'[0m[91m
W: Failed to fetch http://security.debian.org/debian-security/dists/stretch/updates/InRelease  Could not resolve 'security.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/stretch-updates/InRelease  Could not resolve 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
[0mReading package lists...
Building dependency tree...
Reading state information...
Package libxml2 is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

[91mE: Package 'libxml2' has no installation candidate[0m[91m
[0mThe command '/bin/sh -c apt-get update &amp;&amp; yes | apt-get install libxml2' returned a non-zero code: 100
</code></pre>

<p>Thanks!</p>
"
"59877561","selenium.common.exceptions.InvalidCookieDomainException: Message: invalid cookie domain while executing tests in Django with Selenium","<django><selenium><docker><docker-compose><selenium-grid>","61919269","Python Selenium GET with logged cookie or custom cookie","<python><selenium>","<p>I am setting up tests for both chrome and firefox using seleniumgrid.I am using docker images selenium-hub and selenium node-chrome and node-firefox as below.</p>

<pre><code>  app:
    build: .
    command: gunicorn --reload --capture-output --log-level debug --access-logfile - -w 3 -b 0.0.0.0 app.wsgi
    restart: always
    volumes_from:
        - initialize
    ports:
      - ""8000:8000""
    links:
      - db
      - rabbitmq
      - selenium_hub
    env_file: secrets.env
    volumes:
        - ./app/:/code/

  selenium_hub:
    image: selenium/hub
    ports:
      - 4444:4444
    expose:
      - 4444
    tty: true
    environment:
      - GRID_MAX_SESSION=20
      - GRID_NEW_SESSION_WAIT_TIMEOUT=60000
      - GRID_BROWSER_TIMEOUT=300
      - GRID_TIMEOUT=300
      - TIMEOUT=300
  node_1:
    image: selenium/node-chrome
    depends_on:
      - selenium_hub
    environment:
      - HUB_HOST=selenium_hub
      - HUB_PORT=4444
      - NODE_MAX_SESSION=3
      - NODE_MAX_INSTANCES=2
    shm_size: 2GB
  node_2:
    image: selenium/node-firefox
    environment:
      - HUB_HOST=selenium_hub
      - HUB_PORT=4444
      - NODE_MAX_SESSION=3
      - NODE_MAX_INSTANCES=2
    shm_size: 2GB
    depends_on:
      - selenium_hub
</code></pre>

<p>When I try to run the tests I am always running into this error <code>InvalidCookieDomainException: Message: invalid cookie domain</code>. I have already set domain to <code>self.live_server_url</code>.
below is the full traceback with the test setup.</p>

<pre><code>  Traceback (most recent call last):
    File ""/code/frontend/tests/test_user_auth.py"", line 75, in setUp
        ""port"": ""8082"",
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py"", line 894, in add_cookie
        self.execute(Command.ADD_COOKIE, {'cookie': cookie_dict})
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
        self.error_handler.check_response(response)
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
        raise exception_class(message, screen, stacktrace)
    selenium.common.exceptions.InvalidCookieDomainException: Message: invalid cookie domain
    (Session info: chrome=77.0.3865.75)
</code></pre>

<p>Test <a href=""https://elennion.wordpress.com/2018/11/23/django-automated-testing-with-selenium/"" rel=""noreferrer"">reference tutorial</a>.</p>

<pre><code>class TestUserCreate(StaticLiveServerTestCase):
    fixtures = [""test.json""]
    port = 8082

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        caps = {
            ""browserName"": os.getenv(""BROWSER"", ""chrome""),
            ""javascriptEnabled"": True,
         }
        cls.driver = webdriver.Remote(
            command_executor=""http://selenium_hub:4444/wd/hub"",
            desired_capabilities=caps,
         )
        cls.driver.implicitly_wait(10)

    @classmethod
    def tearDownClass(cls):
        cls.driver.quit()
        super().tearDownClass()

    def setUp(self):
        # Login the user
        self.assertTrue(self.client.login(username=""james"", password=""changemequick""))

        # Add cookie to log in the browser
        cookie = self.client.cookies[""sessionid""]
        self.driver.get(self.live_server_url + reverse(""find_patient""))
        self.driver.add_cookie(
            {
                ""name"": ""sessionid"",
                ""value"": cookie.value,
                ""domain"": ""localhost""
            }
         )
        super().setUp()

    def test_form_loader(self):
        # test forms loader is functioning properly

        driver = self.driver
        driver.get(self.live_server_url + ""/accounts/login/"")

        driver.find_element_by_xpath(""//input[@type='submit']"").click()
        driver.get_screenshot_as_file(""login.png"")
        assert len(driver.find_elements_by_css_selector("".loading"")) == 0
</code></pre>
","<p>Is it possible for me to set a cookie with Selenium?</p>

<p>My cookie as I would like it is as follows:</p>

<p><code>cookie=nsjsjshshs; cookie1=djdjjdjddh; ...</code></p>

<p>How can I open an url (GET Method) with custom cookies? </p>
"
"56649582","Substitute environment variables in NGINX config from docker-compose","<docker><nginx><docker-compose><environment-variables>","61932742","Include docker compose startup command in Dockerfile or docker run","<shell><docker><nginx><docker-compose><dockerfile>","<p>I am trying to start an NGINX server within a docker container configured through docker-compose. The catch is, however, that I would like to substitute an environment variable inside of the http section, specifically within the ""upstream"" block. </p>

<p>It would be awesome to have this working, because I have several other containers that are all configured through environment variables, and I have about 5 environments that need to be running at any given time. I have tried using ""envsubst"" (as suggested by the official NGINX docs), perl_set, and set_by_lua, however none of them appear to be working.</p>

<p>Below is the NGINX config, as it is after my most recent trial</p>

<pre><code>user  nginx;
worker_processes  1;
env NGINXPROXY;

load_module modules/ngx_http_perl_module.so;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}

http {
    perl_set $nginxproxy 'sub { return $ENV{""NGINXPROXY""}; }';

    upstream api-upstream {
        server ${nginxproxy};
    }

    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    access_log  /var/log/nginx/access.log  main;

    sendfile        off;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
}

</code></pre>

<p>Below is the NGINX dockerfile</p>

<pre><code># build stage
FROM node:latest
WORKDIR /app
COPY ./ /app
RUN npm install
RUN npm run build

# production stage
FROM nginx:1.17.0-perl
COPY --from=0 /app/dist /usr/share/nginx/html
RUN apt-get update &amp;&amp; apt-get install -y gettext-base
RUN rm /etc/nginx/conf.d/default.conf
RUN rm /etc/nginx/nginx.conf
COPY default.conf /etc/nginx/conf.d
COPY nginx.conf /etc/nginx
RUN mkdir /certs
EXPOSE 80 443
CMD [""nginx"", ""-g"", ""daemon off;""]

</code></pre>

<p>Below is the section of the docker-compose.yml for the NGINX server (with names and IPs changed). The envsubst command is intentionally commented out at this point in my troubleshooting.</p>

<pre><code>front-end:
        environment:
            - NGINXPROXY=172.31.67.100:9300
        build: http://gitaccount:password@gitserver.com/group/front-end.git#develop
        container_name: qa_front_end
        image: qa-front-end
        restart: always
        networks:
            qa_network:
                ipv4_address: 172.28.0.215
        ports:
            - ""9080:80""
        # command: /bin/bash -c ""envsubst '$$NGINXPROXY' &lt; /etc/nginx/nginx.conf &gt; /etc/nginx/nginx.conf &amp;&amp; nginx -g 'daemon off;'""
</code></pre>

<p>What appears to be happening is when I reference the $nginxproxy variable in the upstream block (right after ""server""), I get output that makes it look like it's referencing the string literal ""$nginxproxy"" rather than substituting the value of the variable.</p>

<pre><code>qa3_front_end       | 2019/06/18 12:35:36 [emerg] 1#1: host not found in upstream ""${nginx_upstream}"" in /etc/nginx/nginx.conf:19
qa3_front_end       | nginx: [emerg] host not found in upstream ""${nginx_upstream}"" in /etc/nginx/nginx.conf:19
qa3_front_end exited with code 1
</code></pre>

<p>When I attempt to use envsubst, I get an error that makes it sound like the command messed with the format of the nginx.conf file</p>

<pre><code>qa3_front_end       | 2019/06/18 12:49:02 [emerg] 1#1: no ""events"" section in configuration
qa3_front_end       | nginx: [emerg] no ""events"" section in configuration
qa3_front_end exited with code 1
</code></pre>

<p>I'm pretty stuck, so thanks in advance for your help.</p>
","<p>I am using following command in docker-compose to copy environment values into nginx configuration file at startup of nginx container.</p>

<pre><code>command: /bin/sh -c ""envsubst &lt; /etc/nginx/conf.d/site.template &gt; /etc/nginx/conf.d/default.conf &amp;&amp; exec nginx -g 'daemon off;'""
</code></pre>

<p>I want to get rid of docker-compose and use it with the docker run. I tried to inlcude the command in Dockerfile and use CMD. But the ngix container immediately exit.</p>

<pre><code>FROM mydocker:latest
COPY ./site.template /etc/nginx/conf.d/site.template

CMD envsubst &lt; /etc/nginx/conf.d/site.template &gt; /etc/nginx/conf.d/default.conf &amp;&amp; exec nginx -g 'daemon off;'
</code></pre>

<p>Can someone please help me here.
Thanks in advance</p>
"
"58395566","lsb_release: command not found in latest Ubuntu Docker container","<linux><docker><ubuntu><containers><version>","61972822","Interesting problem with GithubActions+Docker","<docker><ubuntu><github-actions>","<p>I just wanted to test something out real quick. So I ran a docker container and I wanted to check which version I was running:</p>

<pre><code>$ docker run -it ubuntu    
root@471bdb08b11a:/# lsb_release -a
bash: lsb_release: command not found
root@471bdb08b11a:/# 
</code></pre>

<p>So I tried installing it (as <a href=""http://www.andryhacks.com/bash-lsb_release-command-not-found/"" rel=""noreferrer"">suggested here</a>):</p>

<pre><code>root@471bdb08b11a:/# apt install lsb_release
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package lsb_release
root@471bdb08b11a:/# 
</code></pre>

<p>Anybody any idea why this isn't working?</p>
","<p>So it turns out that the Github team <a href=""https://github.com/actions/virtual-environments/issues/228"" rel=""nofollow noreferrer"">hasn't added yet support for Ubuntu 20.04 to GithubActions</a>.</p>

<p>However, <a href=""https://help.github.com/en/actions/creating-actions/creating-a-docker-container-action"" rel=""nofollow noreferrer"">GithubActions has native support for Docker</a>, so I thought, why not use that?</p>

<p>Then I used <a href=""https://github.com/knocte/hello-world-docker-action/commit/159ad202b285af5fae9f593a35db762e9f65caf7"" rel=""nofollow noreferrer"">their hello-world template</a>, <a href=""https://github.com/knocte/hello-world-docker-action/commit/d69e535fcc7ae26c88a8f88753c5ceb324d7e677"" rel=""nofollow noreferrer"">added a workflow to make use of it</a>, and <a href=""https://github.com/knocte/hello-world-docker-action/commit/a97e3f487361082515838b0939bf2ad5903fc2c1"" rel=""nofollow noreferrer"">modified it to use Ubuntu 20.04 instead of Alpine Linux</a>.</p>

<p>However, <a href=""https://github.com/knocte/hello-world-docker-action/runs/702035401"" rel=""nofollow noreferrer"">the logs of it</a> show the error:</p>

<pre><code>Reading package lists...
E: Unable to locate package lsb-release
</code></pre>

<p>How is that possible? lsb-release apt package should exist in Ubuntu 20.04. Am I missing something obvious about Docker?</p>

<p><strong>UPDATE:</strong> Not really a duplicate because I was already installing the <code>lsb-release</code> package in my script, what I was missing is a call to <code>apt update</code> before doing it.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61990336","Kafka - Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092","<docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to get one of my services up and running locally within the docker network, but keep getting <strong>""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092""</strong> exception.</p>

<pre><code>zookeeper:
  image: wurstmeister/zookeeper:3.4.6
  restart: always
  expose:
      - ""2181""



kafka:
      depends_on:
          - zookeeper
      environment:
          KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
          KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      hostname: kafka
      image: wurstmeister/kafka:2.11-2.0.0
      restart: always
      ports:
          - ""9092:9092""
      expose:
          - ""9093""
      links:
          - zookeeper:zookeeper
</code></pre>

<p>That's what I've set in my service's docker-compose for the consumers:</p>

<pre><code>  consumers:
    environment:
      - BROKER_URL=host.docker.internal:9092
</code></pre>

<p>That's the error message:</p>

<pre><code>..`block in execute'"",""/usr/local/bundle/gems/rake-12.3.2/lib/rake/task.rb:273:in `each'""],
""message"":""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092""},
""message"":""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092"",
""@timestamp"":""2020-05-24T18:08:09.434+00:00"",""@version"":""1"",
""severity"":""ERROR"",""host"":""87c5265c6a39""}
</code></pre>

<p><strong>UPDATE:</strong> When checking Kafkacat:</p>

<pre><code>kafkacat -L -b 127.0.0.1:9092
Metadata for all topics (from broker 0: 127.0.0.1:9092/0):
 1 brokers:
  broker 0 at 127.0.0.1:9092 (controller)
 7 topics: ...
</code></pre>

<p>When checking Zookeeper CLI (within Docker):</p>

<pre><code>[zk: localhost:2181(CONNECTED) 2] ls /brokers/ids
[1001]
</code></pre>

<p>I've tried already pretty much everything I could find, but still not able to understand what am I missing.</p>

<p>Any help would be appreciated, thanks!</p>
"
"48745936","Many <none> images created after build a docker image","<docker><dockerfile>","62061332","How to avoid Docker images with Repository and tag <none> while building an new Image","<docker><docker-compose><dockerfile>","<pre><code>FROM scratch
MAINTAINER Aario &lt;AarioAi@gmail.com&gt;
ENV SHARED_GROUP docker
</code></pre>

<p>I build a docker image with above dockerfile.
After runing <code>docker build -t ""aario/centos"" .</code> 
It creates this image aario/centos and the <code>&lt;none&gt;</code> image:</p>

<p><a href=""https://i.stack.imgur.com/CDdH4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CDdH4.png"" alt=""enter image description here""></a></p>

<p>Is it ok? And how to solve it?</p>

<p>When I run <code>docker rmi 2f???????</code> to remove the aario/centeros image. The <code>&lt;none&gt;</code> image will be removed on the same time.</p>
","<p>I am trying to build a new docker image. And this is how my Dockerfile look like</p>

<pre><code>FROM docker-standard-all/dev_env_g3_g5:latest

WORKDIR /root/

COPY controller.pom.xml .
COPY docker_settings.xml .
COPY stage.sh .

RUN mkdir ~/.m2

RUN ln -sf ~/docker_settings.xml ~/.m2/settings.xml

# Initialize Maven repository
RUN mvn dependency:resolve -f controller.pom.xml -q

ENTRYPOINT [""./stage.sh""]
</code></pre>

<p>After building a docker</p>

<pre><code>docker build -t bc/controller-in-docker:1.0.2"" ""${PATH}""
</code></pre>

<p>When i perform </p>

<pre><code>docker images -a
REPOSITORY                                          TAG                 IMAGE ID            CREATED             SIZE
bc/controller-in-docker                             1.0.2               a32e212bc0f7        2 hours ago         2.41GB
&lt;none&gt;                                              &lt;none&gt;              f4e68f5a2720        2 hours ago         2.41GB
&lt;none&gt;                                              &lt;none&gt;              b71751242efd        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              26e3a086889c        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              4acf1759940a        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              0d66510e6a67        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              adc5c7038a4b        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              a994c54e00af        2 hours ago         2.4GB
docker-standard-all/dev_env_g3_g5                   1.17.4              6a83d123f497        6 months ago        2.4GB
</code></pre>

<p>After a bit googling and stack over-overflow, I just realized that : containers are not dangling containers. In fact they appeared to be formed after running the commands like COPY, RUN etc in Dockerfile.</p>

<p>Questions:</p>

<ol>
<li><p>What is the overall size of my new docker images? My each none:none container appears to occupy ~2.41GB from terminal output. In total they all add unto 2.4 * 7 GB</p></li>
<li><p>Can I delete these none:none Images ? They are not dangling images for sure. But they are occupying 2.4 GB each (not sure though) and also confusing name associated with them.</p></li>
<li><p>Is it possible to avoid these none:none images while building the new docker Image ? Any suggestions/modifications to the Dockerfile in order to avoid them?</p></li>
</ol>

<p>Thanks</p>
"
"48593016","Postgresql Docker role does not exist","<database><postgresql><docker>","62084379","Not able to connect to the user using Postgres python package in docker container","<python><postgresql><docker><docker-compose>","<p>I downloaded the docker container for postgres: <a href=""https://hub.docker.com/r/library/postgres/"" rel=""noreferrer"">https://hub.docker.com/r/library/postgres/</a>, and did the following:</p>

<pre><code>$ docker run --name satgres -e POSTGRES_PASSWORD=mysecretpassword -d -p 5432:5432 postgres
satgres | ""docker-entrypoint.s…"" | 5 seconds ago | Up 6 seconds | 0.0.0.0:5432-&gt;5432/tcp | satgres
$ docker exec -it c8f1b22cc41e /bin/bash        
# psql -U postgres
psql (10.1)
postgres=# CREATE DATABASE mytest;
postgres=# \password postgres
postgres=# CREATE ROLE ming WITH LOGIN PASSWORD 'pass1234';
postgres=# ALTER ROLE ming CREATEDB;
postgres=# CREATE DATABASE sjk;
postgres=# GRANT ALL PRIVILEGES ON DATABASE youtube TO ming;
postgres=# \connect sjk
youtube=# ALTER ROLE ming lOGIN;  
</code></pre>

<p>My plan is to use this database on docker with Django, so first want to check I can connect, but I cant.</p>

<pre><code>$ psql -h localhost -p 5432 -U postgres
$ psql: FATAL:  role ""postgres"" does not exist
$ psql -h localhost -p 5432 -U ming
$ psql: FATAL:  role ""ming"" does not exist
</code></pre>

<p>I do the same with PSequal.app, says the same thing,
Im running on Mac High Sierra. </p>

<p>Why am I getting this error? The role exists, or at least it seems it to me...</p>
","<p>I have created a docker image of postgres along with Kafka stream using docker-compose.</p>

<pre><code>version: ""3""

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - stream-network
  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    networks: 
        - stream-network
  streamer:
    build:
      context: ./streamingProducer/
    networks: 
      - stream-network
    depends_on:
      - kafka
  consumer:
    build:
      context: ./streamingConsumer/
    networks: 
      - stream-network
    depends_on:
      - kafka
      - postgres  
  postgres:
    # image: postgres
    # restart: always
    # environment:
    #   POSTGRES_USER: docker
    #   POSTGRES_PASSWORD: docker
    #   POSTGRES_DB: test
    #   PGDATA: /var/lib/postgresql/data/pgdata
    # ports:
    #   - ""5432:5432""
    # healthcheck:
    #   test: [""CMD-SHELL"", ""pg_isready -U docker""]
    # volumes:
    #   - ./data/postgres/pgdata:/var/lib/postgresql/data/pgdata
      build: './db'
      restart: always
      environment:
        - ""FILLA_DB_USER=filla""
        - ""FILLA_DB_PASSWORD=filla""
        - ""FILLA_DB_DATABASE=filladb1""
        - ""POSTGRES_USER=postgres""
        - ""POSTGRES_PASSWORD=password""
      ports:
        - ""5432:5432""
      networks: 
        - stream-network 
</code></pre>

<p>The docker image of Postgres seems to run fine, the log looks completely good</p>

<pre><code>`postgres_1 | creating configuration files … ok
zookeeper_1 | ===&gt; User
postgres_1 | running bootstrap script … ok
zookeeper_1 | uid=0(root) gid=0(root) groups=0(root)
zookeeper_1 | ===&gt; Configuring …
postgres_1 | performing post-bootstrap initialization … sh: locale: not found
postgres_1 | 2020-05-27 11:52:45.209 UTC [29] WARNING: no usable system locales were found
slimmerai_consumer_1 exited with code 0
postgres_1 | ok
postgres_1 | syncing data to disk … ok
postgres_1 | initdb: warning:
postgres_1 | enabling “trust” authentication for local connections
postgres_1 | You can change this by editing pg_hba.conf or using the option -A, or
postgres_1 | --auth-local and --auth-host, the next time you run initdb.
postgres_1 |
postgres_1 | Success. You can now start the database server using:
postgres_1 |
postgres_1 | pg_ctl -D /var/lib/postgresql/data -l logfile start
postgres_1 |
postgres_1 | waiting for server to start…2020-05-27 11:52:46.729 UTC [34] LOG: starting PostgreSQL 12.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 9.2.0) 9.2.0, 64-bit
postgres_1 | 2020-05-27 11:52:46.736 UTC [34] LOG: listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”
postgres_1 | 2020-05-27 11:52:46.767 UTC [35] LOG: database system was shut down at 2020-05-27 11:52:46 UTC
postgres_1 | 2020-05-27 11:52:46.771 UTC [34] LOG: database system is ready to accept connections
postgres_1 | done
postgres_1 | server started
postgres_1 |
postgres_1 | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/01-filladb.sh
postgres_1 | CREATE ROLE
postgres_1 | CREATE DATABASE
postgres_1 | GRANT
postgres_1 |
postgres_1 | 2020-05-27 11:52:46.989 UTC [34] LOG: received fast shutdown request
postgres_1 | waiting for server to shut down…2020-05-27 11:52:46.991 UTC [34] LOG: aborting any active transactions
postgres_1 | 2020-05-27 11:52:46.996 UTC [34] LOG: background worker “logical replication launcher” (PID 41) exited with exit code 1
postgres_1 | 2020-05-27 11:52:46.996 UTC [36] LOG: shutting down
postgres_1 | 2020-05-27 11:52:47.010 UTC [34] LOG: database system is shut down
postgres_1 | done
postgres_1 | server stopped
postgres_1 |
postgres_1 | PostgreSQL init process complete; ready for start up.
postgres_1 |
postgres_1 | 2020-05-27 11:52:47.100 UTC [1] LOG: starting PostgreSQL 12.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 9.2.0) 9.2.0, 64-bit
postgres_1 | 2020-05-27 11:52:47.101 UTC [1] LOG: listening on IPv4 address “0.0.0.0”, port 5432
postgres_1 | 2020-05-27 11:52:47.101 UTC [1] LOG: listening on IPv6 address “::”, port 5432
postgres_1 | 2020-05-27 11:52:47.104 UTC [1] LOG: listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”
postgres_1 | 2020-05-27 11:52:47.125 UTC [45] LOG: database system was shut down at 2020-05-27 11:52:47 UTC
postgres_1 | 2020-05-27 11:52:47.129 UTC [1] LOG: database system is ready to accept connections
`
</code></pre>

<p>And when I log into the database docker container, I can see the users and all the db being created.</p>

<p>But when I try to access the db using python package</p>

<pre><code>from psycopg2 import Error

try:
connection = psycopg2.connect(user = ""postgres"",
password = ""password"",
host = ""localhost"",
port = ""5432"",
database = ""filladb1"")


cursor = connection.cursor()

print(""Table connected successfully in PostgreSQL "")
except (Exception, psycopg2.DatabaseError) as error :
print (“Error while creating PostgreSQL table”, error)`
</code></pre>

<p>It throws an error:</p>

<p><strong>Error while creating PostgreSQL table FATAL: role “postgres” does not exist</strong></p>

<p>Not sure, what am I doing wrong, any help would be appreciated. Thanks.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62136489","Spring can't find running kafka image","<java><spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am new to Kafka and I am trying to send messages via publisher using spring application and kafka docker container. Kafka and zookeeper is up and running but when I start springboot application it shows:</p>

<pre><code>2020-06-01 18:19:05.738  WARN 4824 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Connection to node -1 (/127.0.0.1:9092) could not be established. Broker may not be available.

2020-06-01 18:19:12.203  WARN 4824 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Bootstrap broker 127.0.0.1:9092 (id: -1 rack: null) disconnected
</code></pre>

<p>Here you can see both containers running:</p>

<pre><code>4f74ceac73ba   wurstmeister/kafka:2.12-2.5.0   ""start-kafka.sh""  0.0.0.0:9092-&gt;9092/tcp kafka 

fc958792a4a0  zookeeper:3.6.1 ""/docker-entrypoint.…"" 2888/tcp, 3888/tcp, 0.0.0.0:2181 &gt;2181/tcp, 8080/tcp   zookeeper
</code></pre>

<p>Here you can see zookeeper logs:</p>

<pre><code>2020-06-01 13:17:39,518 [myid:1] - INFO  [main:SnapStream@61] - zookeeper.snapshot.compression.method = CHECKED
2020-06-01 13:17:39,519 [myid:1] - INFO  [main:FileTxnSnapLog@470] - Snapshotting: 0x0 to /data/version-2/snapshot.0
2020-06-01 13:17:39,536 [myid:1] - INFO  [main:ZKDatabase@289] - Snapshot loaded in 45 ms, highest zxid is 0x0, digest is 1371985504
2020-06-01 13:17:39,543 [myid:1] - INFO  [main:FileTxnSnapLog@470] - Snapshotting: 0x0 to /data/version-2/snapshot.0
2020-06-01 13:17:39,544 [myid:1] - INFO  [main:ZooKeeperServer@519] - Snapshot taken in 2 ms
2020-06-01 13:17:39,571 [myid:1] - INFO  [main:RequestThrottler@74] - zookeeper.request_throttler.shutdownTimeout = 10000
2020-06-01 13:17:39,671 [myid:1] - INFO  [main:ContainerManager@83] - Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0
2020-06-01 13:17:39,674 [myid:1] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
2020-06-01 13:17:40,943 [myid:1] - INFO  [SyncThread:0:FileTxnLog@284] - Creating new log file: log.1
</code></pre>

<p>and docker-compose file to bootstrap containers:</p>

<pre><code>version: ""3.7""

networks:
  kafka-net:
    name: kafka-net
    driver: bridge

services:
  zookeeper:
    image: zookeeper:3.6.1
    container_name: zookeeper
    restart: always
    networks:
      - kafka-net
    ports:
      - ""2181:2181""
    volumes:
      - c:/kafka/docker-data/zookeeper:/bitnami/zookeeper

  kafka:
    image: wurstmeister/kafka:2.12-2.5.0
    container_name: kafka
    restart: always
    networks:
      - kafka-net
    ports:
      - ""9092:9092""
    volumes:
      - c:/kafka/docker-data/kafka:/bitnami/kafka
    environment:
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: DOCKER_INTERNAL:PLAINTEXT,DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: DOCKER_INTERNAL://:29092,DOCKER_EXTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: DOCKER_INTERNAL://kafka:29092,DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: ""zookeeper:2181""
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
</code></pre>

<p>There is no additional configuraion in application-properties. Any help would be appreciated, I am pretty much stuck with it :(</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","62137599","Docker requires sudo to run","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>When I try to run any docker command, for example <code>docker ps</code>, I'm getting this error:</p>

<blockquote>
  <p>Got permission denied while trying to connect to the Docker daemon
  socket at unix:///var/run/docker.sock: Get
  <a href=""http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json"" rel=""nofollow noreferrer"">http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json</a>: dial unix
  /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<p>So I manage to run it using sudo:</p>

<pre><code>sudo docker ps
</code></pre>

<p>But I want to be able to run that without sudo.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62213671","How to solve kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs?","<docker><apache-kafka><kafka-python>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>My docker-compose.yml code is:</p>

<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
</code></pre>

<p>and my dockerfile is</p>

<pre><code>FROM python
MAINTAINER Shubham Joshi 
ADD hello.py /
ADD transactions_producer.py /
COPY requirements.txt .
RUN pip3 install -r requirements.txt
CMD [""python"",""./transactions_producer.py""]
</code></pre>

<p>python code for the producer.py is:</p>

<pre><code>from kafka import KafkaProducer
from time import sleep
from kafka.errors import KafkaError
import json

producer = KafkaProducer(bootstrap_servers=['localhost:9092'],api_version=(0, 10, 0),value_serializer=lambda v: json.dumps(v).encode('utf-8'))


for i in range(10):
    producer.send('shubham', create_random_transaction())
    sleep(5)
    print(""Success"",i)

producer.close()
</code></pre>

<p>shubham is the topic I created but I am not able to produce/publish messages in the topic and getting this timeout error.</p>

<p>step 1: Created an image:</p>

<pre><code>docker build -t python_producer2 .
</code></pre>

<p>it built successfully, the I ran</p>

<pre><code>step 2:docker run python_producer2
</code></pre>

<p>this is where I am getting this error</p>

<pre><code>Traceback (most recent call last):
  File ""./transactions_producer.py"", line 52, in &lt;module&gt;
    producer.send('shubham', create_random_transaction())
  File ""/usr/local/lib/python3.8/site-packages/kafka/producer/kafka.py"", line 564, in send
    self._wait_on_metadata(topic, self.config['max_block_ms'] / 1000.0)
  File ""/usr/local/lib/python3.8/site-packages/kafka/producer/kafka.py"", line 690, in _wait_on_metadata
    raise Errors.KafkaTimeoutError(
kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs.
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62228060","How to fix error: kafka.errors.NoBrokersAvailable: NoBrokersAvailable?","<docker><apache-kafka><docker-compose><kafka-python><confluent-platform>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have setup docker-compose.yml as </p>

<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_HOST_NAME: kafka

  client:
    image: python_consumer2
    container_name: python_consumer2
    depends_on: 
      - kafka
</code></pre>

<p>While doing docker-compose -f docker-compose.yml up, I am getting below error, except this kafka and zookeeper sevices are up. </p>

<pre><code>python_consumer2 |   File ""./transactions_consumer.py"", line 8, in &lt;module&gt;
python_consumer2 |     consumer = KafkaConsumer(bootstrap_servers=['localhost:9092'],
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/consumer/group.py"", line 355, in __init__
python_consumer2 |     self._client = KafkaClient(metrics=self._metrics, **self.config)
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/client_async.py"", line 242, in __init__
python_consumer2 |     self.config['api_version'] = self.check_version(timeout=check_timeout)
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/client_async.py"", line 898, in check_version
python_consumer2 |     raise Errors.NoBrokersAvailable()
python_consumer2 | kafka.errors.NoBrokersAvailable: NoBrokersAvailable
</code></pre>

<p>My python_consumer.py contains below configurations: </p>

<pre><code>consumer = KafkaConsumer(bootstrap_servers=['localhost:9092'],
                                 auto_offset_reset='earliest',
                                 value_deserializer=lambda m: json.loads(m.decode('utf-8')),consumer_timeout_ms=10000)
consumer.subscribe(['shubham'])
</code></pre>

<p>I have already published data in the kafka topic 'shubham' I am subscribing to and I have tried changing bootstrap_servers = ['kafka:29092'], still I am getting the same error. How to fix this?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62240417","Unable to create Kafka topics with Kafka and Zookeeper running on Docker","<docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have Kafka and Zookeeper running on two separate Docker containers:</p>

<pre><code>&lt;private-domain&gt;/wurstmeister-kafka:0.10.1.0-2
&lt;private-domain&gt;/wurstmeister-zookeeper:3.4.9
</code></pre>

<p>Both containers seem to be up, but when I try to create Kafka topics by getting in to the first container:</p>

<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
</code></pre>

<p>I get this error:</p>

<pre><code>java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2020-06-07 03:10:55,293] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
</code></pre>

<p>Please notice that I did read other related questions and tried adding arguments to the command, such as <code>-e ZK_HOSTS=""localhost:2181""</code>. I know of other people working in the environment as mine who were able to run the commands successfully, so I suspect this might be a configuration issue on my side. Can you please guide?</p>

<p><strong>EDIT:</strong> Here are the Docker Compose files:</p>

<pre><code>version: '2'
services:
 kafka:
   image: &lt;private-domain&gt;/wurstmeister-kafka:0.10.1.0-2
   container_name: kafka
   ports:
      - 9092:9092
   environment:
     KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
     KAFKA_ADVERTISED_PORT: 9092
     KAFKA_ZOOKEEPER_CONNECT: 127.0.0.1:2181

   restart:
       ""unless-stopped""
</code></pre>

<p>and</p>

<pre><code>version: '2'
services:
 zk:
  image: &lt;private-domain&gt;/wurstmeister-zookeeper:3.4.9
  container_name: zk
  ports:
      - ""2181:2181""
  restart:
   ""unless-stopped""
</code></pre>

<p>and the output of <code>docker ps</code>:</p>

<pre><code>CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                                                NAMES
bf67a49da57a        wurstmeister-kafka:0.10.1.0-2   ""start-kafka.sh""         5 months ago        Up 29 minutes       0.0.0.0:9092-&gt;9092/tcp                               kafka
ef3e908d82b3        wurstmeister-zookeeper:3.4.9    ""/bin/sh -c '/usr/sbin/sshd &amp;&amp; bash /usr/bin/start-zk.sh'""   5 months ago        Up 29 minutes       22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   zk
</code></pre>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","62243616","Node container is crashing without error output","<node.js><docker>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I'm trying to run Node container named Client.</p>

<p>Dockerfile:</p>

<pre><code>FROM node:14.4.0-alpine

WORKDIR /usr/src/app

ENV PATH /usr/src/app/node_modules/.bin:$PATH

COPY package.json /usr/src/app/package.json
COPY package-lock.json /usr/src/app/package-lock.json
RUN npm ci
RUN npm install react-scripts@2.1.8 -g --silent --no-optional

CMD [""npm"", ""start""]
</code></pre>

<p>docker-compose:</p>

<pre><code>version: '3.7'

services:

  users:
    build:
      context: ./services/users
      dockerfile: Dockerfile
    volumes:
      - './services/users:/usr/src/app'
    ports:
      - 5001:5000
    environment:
      - FLASK_ENV=development
      - APP_SETTINGS=project.config.DevelopmentConfig
      - DATABASE_URL=postgres://postgres:postgres@users-db:5432/users_dev
      - DATABASE_TEST_URL=postgres://postgres:postgres@users-db:5432/users_test
    depends_on:
      - users-db

...
...

  client:
    build:
      context: ./services/client
      dockerfile: Dockerfile
    volumes:
      - './services/client:/usr/src/app'
      - '/usr/src/app/node_modules'
    ports:
      - 3007:3000
    environment:
      - NODE_ENV=development
      - REACT_APP_USERS_SERVICE_URL=${REACT_APP_USERS_SERVICE_URL}
    depends_on:
      - users
</code></pre>

<p>All services work just fine on dev server but when I try to containerize Node it crashes as soon as I spin the container. Log gives me this:</p>

<pre><code>ℹ ｢wds｣: Project is running at http://172.18.0.5/
ℹ ｢wds｣: webpack output is served from 
ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...
</code></pre>

<p>Not very helpful to me as you can imagine. What steps I should take to diagnose where is the issue ?</p>
"
"57516777","How can I delete docker images older than X and not in use","<docker>","62247063","remove docker images which are older than 10 minutes","<linux><docker><ubuntu>","<p>I'm running out of disk space on a server and <code>docker images</code> shows some containers from 6 months ago but as old as 2 years ago.  I'd like to remove all the ones older than 8 months.  What magic can I add to <code>docker rmi $(MAGIC)</code> that'll accomplish this?</p>
","<p>I am deleting the docker images by searching a pattern leaving only 2 docker images behind. </p>

<p><code>sudo docker rmi -f $(docker images | grep $search_docker_image | awk '{ print $3 }' | awk '!/'$base_docker_image_id'/ &amp;&amp; !/'$recent_docker_image_id'/')</code>   </p>

<p>In the same command i want also delete the docker images which are older than 10 mins .Is it possible or is there any other way of achieving it.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62280748","kafka container - Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm following <a href=""https://github.com/wurstmeister/kafka-docker/wiki/Connectivity"" rel=""nofollow noreferrer"">this guide</a> in order to start a single kafka container .I'm working on win10 pro and my cli is gitbash.</p>

<p>docker-compose-single-broker.yml : </p>

<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""
  kafka:
    image: wurstmeister/kafka 
    ports:
      - ""9092:9092""
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_CREATE_TOPICS: ""test:1:1""
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>

<p>The containers are starting without any problem : </p>

<pre><code>$ docker-compose -f docker-compose-single-broker.yml up -d
Creating kafka-docker_zookeeper_1 ... done
Creating kafka-docker_kafka_1     ... done

$ docker ps
CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                NAMES
19929c6c3297        wurstmeister/kafka       ""start-kafka.sh""         17 seconds ago      Up 15 seconds       0.0.0.0:9092-&gt;9092/tcp                               kafka-docker_kafka_1
d343a8ecf7ed        wurstmeister/zookeeper   ""/bin/sh -c '/usr/sb…""   17 seconds ago      Up 15 seconds       22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   kafka-docker_zookeeper_1
</code></pre>

<p>However, when I try to run any kafka command(create topic,list topics) from inside the container like the following : </p>

<pre><code>&gt; $KAFKA_HOME/bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server `broker-list.sh`
&gt; $KAFKA_HOME/bin/kafka-topics.sh --describe --topic test --bootstrap-server `broker-list.sh`
</code></pre>

<p>I'm getting the following warnings and afterwards a timeout exception : </p>

<pre><code>$ ./start-kafka-shell.sh localhost
bash-4.4# $KAFKA_HOME/bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server `broker-list.sh`
[2020-06-09 11:47:26,241] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (/172.17.54.145:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2020-06-09 11:47:29,306] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (/172.17.54.145:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
</code></pre>

<p>The content of the start-kafka-shell.sh : </p>

<pre><code>$ cat start-kafka-shell.sh
#!/bin/bash
docker run --rm -v //var/run/docker.sock:/var/run/docker.sock -e HOST_IP=$1 -e ZK=$2 -i -t wurstmeister/kafka /bin/bash
</code></pre>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","62284899","How to limit JVM memory consumption?","<java><spring-boot><web-services><jvm><microservices>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I developed a simple Spring Boot application (microservice you might say) and deployed with a simple <code>java -jar ... -Xmx 128M</code>. Yet, I can see using the <code>ps</code> command that the allocated memory for the process is actually <code>6237.26 Mb</code>.</p>

<p>Now, <a href=""https://stackoverflow.com/questions/6450132/java-seems-to-ignore-xms-and-xmx-options"">I've read</a> that and understood about the fact that <code>-Xmx</code> actually limits the process heap size but my question is:</p>

<ol>
<li>Why does the JVM allocates so much RAM?</li>
<li>How can I limit it to a reasonable size? </li>
</ol>

<p><strong>Update:</strong>
Following the suggestion in the comment, I moved the <code>-Xmx128M</code> and now it's has allocated <code>4300MB</code> of RAM. Still a monster.</p>
"
"54471608","Unable to connect from Intellij to mySql running in docker container - ""specified database user/password combination is rejected""","<mysql><docker><intellij-idea>","62355441","How to establish a MySQL(dockerized) connection in Intellij?","<java><mysql><docker><intellij-idea>","<p>Currently unable to connect from Intellij to mySql running locally on docker container on ubuntu.</p>

<pre><code>+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| tasklogs           |
+--------------------+

+----------+-----------+------+
| DATABASE | HOST      | USER |
+----------+-----------+------+
| tasklogs | localhost | dev  |
| tasklogs | localhost | root |
+----------+-----------+------+

+-----------------------------------------------------------+
| Grants for dev@localhost                                  |
+-----------------------------------------------------------+
| GRANT USAGE ON *.* TO `dev`@`localhost`                   |
| GRANT ALL PRIVILEGES ON `tasklogs`.* TO `dev`@`localhost` |
+-----------------------------------------------------------+
</code></pre>

<p>docker ps -a:</p>

<p><a href=""https://i.stack.imgur.com/eJaM1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eJaM1.png"" alt=""enter image description here""></a></p>

<p>When I connect via intellij:</p>

<p><a href=""https://i.stack.imgur.com/WVnvu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WVnvu.png"" alt=""enter image description here""></a></p>

<p>i.e. <strong>""The specified database user/password combination is rejected: [28000][1045] Access denied for user 'dev'@'localhost' (using password: YES)""</strong></p>

<p>I am putting in the right password.</p>

<p>Any help really appreciated.</p>

<p>Thanks,</p>
","<p>I'm on Windows 10, running Intellij Ultimate Edition, running MySQL 8 in docker -(I'm new to docker, I'm familiarizing myself with the technology because of its relevance).</p>

<p>MySQL is using port 3306 and I'm able to access it otherwise normally using bash and able to modify the the entirety of its content as the root user. I created a new Java project in Intellij and established the connection with the database manager on the right of the window. I've also ran several SQLs to set up the foundation of my new db. </p>

<p>I've created a new user in MySQL specifically for the development of this project. I've granted it full control over <em>only</em> this new db. I'm able to login to this user using bash and am able to modify the contents of my new db to its entirety. </p>

<p>Here's my problem: I wanted to begin the actual coding so first thing I'd thought to do was to test the connection to my db through Java. Unfortunately, I'm unable to. </p>

<pre><code>Connection con = DriverManager.getConnection(""jdbc:mysql://localhost:3306/myDB"",""myUser"",""password"");
</code></pre>

<p>I am thrown <code>java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:3306/myDB</code> which is odd because when I click on Data Source and navigate to MySQL, I see clearly that I have <strong>MySQL Connecter/J ver 8.0.15[latest]</strong>.</p>

<p>Any help is appreciated. Thanks</p>

<p>EDIT: Okay so I've created a maven project instead and added the JDBC dependency. Now I get the infamous <strong>Access denied for user 'myUser'@'172.17.0.1' (using password: YES)</strong>. How would I fix this? </p>

<p>EDIT2: Found my answers in these two threads: 
<a href=""https://stackoverflow.com/questions/54471608/unable-to-connect-from-intellij-to-mysql-running-in-docker-container-specifie"">Unable to connect from Intellij to mySql running in docker container - &quot;specified database user/password combination is rejected&quot;</a></p>

<p><a href=""https://stackoverflow.com/questions/11634084/are-users-user-and-userlocalhost-not-the-same"">Are Users &#39;User&#39;@&#39;%&#39; and &#39;User&#39;@&#39;localhost&#39; not the same?</a></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62367752","Can't Connect to Kafka running on Docker","<docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I deployed Kafka using Docker Compose to a server but I am unable to connect to Kafka. I used the bitnami/kafka images.</p>

<pre><code>$ curl -sSL https://raw.githubusercontent.com/bitnami/bitnami-docker-kafka/master/docker-compose.yml &gt; docker-compose.yml
$ docker-compose up -d
</code></pre>

<p>Using Kafka Tool I've tried connecting to Zookeeper and using bootstrap to try connecting directly to Kafka. It always times out however I have verified I can hit port 2181 and 9092 on the docker host and ping from Kafka tool works as well which leads me to believe those services are up and listening removing the possibility of firewall/network issues.</p>

<p><a href=""https://i.stack.imgur.com/fTyO1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fTyO1.png"" alt=""enter image description here""></a></p>

<p>My docker compose file</p>

<pre><code>version: '2'

services:
  zookeeper:
    image: 'bitnami/zookeeper:3'
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper_data:/bitnami'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: 'bitnami/kafka:2'
    ports:
      - '9092:9092'
      - '9093:9093'
    volumes:
      - 'kafka_data:/bitnami'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
</code></pre>
"
"53650594","Accessing Host Java from Docker container","<java><docker><dockerfile><docker-machine>","62370968","how to mount the Java runtime from the host into the container","<docker><docker-compose><docker-volume>","<p>I have Java installed in my docker host. Now I want that to be able to my docker containers. I don't want to install again on the containers. Do we have any workaround for that?</p>
","<p>At present i am adding a jdk file into my image like below</p>

<p>FROM centos</p>

<p>ADD jdk-11.0.7_linux-x64_bin.tar.gz /opt/java</p>

<p>ENV JAVA_HOME /opt/java/jdk-11.0.7</p>

<p>ENV PATH $PATH:/opt/java/jdk-11.0.7/bin</p>

<p>RUN ls -l /opt/java/jdk-11.0.7</p>

<p>RUN java -version</p>

<p>ADD build/libs/CatalogModel-1.0.jar CatalogModel-1.0.jar</p>

<p>EXPOSE 9081</p>

<p>ENTRYPOINT [""java"", ""-jar"", ""CatalogModel-1.0.jar""]</p>

<hr>

<p>Instead can i mount the Java runtime from the host into the container?</p>
"
"59148975","Running a ECR image locally with helm and Kubernetes","<amazon-web-services><docker><kubernetes><kubernetes-helm><kubectl>","62420919","How to pull a private container from AWS ECR to a local cluster","<amazon-web-services><docker><kubernetes>","<p>I'm new to Kubernetes and as a tutorial for myself I've been working on deploying a basic project to Kubernetes with helm (v3).
I have an image in AWS's ECR as well as a local helm chart for this project.
However, I am struggling to run my image with Kubernetes.</p>

<p>My image is set up correctly. If I try something like <code>docker run my_image_in_ecr</code> locally it behaves as expected (after configuring my IAM access credentials locally).
My helm chart is properly linted and in my image map, it specifies:</p>

<pre><code>image:
  repository: my_image_in_ecr
  tag: latest
  pullPolicy: IfNotPresent
</code></pre>

<p>When I try to use helm to deploy though, I'm running into issues. 
My understanding is to run my program with helm, I should:</p>

<ol>
<li><p>Run helm install on my chart</p></li>
<li><p>Run the image inside my new kubernetes pod</p></li>
</ol>

<p>But when I look at my kubernetes pods, it looks like they never get up and running.</p>

<pre><code>hello-test1-hello-world-54465c788c-dxrc7           0/1     ImagePullBackOff    0          49m
hello-test2-hello-world-8499ddfb76-6xn5q           0/1     ImagePullBackOff    0          2m45s
hello-test3-hello-world-84489658c4-ggs89           0/1     ErrImagePull        0          15s
</code></pre>

<p>The logs for these pods look like this:</p>

<pre><code>Error from server (BadRequest): container ""hello-world"" in pod ""hello-test3-hello-world-84489658c4-ggs89"" is waiting to start: trying and failing to pull image
</code></pre>

<p>Since I don't know how to set up imagePullSecrets properly with Kubernetes I was expecting this to fail. But I was expecting a different error message such as bad auth credentials. </p>

<ol>
<li>How can I resolve the error in image pulling? Is this issue not even related to the fact that my image is in ecr?</li>
<li>How can I properly set up credentials (such as imagePullSecrets) to authorize pulling the image from ecr? I have followed some guides such as <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/"" rel=""nofollow noreferrer"">this one</a> and <a href=""https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry"" rel=""nofollow noreferrer"">this one</a> but am confused on how to tranlate this information into a proper authorization configuration for ecr.</li>
</ol>
","<p>I am currently having trouble trying to pull my remote docker image hosted via AWS ECR. I am getting this error when running a deployment </p>

<p>Step 1)</p>

<p>run </p>

<pre><code>aws ecr get-login-password --region cn-north-1 | docker login --username AWS --password-stdin xxxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn
</code></pre>

<p>Step 2)</p>

<p>run <code>kubectl create -f backend.yaml</code></p>

<p>from here the following happens:</p>

<pre><code>➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     Pending            0          2s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS              RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ContainerCreating   0          4s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ErrImagePull       0          6s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ImagePullBackOff   0          7s
</code></pre>

<p>So then I run <code>kubectl describe pod backend</code> and it will output:</p>

<pre><code>Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  117s                default-scheduler  Successfully assigned default/backend-89d75f7df-qwqdq to minikube
  Normal   Pulling    32s (x4 over 114s)  kubelet, minikube  Pulling image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest""
  Warning  Failed     31s (x4 over 114s)  kubelet, minikube  Failed to pull image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest"": rpc error: code = Unknown desc = Error response from daemon: Get https://xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/v2/baopals/manifests/latest: no basic auth credentials
  Warning  Failed     31s (x4 over 114s)  kubelet, minikube  Error: ErrImagePull
  Warning  Failed     19s (x6 over 113s)  kubelet, minikube  Error: ImagePullBackOff
  Normal   BackOff    4s (x7 over 113s)   kubelet, minikube  Back-off pulling image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest""

</code></pre>

<p>the main error being <code>no basic auth credentials</code></p>

<p>Now what I am confused about is that I can push images to my ECR fine and I can also push to my remote EKS cluster I feel like essentially the only thing I cant do right now is pull from my private repository that is hosted on ECR.</p>

<p>Is there something obvious that I'm missing here that is preventing me from pulling from private repos so i can use them on my local machine?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62422757","org.apache.spark.SparkException: java.nio.channels.ClosedChannelException","<python><docker><apache-spark><pyspark><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am trying to integrate Kafka with Apache spark which is both in separate docker containers. I am producing random data from the Kafka container using Kafka Produce and trying to consume it with Spark.</p>

<p>The consumer code within spark looks like this:</p>

<pre><code>import sys
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from pyspark.sql.context import SQLContext


if __name__ == '__main__':

    n_secs = 1
    topic = ""mytopic""

    conf = SparkConf().setAppName(""KafkaStreamProcessor"").setMaster(""local[*]"")
    sc = SparkContext(conf=conf)
    sc.setLogLevel(""WARN"")
    ssc = StreamingContext(sc, n_secs)

    kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {
                            'bootstrap.servers':'kafka:29092', 
                            'fetch.message.max.bytes':'15728640',
                            'auto.offset.reset':'largest'})
                            # Group ID is completely arbitrary

    lines = kafkaStream.map(lambda x: x[1])
    counts = lines.flatMap(lambda line: line.split("" "")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    time.sleep(600) # Run stream for 10 minutes just in case no detection of producer
    # ssc.awaitTermination()
    ssc.stop(stopSparkContext=True,stopGraceFully=True)
</code></pre>

<p>But when I run this code from the spark container. I get this error:</p>

<pre><code>2020-06-16 10:21:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7db27f95{/metrics/json,null,AVAILABLE,@Spark}
Traceback (most recent call last):
  File ""/app/business_rules/jobs/BusinessRule.py"", line 34, in &lt;module&gt;
    'auto.offset.reset':'largest'})
  File ""/spark/python/lib/pyspark.zip/pyspark/streaming/kafka.py"", line 146, in createDirectStream
  File ""/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o27.createDirectStreamWithoutMessageHandler.
: org.apache.spark.SparkException: java.nio.channels.ClosedChannelException
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at scala.util.Either.fold(Either.scala:98)
    at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:386)
    at org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:223)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStream(KafkaUtils.scala:721)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStreamWithoutMessageHandler(KafkaUtils.scala:689)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p>I am not sure what am I doing wrong, Any help would be appreciated.</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","62455226","React/Spring/MySQL Containerisation: Frontend Unable To Start After Pulling Image","<reactjs><spring><docker><docker-compose><dockerfile>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>Still grasping with Docker and its concepts. I have a React front end, which I am trying to deploy to a backend REST API in Spring Boot and MySQL. Now, I'm able to build and push the images separately, i.e. one for the React application, and the other for the Spring Boot aplication. </p>

<p>This is my <code>Dockerfile</code> for the front-end React application.</p>

<pre><code>FROM node:13.12.0-alpine

WORKDIR /app

ENV PATH /app/node_modules/.bin:$PATH

COPY package.json ./
COPY package-lock.json ./
RUN npm install --silent
RUN npm install react-scripts@3.4.1 -g --silent

COPY . ./

CMD [""npm"", ""start""]
</code></pre>

<p>I'm able to run the front end on <code>localhost:3001</code> with this docker command: </p>

<p><code>docker run -it --rm -v ${PWD}:/app -v /app/node_modules -p 3001:3000 -e CHOKIDAR_USEPOLLING=true dockeruser/app-client</code></p>

<p>And this is for my Spring Boot backend: </p>

<pre><code>FROM openjdk:15-jdk-alpine

COPY app-server-web/target/app-server-web.jar /app-server-web.jar

ENTRYPOINT [""java"", ""-jar"", ""/app-server-web.jar""]
</code></pre>

<p>Now I am able to build both of these and push them to dockerhub. I want to compose them now, but for some reason the front end is not able to start on the port that I assigned it to. Here is my <code>docker-compose.yml</code> file: </p>

<pre><code>version: '3'
services:
  application-db:
    restart: always
    container_name: application-db
    image: 'mysql:5.7.30'
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: application_database
      MYSQL_USER: alee
      MYSQL_PASSWORD: anotherpassword
    ports:
      - '3308:3306'
    volumes:
      - './initial.sql:/docker-entrypoint-initdb.d/initial.sql'
      - null
  application-server:
    restart: on-failure
    image: 'dockeruser/app-server:latest'
    expose:
      - '8080'
    ports:
      - '8080:8080'
    environment:
      SPRING_DATASOURCE_URL: &gt;-
        jdbc:mysql://application-db:3306/application_database?useSSL=false&amp;allowPublicKeyRetrieval=true
      SPRING_DATASOURCE_USERNAME: alee
      SPRING_DATASOURCE_PASSWORD: anotherpassword
    depends-on:
      - application-db
    networks:
      - backend
      - frontend
  application-client:
    restart: on-failure
    image: 'dockeruser/app-client:latest'
    expose:
      - '3001'
    ports:
      - '3001:3000'
    depends-on:
      - application-server
    networks:
      - frontend
networks:
  backend: null
  frontend: null
</code></pre>

<p>These are the logs I get upon <code>docker-compose up</code> </p>

<pre><code>application-client_1  | ℹ ｢wds｣: Project is running at http://192.168.128.3/
application-client_1  | ℹ ｢wds｣: webpack output is served from 
application-client_1  | ℹ ｢wds｣: Content not from webpack is served from /app/public
application-client_1  | ℹ ｢wds｣: 404s will fallback to /
application-client_1  | Starting the development server...
application-client_1  | 
</code></pre>

<p>I'm pretty sure something is wrong in <code>docker-compose.yml</code> but I'm not sure what it is?</p>
"
"61491853","Docker: Can't access binary copied to certain images","<docker>","61512853","Problem with building go proyect in docker (Build in Alpine and execute in Oracle Linux)","<docker><go><alpine><instantclient><oraclelinux>","<p>I am trying to access a binary <code>COPY</code>ed from the <code>migrate</code> container.   When I <code>COPY</code> to <code>python:3.7-alpine</code> it works, but when I <code>COPY</code> to <code>debian:buster-slim</code> it can't be found.</p>

<p>Minimum steps to reproduce:</p>

<p>1.Create Dockerfile.test</p>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM python:3.7-alpine
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""2"">
<li>Build and run. This works.</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>Usage: migrate OPTIONS COMMAND [arg...]
       migrate [ -version | -help ]
</code></pre>

<ol start=""3"">
<li>Stop and remove container</li>
</ol>

<pre><code>docker stop migrate_test;docker rm migrate_test;
</code></pre>

<ol start=""4"">
<li>Change image in Dockerfile.test</li>
</ol>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM debian:buster-slim
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""5"">
<li>Build and run. This doesn't work</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>/bin/sh: 1: /migrate: not found
</code></pre>
","<p>I have a problem building go proyect, I build with golang:alpine and copy the result to oraclelinux:7-slim the problem is when I execute the docker image the result is a error </p>

<pre><code>""standard_init_linux.go:211: exec user process caused ""no such file or directory"""".
</code></pre>

<p>I tried build with other base image by example ""FROM golang:1.14 as builder"" and  execute in oraclelinux:7-slim and the result is satisfactory.</p>

<p>I need execute in oraclelinux:7-slim because a need use Oracle Instant client to connect with database.</p>

<p>I would like build with Alpine because the golang:1.14 is very heavy.</p>

<p>Attached the dockerfile</p>

<pre><code># Start from golang base image
FROM golang:alpine as builder

RUN apk update &amp;&amp; apk add --no-cache git &amp;&amp; \
    apk add build-base

# Set the current working directory inside the container
WORKDIR /app

# Copy go mod and sum files
COPY go.mod go.sum ./

# Download all dependencies. Dependencies will be cached if the go.mod and the go.sum files are not changed 
RUN go mod download 

# Copy the source from the current directory to the working Directory inside the container 
COPY . .

# Build the Go app
RUN CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -o main .

# Start a new stage from scratch
FROM oraclelinux:7-slim

ARG release=19
ARG update=6

# Install instant client
RUN yum -y update &amp;&amp; \
    yum -y install oracle-release-el7 &amp;&amp; yum-config-manager --enable ol7_oracle_instantclient &amp;&amp; \
    yum -y install oracle-instantclient${release}.${update}-basiclite &amp;&amp; \    
    rm -rf /var/cache/yum

ENV LANG=C.UTF-8

# Copy the Pre-built binary file from the previous stage
WORKDIR /root/

COPY --from=builder /app/main .
COPY --from=builder /app/.env .

# Expose port 8080 to the outside world
EXPOSE 8080

#Command to run the executable
ENTRYPOINT [""./main""]
</code></pre>
"
"61713741","docker compose up working fine but browser showing site can't be reached","<docker><docker-compose><dockerfile><docker-machine>","61711737","docker compose up showing that server is running but browser showing site can not be reached","<docker><docker-compose>","<p><strong>docker-compose up</strong> showing this:</p>

<pre><code>Recreating tutorial_product-service_1 ...
Recreating tutorial_product-service_1 ... done
Attaching to tutorial_product-service_1
product-service_1  |  * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
product-service_1  |  * Restarting with stat
product-service_1  |  * Debugger is active!
product-service_1  |  * Debugger PIN: 192-693-276

</code></pre>

<p>and my docker-compose.yml</p>

<pre><code>version: '3'

services:
 product-service:
  container_name: tutorial_product-service_1
  build: ./product
  volumes:
   - ./product:/usr/src/app
  ports:
   - 5002:80

</code></pre>

<p>I have installed docker on AWS EC2 Ubuntu instance and its IP is : <a href=""http://18.225.37.175"" rel=""nofollow noreferrer"">http://18.225.37.175</a></p>

<p>But when I run <a href=""http://18.225.37.175:5002/"" rel=""nofollow noreferrer"">http://18.225.37.175:5002/</a>
it shows me site can not be reached.</p>

<p>Any help?</p>
","<p>after running <strong>docker-compose up</strong> command</p>

<pre><code>Starting tutorial_product-service_1 ...
Starting tutorial_product-service_1 ... done
Attaching to tutorial_product-service_1
product-service_1  |  * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
product-service_1  |  * Restarting with stat
product-service_1  |  * Debugger is active!
product-service_1  |  * Debugger PIN: 176-459-861

</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: '3'

services:
 product-service:
  container_name: tutorial_product-service_1
  build: ./product
  volumes:
   - ./product:/usr/src/app
  networks:
   testing_net:
    ipv4_address: 193.167.10.1
  ports:
   - 5001:80

networks:
 testing_net:
  ipam:
   config:
    - subnet: 193.167.10.0/16
</code></pre>

<p>This is the URL</p>

<pre><code>http://193.167.10.1:5001/
</code></pre>

<p>but it is showing site can not be reached.
Anyone can help me?</p>
"
"61797514","Docker container not hot-reloading Angular","<angular><docker><docker-compose><containers>","61799549","Trying to make hot-reload work Angular Dockerized","<angular><docker><docker-compose><containers><livereload>","<p>I'm configuring a dockerized Angular app but when i modify something, the page does not refresh automatically as it does on my local setup.</p>

<p>Based on this: <a href=""https://stackoverflow.com/questions/44176922/docker-container-doesnt-reload-angular-app/44180317"">Docker container doesn&#39;t reload Angular app</a></p>

<p>I should expose port 49153 but this does not work.</p>

<p>Here's my configuration:</p>

<p>Dockerfile</p>

<pre><code>FROM node:14-slim
RUN npm install @angular/cli@latest -g
RUN mkdir -p /home/boilerplate
WORKDIR /home/boilerplate
EXPOSE 4200 49153
CMD ng serve --port 4200 --poll 1
</code></pre>

<p>docker-compose.yaml</p>

<pre><code>version: ""3""
services:
  app:
    image: project
    build:
      context: .
      dockerfile: font-app/Dockerfile
    volumes:
      - ./font-app:/home/boilerplate
    ports:
      - 4200:4200  #live reload server is 49153 while normal server is 4200 where changes refresh when browser refreshed
      - 49153:49153
</code></pre>

<p>localhost:4200 works fine, and when i do changes, and refresh it works. 
I have also exposed port 49153 like another stackoverflow post, but it does not work. Port 49153 has nothing in it.</p>

<p>Any thoughts what is wrong?</p>

<p>Logs:</p>

<pre><code>192-168-0-243:Spring-and-springboot Jeff$ docker-compose up
Starting spring-and-springboot_app_1 ... done
Attaching to spring-and-springboot_app_1
app_1  | Your global Angular CLI version (9.1.6) is greater than your local
app_1  | version (8.3.26). The local Angular CLI version is used.
app_1  | 
app_1  | To disable this warning use ""ng config -g cli.warnings.versionMismatch false"".
app_1  | WARNING: This is a simple server for use in testing or debugging Angular applications
app_1  | locally. It hasn't been reviewed for security issues.
app_1  | 
app_1  | Binding this server to an open connection can result in compromising your application or
app_1  | computer. Using a different host than the one passed to the ""--host"" flag might result in
app_1  | websocket connection issues. You might need to use ""--disableHostCheck"" if that's the
app_1  | case.
app_1  | ℹ ｢wds｣: Project is running at http://0.0.0.0:4200/webpack-dev-server/
app_1  | ℹ ｢wds｣: webpack output is served from /
app_1  | ℹ ｢wds｣: 404s will fallback to //index.html
app_1  | 
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 270 kB [initial] [rendered]
app_1  | chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
app_1  | chunk {styles} styles.js, styles.js.map (styles) 9.69 kB [initial] [rendered]
app_1  | chunk {vendor} vendor.js, vendor.js.map (vendor) 3.91 MB [initial] [rendered]
app_1  | Date: 2020-05-14T12:24:26.717Z - Hash: d3844cd515038df15e5c - Time: 19389ms
app_1  | ** Angular Live Development Server is listening on 0.0.0.0:4200, open your browser on http://localhost:4200/ **
app_1  | ℹ ｢wdm｣: Compiled successfully.
app_1  | ℹ ｢wdm｣: Compiling...
app_1  | ℹ ｢wdm｣: wait until bundle finished: /
app_1  | ℹ ｢wdm｣: wait until bundle finished: /runtime.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /styles.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /polyfills.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /main.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /vendor.js
app_1  | 
app_1  | Date: 2020-05-14T12:29:46.595Z - Hash: 1d9ec15738fbcb9e6453
app_1  | 4 unchanged chunks
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | Time: 3435ms
app_1  | ℹ ｢wdm｣: Compiled successfully.
</code></pre>
","<p>I'm configuring a dockerized Angular app but when i modify something, <strong>the page does not refresh automatically as it does on my local setup.</strong></p>

<p>Based on this: <a href=""https://stackoverflow.com/questions/44176922/docker-container-doesnt-reload-angular-app/44180317"">Docker container doesn&#39;t reload Angular app</a> , I should expose port 49153 but this does not work.</p>

<p>Here's my configuration:</p>

<p>Dockerfile</p>

<pre><code>FROM node:14-slim
RUN npm install @angular/cli@latest -g
RUN mkdir -p /home/boilerplate
WORKDIR /home/boilerplate
EXPOSE 4200 49153
CMD ng serve --port 4200 --live-reload-port 49153 --poll 1
</code></pre>

<p>docker-compose.yaml</p>

<pre><code>version: ""3""
services:
  app:
    image: project
    build:
      context: .
      dockerfile: font-app/Dockerfile
    volumes:
      - ./font-app:/home/boilerplate
    ports:
      - 4200:4200  
      - 49153:49153
</code></pre>

<p>localhost:4200 works fine but if i change something in my code and save, the page does not automatically refresh.</p>

<p>Any thoughts on what is wrong?</p>

<p>Logs:</p>

<pre><code>192-168-0-243:Spring-and-springboot Jeff$ docker-compose up
Starting spring-and-springboot_app_1 ... done
Attaching to spring-and-springboot_app_1
app_1  | Your global Angular CLI version (9.1.6) is greater than your local
app_1  | version (8.3.26). The local Angular CLI version is used.
app_1  | 
app_1  | To disable this warning use ""ng config -g cli.warnings.versionMismatch false"".
app_1  | WARNING: This is a simple server for use in testing or debugging Angular applications
app_1  | locally. It hasn't been reviewed for security issues.
app_1  | 
app_1  | Binding this server to an open connection can result in compromising your application or
app_1  | computer. Using a different host than the one passed to the ""--host"" flag might result in
app_1  | websocket connection issues. You might need to use ""--disableHostCheck"" if that's the
app_1  | case.
app_1  | ℹ ｢wds｣: Project is running at http://0.0.0.0:4200/webpack-dev-server/
app_1  | ℹ ｢wds｣: webpack output is served from /
app_1  | ℹ ｢wds｣: 404s will fallback to //index.html
app_1  | 
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 270 kB [initial] [rendered]
app_1  | chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
app_1  | chunk {styles} styles.js, styles.js.map (styles) 9.69 kB [initial] [rendered]
app_1  | chunk {vendor} vendor.js, vendor.js.map (vendor) 3.91 MB [initial] [rendered]
app_1  | Date: 2020-05-14T12:24:26.717Z - Hash: d3844cd515038df15e5c - Time: 19389ms
app_1  | ** Angular Live Development Server is listening on 0.0.0.0:4200, open your browser on http://localhost:4200/ **
app_1  | ℹ ｢wdm｣: Compiled successfully.
app_1  | ℹ ｢wdm｣: Compiling...
app_1  | ℹ ｢wdm｣: wait until bundle finished: /
app_1  | ℹ ｢wdm｣: wait until bundle finished: /runtime.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /styles.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /polyfills.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /main.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /vendor.js
app_1  | 
app_1  | Date: 2020-05-14T12:29:46.595Z - Hash: 1d9ec15738fbcb9e6453
app_1  | 4 unchanged chunks
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | Time: 3435ms
app_1  | ℹ ｢wdm｣: Compiled successfully.
</code></pre>
"
"61589254","why docker image with elasticsearch status restarting always?","<docker><elasticsearch>","61844868","run elasticsearch from docker on ec2","<amazon-web-services><docker><elasticsearch><amazon-ec2><jvm>","<p>unbuntu 16.04, ram 1gb, on aws instance</p>

<p>I had to run old instance of elasticsearch so I wanted use a docker image of elasticsearch 5.3.3 version. by seeing the stackoverflow multiple links with same title, i have modified my installation of docker image based elasticsearch as below</p>

<pre><code>sudo docker run -p 9200:9200 -p 9300:9300 -d -e ""http.host=0.0.0.0"" -e ""transport.host=127.0.0.1"" -e ""xpack.security.enabled=false"" --restart=unless-stopped --name careerassistant-elastic  docker.elastic.co/elasticsearch/elasticsearch:5.3.3
</code></pre>

<p>the installation is finished and have problem with accessing the elasticsearch, though I had mutliple modifications as above command, I couldnt resolve the issue. when on</p>

<pre><code>sudo docker ps
</code></pre>

<p>the status is still --> restarting(1) 48 seconds ago</p>

<p>when i was checking the log of the docker i couldnt understand anything as i am new to docker and its utilization</p>

<pre><code>&gt; docker logs --tail 50 --follow --timestamps careerassistant-elastic
</code></pre>

<p>i got the following output</p>

<pre><code>2020-05-04T09:36:00.552415247Z CmaTotal:              0 kB
2020-05-04T09:36:00.552418314Z CmaFree:               0 kB
2020-05-04T09:36:00.552421364Z HugePages_Total:       0
2020-05-04T09:36:00.552424343Z HugePages_Free:        0
2020-05-04T09:36:00.552427401Z HugePages_Rsvd:        0
2020-05-04T09:36:00.552430358Z HugePages_Surp:        0
2020-05-04T09:36:00.552433336Z Hugepagesize:       2048 kB
2020-05-04T09:36:00.552436334Z DirectMap4k:       67584 kB
2020-05-04T09:36:00.552439415Z DirectMap2M:      980992 kB
2020-05-04T09:36:00.552442390Z 
2020-05-04T09:36:00.552445460Z 
2020-05-04T09:36:00.552448777Z CPU:total 1 (initial active 1) (1 cores per cpu, 1 threads per core) family 6 model 63 stepping 2, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, lzcnt, tsc, bmi1, bmi2
2020-05-04T09:36:00.552452312Z 
2020-05-04T09:36:00.552455227Z /proc/cpuinfo:
2020-05-04T09:36:00.552458471Z processor    : 0
2020-05-04T09:36:00.552461695Z vendor_id    : GenuineIntel
2020-05-04T09:36:00.552464872Z cpu family   : 6
2020-05-04T09:36:00.552467992Z model        : 63
2020-05-04T09:36:00.552471311Z model name   : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
2020-05-04T09:36:00.552474616Z stepping : 2
2020-05-04T09:36:00.552477715Z microcode    : 0x43
2020-05-04T09:36:00.552480781Z cpu MHz      : 2400.040
2020-05-04T09:36:00.552483934Z cache size   : 30720 KB
2020-05-04T09:36:00.552486978Z physical id  : 0
2020-05-04T09:36:00.552490023Z siblings : 1
2020-05-04T09:36:00.552493103Z core id      : 0
2020-05-04T09:36:00.552496146Z cpu cores    : 1
2020-05-04T09:36:00.552511390Z apicid       : 0
2020-05-04T09:36:00.552515457Z initial apicid   : 0
2020-05-04T09:36:00.552518523Z fpu      : yes
2020-05-04T09:36:00.552521677Z fpu_exception    : yes
2020-05-04T09:36:00.552524702Z cpuid level  : 13
2020-05-04T09:36:00.552527802Z wp       : yes
2020-05-04T09:36:00.552531691Z flags        : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single kaiser fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
2020-05-04T09:36:00.552535638Z bugs     : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit
2020-05-04T09:36:00.552538954Z bogomips : 4800.08
2020-05-04T09:36:00.552545171Z clflush size : 64
2020-05-04T09:36:00.552548419Z cache_alignment  : 64
2020-05-04T09:36:00.552551514Z address sizes    : 46 bits physical, 48 bits virtual
2020-05-04T09:36:00.552554916Z power management:
2020-05-04T09:36:00.552558030Z 
2020-05-04T09:36:00.552561090Z 
2020-05-04T09:36:00.552564141Z 
2020-05-04T09:36:00.552567135Z Memory: 4k page, physical 1014424k(76792k free), swap 0k(0k free)
2020-05-04T09:36:00.552570458Z 
2020-05-04T09:36:00.552573441Z vm_info: OpenJDK 64-Bit Server VM (25.131-b11) for linux-amd64 JRE (1.8.0_131-b11), built on Jun 16 2017 13:51:29 by ""buildozer"" with gcc 6.3.0
2020-05-04T09:36:00.552576947Z 
2020-05-04T09:36:00.552579894Z time: Mon May  4 09:36:00 2020
2020-05-04T09:36:00.552582956Z elapsed time: 0 seconds (0d 0h 0m 0s)
2020-05-04T09:36:00.552586052Z
</code></pre>

<p>can someone help me out to figure what could be the problem for docker status to be restarting ?</p>
","<p>im trying to start a elasticsearch instance for dev porpuse(the smallest possible , dont care), is inside ec2 t2.micro (in order to avoid cost running the elasticsearch service from aws)</p>

<p>so ... i pulled the instance </p>

<pre><code>docker pull docker.elastic.co/elasticsearch/elasticsearch:6.8.9
</code></pre>

<p><a href=""https://i.stack.imgur.com/LSdxz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LSdxz.png"" alt=""enter image description here""></a></p>

<p>now , when i try to run the image</p>

<pre><code>docker run -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" docker.elastic.co/elasticsearch/elasticsearch:6.8.9
</code></pre>

<p>the following error apperars</p>

<pre><code>OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) 

    <code>failed; error='Not enough space' (errno=12)
    <code>#
    <code># There is insufficient memory for the Java Runtime Environment to continue.
    <code># Native memory allocation (mmap) failed to map 1073741824 bytes for committing reserved memory.
    <code># An error report file with more information is saved as:
    <code># logs/hs_err_pid1.log</code></code></code></code></code></code>

</pre>

<p>i heard in some places that you have two options 
Either give more memory to your VM (is this posible for a t2.micro instance), how can achive that?
 or change Elasticsearch JVM settings<code>/etc/elasticsearch/jvm.options</code> and lower the values of the following parameters -Xms512m -Xmx512m</code> but how can i do this if i the image its not available to get in </p>

<p>i also tried to install elasticserach via rpm but requires coreutils >= 8.4 , so another failure trying to get the instance up</p>

<p>using -Xms512m -Xmx512m on jvm.options with avaible space (i guess)
i got the same error <strong>Not enough space</strong>
<a href=""https://i.stack.imgur.com/tekPy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tekPy.png"" alt=""enter image description here""></a></p>
"
"62169568","Docker Alpine Linux python (missing)","<docker><alpine>","62169844","ERROR: unsatisfiable constraints - while installing python using APK","<python><docker><dockerfile><alpine>","<p>I have a pipeline which deploys my container from GitLab. Last deployment was 5 days ago and went without any problems. Today I deploy it and get the following error:</p>
<pre><code>$ apk add --no-cache curl python py-pip
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
 ERROR: unsatisfiable constraints:
   python (missing):
     required by: world[python]
</code></pre>
<p>My job definition is:</p>
<pre><code>my-deploy:
  type: my-deploy
  image: docker:stable
  script:
    - apk update
    - apk add --no-cache curl python py-pip &lt;-- Here the erorr happens
    ...
</code></pre>
","<p>My dockerfile looks like this:</p>

<pre><code>FROM alpine

RUN apk add --no-cache \
    ca-certificates \
    curl \
    python \
    py-pip
RUN pip install awscli

# remaining code...
</code></pre>

<p>But when I build this, I get the following logs with an error:</p>

<pre><code>RUN apk add --no-cache  ca-certificates     curl    python  py-pip

fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz

fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz

ERROR: unsatisfiable constraints:

  python (missing):

    required by: world[python]

The command '/bin/sh -c apk add --no-cache  ca-certificates     curl    python  py-pip' returned a non-zero code: 1
</code></pre>

<p>I have found some questions which described the same error but the solution doesn't help me.</p>

<p>Do I need to mention the <strong>python</strong> version? I would really like to use to install latest version only.</p>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","62558603","error during connect: The system cannot find the file specified..,the docker client must be run elevated to connect","<docker><windows-10>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>I have installed docker on my windows 10 but when I am trying to execute any docker commands (except - v) it gives me below error. Any help ?</p>
<p><em><strong>error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/images/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></em></p>
"
"55343991","Error running docker container: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown","<docker><flask><docker-compose><dockerfile><docker-image>","62584751","unable to run flask app as a docker image","<docker><flask><dockerfile>","<p>I am trying to dockerize a simple Python-Flask application but I am getting an error while running my container.</p>

<p>docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown.</p>

<p>Workdir on localhost:</p>

<pre><code>/home/ubuntu/flask_web
- app.py
- Dockerfile
- requirements.txt
</code></pre>

<p>app.py</p>

<pre><code>#flask_web/app.py

from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hey, we have Flask in a Docker container'


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0')
</code></pre>

<p>Dockerfile</p>

<pre><code>FROM ubuntu:16.04

MAINTAINER xyz ""xyz@gmail.com""

RUN apt-get update \
    &amp;&amp; apt-get install -y software-properties-common vim \
    &amp;&amp; add-apt-repository ppa:jonathonf/python-3.6 \
    &amp;&amp; apt-get update -y \
    &amp;&amp; apt-get install -y build-essential python3.6 python3.6-dev python3-pip python3.6-venv \
    &amp;&amp; pip3 install --upgrade pip

# We copy just the requirements.txt first to leverage Docker cache
COPY ./requirements.txt /app/requirements.txt

WORKDIR /app

RUN pip install -r requirements.txt

COPY . /app

ENTRYPOINT [ ""python"" ]

CMD [ ""app.py"" ]
</code></pre>

<p>Commands:</p>

<pre><code>docker build -t flask-test:latest .
docker run -p 5000:5000 flask-test
</code></pre>

<p>Expected : Flask web should run on port 5000</p>

<p>Actual Result: </p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown.
</code></pre>
","<p>I'm a complete beginner in docker in windows 10 and I have a flask project with some html templates inside a directory . I create a dockerfile and insert it inside my directory and then I build a docker image to run . However Instead of running my docker image I get the error <code>C:\Program Files\Docker\Docker\resources\bin\docker.exe: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;python\&quot;: executable file not found in $PATH&quot;: unknown.</code> which I do not understand or know how to solve .</p>
<p>My dockerfile :</p>
<pre><code>FROM ubuntu:16.04
MAINTAINER bill&lt;bill@gmailcom&gt;
RUN apt-get update
RUN apt-get install -y python3 python3-pip 
RUN pip3 install flask pymongo
EXPOSE 5000
WORKDIR &quot;/MovieFlix2020_E17136_SKENTOS_VASILIS&quot;
CMD [&quot;python&quot;, &quot;webservice.py&quot;, &quot;--host&quot;, &quot;127.0.0.1&quot;, &quot;--port&quot;, &quot;5000&quot;]
</code></pre>
<p>I build the image inside my <code>&quot;/MovieFlix2020_E17136_SKENTOS_VASILIS&quot;</code> directory with <code>docker build -t flask .</code>
The image is succesfully built but I get the warning :
<a href=""https://i.stack.imgur.com/3QxUH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3QxUH.png"" alt=""enter image description here"" /></a></p>
<p>And in the end with <code>run flask</code> which has to run my newly created image I get the error I wrote above .</p>
<p>I would appreciate your help with guiding me to solve this issue . Thank you in advance</p>
"
"55522726","Docker failing to start rails","<ruby-on-rails><docker><redis><resque>","62603593","Docker: failed to open TCP connection to localhost:12345","<mysql><ruby><api><docker><docker-compose>","<p>I'm trying to start my rails application using rails. It depends on redis, which seems to be working but when rails server starts it fails with <code>Cannot assign requested address - connect(2) for [::1]:6379 (Errno::EADDRNOTAVAIL)</code>.</p>

<h3>Dockerfile</h3>

<pre><code>FROM ruby:2.6.1

RUN apt-get update -yqq &amp;&amp; \
  apt-get install -yqq --no-install-recommends \
  nodejs \
  nano

COPY Gemfile* /usr/src/app/
WORKDIR /usr/src/app
RUN bundle install
RUN gem install foreman
RUN gem install rake -v 12.3.2

COPY . /usr/src/app/

CMD [ ""bin/rails"", ""s"", ""-b"", ""0.0.0.0"" ]
</code></pre>

<h3>.docker-compose.yml</h3>

<pre><code>version: '3'

services:
  postgres:
    image: 'postgres:10.3-alpine'
    volumes:
      - 'postgres:/var/lib/postgresql/data'
    env_file:
      - '.env'

  redis:
    image: 'redis'
    # volumes:
    #   - 'redis:/data'

  rails:
    depends_on:
      - 'postgres'
      - 'redis'
    build: .
    ports:
      - '3000:3000'
    volumes:
      - '.:/usr/src/app'
    env_file:
      - '.env'

volumes:
  redis:
  postgres:
</code></pre>

<h3>log</h3>

<pre><code>rails_1     | =&gt; Booting Puma
rails_1     | =&gt; Rails 5.2.2.1 application starting in development
rails_1     | =&gt; Run `rails server -h` for more startup options
rails_1     | Exiting
rails_1     | /usr/local/lib/ruby/2.6.0/socket.rb:1213:in `__connect_nonblock': Cannot assign requested address - connect(2) for [::1]:6379 (Errno::EADDRNOTAVAIL)
rails_1     |   from /usr/local/lib/ruby/2.6.0/socket.rb:1213:in `connect_nonblock'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:180:in `connect_addrinfo'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:220:in `block in connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `each'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `each_with_index'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:296:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:342:in `establish_connection'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:104:in `block in connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:299:in `with_reconnect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:103:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:372:in `ensure_connected'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:224:in `block in process'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:312:in `logging'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:223:in `process'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:123:in `call'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:1347:in `block in sadd'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:50:in `block in synchronize'
rails_1     |   from /usr/local/lib/ruby/2.6.0/monitor.rb:230:in `mon_synchronize'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:50:in `synchronize'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:1346:in `sadd'
rails_1     |   from /usr/local/bundle/gems/redis-namespace-1.6.0/lib/redis/namespace.rb:442:in `call_with_namespace'
rails_1     |   from /usr/local/bundle/gems/redis-namespace-1.6.0/lib/redis/namespace.rb:328:in `method_missing'
rails_1     |   from /usr/local/bundle/gems/resque-2.0.0/lib/resque/data_store.rb:65:in `method_missing'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:94:in `set_schedule'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:51:in `block in schedule='
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:50:in `each'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:50:in `schedule='
rails_1     |   from /usr/src/app/config/initializers/resque.rb:8:in `&lt;top (required)&gt;'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `load'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `block in load'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:257:in `load_dependency'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `load'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:657:in `block in load_config_initializer'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/notifications.rb:170:in `instrument'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:656:in `load_config_initializer'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:614:in `block (2 levels) in &lt;class:Engine&gt;'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:613:in `each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:613:in `block in &lt;class:Engine&gt;'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:32:in `instance_exec'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:32:in `run'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:61:in `block in run_initializers'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:228:in `block in tsort_each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:350:in `block (2 levels) in each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:422:in `block (2 levels) in each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:431:in `each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:421:in `block in each_strongly_connected_component_from'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:50:in `each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:50:in `tsort_each_child'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:415:in `call'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:415:in `each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:349:in `block in each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `call'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:226:in `tsort_each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:205:in `tsort_each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:60:in `run_initializers'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/application.rb:361:in `initialize!'
rails_1     |   from /usr/src/app/config/environment.rb:5:in `&lt;top (required)&gt;'
rails_1     |   from config.ru:3:in `require_relative'
rails_1     |   from config.ru:3:in `block in &lt;main&gt;'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:55:in `instance_eval'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:55:in `initialize'
rails_1     |   from config.ru:in `new'
rails_1     |   from config.ru:in `&lt;main&gt;'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:49:in `eval'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:49:in `new_from_string'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:40:in `parse_file'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:319:in `build_app_and_options_from_config'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:219:in `app'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:27:in `app'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:354:in `wrapped_app'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:283:in `start'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:53:in `start'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:147:in `block in perform'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:142:in `tap'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:142:in `perform'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor/command.rb:27:in `run'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor/invocation.rb:126:in `invoke_command'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor.rb:387:in `dispatch'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/command/base.rb:65:in `perform'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/command.rb:46:in `invoke'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands.rb:18:in `&lt;top (required)&gt;'
rails_1     |   from bin/rails:4:in `require'
rails_1     |   from bin/rails:4:in `&lt;main&gt;'
</code></pre>
","<p>my problem is that I currently cannot login to my web ap that's running on docker. My docker-compose consists of my rails app, a mysql db, a restful api and a reverse proxy (someone recently just helped me out with this!). Everything is running fine currently (or at least I think so) and I can even test my api endpoint by going to localhost/api (and I see my doc for the api).</p>
<p>Problem is, when I login, it goes throug the restful api but I get an error as follow :</p>
<pre><code>Failed to open TCP connection to localhost:12345 (Cannot assign requested address - connect(2) for &quot;localhost&quot; port 12345)
</code></pre>
<p>I've tried to expose the port in the docker-compose, but it didn't work. Here are the important files :</p>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: '3.5'
services:
  app:
    image: 'test/testapp:first-test'
    depends_on:
      - db
    environment:
      DB_USER: root
      DB_NAME: test_dev
      DB_PASSWORD: root
      DB_HOST: db
      DB_PORT: 3306
      RAILS_ENV: development
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - .:/test_app
    networks:
      test-network:
        aliases:
          - app

  api:
    depends_on:
      - db
    image: 'test/testapi:first-test'
    ports:
      - &quot;12345:12345&quot;
    expose:
      - &quot;12345&quot;
    volumes:
      - .:/test_api
    networks:
      test-network:
        aliases:
          - api  

  reverse-proxy:
    depends_on:
      - app
      - api
    image: nginx:alpine
    volumes: 
      - $PWD/default.conf:/etc/nginx/conf.d/default.conf
    networks:
      test-network:
        aliases:
          - reverse-proxy
    ports:
      - 80:80
      - 443:443

  db:
    image: mysql:latest
    restart: always
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: test_dev
      MYSQL_USERNAME: root
      MYSQL_PASSWORD: root
    ports:
      - '3306:3306'
    volumes:
      - .:/test_db
    networks:
      test-network:
        aliases:
          - mysql-db
        
networks:
  test-network:

</code></pre>
<p>nginx conf</p>
<pre><code># This is a default site configuration which will simply return 404, preventing
# chance access to any other virtualhost.

server {
    listen 80 default_server;
    listen [::]:80 default_server;

    # Frontend
    location / {
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        proxy_pass http://hubsite:3000; # same name as network alias
    }

    # Backend
    location /api {
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        proxy_pass http://hubapi:12345/;  # &lt;--- note this has an extra /
    }

    # location = /404.html {
    #     internal;
    # }
}
</code></pre>
<p>Now this is very &quot;alpha&quot;, I'm pretty sure my volumes are all wrong and I should &quot;link&quot; my services altogether.</p>
<p>If anyone has information on this, I'd very much appreciate it! Thank you!</p>
"
"62708135","How to create separate volumes for each container in docker compose","<docker><docker-compose><mounted-volumes>","62692255","Multiple containers of docker image using same volumes in docker compose?","<docker><docker-compose><docker-volume>","<p>I have created a <code>docker</code> image for which I want to run multiple containers. This docker image is dependent on few things which will remain same for all the containers but the only difference will be configuration of the containers.</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
</code></pre>
<p>In the above file, you can see that I am giving <code>s1config</code> and <code>s2config</code> for <code>s1</code> <code>s2</code> containers but rest of the things remains same and this is why <code>/home/andrew/Documents/CVAI</code> volume is same for both the containers. Due to this, docker is sharing the volume between both the containers and thus all the data is mixing between containers and not getting separated.</p>
<p><strong>Is there any way we can separate the volumes between multiple containers.?</strong> I do not want to create multiple dockers for this. Please help. Thanks</p>
<p><strong>EDIT</strong></p>
<p>Updated docker-compose file :</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/s1/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/s1:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/s2/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/s2:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
</code></pre>
","<p>I have a python project which I have converted to a docker image. I need to run 2 containers of this docker image with different configs and different volumes. So I have put everything in docker compose file. Below is the file content:</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
         
            
        
</code></pre>
<p>As you can see that I am starting two containers i.e. <code>s1</code> <code>s2</code> of docker image <code>testdockerimg</code>. Both of them has same volume but difference is I am giving <code>s1config.json</code> as <code>config.json</code> to s1 container and <code>s2config.json</code> as <code>config.json</code> to s2 container.  This <code>config.json</code> has different configuration for different containers and is read by python file in docker to perform all the logic.</p>
<p>Now as per my understanding, both of these containers will have their own separate volumes, which means they are acting as two different machines having the volumes as mentioned above. But looks like both of the containers are using the 1 shared volume instead of having 2 separate volumes. I can confirm this as python code inside the docker pushes all the data to iot hub portal and I can see that <code>s1</code> has same data  as <code>s2</code>. I can also confirm this looking at the log file as their are 2 entries for each log in file.</p>
<p>Can anyone please tell me what is wrong. Does volumes across each containers remains same or its different. I do not want to create another docker image and would like to simply run another container having different config and volume so that all the data of the container is separate from other containers. Please help. Thanks</p>
"
"58573504","Docker port fowarding working on IPv6 but not IPv4","<docker><networking><docker-compose><portforwarding>","62703833","Unable to bind any program to IPv4 TCP port 80 on Mac","<macos><tcp><port><ipv4>","<p>I have the following very simple docker-compose.yml, running on a Mac:</p>
<pre><code>version: &quot;3.7&quot;
services:
  apache:
    image: httpd:2.4.41
    ports:
      - 80:80
</code></pre>
<p>I run <code>docker-compose up</code>, then I run this <code>curl</code> and Apache returns content:</p>
<pre><code>/tmp/test $ curl -v http://localhost
*   Trying ::1:80...
* TCP_NODELAY set
* Connected to localhost (::1) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: localhost
&gt; User-Agent: curl/7.66.0
&gt; Accept: */*
&gt; 
* Mark bundle as not supporting multiuse
&lt; HTTP/1.1 200 OK
&lt; Date: Sat, 26 Oct 2019 18:30:03 GMT
&lt; Server: Apache/2.4.41 (Unix)
&lt; Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT
&lt; ETag: &quot;2d-432a5e4a73a80&quot;
&lt; Accept-Ranges: bytes
&lt; Content-Length: 45
&lt; Content-Type: text/html
&lt; 
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
* Connection #0 to host localhost left intact
</code></pre>
<p>However, if I try to access the container using 127.0.0.1 instead of localhost, I get connection refused:</p>
<pre><code>/tmp/test $ curl -v http://127.0.0.1
*   Trying 127.0.0.1:80...
* TCP_NODELAY set
* Connection failed
* connect to 127.0.0.1 port 80 failed: Connection refused
* Failed to connect to 127.0.0.1 port 80: Connection refused
* Closing connection 0
curl: (7) Failed to connect to 127.0.0.1 port 80: Connection refused
</code></pre>
<p>Localhost does point to 127.0.0.1:</p>
<pre><code>/tmp/test $ ping localhost
PING localhost (127.0.0.1): 56 data bytes
</code></pre>
<p>And netstat shows all local IP addresses port 80 to be forwarded:</p>
<pre><code>/tmp/test $ netstat -tna | grep 80
...
tcp46      0      0  *.80                   *.*                    LISTEN     
...
</code></pre>
<p>I came to this actually trying to access the container using a custom domain I had on my <code>/etc/hosts</code> file pointing to 127.0.0.1. I thought there was something wrong with that domain name, but then I tried 127.0.0.1 and didn't work either, so I'm concluding there is something very basic about docker I'm not doing right.</p>
<p>Why is <code>curl http://localhost</code> working but <code>curl http://127.0.0.1</code> is not?</p>
<p><strong>UPDATE</strong></p>
<p>It seems localhost is resolving to IPv6 <code>::1</code>, so port forwarding seems to be working on IPv6 but not IPv4 addresses. Does that make any sense?</p>
<p><strong>UPDATE 2</strong></p>
<p>I wasn't able to fix it, but pointing my domain name to <code>::1</code> instead of <code>127.0.0.1</code> in my <code>/etc/hosts</code> serves as a workaround for the time being.</p>
<p><strong>UPDATE 3</strong></p>
<p>8 months later I bumped into the same issue and found my own question here, still unanswered. But this time I can't apply the same workaround, because I need to bind the port forwarding to my IPv4 address so it can be accessed from other hosts.</p>
","<p>I started debugging an issue with docker containers, confirmed it also happened on a plain nodejs + express on my Mac, and even netcat. Basically, I can bind to IPv4 TCP port 81 but not 80. It seems nothing else is already listening on that port, and the firewall is disabled.</p>
<p>To reproduce this, on a fresh boot I run the following to check that nothing is listening on port 80:</p>
<pre><code>$ sudo lsof -i tcp:80
$ sudo netstat -an | grep 80.*LISTEN
</code></pre>
<p>Both commands show nothing.</p>
<p>Then, on one terminal I run netcat to echo everything received in port 80: <code>sudo nc -l -p 80</code>.</p>
<p>Now:</p>
<pre><code>$ sudo lsof -i tcp:80
netcat    1700   root    3u  IPv4 0xcd04564dc212a9f      0t0  TCP *:http (LISTEN)

$ sudo netstat -an | grep 80.*LISTEN
tcp4       0      0  *.80                   *.*                    LISTEN  
</code></pre>
<p>But on a separate terminal I try to connect and get connection refused:</p>
<pre><code>$ telnet localhost 80
Trying ::1...
telnet: connect to address ::1: Connection refused
Trying 127.0.0.1...
telnet: connect to address 127.0.0.1: Connection refused
Trying 192.168.10.16...
telnet: connect to address 192.168.10.16: Connection refused
telnet: Unable to connect to remote host
</code></pre>
<p>This actually works fine with port 81, I can connect using IP 127.0.0.1 and everything I type on one terminal appears on the other:</p>
<pre><code>Terminal 1:
$ sudo nc -l -p 81
asdf

Terminal 2:
$ telnet localhost 81
Trying ::1...
telnet: connect to address ::1: Connection refused
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
asdf
</code></pre>
<p>Additionally:</p>
<ul>
<li><code>docker run -p 0.0.0.0:80:80 httpd:2.4</code> binds successfully to IPv6 ::1:80, but not IPv4 addresses. <code>curl http://[::1]</code> returns the homepage, but <code>curl http://127.0.0.1</code> shows connection refused.</li>
<li>Trying to bind to a specific IP address with <code>docker run -p 192.168.10.16:80:80 httpd:2.4</code>, froze for exactly 2 minutes (!) and ended up showing in <code>sudo lsof</code> as LISTEN, but I still get connection refused.</li>
<li>A simple nodejs + express server shows a similar behavior.</li>
<li>Same happens with ports 443 (doesn't work) and 444 (works as expected).</li>
<li>Since I'm not running any SSH server, I tested this on port 22 and it works just fine too. So far, the only 2 ports having problems that I know of are 80 and 443.</li>
<li>I used to run the builtin Apache server on this Mac a long time ago, and also through <code>homebrew</code>, in both cases using ports 80 and 443. I don't think this would be an issue because port 80 and 443 are actually free when I boot, but I mention this just in case this means something to somebody.</li>
</ul>
<p>It smells like something is blocking the ports rather than using them, but I checked the firewall in System Preferences &gt; Security &amp; Privacy &gt; Firewall and it's disabled.</p>
<p>What am I missing?</p>
"
"57331050","How can I install package from private repository using docker","<docker><npm><npm-install>","62712426","npm WARN deprecated on node v14+","<node.js><linux><docker><npm><openshift>","<p>I am installing a package from my private repository. I am able to install it using:
<code>npm i -S git+https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git</code></p>

<p>I am using docker container but while installation process I am getting an error:</p>

<pre><code>npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent
</code></pre>

<p>How can I solve it?</p>

<p>My docker file:</p>

<pre><code>FROM node:alpine

COPY package.json package.json
COPY src src
COPY .babelrc .babelrc

RUN npm install  
RUN npm run gitlab-build

RUN ls
EXPOSE 8080
CMD [""npm"", ""run"", ""docker-start""]
</code></pre>
","<p>I'm facing this issue for past 2days. But then the docker image fails. I've update chokidar,
fsevents doesn't update.
for the request I'm not sure what to do&quot; request,request,urix,resolve-url@,left-pad, core-js</p>
<pre><code>                  npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
                    npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
                    npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
                    npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
                    npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
                    npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
                    npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
                    npm ERR! path git
                    npm ERR! code ENOENT
                    npm ERR! errno ENOENT
                    npm ERR! syscall spawn git
                    npm ERR! enoent Error while executing:
                    npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
                    npm ERR! enoent
                    npm ERR! enoent
                    npm ERR! enoent spawn git ENOENT
                    npm ERR! enoent This is related to npm not being able to find a file.
                    npm ERR! enoent


                    npm ERR! A complete log of this run can be found in:
                    npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
                    The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please lemme know where do i do the changes I'm new to it</p>
"
"62712426","npm WARN deprecated on node v14+","<node.js><linux><docker><npm><openshift>","62715789","npm WARN deprecated on node v14+ Nodejs on openshift cluster","<node.js><linux><docker><npm><openshift>","<p>I'm facing this issue for past 2days. But then the docker image fails. I've update chokidar,
fsevents doesn't update.
for the request I'm not sure what to do&quot; request,request,urix,resolve-url@,left-pad, core-js</p>
<pre><code>                  npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
                    npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
                    npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
                    npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
                    npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
                    npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
                    npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
                    npm ERR! path git
                    npm ERR! code ENOENT
                    npm ERR! errno ENOENT
                    npm ERR! syscall spawn git
                    npm ERR! enoent Error while executing:
                    npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
                    npm ERR! enoent
                    npm ERR! enoent
                    npm ERR! enoent spawn git ENOENT
                    npm ERR! enoent This is related to npm not being able to find a file.
                    npm ERR! enoent


                    npm ERR! A complete log of this run can be found in:
                    npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
                    The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please lemme know where do i do the changes I'm new to it</p>
","<p>Please, any help would be appreciated.</p>
<p>I'm facing this issue for past 2 days. But then the docker image fails. I've update <code>chokidar</code>, <code>fsevents</code> doesn't update.</p>
<p>For the request I'm not sure what to do about <code>request</code>, <code>request</code>, <code>urix</code>, <code>resolve-url@</code>, <code>left-pad</code>, <code>core-js</code>.</p>
<pre class=""lang-none prettyprint-override""><code>npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent


npm ERR! A complete log of this run can be found in:
npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
         The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please let me know where do I do the changes. I'm new to it.</p>
"
"62011537","How can I run Selenium tests in a docker container with a visible browser?","<docker><selenium><selenium-webdriver><docker-compose><dockerfile>","62757187","is it possible to run selenium tests in docker container with selenium WebDriver?","<docker><selenium><grid>","<p>If I want to run Selenium tests inside a Docker container with a visible (not headless) browser, what are my options?</p>

<ul>
<li>Do I need to use a remote display viewer such as VNC?</li>
<li>Is it possible to use a browser on the host? (I.e. a browser that is not in the Docker container). How does this work?</li>
<li>Any other option?</li>
</ul>
","<p>I want to build a docker image from the sources of my Selenium project without selenium Grid is it possible?</p>
"
"62748938","Docker container to connect to Postgres not in docker","<postgresql><docker><rest>","62804279","Windows Docker container Networking to Postges on host (windows 10)","<docker><windows-10-desktop><postgresql-12>","<p>I have a .NET application running on a windows 10 computer using docker and postgres. When I run using the</p>
<pre><code> &quot;DockerTest&quot;: {
      &quot;commandName&quot;: &quot;Project&quot;,
      &quot;launchBrowser&quot;: true,
      &quot;launchUrl&quot;: &quot;asset&quot;,
      &quot;environmentVariables&quot;: {
        &quot;ASPNETCORE_ENVIRONMENT&quot;: &quot;Development&quot;
      },
      &quot;applicationUrl&quot;: &quot;https://localhost:5001;http://localhost:5000&quot;
    },
</code></pre>
<p>Connection string =  &quot;Server=localhost;Port=5433;Database=xxx;Uid=yyy;Pwd=zzz;&quot;,</p>
<p>option in VS2019 I can connect to my postgres database without problems.</p>
<p>When I try to run the application using the</p>
<pre><code> &quot;Docker&quot;: {
      &quot;commandName&quot;: &quot;Docker&quot;,
      &quot;launchBrowser&quot;: true,
      &quot;launchUrl&quot;: &quot;{Scheme}://{ServiceHost}:{ServicePort}/asset&quot;,
      &quot;publishAllPorts&quot;: true,
      &quot;useSSL&quot;: true
    },
</code></pre>
<p>option, I am getting errors connecting. Here is my connection string.</p>
<pre><code> &quot;Database1&quot;: &quot;Server=host.docker.internal;Port=5433;Database=xxx;Uid=yyy;Pwd=zzz;&quot;,
</code></pre>
<p>No matter what I try for a connection string I get CONNECTION WAS REFUSED or TIMEOUT errors.</p>
<p>Can anyone give me pointer as to what to try? I have gone over the message boards for two days and tries WINDOWS DEFENDER additions for com.docker.backend, I have tried different connection strings, I have tried many many other ways to solve this and have come up blank.</p>
<p>The end goal is to have docker run my REST API and have postgres installed on the same computer but not running in docker.</p>
<p><strong>NEW EDIT:</strong> July 6 1:45 AM MST</p>
<p>inside the container I ran the following:</p>
<p>C:\app&gt;ipconfig</p>
<p>Windows IP Configuration</p>
<pre><code>Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . : allworx.activeis.ca
   Link-local IPv6 Address . . . . . : fe80::c06e:18b5:f9e0:48bb%4
   IPv4 Address. . . . . . . . . . . : 172.30.81.112
   Subnet Mask . . . . . . . . . . . : 255.255.240.0
   Default Gateway . . . . . . . . . : 172.30.80.1
</code></pre>
<p>C:\app&gt;ping host.docker.internal</p>
<pre><code>Pinging host.docker.internal [10.0.0.47] with 32 bytes of data:
Reply from 10.0.0.47: bytes=32 time&lt;1ms TTL=127
</code></pre>
<p>NOTE : if I try to run my own app in the environment it actually works in with the 10.0.0.47 it works (even with host.docker.internal)... It is just from inside the container that it does not make the Postgres connection.</p>
<pre><code>**PG_HBA.CONF FILE :** 
host    replication     all             127.0.0.1/32            md5
host    replication     all             ::1/128                 md5
host    all     all     172.17.0.1/16       md5
</code></pre>
<p><strong>NEW EDIT:</strong> July 6 2:00 AM MST
If I use the IP address of my other network IPs on my host they all work as long as I put them into the PG_HBA.CONF FILE (in the dockertest option)</p>
<p>Nothing seems to make the transition from container to host when I run with the DOCKER option. Even when I add the 10.0.0.47 to the file and run with HOST.DOCKER.INTERNAL (which translates to 10.0.0.47) it still does not work</p>
<pre><code>**PG_HBA.CONF FILE :** 
host    replication     all             127.0.0.1/32            md5
host    replication     all             ::1/128                 md5
host    all     all     172.17.0.1/16       md5
host    all     all     10.0.0.47/16        md5
host    all     all     169.254.214.72/16   md5

C:\Users\Chris\source\repos\QuickTech.Com\QuickTechAPI&gt;docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
661118d67505        Default Switch      ics                 local
eae6a3536ef1        nat                 nat                 local
9d0b1f0209f6        none                null                local
</code></pre>
<p>I've seen a bridge or other network added in my hours of scouring the internet. Do I need to add a network to docker to make this transition?</p>
<p><strong>NEW EDIT:</strong> July 6 11:00 AM MST</p>
<pre><code>Host File in host computer
10.0.0.47 host.docker.internal
10.0.0.47 gateway.docker.internal
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
</code></pre>
<p>I cannot open the hosts file (c:\windows\system32\drivers\etc) in the container to see what it contains.</p>
","<p>OK.. Sorry to clog up this site with endless questions.
I have a .NET REST API that works in DOCKER. (Windows container)
But, the moment I try to connect to Postgres on my host I am unable to connect. I get unable to connect, request timed out, connection was actively refused... I have modified my connection string over a thousand times trying to get this to work.</p>
<p>when I look at docker networks is get:</p>
<pre><code>C:\Windows\SysWOW64&gt;docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
4c79ae3895aa        Default Switch      ics                 local
40dd0975349e        nat                 nat                 local
90a25f9de905        none                null                local
</code></pre>
<p>when I inspect my container, it says it is using NAT for network.</p>
<pre><code>C:\Windows\SysWOW64&gt;docker network inspect nat
[
    {
        &quot;Name&quot;: &quot;nat&quot;,
        &quot;Id&quot;: &quot;40dd0975349e1f4b334e5f7b93a3e8fb6aef864315ca884d8587c6fa7697dec5&quot;,
        &quot;Created&quot;: &quot;2020-07-08T15:02:17.5277779-06:00&quot;,
        &quot;Scope&quot;: &quot;local&quot;,
        &quot;Driver&quot;: &quot;nat&quot;,
        &quot;EnableIPv6&quot;: false,
        &quot;IPAM&quot;: {
            &quot;Driver&quot;: &quot;windows&quot;,
            &quot;Options&quot;: null,
            &quot;Config&quot;: [
                {
                    &quot;Subnet&quot;: &quot;172.22.96.0/20&quot;,
                    &quot;Gateway&quot;: &quot;172.22.96.1&quot;
                }
            ]
        },
        &quot;Internal&quot;: false,
        &quot;Attachable&quot;: false,
        &quot;Ingress&quot;: false,
        &quot;ConfigFrom&quot;: {
            &quot;Network&quot;: &quot;&quot;
        },
        &quot;ConfigOnly&quot;: false,
        &quot;Containers&quot;: {
            &quot;0d2dc2658a9948d84b01eaa9f5eb5a0e7815933f5af17e5abea17b82a796e1ec&quot;: {
                &quot;Name&quot;: &quot;***MyAPI***&quot;,
                &quot;EndpointID&quot;: &quot;3510dac9e5c5d49f8dce18986393e2855008980e311fb48ed8c4494c9328c353&quot;,
                &quot;MacAddress&quot;: &quot;00:15:5d:fc:4f:8e&quot;,
                &quot;IPv4Address&quot;: &quot;172.22.106.169/16&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            }
        },
        &quot;Options&quot;: {
            &quot;com.docker.network.windowsshim.hnsid&quot;: &quot;3007307C-49DC-4DB5-91C8-0E05DAC8E2B6&quot;,
            &quot;com.docker.network.windowsshim.networkname&quot;: &quot;nat&quot;
        },
        &quot;Labels&quot;: {}
    }
]
</code></pre>
<p>When I look at my network properties of my host I have :</p>
<pre><code>Name:   vEthernet (nat)
Description:    Hyper-V Virtual Ethernet Adapter #2
Physical address (MAC): 00:15:5d:fc:43:56
Status: Operational
Maximum transmission unit:  1500
IPv4 address:   172.22.96.1/20
IPv6 address:   fe80::d017:d598:692a:2e67%63/64
DNS servers:    fec0:0:0:ffff::1%1, fec0:0:0:ffff::2%1, fec0:0:0:ffff::3%1
Connectivity (IPv4/IPv6):   Disconnected
</code></pre>
<p>I am guessing that the NAT in the docker network ls linking to this network hyper v adapter.
both have 172.22.96.1 as the IPAddress</p>
<p>connection string:</p>
<p>Server=172.22.96.1;Port=5433;Database=QuickTechAssetManager;Uid=QuickTech;Pwd=password;</p>
<p>SO... when I try to connect from container to host to connect to postgres I get errors even though the I can ping the UP address.</p>
<p>when I look at my host file, host.docker.internal is set to 10.0.0.47 (my wifi connection).
Is this &quot;disconnect&quot; part of my network problems.</p>
<p>I have posted a few questions on this and I get one answer and then nothing further.
I am would absolutely love to have someone work with me for a bit to resolve this one - what should be minor - issue.</p>
<p>I have modified my pg_hba.conf file, I have done everything I can find...</p>
<p>I will give a phone number or email to anyone who wants to help me solve this. I have been killing myself for over a week and am getting nowhere. I am not even sure is this sort of request is allowed here but I am desperate. I am three months into a project and cant get paid until I get this one minor problem solved.</p>
<p>here is the other question I asked a few days ago:
<a href=""https://stackoverflow.com/questions/62748938/docker-container-to-connect-to-postgres-not-in-docker"">Docker container to connect to Postgres not in docker</a></p>
<p>rentedtrout@gmail.com for anyone who wants to work with me on this.</p>
<p>Please and thank you in advance.</p>
"
"54981900","Can we exec into container in a POD in K8S?","<docker><kubernetes>","62874800","Is it possible to exec in to a K8s pod the same way we exec in to a docker containers or containers running inside of a pod?","<docker><kubernetes><namespaces><kubernetes-pod><nsenter>","<p>I am putting docker image into POD.
We can exec into a Docker container using ""docker exec...""
Similarly is there a way to exec into container in a POD to check some data ?</p>
","<p>Is it possible to <code>exec</code> in to a K8s pod the same way we <code>exec</code> into docker containers or containers running inside of a pod?p</p>
<p><strong>Edit -</strong><br />
This question not about <code>exec</code>ing into a container in a pod. This is about the pod itself. Might be it's not possible but that is what the question is about. So stop marking it duplicate with - <a href=""https://stackoverflow.com/questions/54981900/can-we-exec-into-container-in-a-pod-in-k8s"">Can we exec into container in a POD in K8S?</a></p>
"
"54411883","Reload configuration when env variable has changed","<c#><docker><asp.net-core><docker-compose>","62883939","ASP.NET Core environment variables reload configuration on change","<c#><asp.net-core><configuration>","<p>In Startup.cs file I have</p>

<pre><code>public Startup(IHostingEnvironment env)
{
    var builder = new ConfigurationBuilder()
        .SetBasePath(env.ContentRootPath)
        .AddJsonFile(""appsettings.json"", optional: false, reloadOnChange: true)            
        .AddEnvironmentVariables();
    Configuration = builder.Build();
}
</code></pre>

<p>There is <em>appsettings.json</em> file with configurations. Like :</p>

<pre><code>{
  ""Log"" : {
     ""Type"" : ""value from appsettings.json""
   }
}
</code></pre>

<p><code>reloadOnChange</code> set to <code>true</code>, so, when I change <em>appsettings.json</em> then I get immediately a new value of log type in my program.</p>

<p>But I'm using Docker with docker-compose and pass a value of setting by env variables. My <em>docker-compose.override.yml</em> file is: </p>

<pre><code>version: '3.7'

services:

  myservice:
    environment:
      ASPNETCORE_ENVIRONMENT: Development
      Log__Type: ""value from docker-compose""
</code></pre>

<p>To run I use `docker-compose up. Now my app has value ""value from docker-compose"" for log type. </p>

<p><strong>Question:</strong> Are there any ways to change a value of env variable (<code>Log__Type</code>) at runtime (without restarting docker container) and reload configuration in my app as it has done with <code>reloadOnChange</code> and appsettings.json?</p>

<p>I tried to connect to the container (<code>docker exec</code>) and set a new value of env variable </p>

<pre><code>printenv Log__Type  // -&gt; value from docker-compose
export Log__Type=new value 
printenv Log__Type  // -&gt; new value 
</code></pre>

<p>but my app didn't reload configuration and still shows log type ""value from docker-compose"". </p>

<p>Could you please advise how to change settings at runtime using docker? Or explain why there is only reload when the file has changed but not env variable.</p>
","<p>When using standard appsettings configuration for your ASP.NET Core application there is an option to reload configs on file change by specifying <code>reloadOnChange</code> parameter:</p>
<pre class=""lang-cs prettyprint-override""><code>    var builder = new ConfigurationBuilder()
        .SetBasePath(environment.ContentRootPath)
        .AddJsonFile(&quot;appsettings.json&quot;, optional: true, reloadOnChange: true)
        .AddEnvironmentVariables();
</code></pre>
<p>When calling <code>AddEnvironmentVariables()</code> it allows you to use environment variables as well for generating your config. So the question is if there is a way to set reload on change for environment variables? So my application config is getting reset on environment variable change</p>
"
"61793815","Cannot create deno docker image","<docker><deno>","62960813","Dockerfile error no such file or directory""","<docker><deno>","<p>I want to create <code>deno</code> docker image using Dockerfile</p>

<pre><code>FROM alpine:latest

WORKDIR /

RUN apk update &amp;&amp; \
    apk upgrade

RUN apk add curl

RUN curl -fsSL https://deno.land/x/install/install.sh | sh

ENV DENO_INSTALL=""/root/.deno""

ENV PATH=""${DENO_INSTALL}/bin:${PATH}""

RUN deno --help
</code></pre>

<p>But when run <code>docker build -t deno .</code> it shows at last <code>/bin/sh: deno: not found</code></p>

<p>full output:</p>

<pre><code>Sending build context to Docker daemon  54.78kB
Step 1/8 : FROM alpine:latest
 ---&gt; f70734b6a266
Step 2/8 : WORKDIR /
 ---&gt; Using cache
 ---&gt; b1bbfa810906
Step 3/8 : RUN apk update &amp;&amp;     apk upgrade
 ---&gt; Using cache
 ---&gt; a7761425faba
Step 4/8 : RUN apk add curl
 ---&gt; Using cache
 ---&gt; 9099d4f65cb1
Step 5/8 : RUN curl -fsSL https://deno.land/x/install/install.sh | sh
 ---&gt; Using cache
 ---&gt; b4ea95c69a73
Step 6/8 : ENV DENO_INSTALL=""/root/.deno""
 ---&gt; Using cache
 ---&gt; bdc7e1e85e9c
Step 7/8 : ENV PATH=""${DENO_INSTALL}/bin:${PATH}""
 ---&gt; Using cache
 ---&gt; d35db1caba71
Step 8/8 : RUN deno --help
 ---&gt; Running in d1ca4e1d0dc6
/bin/sh: deno: not found
The command '/bin/sh -c deno --help' returned a non-zero code: 127
</code></pre>
","<p>I try to use my Dockerfile at <a href=""https://labs.play-with-docker.com/"" rel=""nofollow noreferrer"">https://labs.play-with-docker.com</a>. Its EOL conversion is LF. My file:</p>
<pre><code>FROM alpine
RUN apk --no-cache add curl
RUN curl -fsSL https://deno.land/x/install/install.sh | sh
WORKDIR /root/.deno/bin
# To print deno version at container start.
CMD [&quot;/root/.deno/bin/deno&quot;, &quot;--version&quot;]
</code></pre>
<p>I start the commands:</p>
<pre><code>docker build -t deno .
docker run --rm --name deno deno
</code></pre>
<p>But I get the error:</p>
<pre><code>standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Why does it happen? How can I fix it?</p>
"
"62011537","How can I run Selenium tests in a docker container with a visible browser?","<docker><selenium><selenium-webdriver><docker-compose><dockerfile>","62984510","How do i run my ""window based application"" in docker container","<selenium><docker-machine>","<p>If I want to run Selenium tests inside a Docker container with a visible (not headless) browser, what are my options?</p>

<ul>
<li>Do I need to use a remote display viewer such as VNC?</li>
<li>Is it possible to use a browser on the host? (I.e. a browser that is not in the Docker container). How does this work?</li>
<li>Any other option?</li>
</ul>
","<p>I want to run my selenium test case in a window based docker container.Since docker support only linux OS. But I wanted to run in window based container.since my application support only windows OS.  Is there any free or open ware which  support this. And i need free scalable window based contianer.
I have tried zalenium. But that too linux os based.
I am looking for open/freeware application for scalable selenium grid .</p>
<p>I have tried zalenium . But it operate in linus OS</p>
"
"58462473","Google Cloud VM Image to docker image","<docker><google-cloud-platform><google-kubernetes-engine><google-cloud-build>","63003082","How can I clone my Google Cloud Instance so I can download it and host it locally using Docker","<docker><ubuntu><hosting>","<p>I have a Google Cloud VM that installed with my application. The installation step is completed and I:</p>

<ol>
<li>Turned off the VM instance. </li>
<li>Exported the disk to disk image called <code>MY_CUSTOM_IMAGE_1</code></li>
</ol>

<p>My wish now is to use <code>MY_CUSTOM_IMAGE_1</code> as the starting image of my docker image build. For building the images I'm using Google Cloud Build. </p>

<p>My docker file should look like this:</p>

<pre><code>FROM MY_CUSTOM_IMAGE_1 AS BUILD_ENV
...
</code></pre>

<p>When I tried to use this image I got the build error:</p>

<pre><code>ERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: exit status 1
ERROR
pull access denied for MY_CUSTOM_IMAGE_1, repository does not exist or may require 'docker login'
Step 1/43 : FROM MY_CUSTOM_IMAGE_1 AS BUILD_ENV
</code></pre>

<p>The reason is that VM images are not the same as Docker images. </p>

<p>Is this possible to make this transform (GCP VM Image -> Docker image), without external tools (outside GCP, like ""docker private repositories"")?</p>

<p>Thanks!</p>
","<p>I have an ubuntu server in Google Cloud which I'd like move completely to host in Docker on my desktop machine. How can I just clone it and download it for local usage on ubuntu using Docker?</p>
<p>Is there a way to install docker on the VM and create an image of the machine which i can import into my local machine?</p>
<p>PS: Docker Noob</p>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","63005184","docker daemon is not running for windows 10","<docker>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>I am trying to install Docker for my Windows 10 PC by using virtualbox,docker is configured to use the default machine with IP but I am getting the following error</p>
<p>C:\Program Files\Docker Toolbox\docker.exe: error during connect: Post http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/containers/create: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.<a href=""https://i.stack.imgur.com/19Tpg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19Tpg.png"" alt=""enter image description here"" /></a></p>
"
"55174274","Understanding docker layers and future changes","<docker><ubuntu><debian>","63057529","Do docker images share identical layers?","<docker><dockerfile><docker-image>","<p><a href=""https://medium.com/@nagarwal/docker-containers-filesystem-demystified-b6ed8112a04a"" rel=""nofollow noreferrer"">So</a>, </p>

<p><a href=""https://i.stack.imgur.com/fotPN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fotPN.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem. </p>
</blockquote>

<p>and, </p>

<p><a href=""https://i.stack.imgur.com/qEm9R.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qEm9R.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Because each container has its own thin writable container layer, and all changes are stored in this container layer, this means that multiple containers can <strong>share access</strong> to the <strong>same underlying image</strong> and yet have their own data state.</p>
</blockquote>

<p>and <a href=""https://medium.com/@jessgreb01/digging-into-docker-layers-c22f948ed612"" rel=""nofollow noreferrer"">also</a>, </p>

<blockquote>
  <p>Layers of a Docker image are essentially just files generated from running some command. You can view the contents of each layer on the Docker host at <code>/var/lib/docker/aufs/diff</code>. </p>
</blockquote>

<p>Now, questions, </p>

<ul>
<li>Say I build my docker images <strong>layer by layer</strong>. <code>A &lt; B &lt; C &lt; D</code>, etc. </li>
<li>Now if I <strong>update my docker image</strong> <code>A</code>, would the rest of docker images <code>B, C, D</code> see the changes as well, provided that the changes are not touched by them when building them? E.g., adding <code>/etc/apt/sources.list.d/somethingnew</code> that was never there before.</li>
<li>If I had built <strong>another set</strong> of docker images layer by layer. <code>A &lt; X &lt; Y &lt; Z</code>, then the above changes will be reflected in <code>X, Y, Z</code> as well, right? </li>
<li>Now if however, the future changes to <code>A</code>, is <strong>made to the same file</strong> that will be changed when building <code>B, C, D</code>, then what would happen? For e.g., let's make it simple that docker images <code>B, C, D</code> each only add pkgB, pkgC, and pkgD in its layer. If I add a pkgA to <code>A</code> <em>after</em> <code>B, C, D</code> are build, what would happen? -- I guess there should be one single version of truth as what packages are in, for a single system, so what would it be for this case? </li>
<li>What if I'm <strong>only upgrading</strong> the packages in <code>A</code>? This should be OK right? Would the rest of docker images see the changes as well? </li>
</ul>
","<p>I know that each container is an image with a readable/writeable layer on top of a bunch of read-only layers, and that multiple containers can share the read-only layers of the image. Do two images that were created from the same base image share their identical images?</p>
<p>Example:</p>
<ul>
<li>Image A has 5 layers and weighs 1GB.</li>
<li>Image B is created with A as its base image and adds another layer and weighs 1.1GB.</li>
<li>Image C is created with A as its base image and adds another layer and weighs 1.5GB</li>
</ul>
<p>Is the total disk space now 3.6GB or 1.6GB?</p>
"
"63177219","Docker: How to access files from another container from a given container?","<docker><docker-compose><dockerfile><grpc><grpc-node>","63182856","Can One Container use the file of another Container?","<docker><docker-compose><dockerfile><docker-registry><docker-volume>","<p>Basically I have a main directory and Books Directory (General file structure, there's more but these are the important pieces). So when I fire a request from main to booksServer, it doesn't work because the node modules are missing.</p>
<p>That's because the node modules are inside the docker container at a specific path:  <strong>'/usr/src/app'</strong></p>
<p>How can I have main.js see that books (service/container) does have the proper node packages inside this specific path?</p>
<p>I think I can use docker-compose, but I wanted to test it individually without docker-compose first.</p>
<pre><code>**-Main Directory (Individual Service, has its own container)**
  -Initiator (Fires commands) 
  -DockerFile

**-Books Directory (Individual Service, has its own container)**
  -Stubs
     -BooksStub.js (NEED THIS!, but it won't work because needs npm modules which is located in its container @/usr/src/app. How can I access the nodemodules that it's using?)

  -booksServer.js
  -Package*.json (lock and package.json)
  -DockerFile
</code></pre>
<p>Inside the</p>
<p><strong>Error:</strong></p>
<pre><code>internal/modules/cjs/loader.js:800
    throw err;
    ^

Error: Cannot find module 'grpc'
</code></pre>
<p><strong>Books Dockerfile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 30043
CMD [&quot;node&quot;, &quot;booksServer.js&quot;]
</code></pre>
<p><strong>Main DockerFile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 4555
CMD [&quot;node&quot;, &quot;main.js&quot;]
</code></pre>
","<p>Basically I have a main directory and Books Directory (General file structure, there's more but these are the important pieces). So when I fire a request from main to booksServer, it doesn't work because the node modules are missing.</p>
<p>That's because the node modules are inside the docker container at a specific path:  <strong>'/usr/src/app'</strong></p>
<p>How can I have main.js see that books (service/container) does have the proper node packages inside this specific path?</p>
<p>I think I can use docker-compose, but I wanted to test it individually without docker-compose first.</p>
<pre><code>**-Main Directory (Individual Service, has its own container)**
  -Initiator (Fires commands) 
  -DockerFile

**-Books Directory (Individual Service, has its own container)**
  -Stubs
     -BooksStub.js (NEED THIS!, but it won't work because needs npm modules which is located in its container @/usr/src/app. How can I access the nodemodules that it's using?)

  -booksServer.js
  -Package*.json (lock and package.json)
  -DockerFile
</code></pre>
<p>Inside the</p>
<p><strong>Error:</strong></p>
<pre><code>internal/modules/cjs/loader.js:800
    throw err;
    ^

Error: Cannot find module 'grpc'
</code></pre>
<p><strong>Books Dockerfile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 30043
CMD [&quot;node&quot;, &quot;booksServer.js&quot;]
</code></pre>
<p><strong>Main DockerFile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 4555
CMD [&quot;node&quot;, &quot;main.js&quot;]
</code></pre>
"
"61762219","Kubernetes services are not accessible through nodeport with Desktop Docker setup","<docker><kubernetes><docker-desktop><kubernetes-service>","63225266","Can't access my node in k8s via browser (with service & deployment)","<docker><kubernetes><deployment>","<p>I am using Docker Desktop on windows 10. And I generate kubernetes NodePort Service to access from client web browser (<a href=""http://10.110.201.24:30008/hello/praveen"" rel=""nofollow noreferrer"">http://10.110.201.24:30008/hello/praveen</a>) but service is not accessible.</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      run: spring-app
  replicas: 1
  template:
    metadata:
      labels:
        run: spring-app
    spec:
      containers:
        - name: spring-boot
          image: praveen1233/spring-boot:v1
          ports:
            - containerPort: 80
</code></pre>

<p>service </p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: service
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30008
</code></pre>

<p>DockerFile </p>

<pre><code>FROM openjdk:8u212-jdk-slim
LABEL maintainer=""praveen.ambati33@gmail.com""
VOLUME /tmp
EXPOSE 5001
ARG JAR_FILE=target/spring-boot-app.jar
ADD ${JAR_FILE} spring-boot-app.jar
ENTRYPOINT [""java"",""-Djava.security.egd=file:/dev/./urandom"",""-jar"",""/spring-boot-app.jar""]
</code></pre>

<p>I have wrote simple web based spring boot application as below </p>

<pre><code>@RestController
public class HelloController {
    @GetMapping( value = ""hello/{input}"")
    public String getMessage(@PathVariable String input){
        return ""Hi ""+input;
    }
}
</code></pre>

<p>application.properties</p>

<pre><code>server.port=5000
</code></pre>

<p>I could see everything looks good in terms of Deployment, Pod and Services status. However I am not able to access the application. </p>

<pre><code>kubectl get all
NAME                            READY   STATUS             RESTARTS   AGE
pod/my-app-7b77675f79-wwbfl     1/1     Running            0          32m

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        6d1h
service/service      NodePort    10.110.201.24   &lt;none&gt;        80:30008/TCP   23m

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-app     1/1     1            1           32m

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/my-app-7b77675f79     1         1         1       32m
</code></pre>

<p>I am guessing their could be different way of accessing ip in Docker Hub which I am not able figure it out! Could you please help. Appreciated !</p>

<p>Docker and k8s versions as below </p>

<pre><code>Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b
 Built:             Wed Mar 11 01:23:10 2020
 OS/Arch:           windows/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.8
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.17
  Git commit:       afacb8b
  Built:            Wed Mar 11 01:29:16 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.2.13
  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429
 runc:
  Version:          1.0.0-rc10
  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
</code></pre>
","<p>There is the deployment:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-task-tracker-deployment
spec:
  selector:
    matchLabels:
      app: my-task-tracker
  replicas: 5
  template:
    metadata:
      labels:
        app: my-task-tracker
    spec:
      containers:
        - name: hello-world
          image: shaikezam/task-tracker:1.0
          ports:
            - containerPort: 8080
              protocol: TCP
</code></pre>
<p>This is the service (NodePort):</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: my-task-tracker-service
  labels:
    app: my-task-tracker
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8085
      nodePort: 30001
      protocol: TCP
  selector:
    app: my-task-tracker
</code></pre>
<p>Now, I try to access localhost:8085 or localhost:30001, and nothing happened.</p>
<p>I'm running using K8S in docker desktop.</p>
<p>Any suggestion what I'm doing wrong?</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","63265549","Problem running 'react-scripts start' from docker-compose","<reactjs><docker>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I like to use Docker as development environment and it worked quite well until react 2.1.8, but after upgrade the react to version 3.4.1, it stopped to work.</p>
<p>Now, the Docker container with the React application close with status exit 0, just after start the server.</p>
<p>It is strange because there is no log error. Any suggestion?</p>
<p>There is no error, just this log when I try <code>docker run react-frontend</code>:</p>
<pre><code>ℹ ｢wds｣: Project is running at http://172.17.0.2/
ℹ ｢wds｣: webpack output is served from 
ℹ ｢wds｣: Content not from webpack is served from /app/public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...

marcos@marcos-Inspiron-7472:~/Projetos/Virtus/Softex/edgeframework/maestro/frontend$ 
</code></pre>
"
"63325112","torch.cuda.is_avaiable returns False with nvidia-smi not working","<docker><gpu><nvidia><torch>","63324124","torch.cuda.is_available() returns false - torch.version.cuda different from torch._C._cuda_getDriverVersion()","<docker><cuda><dockerfile><nvidia><torch>","<p>I'm trying to build a docker image that can run using GPUS, this my situation:
<a href=""https://i.stack.imgur.com/vZivE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vZivE.png"" alt=""situation inside docker images"" /></a></p>
<p>I have python 3.6 and I am starting from image nvidia/cuda:10.0-cudnn7-devel.
Torch does not see my GPUs.</p>
<p><code>nvidia-smi</code> is not working too, returning error:</p>
<pre><code>&gt; Failed to initialize NVML: Unknown Error
&gt; The command '/bin/sh -c nvidia-smi' returned a non-zero code: 255
</code></pre>
<p>I installed nvidia toolkit and nvidia-smi with</p>
<pre><code> RUN apt install nvidia-cuda-toolkit -y
 RUN apt-get install nvidia-utils-410 -y
</code></pre>
","<p>My problem is that I can't se my GPUs, as torch.cuda.device_count() returns 0 and torch.cuda.is_available() False .</p>
<p>I found out that torch._C._cuda_getDriverVersion() returns 10000, nvcc --version is</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
</code></pre>
<p>BUT torch.version.cuda is 10.2.</p>
<p>Moreover, <code>nvidia-smi</code> does not work inside the container, even if I installed cuda toolkit and utils with:</p>
<pre><code>RUN apt install nvidia-cuda-toolkit -y
RUN apt-get install nvidia-utils-410 -y
</code></pre>
<p>The <code>nvidia-smi</code> error is: Failed to initialize NVML: Unknown Error</p>
<p>My base image is nvidia/cuda:10.0-cudnn7-devel.</p>
<p>Edit:
I tried <code>docker run --gpus all nvidia/cuda:10.0-cudnn7-devel nvidia-smi</code> and it works, showing the same nvidia-smi as the local machine</p>
"
"54269973","How to convert a Spring-Boot web service into a Docker image?","<spring-boot><docker><dockerfile>","63567621","Docker container port unreachable Spring Boot App","<spring><spring-boot><docker>","<p>I want to access my website from a Docker container but I can't. The steps I am trying to implement are as follows. After all the steps I can't access the <code>http://localhost:8080/index</code> page, where did I make a mistake?</p>

<p>The Spring-Boot project name is <code>demo</code>. Parts of my code:</p>

<pre><code>package com.qwerty.demo.rest;

import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class FunRestService {

    @GetMapping(""/index"")
    public String setIndex() {
        return ""HELLO WORLD!"";
    }
}
</code></pre>

<p>My <code>dockerfile</code> code:</p>

<pre><code>FROM openjdk:8
COPY . /usr/var/www/MYPROJECT
WORKDIR /usr/var/www/MYPROJECT
EXPOSE 8080
CMD ./mvnw spring-boot:run
</code></pre>

<p>I build the Spring-Boot project to <code>myimage1</code> with this command.</p>

<pre><code>docker build -t myimage1 .
</code></pre>

<p>Then, I create a new container from <code>myimage1</code> with this command.</p>

<pre><code>docker run --name mycontainer1 myimage1
</code></pre>

<p>And Maven downloads necessary files and starts my app for me. Last output:</p>

<pre><code>  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.2.RELEASE)

2019-01-19 17:40:32.604  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : Starting DemoApplication on 8086b6e010fb with PID 6 (/usr/var/www/MYPROJECT/target/classes started by root in /usr/var/www/MYPROJECT)
2019-01-19 17:40:32.613  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : No active profile set, falling back to default profiles: default
2019-01-19 17:40:34.119  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2019-01-19 17:40:34.170  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-01-19 17:40:34.171  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.14]
2019-01-19 17:40:34.186  INFO 6 --- [           main] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib]
2019-01-19 17:40:34.288  INFO 6 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2019-01-19 17:40:34.289  INFO 6 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1559 ms
2019-01-19 17:40:34.602  INFO 6 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2019-01-19 17:40:34.882  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''2019-01-19 17:40:34.888  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : Started DemoApplication in 3.176 seconds (JVM running for 69.839)
</code></pre>

<p>What should we do to convert one such Spring-Boot project (using a <code>Dockerfile</code>) into an image? How can I access my web page?</p>
","<p>I run my application in my host.</p>
<pre><code>java -jar docker-spring-boot.war


 http://localhost:8083/
</code></pre>
<p>On the browser, it works fine.</p>
<p>In the docker container, it didn't work.</p>
<pre><code> docker run docker-spring-boot -p 8083:8083

 http://localhost:8083/
 http://192.168.99.100:8083/
</code></pre>
<p>I have this message:</p>
<p><code>This site is inaccessible</code></p>
<p>Dockerfile</p>
<pre><code>FROM openjdk:8-jdk-alpine
EXPOSE 8083
ADD target/docker-spring-boot.war docker-spring-boot.war
ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/docker-spring-boot.war&quot;]
</code></pre>
<p>Have you an idea about the solution ?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63570332","Accessing Kafka broker in Docker container","<docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I can't for the life of me find information on this so forgive me if this is obvious.</p>
<p>I am publishing a Kafka broker's port to my host machine (MacOS) and cannot connect from the host.  Then, even when I <code>exec</code> into the container I can't <code>ping localhost:9092</code> or <code>curl</code> it.</p>
<p>I've also tried hitting the <code>KAFKA_ADVERTISED_LISTENERS</code> value which is <code>PLAINTEXT://kafka1:9092</code> and that gives me a <code>ping: cannot resolve PLAINTEXT://kafka1:9092</code></p>
<p>This is the relevant bit of my docker-compose file:</p>
<pre><code>version: '3'
services:
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_OPTS: &quot;-Dzookeeper.4lw.commands.whitelist=*&quot;
    ports:
      - &quot;2181:2181&quot;

  kafka1:
    build: .
    container_name: &quot;kafka1&quot;
    depends_on:
      - zookeeper
    ports:
      - &quot;8778&quot;
      - &quot;9092:9092&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OPTS: '-javaagent:/usr/jolokia/agents/jolokia-jvm.jar=host=0.0.0.0'
</code></pre>
<p>Dockerfile is as follows:</p>
<pre><code>FROM confluentinc/cp-kafka:latest

ENV JOLOKIA_VERSION 1.3.5
ENV JOLOKIA_HOME /usr/jolokia-${JOLOKIA_VERSION}
RUN curl -sL --retry 3 \
  &quot;https://github.com/rhuss/jolokia/releases/download/v${JOLOKIA_VERSION}/jolokia-${JOLOKIA_VERSION}-bin.tar.gz&quot; \
  | gunzip \
  | tar -x -C /usr/ \
 &amp;&amp; ln -s $JOLOKIA_HOME /usr/jolokia \
 &amp;&amp; rm -rf $JOLOKIA_HOME/client \
 &amp;&amp; rm -rf $JOLOKIA_HOME/reference
</code></pre>
<p>Any ideas?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63596455","Kafdrop - Cannot connect to Kafka Cluster setup using bitnami/kafka","<docker><apache-kafka><docker-compose><kafka-cluster>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I setup a kafka cluster using bitnami kafka and zookeeper and I wanted to view this cluster or at least one broker using kafdrop. I used docker compose to build all the components. I initially followed this <a href=""https://itnext.io/how-to-install-kafka-using-docker-a2b7c746cbdc"" rel=""nofollow noreferrer"">tutorial</a> and then added the kafdrop config in the docker-compose.yml</p>
<pre><code>version: '2'

networks:
  kafka-net:
    driver: bridge

services:
  zookeeper-server:
    image: 'bitnami/zookeeper:latest'
    networks:
      - kafka-net
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafdrop:
    image: obsidiandynamics/kafdrop
    networks:
      - kafka-net
    restart: &quot;no&quot;
    ports:
      - &quot;9000:9000&quot;
    environment:
      KAFKA_BROKERCONNECT: &quot;PLAINTEXT://localhost:9092,PLAINTEXT://localhost:9093,PLAINTEXT://localhost:9094&quot;
      JVM_OPTS: &quot;-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify&quot;
    depends_on:
      - &quot;kafka-server1&quot;
      - &quot;kafka-server2&quot;
      - &quot;kafka-server3&quot;
  kafka-server1:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
  kafka-server2:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9093:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9093
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
  kafka-server3:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9094:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9094
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
</code></pre>
<p>My main issue is that kafdrop always throw this error:</p>
<pre><code>020-08-26 10:53:53.517  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -3 (localhost/127.0.0.1:9094) could not be established. Broker may not be available.
2020-08-26 10:53:53.522  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -2 (localhost/127.0.0.1:9093) could not be established. Broker may not be available.
2020-08-26 10:53:53.526  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-08-26 10:53:53.627  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdro
</code></pre>
<p>I have tried changing the value of <code>KAFKA_BROKERCONNECT</code> with the ff values but all didn't work.</p>
<ul>
<li>PLAINTEXT://localhost:9092,PLAINTEXT://localhost:9093,PLAINTEXT://localhost:9094</li>
<li>localhost:9092,localhost:9093,localhost:9094</li>
<li>PLAINTEXT://kafka-server1:9092,PLAINTEXT://kafka-server2:9093,PLAINTEXT://kafka-server3:9094</li>
<li>kafka-server1:9092,kafka-server2:9093,kafka-server3:9094</li>
</ul>
<p>I am actually just guessing the correct config syntax so any explanation with this one is appreciated :).</p>
<p>Also, is adding <code>networks</code> property needed on kafdrop config? Kafdrop have sample <a href=""https://github.com/obsidiandynamics/kafdrop/tree/master/docker-compose/kafka-kafdrop"" rel=""nofollow noreferrer"">docker-compose</a> file and this one doesn't have network config so I wonder why/if <code>network</code> is needed.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63603430","How to access Docker Kafka from host machine","<java><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>looking for help as I have been stuck for a week now. I have a Kafka (0.8.2.1_11) and Zookeeper instance running inside of docker containers, and I am trying to get a Java app to write data to Kafka. I see that my topic is created, but no data is being saved.</p>
<p>My Java app is pretty straightforward, for loop that writes the iteration number. I also included a snippet of my docker-compose.yml file for the two services.</p>
<p>I start up my Kafka and Zookeeper by issuing a <code>docker-compose -f docker-compose.yml up -d</code> command. Then I run my Java file through intelliJ</p>
<pre><code>import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

public class MainFake {

    private static final Logger LOGGER = LoggerFactory.getLogger(MainFake.class);
    private static Properties kafkaProperties = new Properties();

    static {

        // set up Kafka
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);
        kafkaProperties.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        kafkaProperties.put(&quot;retries&quot;, 0);
        kafkaProperties.put(&quot;batch.size&quot;, 16384);
        kafkaProperties.put(&quot;linger.ms&quot;, 1);
        kafkaProperties.put(&quot;buffer.memory&quot;, 33554432);
    }

    public static void main(String[] args) {
        LOGGER.info(&quot;Started fake Main&quot;);
        KafkaProducer kafkaProducer = new KafkaProducer(kafkaProperties);
        try {
            for (int i = 0; i &lt; 30; i++) {
                LOGGER.info(&quot;logging &quot; + i);
                String kafkaPayload = String.valueOf(i);
                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test&quot;, generateKey(kafkaPayload), kafkaPayload);
                LOGGER.info(&quot;sending...&quot;);
                kafkaProducer.send(record).get();
                LOGGER.info(&quot;sent.&quot;);
            }
        } catch (Exception e) {
            LOGGER.error(String.valueOf(e));
        } finally {
            LOGGER.info(&quot;closing producer...&quot;);
            kafkaProducer.close();
            LOGGER.info(&quot;closed producer.&quot;);
        }
        LOGGER.info(&quot;Finished fake Main&quot;);
    }

    private static String generateKey(String payload) {
        return String.valueOf(System.nanoTime() + payload.hashCode());
    }

}

</code></pre>
<pre><code>zk:
    image: registry.cloud.cas.org/osd-platform/baseimage-zookeeper:3.4.8_11
    restart: always
    ports:
      - &quot;2181:2181&quot;
    networks:
      - chad

  kafka:
    image: registry.cloud.cas.org/osd-platform/kafka:0.8.2.1_11
    restart: always
    ports:
      - &quot;9092:9093&quot;
    depends_on:
      - zk
    environment:
      KAFKA_NAME: 1
      LOG_LEVEL: INFO
      KAFKA_ZOOKEEPER_CONNECT: localhost:2181
      KAFKA_LISTENERS: INTERNAL://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    hostname: ${VM_HOSTNAME}
    networks:
      chad:
        aliases:
          - kafka
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63608777","Spring Kafka Group Coordinator is invalid or unavailable","<java><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>This is a follow up question on my previous <a href=""https://stackoverflow.com/questions/63596455/kafdrop-cannot-connect-to-kafka-cluster-setup-using-bitnami-kafka/63600496#63600496"">post</a>. I am fairly new on both Kafka and Docker and I appreciate any detailed explanation.</p>
<p>I successfully setup my Kafka cluster using docker with zookeeper and kafdrop. Now, I want to connect my Spring project to connect to this cluster using Spring Kafka.</p>
<p>At first it successfully connected and I saw topics were automatically created. But then, I encountered this error that kept on running:</p>
<pre><code>11:27:27.727 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Group coordinator 66aa896dd4c0:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Discovered group coordinator 66aa896dd4c0:9092 (id: 2147482646 rack: null)
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=c*****] Discovered group coordinator 6043a3559f58:9092 (id: 2147482644 rack: null)
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Discovered group coordinator 6043a3559f58:9092 (id: 2147482644 rack: null)
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Group coordinator 66aa896dd4c0:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Group coordinator 6043a3559f58:9092 (id: 2147482644 rack: null) is unavailable or invalid, will attempt rediscovery
11:27:27.730 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Group coordinator 6043a3559f58:9092 (id: 2147482644 rack: null) is unavailable or invalid, will attempt rediscovery
11:27:27.732 INFO  o.a.k.c.c.i.AbstractCoordinator         - [Consumer clientId=*****, groupId=*****] Discovered group coordinator 66aa896dd4c0:9092 (id: 2147482646 rack: null)
</code></pre>
<p>Based on this log, the group coordinator keeps being discovered and lost.</p>
<p>This is my Consumer Config</p>
<pre><code> @Bean
    public Map&lt;String, Object&gt; consumerConfigs() {

        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092,localhost:9093,localhost:9094&quot;);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, 1);
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, &quot;1500&quot;);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        return props;
    }
</code></pre>
<p>Producer Config</p>
<pre><code> @Bean
    public Map&lt;String, Object&gt; producerConfigs() {

        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092,localhost:9093,localhost:9094&quot;);
        props.put(ProducerConfig.RETRIES_CONFIG, 0);
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        props.put(ProducerConfig.LINGER_MS_CONFIG, 1);
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        return props;
    }
</code></pre>
"
"57331050","How can I install package from private repository using docker","<docker><npm><npm-install>","63672427","npm install failed on compilation dockerfile","<docker><npm><svelte>","<p>I am installing a package from my private repository. I am able to install it using:
<code>npm i -S git+https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git</code></p>

<p>I am using docker container but while installation process I am getting an error:</p>

<pre><code>npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent
</code></pre>

<p>How can I solve it?</p>

<p>My docker file:</p>

<pre><code>FROM node:alpine

COPY package.json package.json
COPY src src
COPY .babelrc .babelrc

RUN npm install  
RUN npm run gitlab-build

RUN ls
EXPOSE 8080
CMD [""npm"", ""run"", ""docker-start""]
</code></pre>
","<p>I have my dockerfile:</p>
<pre><code>FROM node:14-alpine
 
WORKDIR /usr/src/app
 
COPY rollup.config.js ./
COPY package.json ./
 
RUN npm install
 
COPY ./src ./src
COPY ./static ./static
 
RUN npm run-script build
 
EXPOSE 4000
 
ENV HOST=0.0.0.0
 
CMD [ &quot;npm&quot;, &quot;start&quot; ]
</code></pre>
<p>When i execute</p>
<pre><code>docker build -t Dockerfile .
</code></pre>
<p>I have this error:</p>
<pre><code>npm ERR! code ENOENT
npm ERR! syscall spawn git
npm ERR! path git
npm ERR! errno -2
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t http://username:password@192.168.0.117:3000/username/example-component.git
npm ERR! enoent 
npm ERR! enoent 
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent 
</code></pre>
<p>Because some of my package on my package.json are</p>
<pre><code>&quot;package&quot;:  &quot;git+http://username:password@192.168.0.117:3000/username/example-component.git&quot;,
</code></pre>
<p>that is reachable only with a vpn. When i execute docker build i'm under vpn, i don't understand why is unreachable</p>
"
"59299133","How to silent install Postgresql in Ubuntu via. Dockerfile?","<postgresql><docker><dockerfile>","63737359","Trying to create a custom Docker image but getting asked for question during a RUN step","<python><docker><opencv>","<p>I have the following docker file, and I am using the command <code>docker build -t demo:v1 .</code> to build the image.</p>

<pre><code>FROM ubuntu:18.04
WORKDIR /app
RUN apt update \
    &amp;&amp; apt -y upgrade \
    &amp;&amp; apt install -y python3 \
    &amp;&amp; apt install -y python3-pip \
    &amp;&amp; apt install -y poppler-utils \
    &amp;&amp; apt install -y libsm6 libxext6 libxrender-dev

RUN apt install -y postgresql

COPY requirements.txt /app/requirements.txt

RUN pip3 install -r requirements.txt

COPY . /app

CMD gunicorn -t 300 --workers 5 --bind  0.0.0.0:8080 wsgi
</code></pre>

<p>When I build an image using this, while installing postgresql, it expects input and stops the building process like this </p>

<pre><code>.
.
.
.
Setting up libpopt0:amd64 (1.16-11) ...
Setting up tzdata (2019c-0ubuntu0.18.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area:
</code></pre>

<p>So, how can I setup postgresql inside my image, so that it builds without expecting this input? Also, surprisingly, even after I input my option, nothing happens further, and the process gets stuck. </p>
","<p>I'm trying to create a custom Docker image and during one of the <code>RUN</code> steps I'm getting asked to select some 'menu' choice .. which hangs/stops the image from continuing/getting made.</p>
<p>Here's the Dockerfile:</p>
<pre><code>FROM ubuntu:latest

RUN apt update

# install OpenCV
RUN apt install python3-opencv -y

# Test that OpenCV has been installed
RUN python3 -c &quot;import cv2; print(cv2.__version__)&quot;
</code></pre>
<p>and here's the last few lines in the build output:</p>
<pre><code>....
Setting up libsnappy1v5:amd64 (1.1.8-1build1) ...
Setting up poppler-data (0.4.9-2) ...
Setting up libkrb5support0:amd64 (1.17-6ubuntu4) ...
Setting up libsasl2-modules-db:amd64 (2.1.27+dfsg-2) ...
Setting up tzdata (2020a-0ubuntu0.20.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area: 
</code></pre>
<p>This is part of the 3rd step =&gt; <code>RUN p3 p3-opencv</code></p>
<p>How can I get this step to sorta auto completed? I'm <em>guessing</em> that when trying to install <code>python-opencv</code> I am missing dependencies and/or needing some dependencies to be updated. This is fine, and it's trying to update/install these, but needs some user-interaction.</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","63742046",":cached and :delegated mounted volumed performance in OSX","<macos><performance><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>Has anyone been able to solve the filesystem performance issues with mounted volumes on OSX?</p>
<p>I’m running <code>time du -d0</code> in some directory, and my baseline (on the host) is this:</p>
<pre><code>&gt; time du -d0
117496  .
________________________________________________________
Executed in   87.45 millis    fish           external
   usr time    7.43 millis   88.00 micros    7.34 millis
   sys time   75.34 millis  735.00 micros   74.60 millis
</code></pre>
<p>When doing that inside a container, on a mounted volume:</p>
<pre><code>&gt; docker run -ti --rm -v $PWD:$PWD -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 6.42s
user    0m 0.06s
sys 0m 0.29s
</code></pre>
<p>After reading <a href=""https://docs.docker.com/docker-for-mac/osxfs-caching/"" rel=""nofollow noreferrer"">this</a>, I also tried the <code>:cached</code> and <code>:delegated</code> options, to no avail:</p>
<pre><code>&gt; docker run -ti --rm -v $PWD:$PWD:cached -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 6.09s
user    0m 0.06s
sys 0m 0.36s
&gt; docker run -ti --rm -v $PWD:$PWD:delegated -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 5.28s
user    0m 0.06s
sys 0m 0.49s
</code></pre>
<p>Tested on:
macOS Catalina 10.15.6
Docker desktop 2.3.0.4
Docker engine: 19.03.12</p>
"
"63764895","Redis connect ECONNREFUSED 127.0.0.1:6379","<javascript><node.js><docker><redis><containers>","63765804","Can't connect to DB from container","<javascript><node.js><docker><redis><containers>","<p>Im running redis on my server (Redis is not running in a container)  and I am trying to connect to redis inside of my Container which has a Node.js app runninng. When starting the Docker Container with the node app from my Dockerfile in the logs there is the following error: <code>Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code></p>
","<p>I have a Docker container with a Node.js app and a Redis Database which is running on the server (not in a container). In the container when the app starts I can't connect to the DB and I am getting the following errror <code>Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code> but I dont understand why because if I open redis-cli on the server and ping it answers with pong so the db is up.</p>
"
"63373403","org.openqa.selenium.NoSuchSessionException: Unable to find session with ID error testing with Behat/Mink and Selenium2Driver in docker container","<docker><selenium><symfony><selenium-chromedriver><behat>","63792374","org.openqa.selenium.NoSuchSessionException: Session ID is null. -getting this error in automation (using testng,selenium,java)","<java><selenium><cucumber-java>","<p>I'm trying to test a Symfony3 web application with <a href=""http://mink.behat.org/en/latest/index.html"" rel=""nofollow noreferrer"">Behat/Mink</a> and <a href=""http://mink.behat.org/en/latest/drivers/selenium2.html"" rel=""nofollow noreferrer"">Selenium2Driver</a> so that I can test Javascript functionallity too.</p>
<p>The application runs in a docker container, so I added a new docker container for selenium-hub and chrome as described <a href=""https://github.com/SeleniumHQ/docker-selenium"" rel=""nofollow noreferrer"">here</a>:</p>
<pre class=""lang-yaml prettyprint-override""><code># docker-compose.yml
version: '3.5' # Docker Engine release 17.12.0+

networks:
    servicesnet:
      driver: bridge

services:
   apache:
      build:
         context: './apache2'
      container_name: apache-service
      ports:
         - &quot;80:80&quot;
         - &quot;443:443&quot;
      tty: true
      networks:
         - servicesnet
      volumes:
         - ${HOST_APACHE_CONFIG}:/etc/apache2
         - ${HOST_PAGES_PATH}:/var/www/localhost/htdocs

   selenium-hub:
      image: selenium/hub:4.0.0-alpha-6-20200730
      container_name: selenium-hub
      ports:
         - &quot;4444:4444&quot;
      networks:
         - servicesnet

   chrome:
      image: selenium/node-chrome:4.0.0-alpha-6-20200730
      volumes:
         - /dev/shm:/dev/shm
      depends_on:
         - selenium-hub
      environment:
         - HUB_HOST=selenium-hub
      networks:
         - servicesnet
</code></pre>
<p>When I run <code>docker-compose up</code> it outputs for the new containers:</p>
<pre><code>chrome          | 2020-08-12 07:36:19,917 INFO Included extra file &quot;/etc/supervisor/conf.d/selenium.conf&quot; during parsing
chrome          | 2020-08-12 07:36:19,918 INFO supervisord started with pid 7
selenium-hub    | 2020-08-12 07:36:19,297 INFO Included extra file &quot;/etc/supervisor/conf.d/selenium-grid-hub.conf&quot; during parsing
selenium-hub    | 2020-08-12 07:36:19,298 INFO supervisord started with pid 7
selenium-hub    | 2020-08-12 07:36:20,301 INFO spawned: 'selenium-grid-hub' with pid 10
selenium-hub    | Starting Selenium Grid Hub...
selenium-hub    | 2020-08-12 07:36:20,311 INFO success: selenium-grid-hub entered RUNNING state, process has stayed up for &gt; than 0 seconds (startsecs)
selenium-hub    | 07:36:20.588 INFO [LoggingOptions.getTracer] - Using OpenTelemetry for tracing
selenium-hub    | 07:36:20.589 INFO [LoggingOptions.createTracer] - Using OpenTelemetry for tracing
selenium-hub    | 07:36:20.607 INFO [EventBusOptions.createBus] - Creating event bus: org.openqa.selenium.events.zeromq.ZeroMqEventBus
selenium-hub    | 07:36:20.638 INFO [BoundZmqEventBus.&lt;init&gt;] - XPUB binding to [binding to tcp://*:4442, advertising as tcp://172.28.0.3:4442], XSUB binding to [binding to tcp://*:4443, advertising as tcp://172.28.0.3:4443]
selenium-hub    | 07:36:20.676 INFO [UnboundZmqEventBus.&lt;init&gt;] - Connecting to tcp://172.28.0.3:4442 and tcp://172.28.0.3:4443
selenium-hub    | 07:36:20.680 INFO [UnboundZmqEventBus.&lt;init&gt;] - Sockets created
selenium-hub    | 07:36:20.681 INFO [UnboundZmqEventBus.lambda$new$2] - Bus started
chrome          | 2020-08-12 07:36:21,136 INFO success: xvfb entered RUNNING state, process has stayed up for &gt; than 0 seconds (startsecs)
chrome          | 2020-08-12 07:36:21,136 INFO success: fluxbox entered RUNNING state, process has stayed up for &gt; than 0 seconds (startsecs)
chrome          | 2020-08-12 07:36:21,136 INFO success: vnc entered RUNNING state, process has stayed up for &gt; than 0 seconds (startsecs)
chrome          | 2020-08-12 07:36:21,137 INFO success: selenium-node entered RUNNING state, process has stayed up for &gt; than 0 seconds (startsecs)
selenium-hub    | 07:36:21.308 INFO [Hub.execute] - Started Selenium hub 4.0.0-alpha-6 (revision 5f43a29cfc): http://172.28.0.3:4444
chrome          | 07:36:21.774 INFO [LoggingOptions.getTracer] - Using OpenTelemetry for tracing
chrome          | 07:36:21.775 INFO [LoggingOptions.createTracer] - Using OpenTelemetry for tracing
chrome          | 07:36:21.791 INFO [EventBusOptions.createBus] - Creating event bus: org.openqa.selenium.events.zeromq.ZeroMqEventBus
chrome          | 07:36:21.829 INFO [UnboundZmqEventBus.&lt;init&gt;] - Connecting to tcp://selenium-hub:4442 and tcp://selenium-hub:4443
chrome          | 07:36:21.857 INFO [UnboundZmqEventBus.&lt;init&gt;] - Sockets created
chrome          | 07:36:21.859 INFO [UnboundZmqEventBus.lambda$new$2] - Bus started
chrome          | 07:36:22.121 INFO [NodeServer.execute] - Reporting self as: http://172.28.0.5:5555
chrome          | 07:36:22.175 INFO [NodeOptions.report] - Adding Chrome for {&quot;browserName&quot;: &quot;chrome&quot;} 8 times
chrome          | 07:36:22.298 INFO [NodeServer.execute] - Started Selenium node 4.0.0-alpha-6 (revision 5f43a29cfc): http://172.28.0.5:5555
chrome          | 07:36:22.302 INFO [NodeServer.execute] - Starting registration process for node id ff0154a7-ed4b-438a-887c-0a7f3a988cb4
selenium-hub    | 07:36:22.355 INFO [LocalDistributor.refresh] - Creating a new remote node for http://172.28.0.5:5555
selenium-hub    | 07:36:22.763 INFO [LocalDistributor.add] - Added node ff0154a7-ed4b-438a-887c-0a7f3a988cb4.
selenium-hub    | 07:36:22.770 INFO [Host.lambda$new$0] - Changing status of node ff0154a7-ed4b-438a-887c-0a7f3a988cb4 from DOWN to UP. Reason: http://172.28.0.5:5555 is ok
chrome          | 07:36:22.774 INFO [NodeServer.lambda$execute$0] - Node has been added
</code></pre>
<p>Then I have the next method for every test:</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

namespace Tests\AppBundle\Controller;

use Behat\Mink\Driver\Selenium2Driver;
use Behat\Mink\Mink;
use Behat\Mink\Session;
use Symfony\Bundle\FrameworkBundle\Client;
use Symfony\Bundle\FrameworkBundle\Test\WebTestCase;

abstract class BaseControllerTest extends WebTestCase
{
    /**
     * @var Client
     */
    protected $client;
    /**
     * @var Session
     */
    protected $session;

    public function visitUri($uri)
    {
        $this-&gt;client = static::createClient();
        $pass = $this-&gt;client-&gt;getKernel()-&gt;getContainer()-&gt;getParameter('http_basic_auth_pass');
        $host = 'localhost'; // I've tried several things here (like 172.28.0.5:5555)

        $driver = new Selenium2Driver('chrome');
        $mink = new Mink(array(
            'chrome' =&gt; new Session($driver)
        ));
        $driver-&gt;setTimeouts(['page load' =&gt; 900000]);

        $mink-&gt;setDefaultSessionName('chrome');

        $this-&gt;session = $mink-&gt;getSession();

        $this-&gt;session-&gt;visit('http://user:' . $pass . '@' . $host . $uri);
    }
}
</code></pre>
<p>And I call this method from a specific test:</p>
<pre class=""lang-php prettyprint-override""><code>    public function testClickOnSearch()
    {
        $this-&gt;visitUri(/mi-custom-uri);
        $page = $this-&gt;session-&gt;getPage();

        $this-&gt;session-&gt;wait(
            200000,
            &quot;typeof jQuery !== 'undefined'&quot;
        );

        $page-&gt;findButton('Buton text')-&gt;click();
        $this-&gt;assertContains('my-custom-uri-2', $this-&gt;session-&gt;getCurrentUrl());
    }
</code></pre>
<p>but I never get the session started. If I go to <code>http://localhost:4444/wd/hub/session/url</code> I see this error message:</p>
<blockquote>
<p>&quot;org.openqa.selenium.NoSuchSessionException: Unable to find session with ID: url\nBuild info: version: '4.0.0-alpha-6', revision: '5f43a29cfc'\nSystem info: host: 'fca78c7f81e6', ip: '172.28.0.3', os.name: 'Linux', os.arch: 'amd64', os.version: '5.4.0-42-generic', java.version: '1.8.0_252'\nDriver info: driver.version: unknown&quot;</p>
</blockquote>
<p>And executing the test, after 200 seconds this error is thrown:</p>
<blockquote>
<p>PHP Fatal error:  Call to a member function click() on null</p>
</blockquote>
<p>I'm sure something is missing but don't know what. Any idea?</p>
","<pre><code>My files are given below. please help me in parallel execution in same machine. Multiple Browsers are opening  but URL in one browser is not getting added also. and this is what error is coming which is mentioned in output .Thanks in advance.








This is testng file.
-----## testng##-----
    
    
    &lt;!DOCTYPE suite SYSTEM &quot;http://testng.org/testng-1.0.dtd&quot;&gt;
    
    &lt;suite name=&quot;BOC UI AUTOMATION&quot;&gt;
        &lt;test name=&quot;BOC Feature Testing&quot; parallel=&quot;methods&quot; thread-count=&quot;2&quot;&gt;
            &lt;classes&gt;
                &lt;class name=&quot;runner.loginPageRunner&quot;&gt;&lt;/class&gt;
              
                &lt;class name=&quot;runner.B2BSmokeRunner&quot;&gt;&lt;/class&gt;
            &lt;/classes&gt;
        &lt;/test&gt;
    &lt;/suite&gt;
    
    
    
    


----------

This is driver settings file where i am selecting driver of browser.
    ## Driversetting ##
    


----------


    package base;
    
    import java.io.File;
    import java.util.HashMap;
    import java.util.concurrent.TimeUnit;
    
    //import org.openqa.selenium.Dimension;
    import org.openqa.selenium.chrome.ChromeDriver;
    import org.openqa.selenium.chrome.ChromeOptions;
    import org.openqa.selenium.firefox.FirefoxDriver;
    import org.openqa.selenium.firefox.FirefoxOptions;
    
    import org.openqa.selenium.remote.CapabilityType;
    import org.openqa.selenium.remote.DesiredCapabilities;
    import utils.FileReference;
    
    
    public class DriverSettings
    {
        /**
         * Overloaded browser setup with browser name and time-out being
         * sent as parameter
         * @param browserName : name of the browser (like: firefox, ie, chrome) case insensitive
         * @param timeUnitInSecond : Timeout second for implicit time-out
         */
        public void setUpDriver(String browserName, int timeUnitInSecond)
        {
            final FirefoxOptions options = new FirefoxOptions();
            switch(browserName.trim().toLowerCase())
            {
            case &quot;firefox&quot; :
                System.setProperty(&quot;webdriver.gecko.driver&quot;, FileReference.driversFilePath+File.separator+&quot;geckodriver.exe&quot;);
                System.setProperty(FirefoxDriver.SystemProperty.DRIVER_USE_MARIONETTE, &quot;true&quot;);
                options.addPreference(&quot;browser.popups.showPopupBlocker&quot;, false);
                options.addPreference(&quot;security.sandbox.content.level&quot;, 5);
                Driver.driver = new FirefoxDriver(options);
                break;
    
            case &quot;chrome&quot; :
                System.setProperty(&quot;webdriver.chrome.driver&quot;, FileReference.driversFilePath+File.separator+&quot;chromedriver.exe&quot;);
                HashMap&lt;String, Object&gt; chromePrefs = new HashMap&lt;String, Object&gt;();
                chromePrefs.put(&quot;profile.default_content_settings.popups&quot;, 0);
                chromePrefs.put(&quot;download.default_directory&quot;, FileReference.download);
                ChromeOptions option = new ChromeOptions();
                option.setExperimentalOption(&quot;prefs&quot;, chromePrefs);
                option.setAcceptInsecureCerts(true);
                Driver.driver = new ChromeDriver(option);
                break;
    
            default : 
                System.setProperty(&quot;webdriver.gecko.driver&quot;, System.getProperty(&quot;user.dir&quot;) + &quot;\\geckodriver.exe&quot;);
                System.setProperty(FirefoxDriver.SystemProperty.DRIVER_USE_MARIONETTE, &quot;true&quot;);
                options.addPreference(&quot;browser.popups.showPopupBlocker&quot;, false);
                options.addPreference(&quot;security.sandbox.content.level&quot;, 5);
                Driver.driver = new FirefoxDriver(options);
                break;
            }
            setWait(timeUnitInSecond);
            setBrowserAtMaxSize();
        }
    
        /**
         * Sets the implicit time out as parameter
         * @param timeUnitInSecond : Timeout second for implicit time-out
         */
        protected static void setWait(int timeUnitInSecond)
        {
            Driver.driver.manage().timeouts().implicitlyWait(timeUnitInSecond, TimeUnit.SECONDS);
        }
    
        /**
         * Sets the browser at maximum size of the screen
         */
        protected void setBrowserAtMaxSize()
        {
            Driver.driver.manage().window().maximize();
        }
    
        /**
         * Closes the driver, deletes all cookies
         */
        public void closeDriver()
        {
    //      Driver.driver.close();
            Driver.driver.quit();
        }
    
        public void closeWindow()
        {
            Driver.driver.close();
        }
    }
    


----------


    
    ----------##Driver## ----------
    
    
    package base;
    
    import org.openqa.selenium.WebDriver;
    
    public class Driver
    {
        
        public static WebDriver driver;
    
    }
    
    






    ----------
   ##Browser## 
    
    
    ----------
    package base;
    
    import org.dom4j.DocumentException;
    
    import org.openqa.selenium.JavascriptExecutor;
    import org.openqa.selenium.NoSuchSessionException;
    import utils.LoadEnvironment;
    
    public class Browser {
    
        private DriverSettings driverSetting;
        protected static int timeToWait = 30;
    
        public Browser() throws DocumentException
        {
            LoadEnvironment.loadProperties();
            String browserName=LoadEnvironment.browser;
    
            driverSetting = new DriverSettings();
            driverSetting.setUpDriver(browserName, timeToWait);
        }
    
        public Browser(String browserName)
        {
            driverSetting = new DriverSettings();
            driverSetting.setUpDriver(browserName, timeToWait);
        }
    
        public Browser(String browserName, int timeUnitInSecond)
        {
            driverSetting = new DriverSettings();
            timeToWait = timeUnitInSecond;
            driverSetting.setUpDriver(browserName, timeUnitInSecond);
        }
    
        public void close()
        {
            try
            {
                driverSetting.closeDriver();
            }
            catch(Exception e)
            {
                System.out.println(e.getMessage());
            }
        }
    
        public static void navigateTo(String URL)
        {
            Driver.driver.get(URL);
        }
    
        public void refresh()
        {
            Driver.driver.navigate().refresh();
        }
    
        /**
         * Read the URL of the current page
         * @return : String URL
         */
        public String getCurrntUrl()
        {
            return Driver.driver.getCurrentUrl();
        }
    
        public static void scrollToBottomOfThePage() throws InterruptedException {
            Thread.sleep(100);
            JavascriptExecutor js = ((JavascriptExecutor) Driver.driver);
            js.executeScript(&quot;window.scrollTo(0,document.body.scrollHeight)&quot;);
            Thread.sleep(400);
        }
    
        public static boolean isBrowserReachable()
        {
            boolean browserReachable=true;
            try {
                Driver.driver.getTitle();
            }catch(NoSuchSessionException e)
            {
                browserReachable=false;
            }
            return browserReachable;
        }
    
        
    }








----------
##Pom.xml##


----------
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;B2BUI_Automation&lt;/groupId&gt;
    &lt;artifactId&gt;com.softwareag&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;aspectj.version&gt;1.9.4&lt;/aspectj.version&gt;
         &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;suite.file&gt;testng.xml&lt;/suite.file&gt;
        &lt;browser&gt;chrome&lt;/browser&gt;
        &lt;tagsToRun&gt;&lt;/tagsToRun&gt;
    &lt;/properties&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.19.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;testFailureIgnore&gt;true&lt;/testFailureIgnore&gt;
                    &lt;argLine&gt;
                        -javaagent:&quot;${settings.localRepository}/org/aspectj/aspectjweaver/${aspectj.version}/aspectjweaver-${aspectj.version}.jar&quot;
                       -Dcucumber.options=&quot;&amp;#45;&amp;#45;plugin utils.CustomCucumberAfterStepListener &amp;#45;&amp;#45;plugin io.qameta.allure.cucumber4jvm.AllureCucumber4Jvm ${tagsToRun}&quot;
                    &lt;/argLine&gt;
                    &lt;suiteXmlFiles&gt;
                        &lt;suiteXmlFile&gt;${suite.file}&lt;/suiteXmlFile&gt;
                    &lt;/suiteXmlFiles&gt;
                    &lt;systemPropertyVariables&gt;
                        &lt;browser&gt;${browser}&lt;/browser&gt;
                    &lt;/systemPropertyVariables&gt;
                &lt;/configuration&gt;
                &lt;dependencies&gt;
                    &lt;dependency&gt;
                        &lt;groupId&gt;org.aspectj&lt;/groupId&gt;
                        &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;
                        &lt;version&gt;${aspectj.version}&lt;/version&gt;
                    &lt;/dependency&gt;
                &lt;/dependencies&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;filesets&gt;
                        &lt;fileset&gt;
                            &lt;directory&gt;${basedir}/allure-results&lt;/directory&gt;&lt;/fileset&gt;
                    &lt;/filesets&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

    &lt;reporting&gt;
        &lt;excludeDefaults&gt;true&lt;/excludeDefaults&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;io.qameta.allure&lt;/groupId&gt;
                &lt;artifactId&gt;allure-maven&lt;/artifactId&gt;
                &lt;version&gt;2.10.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;resultsDirectory&gt;../allure-results&lt;/resultsDirectory&gt;
                &lt;/configuration&gt;

            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/reporting&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.cucumber&lt;/groupId&gt;
            &lt;artifactId&gt;cucumber-testng&lt;/artifactId&gt;
            &lt;version&gt;4.4.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;junit&lt;/groupId&gt;
                    &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.cucumber&lt;/groupId&gt;
            &lt;artifactId&gt;cucumber-picocontainer&lt;/artifactId&gt;
            &lt;version&gt;4.4.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt;
            &lt;artifactId&gt;selenium-server&lt;/artifactId&gt;
            &lt;version&gt;3.11.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.testng&lt;/groupId&gt;
            &lt;artifactId&gt;testng&lt;/artifactId&gt;
            &lt;version&gt;6.10&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.qameta.allure&lt;/groupId&gt;
            &lt;artifactId&gt;allure-cucumber4-jvm&lt;/artifactId&gt;
            &lt;version&gt;2.12.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;dom4j&lt;/groupId&gt;
            &lt;artifactId&gt;dom4j&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.glassfish.jersey.core&lt;/groupId&gt;
            &lt;artifactId&gt;jersey-client&lt;/artifactId&gt;
            &lt;version&gt;2.5.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/jaxen/jaxen --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;jaxen&lt;/groupId&gt;
            &lt;artifactId&gt;jaxen&lt;/artifactId&gt;
            &lt;version&gt;1.1.6&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>
<hr />
<h2>Output</h2>
<hr />
<p>org.openqa.selenium.NoSuchSessionException: Session ID is null. Using WebDriver after calling quit()?
Build info: version: '3.11.0', revision: 'e59cfb3', time: '2018-03-11T20:26:55.152Z'
System info: host: 'SAG-DMPJ622', ip: '10.91.56.203', os.name: 'Windows 10', os.arch: 'amd64', os.version: '10.0', java.version: '11.0.5'
Driver info: driver.version: RemoteWebDriver</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","63843768","Java process memory used 3.2GB, but heap used only 1053MB, I want to know how to solve this","<java><memory-leaks><jvm><heap-memory>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I want to check the backends' healthy condition, I use the ThreadPool start a Thread to check backend per10s. In this times, this will query mysql 6 times per 10s and not use the database connection pool.
When I deploy this app in the cloud platform, I received message that the usage of memory is continued growth.</p>
<p><a href=""https://i.stack.imgur.com/8yBnt.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>At first, I use the <code>top</code> command to watch the usage of Java Heap.
In this picture, I find this app use about 3.2GB memory.</p>
<p><a href=""https://i.stack.imgur.com/etpOI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>And then I exec the <code>jmap -heap 9</code>,but i find the heap usage of memory is only 1053MB.</p>
<p><a href=""https://i.stack.imgur.com/18PZK.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I guess the reason of this is the number of thread. I exec the <code>pstree -p | grep java | wc -l</code>. I find only 70 threads.</p>
<p><a href=""https://i.stack.imgur.com/3p4TQ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Finally, I exec the <code>jcmd 9 VM.native_memory scale=MB</code>. I see the Heap usage of memory is 4096MB. This confused me.
<a href=""https://i.stack.imgur.com/D1BoB.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I want to decrease the usage of memory in this app. But I don't know how to solve it.</p>
"
"52570093","What is the practical purpose of VOLUME in Dockerfile?","<docker><dockerfile>","63845709","What is the actual advantage of declaring a VOLUME in a Dockerfile?","<docker><dockerfile><docker-volume>","<p>First of all, I want to make it clear I've done due diligence in researching this topic. Very closely related is <a href=""https://stackoverflow.com/questions/34809646/what-is-the-purpose-of-volume-in-dockerfile"">this SO question</a>, which doesn't really address my confusion.</p>

<p>I understand that when <code>VOLUME</code> is specified in a Dockerfile, this instructs Docker to create an unnamed volume for the duration of the container which is mapped to the specified directory inside of it. For example:</p>

<pre><code># Dockerfile
VOLUME [""/foo""]
</code></pre>

<p>This would create a volume to contain any data stored in <code>/foo</code> inside the container. The volume (when viewed via <code>docker volume ls</code>) would show up as a random jumble of numbers.</p>

<p>Each time you do <code>docker run</code>, this volume is not reused. This is the key point causing confusion here. To me, the goal of a volume is to contain state persistent across <em>all instances</em> of an image (all containers started from it). So basically if I do this, <em>without explicit volume mappings</em>:</p>

<pre><code>#!/usr/bin/env bash
# Run container for the first time
docker run -t foo

# Kill the container and re-run it again. Note that the previous 
# volume would now contain data because services running in `foo`
# would have written data to that volume.
docker container stop foo
docker container rm foo

# Run container a second time
docker run -t foo
</code></pre>

<p>I expect the unnamed volume to be reused between the 2 <code>run</code> commands. However, this is not the case. Because I did not explicitly map a volume via the <code>-v</code> option, a new volume is created for each <code>run</code>.</p>

<p>Here's important part number 2: Since I'm required to explicitly specify <code>-v</code> to share persistent state between <code>run</code> commands, why would I ever specify <code>VOLUME</code> in my Dockerfile? Without <code>VOLUME</code>, I can do this (using the previous example):</p>

<pre><code>#!/usr/bin/env bash
# Create a volume for state persistence
docker volume create foo_data

# Run container for the first time
docker run -t -v foo_data:/foo foo

# Kill the container and re-run it again. Note that the previous 
# volume would now contain data because services running in `foo`
# would have written data to that volume.
docker container stop foo
docker container rm foo

# Run container a second time
docker run -t -v foo_data:/foo foo
</code></pre>

<p>Now, truly, the second container will have data mounted to <code>/foo</code> that was there from the previous instance. I can do this without <code>VOLUME</code> in my Dockerfile. From the command line, I can turn any directory inside the container into a mount to either a bound directory on the host or a volume in Docker.</p>

<p>So my question is: What is the point of <code>VOLUME</code> when you have to explicitly map named volumes to containers via commands on the host anyway? Either I'm missing something or this is just confusing and obfuscated.</p>

<p>Note that all of my assertions here are based on my observations of how docker behaves, as well as what I've gathered from the documentation.</p>
","<p>What is the unique value of using the <code>VOLUME</code> keyword in a Dockerfile given that it's possible to mount volumes to images that don't use it?</p>
"
"63801165","composer could not load package Invalid version string","<docker><symfony><composer-php>","63862580","Composer RuntimeException - Could not load package mews/purifier","<laravel><composer-php>","<p>I faced with range case, when I was try to install some bundle in locally I faced with erro, but when I try install the same bundle in test server everything installed without error. I use docker-compose and install bundle inside in image. docker-copose and other docker file with all dependecies absolutelty the same, all in git.</p>
<p><code>composer require league/flysystem-bundle</code></p>
<p>and in locally I faced with that</p>
<p>Using version dev-master for league/flysystem-bundle
./composer.json has been updated
Loading composer repositories with package information
Updating dependencies (including require-dev)
Restricting packages listed in &quot;symfony/symfony&quot; to &quot;5.0.*&quot;</p>
<p>Installation failed, reverting ./composer.json to its original content.</p>
<pre><code>  [RuntimeException]                                                           
  Could not load package ezsystems/ezplatform in http://repo.packagist.org: [  
  UnexpectedValueException] Could not parse version constraint dev-load-varni  
  sh-only-when-used as ^2.0@dev: Invalid version string &quot;^2.0@dev&quot;             
                                                                               

                                                                               
  [UnexpectedValueException]                                                   
  Could not parse version constraint dev-load-varnish-only-when-used as ^2.0@  
  dev: Invalid version string &quot;^2.0@dev&quot; 
</code></pre>
<p>locally composer version Composer version 1.10.11 2020-09-08 16:53:44</p>
<p>and test server</p>
<pre><code>/var/www/symfony # composer require league/flysystem-bundle
Using version dev-master for league/flysystem-bundle
./composer.json has been updated
Loading composer repositories with package information
Updating dependencies (including require-dev)
Restricting packages listed in &quot;symfony/symfony&quot; to &quot;5.0.*&quot;

Prefetching 3 packages 🎶 💨
  - Downloading (100%)

Package operations: 3 installs, 0 updates, 0 removals
  - Installing league/mime-type-detection (1.4.0): Loading from cache
  - Installing league/flysystem (1.x-dev 53f16fd): Loading from cache
  - Installing league/flysystem-bundle (dev-master 525845a): Loading from cache
Package easycorp/easy-log-handler is abandoned, you should avoid using it. No replacement was suggested.
Package zendframework/zend-code is abandoned, you should avoid using it. Use laminas/laminas-code instead.
Package zendframework/zend-eventmanager is abandoned, you should avoid using it. Use laminas/laminas-eventmanager instead.
Writing lock file
Generating autoload files
20 packages you are using are looking for funding.
Use the `composer fund` command to find out more!
Symfony operations: 1 recipe (c67222ac592a52b7dec1c2cd56763685)
  -  WARNING  league/flysystem-bundle (&gt;=1.0): From github.com/symfony/recipes-contrib:master
    The recipe for this package comes from the &quot;contrib&quot; repository, which is open to community contributions.
    Review the recipe at https://github.com/symfony/recipes-contrib/tree/master/league/flysystem-bundle/1.0

    Do you want to execute this recipe?
    [y] Yes
    [n] No
    [a] Yes for all packages, only for the current installation session
    [p] Yes permanently, never ask again for this project
    (defaults to n): 
ocramius/package-versions:  Generating version class...
ocramius/package-versions: ...done generating version class
Executing script cache:clear [OK]
Executing script assets:install public [OK]
</code></pre>
<p>test server composer version Composer version 1.10.10 2020-08-03 11:35:19</p>
<p>my dockerfile</p>
<pre><code>FROM alpine:edge

LABEL maintainer=&quot;Vincent Composieux &lt;vincent.composieux@gmail.com&gt;&quot;

RUN apk add --update --no-cache \
    coreutils \
    yarn \
    php7-fpm \
    php7-apcu \
    php7-ctype \
    php7-curl \
    php7-dom \
    php7-gd \
    php7-iconv \
    php7-imagick \
    php7-json \
    php7-intl \
    php7-mcrypt \
    php7-fileinfo\
    php7-mbstring \
    php7-opcache \
    php7-openssl \
    php7-pdo \
    php7-pdo_mysql \
    php7-mysqli \
    php7-pdo_pgsql \
    php7-pgsql \
    php7-xml \
    php7-zlib \
    php7-phar \
    php7-tokenizer \
    php7-session \
    php7-simplexml \
    php7-xdebug \
    php7-zip \
    php7-xmlwriter \
    make \
    curl \
    zlib-dev \
    libxml2-dev \
    rabbitmq-c-dev \
    oniguruma-dev \
    php7-pecl-amqp \
    php7-amqp \
    php7-redis

RUN apk add --no-cache --repository=http://dl-cdn.alpinelinux.org/alpine/edge/testing/ php7-pecl-mongodb

RUN echo &quot;$(curl -sS https://composer.github.io/installer.sig) -&quot; &gt; composer-setup.php.sig \
        &amp;&amp; curl -sS https://getcomposer.org/installer | tee composer-setup.php | sha384sum -c composer-setup.php.sig \
        &amp;&amp; php composer-setup.php &amp;&amp; rm composer-setup.php* \
        &amp;&amp; chmod +x composer.phar &amp;&amp; mv composer.phar /usr/bin/composer

COPY symfony.ini /etc/php7/conf.d/
COPY symfony.ini /etc/php7/cli/conf.d/
COPY xdebug.ini  /etc/php7/conf.d/

COPY symfony.pool.conf /etc/php7/php-fpm.d/

CMD [&quot;php-fpm7&quot;, &quot;-F&quot;]

WORKDIR /var/www/symfony
EXPOSE 9001

</code></pre>
<p>Why in the same time I faced with differnt version composer. Composer installed by the same way, by the same Dockerfile. How to fix this problem ?
I don't belive, how it's possible, this problem don't should be appear when using docker structure.. ?</p>
","<p>I have previously installed mews/purifier package and it was working fine. Now I want to update packages using <code>composer update</code>, this is where I get this error:</p>
<pre><code> [RuntimeException]                                                                             
  Could not load package mews/purifier in http://repo.packagist.org: [UnexpectedValueException]  
   Could not parse version constraint ~4.*: Invalid version string &quot;~4.*&quot;
</code></pre>
<p>I already checked the package list and it has this package <a href=""https://packagist.org/packages/mews/purifier"" rel=""noreferrer"">https://packagist.org/packages/mews/purifier</a></p>
<p>I tried to remove it from <code>composer.json</code> by removing this line:</p>
<pre><code>&quot;mews/purifier&quot;: &quot;^3.2&quot;,
</code></pre>
<p>But the error persists. I also removed associated lines in the <code>app.php</code> file.</p>
<pre><code>'providers' =&gt; [
Mews\Purifier\PurifierServiceProvider::class
],
'aliases' =&gt; [
'Purifier' =&gt; Mews\Purifier\Facades\Purifier::class
]

</code></pre>
<p>I really don't know what the problem is here. It all began when I wanted to install a package and I realized this issue.</p>
"
"53681522","Share variable in multi-stage Dockerfile: ARG before FROM not substituted","<docker><dockerfile><docker-multi-stage-build>","63957831","passing arguments to docker","<bash><docker><continuous-integration>","<p>I'm writing a multi-stage Dockerfile for the <a href=""https://www.mcs.anl.gov/research/projects/darshan/"" rel=""noreferrer"">darshan utils</a>:</p>

<pre><code>ARG DARSHAN_VER=3.1.6

FROM fedora:29 as build
RUN dnf install -y \
        gcc \
        make \
        bzip2 bzip2-devel zlib zlib-devel
RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz"" \
    &amp;&amp; tar ...


FROM fedora:29
COPY --from=build ""/usr/local/darshan-${DARSHAN_VER}"" ""/usr/local/darshan-${DARSHAN_VER}""
...
</code></pre>

<p>I build it with <code>docker build -t darshan-util:3.6.1 .</code> and the error I get is:</p>

<pre><code>Step 5/10 : RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...

 ---&gt; Running in 9943cce1669c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
...
curl: (78) RETR response: 550
The command '/bin/sh -c curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...' returned a non-zero code: 78
</code></pre>

<p>I'd like to reuse the same ARG in both stages, so that I can define a default build variable just once.
If I duplicate ARG in both stages, just below the two FROMs, it builds correctly.</p>

<p>What is the correct way to define a ""global"" multi-stage ARG variable with a default?</p>
","<p>I am trying to pass an argument to docker. For that, I use build-arg in my docker build command.</p>
<pre><code>echo &quot;[INFO] Hadoop version is ${HDP_VERSION}&quot;
docker build --no-cache=true --squash \
--build-arg HDP_VERSION=${HDP_VERSION} \
</code></pre>
<p>This is my docker</p>
<pre><code>ARG HDP_VERSION
FROM host:5000/runner-hadoop:${HDP_VERSION}
RUN echo &quot;HDP_VERSION=&quot;${HDP_VERSION}
COPY oozie/${HDP_VERSION} ${PATH_UNIX_PROJECT}
</code></pre>
<p>The first and second row execute without failing, but after that, I see that <code>HDP_VERSION</code> is actually empty. So at step 4, the wrong directory is taken.</p>
<p><a href=""https://i.stack.imgur.com/QZ4fb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QZ4fb.png"" alt=""enter image description here"" /></a></p>
<p>Why is that and how do you correct it?</p>
<p>EDIT</p>
<p>This is the result of echo</p>
<p><a href=""https://i.stack.imgur.com/HxSo5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HxSo5.png"" alt=""enter image description here"" /></a></p>
<p>This is what I get if I use</p>
<pre><code>--build-arg HDP_VERSION=1 \
</code></pre>
<p><a href=""https://i.stack.imgur.com/A4ATn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A4ATn.png"" alt=""enter image description here"" /></a></p>
<p>EDIT2</p>
<p>This is what happens if I use ENV. The result is the same.</p>
<pre><code>ENV HDP_VERSION=${HDP_VERSION}
RUN echo &quot;HDP_VERSION=&quot;${HDP_VERSION}
</code></pre>
<p><a href=""https://i.stack.imgur.com/Kpvwh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kpvwh.png"" alt=""enter image description here"" /></a></p>
"
"54121031","Multiple commands on docker ENTRYPOINT","<docker><dockerfile>","64006872","Docker: How can I execute 2 scripts one after the another using DockerFile","<docker><dockerfile>","<p>I'm trying to build a custom tcserver docker image. But I'm having some problems starting the webserver and the tomcat.<br/>
As far as I understand I should use ENTRYPOINT to run the commands I want.<br/>
The question is, is it possible to run multiple commands with ENTRYPOINT?<br/>
Or should I create a small bash script to start all?<br/><br/></p>

<p>Basically what I would like to do is:<br/></p>

<pre><code>ENTRYPOINT /opt/pivotal/webserver/instance1/bin/httpdctl start &amp;&amp; /opt/pivotal/webserver/instance2/bin/httpdctl start &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance1 -i /opt/pivotal/pivotal-tc-server-standard &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance2 -i /opt/pivotal/pivotal-tc-server-standard
</code></pre>

<p>But I don't know if that is a good practice or if that would even work.</p>
","<p>In my Dockerfile, I want to run 2 scripts.</p>
<p>I want once the first script ran successfully then only it will run another script because another script is consuming the output of the first script.</p>
<p>What changes do I need to make in Dockerfile to complete this task?</p>
<pre><code>FROM ubuntu

WORKDIR /usr/bin/

COPY ./example.py .

RUN chmod +x example.py

ENTRYPOINT [&quot;python3&quot;, &quot;example.py&quot;]

</code></pre>
<p>I want to include one move ENTRYPOINT to execute another script.</p>
"
"53375613","Why is the Java 11 base Docker image so large? (openjdk:11-jre-slim)","<java><docker><alpine><java-11>","64009638","Why image base of spring boot microservices changed from alpine to ubuntu","<spring-boot><docker><ubuntu><jhipster><alpine>","<p>Java 11 is announced to be the most recent LTS version. So, we're trying to start new services based on this Java version.</p>

<p>However, the base Docker image for Java 11 is much larger than the equivalent  for Java 8:</p>

<ul>
<li><p><a href=""https://github.com/docker-library/openjdk/blob/master/8/jre/alpine/Dockerfile"" rel=""noreferrer""><code>openjdk:8-jre-alpine</code></a>: 84 MB</p></li>
<li><p><a href=""https://github.com/docker-library/openjdk/blob/master/11/jre/slim/Dockerfile"" rel=""noreferrer""><code>openjdk:11-jre-slim</code></a>: <strong>283</strong> MB</p></li>
</ul>

<p>(I'm considering only the <a href=""https://hub.docker.com/_/openjdk/"" rel=""noreferrer""><strong><em>official OpenJDK</em></strong></a> and <strong><em>the most lightweight</em></strong> images for each Java version.)</p>

<p>Deeper digging uncovered the following ""things"":</p>

<ul>
<li><p>the <a href=""https://github.com/docker-library/openjdk/blob/master/11/jre/slim/Dockerfile"" rel=""noreferrer""><code>openjdk:11-jre-slim</code></a> image uses the base image <code>debian:sid-slim</code>. This brings 2 issues: </p>

<ul>
<li><p>this is 60 MB larger than <code>alpine:3.8</code></p></li>
<li><p>the <a href=""https://www.debian.org/releases/sid/"" rel=""noreferrer"">Debian <code>sid</code></a> versions are unstable</p></li>
</ul></li>
<li><p>the <code>openjdk-11-jre-headless</code> package installed in the image is <strong>3 times larger</strong> than <code>openjdk8-jre</code> (inside running Docker container):</p>

<ul>
<li><p><code>openjdk:8-jre-alpine</code>: </p>

<blockquote>
<pre><code>/ # du -hs /usr/lib/jvm/java-1.8-openjdk/jre/lib/
57.5M   /usr/lib/jvm/java-1.8-openjdk/jre/lib/
</code></pre>
</blockquote></li>
<li><p><code>openjdk:11-jre-slim</code>:</p>

<blockquote>
<pre><code># du -sh /usr/lib/jvm/java-11-openjdk-amd64/lib/
179M    /usr/lib/jvm/java-11-openjdk-amd64/lib/
</code></pre>
</blockquote>

<p>Going deeper I discovered the ""root"" of this heaviness - it's the <code>modules</code> file of the JDK:</p>

<blockquote>
<pre><code># ls -lhG /usr/lib/jvm/java-11-openjdk-amd64/lib/modules
135M    /usr/lib/jvm/java-11-openjdk-amd64/lib/modules
</code></pre>
</blockquote></li>
</ul></li>
</ul>

<p>So, now the questions which came:</p>

<ul>
<li><p>Why is <code>alpine</code> not used any more as a base image for Java 11 slim images?</p></li>
<li><p>Why is the unstable <em>sid</em> version used for LTS Java images?</p></li>
<li><p>Why is the slim/headless/JRE package for OpenJDK 11 so large compared to the similar OpenJDK 8 package? </p>

<ul>
<li>What is this <em>modules</em> file which brings 135 MB in OpenJDK 11?</li>
</ul></li>
</ul>

<p><strong>UPD</strong>: as a solutions for these challenges one could use this answer: <a href=""https://stackoverflow.com/questions/53669151/java-11-application-as-docker-image/53669152#53669152"">Java 11 application as docker image</a></p>
","<p>Why image base in containers of spring boot microservices changed from alpine to ubuntu</p>
<ul>
<li>In Jhipster 5.3.4: openjdk:8-jre-alpine</li>
<li>In Jhipster 6.10.1: ubuntu...</li>
</ul>
<p>It seems, the image ubuntu have more MB of older image base...</p>
<p>Thanks</p>
"
"62649255","How to increase/check default memory Docker has on linux?","<docker>","64073019","Increasing the memory allocation to docker daemon (dockerd) on Linux","<docker><docker-engine>","<p>I've seen that on Windows and Mac it's very easy to change the RAM containers are given - you just go into the GUI. But how do you do this on Linux, where it's a CLI instead of a GUI?</p>
<p>The Docker docs it mention an -m flag, but this flag doesn't give any response (just prints the entirety of the help output again) so I don't know whether it worked. It also seems specific to containers, whereas I'd like to change the global default.</p>
<p>Lastly, is there a way to check the current default RAM, so I can make sure whatever I do in the end actually worked?</p>
","<p>When using the Docker for macOS or Windows we can set the amount of memory that is allocated to the Docker daemon (by default it is 2GB). My question is there such a setting on Linux (Ubuntu 20.04.1) as well or is the docker daemon free to use as much memory as it wants on the Linux environment.</p>
<p>I was reading that we can control some configuration to dockerd using the /etc/docker/daemon.json file but got no example of memory allocation.</p>
"
"53211703","x509 certificate signed by unknown authority - go-pingdom","<http><docker><go><x509>","64099160","Running into x509: certificate signed by unknown authority when making soap wsdl call","<go><ssl-certificate><soap-client>","<p>I'm using the Go package <code>pingdom-go</code> to query Pingdom. The application is containerized as this:</p>

<pre><code>FROM alpine:3.8

USER nobody

ADD build/_output/bin/app /usr/local/bin/app
</code></pre>

<p>However I get the following error:</p>

<pre><code>Get https://api.pingdom.com/api/2.1/checks/0: x509: certificate signed by unknown authority
</code></pre>

<p>I've already tried what suggested here <a href=""https://stackoverflow.com/questions/45165813/x509-certificate-signed-by-unknown-authority#45165813"">x509 certificate signed by unknown authority</a> but without luck. Any ideas?</p>
","<p>I am attempting to make a soap call using gowsdl lib, the soap endpoint expects TLS connection. Saw there are multiple threads here that helped me to restructure however, now I am stuck with error and not sure what where i am going wrong.</p>
<p>Here's how I read and extract the cert package from the .pfx certificate file</p>
<pre><code>func loadCert() (interface{}, *x509.Certificate) {
b, err := ioutil.ReadFile(fileName)
if err != nil {
    log.Println(&quot;failed to find / open cert file&quot;, err)
}

p, c, err := pkcs12.Decode(b, password)
if err != nil {
    log.Fatalln(&quot;Failed to read cert content&quot;, err)
}

return p, c
</code></pre>
<p>}</p>
<p>And then I am calling this in the following function to make the soap request</p>
<pre><code>func soapAction() {

// Loading cert
_, c := a.loadCert()

// adding it to local pool
// if i use x509.SystemCertPool() I get error with system cert pool not being availabe on windows

caCertPool := x509.NewCertPool()
caCertPool.AddCert(c)

// adding cert pool to transport using http client
hClient := &amp;http.Client{
    Transport: &amp;http.Transport{
        TLSClientConfig: &amp;tls.Config{
            RootCAs:    caCertPool,
            ClientCAs:  caCertPool,
            ClientAuth: tls.RequireAndVerifyClientCert,
            InsecureSkipVerify: false,
        },
    },
}

// create soap client with options
client := soap.NewClient(pURL,
    soap.WithHTTPClient(hClient),
)
</code></pre>
<p>// removed the data structuring for SOAP request
}</p>
<p>Above, throws</p>
<blockquote>
<p>Post &quot;https://someURL.com&quot;: x509: certificate signed by unknown authority
panic: runtime error: invalid memory address or nil pointer dereference</p>
</blockquote>
<p>Would appreciate some hints where I could be going wrong. Thanks.</p>
"
"52848004","How to increase docker disk image size in Ubuntu","<docker><ubuntu-16.04>","64232127","Extending Docker Container Size on Amazon Linux 2","<amazon-web-services><docker><amazon-ec2>","<p>I am trying to increase the docker image size on ubuntu. When I do docker info I get following info</p>

<pre><code>Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 17.09.0-ce
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host macvlan null overlay
 Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0
runc version: 3f2f8b84a77f73d38244dd690525642a72156c64
init version: 949e6fa
Security Options:
 apparmor
 seccomp
  Profile: default
Kernel Version: 4.4.0-87-generic
Operating System: Ubuntu 16.04.3 LTS
OSType: linux
Architecture: x86_64
CPUs: 8
Total Memory: 15.67GiB
Name: no1010042033112.corp.adobe.com
ID: PYZE:KYTG:DXED:QI37:43ZM:56BB:TLM6:X2OJ:WDPA:35UP:Z4CU:DSNC
Docker Root Dir: /var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Experimental: false
Insecure Registries:
 127.0.0.0/8
Live Restore Enabled: false
</code></pre>

<p>As you can see that total memory is Total Memory: 15.67GiB. I couldn't find a way to do it on Ubuntu. I tried following ways</p>

<p>1) sudo dockerd --storage-opt dm.basesize=100G
2) Changing DOCKER_OPTS =""--storage-opt dm.basesize=50G"" in /etc/default/docker.</p>

<p>But none of these helped. This option is easily available in Docker config in Windows. But how to do it from a ubuntu terminal</p>
","<p>I have Docker installed on my Amazon Linux Instance. Below are the details of the docker install. I am trying to increase the default container size and am having errors in doing so. I have added on &quot;--storage-opt dm.basesize=100G&quot; to my /etc/sysconfig/docker-storage file, but am unable to get docker running after that statement. In my research I think it may have something to do with the storage driver that it is using: overlay2. Any advice on how I can increase would be helpful.</p>
<p><strong>Docker Info</strong>
<a href=""https://i.stack.imgur.com/DKJxP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DKJxP.png"" alt=""enter image description here"" /></a></p>
<p><strong>Docker Storage Options</strong></p>
<pre><code>DOCKER_STORAGE_OPTIONS=&quot;--storage-opt dm.basesize=100G&quot;
</code></pre>
<p><strong>Error when restarting Docker</strong></p>
<pre><code>    [root@ip-172-31-35-212 sysconfig]# systemctl status docker.service
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: failed (Result: start-limit) since Tue 2020-10-06 18:52:25 UTC; 2s ago
     Docs: https://docs.docker.com
  Process: 21254 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_ADD_RUNTIMES (code=exited, status=1/FAILURE)
</code></pre>
"
"54292282","Clarification of meaning new JVM memory parameters InitialRAMPercentage and MinRAMPercentage","<java><docker><jvm>","64297261","java -XX:+PrintFlagsFinal -version | grep RAMPercentage --> Should not MinRAMPercentage be less than MaxRAMPercentage?","<java><java-8><jvm><g1gc><concurrent-mark-sweep>","<p>Reference: <a href=""https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8186315"" rel=""noreferrer"">https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8186315</a></p>

<p>I'm really struggling to find out what MinRAMPercentage does, especially compared to InitialRAMPercentage.</p>

<p>I assumed that InitialRAMPercentage sets the amount of heap at startup, that MinRAMPercentage and MaxRAMPercentage set the bottom and top limit of heap that the JVM is allowed to shrink/grow to.</p>

<p>Apparently that is not the case. When I start a JVM (with UseContainerSupport, having these new memory setting parameters) like so:</p>

<pre><code>java -XX:+UseContainerSupport -XX:InitialRAMPercentage=40.0 -XX:MinRAMPercentage=20.0 -XX:MaxRAMPercentage=80.0 -XX:+PrintFlagsFinal -version | grep Heap
</code></pre>

<p>InitialHeap and MaxHeap get set, there is no ""Minimum Heap Size"" value that I can find; Consequently, that MinRAMPercentage never seems to get used.</p>

<p>Super confused, and apparently, I'm not the only one; the OpenJ9 dudes seem to also not fully parse the intent of these options, as I've gathered <a href=""https://github.com/eclipse/openj9/issues/1428"" rel=""noreferrer"">here</a> and <a href=""https://github.com/eclipse/openj9/pull/1699"" rel=""noreferrer"">here</a>. They seem to have opted to simply not implement MinRAMPercentage afaics.</p>

<p>So: What is the real intended usage and effect of setting MinRAMPercentage?</p>
","<pre><code>$ java -XX:+PrintFlagsFinal -version | grep RAMPercentage
   double InitialRAMPercentage                      = 1.562500                            {product}
   double MaxRAMPercentage                          = 25.000000                           {product}
   double MinRAMPercentage                          = 50.000000                           {product}
openjdk version &quot;1.8.0_265&quot;
OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~20.04-b01)
OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)
</code></pre>
<p>Unable to belive that <code>MinRAMPercentage=50</code> while <code>MaxRAMPercentage=25</code>??</p>
<p><strong>How is this possible??</strong> Could someone help me understand this better !!</p>
"
"62169568","Docker Alpine Linux python (missing)","<docker><alpine>","64301417","I want to build a docker environment, but I got ERROR: unsatisfiable constraints: python (missing):","<python><docker>","<p>I have a pipeline which deploys my container from GitLab. Last deployment was 5 days ago and went without any problems. Today I deploy it and get the following error:</p>
<pre><code>$ apk add --no-cache curl python py-pip
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
 ERROR: unsatisfiable constraints:
   python (missing):
     required by: world[python]
</code></pre>
<p>My job definition is:</p>
<pre><code>my-deploy:
  type: my-deploy
  image: docker:stable
  script:
    - apk update
    - apk add --no-cache curl python py-pip &lt;-- Here the erorr happens
    ...
</code></pre>
","<p>I am trying to run a model and it asks me to build a docker environment. And I think I get an error message about my python? I do not understand it very much.</p>
<p>I have tried the top answer in <a href=""https://stackoverflow.com/questions/62169568/docker-alpine-linux-python-missing"">Docker Alpine Linux python (missing)</a>, but I still receive the same error message. I wonder if it is the specific python package requirement the Dockerfile asks for? Do I need to install these packages one-by-one on the directory that the model is in?</p>
<pre><code>RUN apk update &amp;&amp; apk add gfortran \
    musl-dev bash python py-pip doxygen git graphviz
</code></pre>
<p>I have both Python and Gfortran installed.</p>
<pre><code>$ which python
/opt/anaconda3/bin/python

$ which gfortran
/usr/local/bin/gfortran

$ python
Python 3.7.6 (default, Jan  8 2020, 13:42:34) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
</code></pre>
<p><strong>This is what I have in the terminal</strong></p>
<pre><code>$ sudo docker build -t simstrat:alpine .
Sending build context to Docker daemon  13.31kB
Step 1/6 : FROM alpine:latest
 ---&gt; a24bb4013296
Step 2/6 : MAINTAINER SURF Team &quot;davide.vanzo@eawag.ch&quot;
 ---&gt; Using cache
 ---&gt; e84c1b2e9e15
Step 3/6 : RUN apk update &amp;&amp; apk add gfortran   musl-dev bash python py-pip doxygen git graphviz
 ---&gt; Running in 7f7ed9219d01
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
v3.12.0-376-gb3fc85f1ac [http://dl-cdn.alpinelinux.org/alpine/v3.12/main]
v3.12.0-381-g200ba6e281 [http://dl-cdn.alpinelinux.org/alpine/v3.12/community]
OK: 12750 distinct packages available
ERROR:   python (missing):
unsatisfiable constraints:
    required by: world[python]
The command '/bin/sh -c apk update &amp;&amp; apk add gfortran  musl-dev bash python py-pip doxygen git graphviz' returned a non-zero code: 1
</code></pre>
<p><strong>This is the Docker file I have:</strong></p>
<pre><code>FROM alpine:latest
MAINTAINER SURF Team &quot;davide.vanzo@eawag.ch&quot;

# get the tools we need
RUN apk update &amp;&amp; apk add gfortran \
    musl-dev bash python py-pip doxygen git graphviz

RUN pip install FoBiS.py ford pygooglechart

# root dir
RUN mkdir /home/Simstrat
WORKDIR /home/Simstrat

# calls that are needed to build and start the container with the build environment:
# docker build -t simstrat:alpine .
# docker create --name simstrat -it -v &lt;pathToLocalGitRepoDirectory&gt;:/home/Simstrat simstrat:alpine
# docker start simstrat
# docker exec -it simstrat bash
</code></pre>
"
"52314900","How to connect local Mongo database to docker","<mongodb><docker><go>","64307867","Docker container of flask app is possible connect to localhost MongoDB","<mongodb><docker><flask><connection>","<p>I am working on golang project, recently I read about docker and try to use docker with my app. I am using mongoDB for database.
Now problem is that, I am creating Dockerfile to install all packages and compile and run the go project. 
I am running mongo data as locally, if I am running go program without docker it gives me output, but if I am using docker for same project (just installing dependencies with this and running project), it compile successfully but not gives any output, having error::</p>

<pre><code>CreateSession: no reachable servers 
</code></pre>

<p>my Dockerfile::</p>

<pre><code># Start from a Debian image with the latest version of Go installed
# and a workspace (GOPATH) configured at /go.
FROM golang
WORKDIR $GOPATH/src/myapp

# Copy the local package files to the container's workspace.
ADD . /go/src/myapp

#Install dependencies
RUN go get ./...

# Build the installation command inside the container.
RUN go install myapp

# Run the outyet command by default when the container starts.
ENTRYPOINT /go/bin/myapp

# Document that the service listens on port 8080.
EXPOSE 8080
EXPOSE 27017
</code></pre>
","<p>I’m new to docker.
I have the one flask app was running on my docker container and the application needs to connect to MongoDB for the CRUD action.</p>
<p>but I have some connection problems between the docker and the localhost. The container cannot connect to my localhost MongoDB.</p>
<p>So is a possible flask app from docker container connect to the localhost MongoDB?</p>
<p>My Flask app MongoDB config setup:</p>
<pre><code>cilent = pymongo.MongoClient('127.0.0.1',27017)
</code></pre>
<p>My Dockerfile config:</p>
<pre><code>FROM ubuntu:latest

MAINTAINER Michael Levan

CMD tail -f /dev/null

RUN apt-get update -y &amp;&amp; apt-get install -y python3-pip python-dev

EXPOSE 8080
EXPOSE 5000

COPY ./requirements.txt /app/requirements.txt

WORKDIR /app

RUN pip3 install -r requirements.txt

COPY . /app

ENTRYPOINT [ &quot;python3&quot; ]
CMD [ &quot;app.py&quot; ]
</code></pre>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","64310899","unknown shorthand flag: 'a' in -aq","<docker>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I follow tutorial <a href=""https://phoenixnap.com/kb/remove-docker-images-containers-networks-volumes"" rel=""nofollow noreferrer"">https://phoenixnap.com/kb/remove-docker-images-containers-networks-volumes</a> . My error</p>
<p><a href=""https://i.stack.imgur.com/g6jam.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g6jam.png"" alt=""enter image description here"" /></a></p>
<pre><code>Microsoft Windows [Version 10.0.17134.1304]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Users\MyPC&gt;docker container stop $(docker container ls -aq)
unknown shorthand flag: 'a' in -aq)
See 'docker container stop --help'.
</code></pre>
<p>How to fix it?</p>
"
"64338203","how to do reverse proxy on docker","<docker><docker-compose><node-red><nginx-reverse-proxy><mosquitto>","64372533","edited how to make nginx pont to my mosquitto broker in docker","<docker><nginx><docker-compose><mqtt>","<p>I have a server and I am using Ubuntu 20.04, nginx , mosquitto and node-red and docker , let's call the website <code>http://mywebsite.com</code>. The problem that I am facing that I have created a client lets call it client1 in docker so the URL will be <code>http://mywebsite.com/client1</code>
and I want to establish an MQTT connection via mosquitto and I'm sending the data on topic <code>test</code></p>
<p>The problem that on node red node of MQTT when I write the IP address of my mosquitto container it works</p>
<p><img src=""https://i.stack.imgur.com/MU79C.png"" alt=""enter image description here"" /></p>
<p>But if I change the IP address 192.144.0.5 with mywebsite.com/client1 I can't connect to mosquitto and I can't send or receive any form of data</p>
<p><img src=""https://i.stack.imgur.com/QVbpS.png"" alt=""enter image description here"" /></p>
<p>any idea on how to solve this problem</p>
","<p>I have an web server with ubuntu 20.04 lets call it mywebsite.com that i've installed nginx and [2 nodered 2 mosquitto 2 grafana those are runing on docker, docker-compose called client1  that contain 1 grafana 1 mosquitto 1 nodered  ]
client_mosquitto ha a local ip address of 192.170.0.5 and use port 60007
here is my nginx.conf</p>
<pre><code>server {
     rewrite ^/client1/grafana(.*) $1 break;
            proxy_pass &quot;http://mywebsite.com:60007/grafana/&quot;;
    }

    location /client1/node-red/ {
            rewrite ^/node-red(.*) $1 break;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection &quot;upgrade&quot;;
            proxy_pass http://mywebsite.com:60008;
    }
   location /client1/mqtt/ {
            rewrite ^/node-red(.*) $1 break;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection &quot;upgrade&quot;;
            proxy_pass http://mywebsite.com:60009;
    }
   }
</code></pre>
<p>the problem that I am facing is that I can't access my mosquitto broker, so if i send data to my clien1_broker with &quot;mywebsite.com:60009&quot; i don't receive any thing</p>
"
"60438128","Swarm mode routing mesh not working, instead is working like host mode by default","<docker><docker-swarm><docker-swarm-mode>","64372556","Docker Swarm Overlay Network - Can't Ping Other Hosts","<docker><networking><docker-swarm><docker-networking>","<p><strong>Description</strong></p>

<p>Swarm mode routing mesh not working, instead, it is working like using host mode by default.</p>

<p>We were deploying a swarm of 3 masters nodes and 8 worker nodes, each of them in a different instance of a cloud service <strong>OpenStack</strong> using Terraform and Ansible. The swarm and routing mesh was working perfectly since it stopped working and started working like in a host mode. We didn't change anything nor done any update or deploy new services. We tried to restart the swarm and re-deploy the swarm and all services, but nothing worked, we couldn't make it work in routing mesh mode again. So, we decided to destroy all instances and start from scratch (the issue reported below). We did a clean installation of <strong>Ubuntu 18.04 LTS</strong> and docker as we did before. Then we set 1 master node and 2 workers (this time manually) and deploy one service, but the swarm is still working like in host mode.</p>

<p>The only way to access the services is by the IP address of the node where it is running, otherwise, there is no answer (time out). We tried to access using the IP of the manager or the other worker instances, but it is not possible to access to the service. That is why we supposed that the swarm is using host mode by default instead of the ingress network and routing mesh.</p>

<p>We also tried with different services like Mongo or Cassandra but the behaviour is the same, the swarm looks like working using host mode. You can only access the service by using the instance IP address where the service is running.</p>

<p>Any ideas to how to bypass the host most and go back to the routing mesh?
We want to access to any service only by using the IP address of the manager nodes which are supossed to be in Drain mode.</p>

<p><strong>Steps to reproduce the issue:</strong></p>

<ol>
<li>[<strong>manager</strong>] <code>sudo docker swarm init --advertise-addr 158.39.201.14</code></li>
<li>[<strong>worker-0</strong>] <code>sudo docker swarm join --token SWMTKN-1-3np0cy0msnfurecckl4863hkftykuqkgeq998s1hix6jsoiarq-758o52hyma
iyzv74w3u1yzltt 158.39.201.14:2377</code></li>
<li>[<strong>worker-1</strong>] <code>sudo docker swarm join --token SWMTKN-1-3np0cy0msnfurecckl4863hkftykuqkgeq998s1hix6jsoiarq-758o52hyma
iyzv74w3u1yzltt 158.39.201.14:2377</code></li>
<li>[<strong>manger</strong>] sudo docker stack deploy -c docker-compose.yml nh</li>
</ol>

<p><strong>Describe the results you received:</strong></p>

<p>curl <a href=""http://[worker-0-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-0-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p>curl <a href=""http://[worker-1-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-1-ip]:8089/bigdata</a> <strong>FAIL TIME OUT</strong></p>

<p><strong>Describe the results you expected:</strong></p>

<p>curl <a href=""http://[worker-0-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-0-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p>curl <a href=""http://[worker-1-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-1-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p><strong>Additional information you deem important (e.g. issue happens only occasionally):</strong></p>

<p>This issue was not happening 2 days ago and suddently it started happening. We didn't made any modification nor touch the servers.</p>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3.7'

networks:
  news-hunter:
    name: &amp;network news-hunter

x-network: &amp;network-base
  networks:
    - *network

services:
   blazegraph:
    &lt;&lt;: *network-base
    image: lyrasis/blazegraph:2.1.5
    ports:
      - published: 8089
        target: 8080
    deploy:
      placement:
        constraints:
          - node.role == worker 
</code></pre>

<p><strong>IP tables of manager, worker-1 and worker-2</strong> (all are the same): <code>sudo iptables -L</code></p>

<pre><code>Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy DROP)
target     prot opt source               destination
DOCKER-USER  all  --  anywhere             anywhere
DOCKER-INGRESS  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
DROP       all  --  anywhere             anywhere

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain DOCKER (2 references)
target     prot opt source               destination

Chain DOCKER-INGRESS (1 references)
target     prot opt source               destination
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:8089
ACCEPT     tcp  --  anywhere             anywhere             state RELATED,ESTABLISHED tcp spt:8089
RETURN     all  --  anywhere             anywhere

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
target     prot opt source               destination
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-ISOLATION-STAGE-2 (2 references)
target     prot opt source               destination
DROP       all  --  anywhere             anywhere
DROP       all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-USER (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
</code></pre>

<p><strong>Manager ports</strong>: <code>sudo netstat -tuplen</code></p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        46731      14980/systemd-resol
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          17752      865/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      0          17757      865/sshd
tcp6       0      0 :::8089                 :::*                    LISTEN      0          306971     24992/dockerd
tcp6       0      0 :::2377                 :::*                    LISTEN      0          301970     24992/dockerd
tcp6       0      0 :::7946                 :::*                    LISTEN      0          301986     24992/dockerd
udp        0      0 127.0.0.53:53           0.0.0.0:*                           101        46730      14980/systemd-resol
udp        0      0 158.39.201.14:68        0.0.0.0:*                           100        46591      14964/systemd-netwo
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          302125     -
udp6       0      0 fe80::f816:3eff:fef:546 :::*                                100        46586      14964/systemd-netwo
udp6       0      0 :::7946                 :::*                                0          301987     24992/dockerd
</code></pre>

<p><strong>Worker ports</strong>: <code>sudo netstat -tuplen</code></p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        44998      15283/systemd-resol
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          15724      1010/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      0          15726      1010/sshd
tcp6       0      0 :::8089                 :::*                    LISTEN      0          300227     25355/dockerd
tcp6       0      0 :::7946                 :::*                    LISTEN      0          283636     25355/dockerd
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          285465     -
udp        0      0 127.0.0.53:53           0.0.0.0:*                           101        44997      15283/systemd-resol
udp        0      0 158.39.201.15:68        0.0.0.0:*                           100        233705     15247/systemd-netwo
udp6       0      0 :::7946                 :::*                                0          283637     25355/dockerd
udp6       0      0 fe80::f816:3eff:fee:546 :::*                                100        48229      15247/systemd-netwo
</code></pre>

<p><strong>Services running</strong>: <code>sudo docker service ls</code></p>

<pre><code>ID                  NAME                MODE                REPLICAS            IMAGE                      PORTS
m7eha88ff4wm        nh_blazegraph       replicated          1/1                 lyrasis/blazegraph:2.1.5   *:8089-&gt;8080/tcp
</code></pre>

<p><strong>Stack</strong>: <code>sudo docker stack ps nh</code></p>

<pre><code>ID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS
tqkd9t4i03yt        nh_blazegraph.1     lyrasis/blazegraph:2.1.5   nh-worker-0         Running             Running 3 hours ago
</code></pre>

<p><strong>Output of <code>docker version</code>:</strong></p>

<pre><code>Client: Docker Engine - Community
 Version:           19.03.6
 API version:       1.40
 Go version:        go1.12.16
 Git commit:        369ce74a3c
 Built:             Thu Feb 13 01:27:49 2020
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.6
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.16
  Git commit:       369ce74a3c
  Built:            Thu Feb 13 01:26:21 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.2.10
  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:          1.0.0-rc8+dev
  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
</code></pre>

<p><strong>Output of <code>docker info</code>:</strong></p>

<pre><code>Client:
 Debug Mode: false

Server:
 Containers: 1
  Running: 0
  Paused: 0
  Stopped: 1
 Images: 1
 Server Version: 19.03.6
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Native Overlay Diff: true
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: active
  NodeID: hpcm67vxrmkm1wvlhfqbjevox
  Is Manager: true
  ClusterID: gnl96swlf7o3a976oarvjrazt
  Managers: 1
  Nodes: 3
  Default Address Pool: 10.0.0.0/8
  SubnetSize: 24
  Data Path Port: 4789
  Orchestration:
   Task History Retention Limit: 5
  Raft:
   Snapshot Interval: 10000
   Number of Old Snapshots to Retain: 0
   Heartbeat Tick: 1
   Election Tick: 10
  Dispatcher:
   Heartbeat Period: 5 seconds
  CA Configuration:
   Expiry Duration: 3 months
   Force Rotate: 0
  Autolock Managers: false
  Root Rotation In Progress: false
  Node Address: 158.39.201.14
  Manager Addresses:
   158.39.201.14:2377
 Runtimes: runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339
 runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657
 init version: fec3683
 Security Options:
  apparmor
  seccomp
   Profile: default
 Kernel Version: 4.15.0-74-generic
 Operating System: Ubuntu 18.04.4 LTS
 OSType: linux
 Architecture: x86_64
 CPUs: 1
 Total Memory: 3.852GiB
 Name: nh-manager-0
 ID: PHBO:E6UZ:RNJL:5LVU:OZXW:FM5M:GQVW:SCAQ:EEQW:7IIW:GARL:AUHI
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false
</code></pre>

<p><strong>Service inspect</strong>:<code>sudo docker service inspect --pretty nh_blazegraph</code></p>

<pre><code>ID:             ef9s5lesysovh5x2653qc6dk9
Name:           nh_blazegraph
Labels:
 com.docker.stack.image=lyrasis/blazegraph:2.1.5
 com.docker.stack.namespace=nh
Service Mode:   Replicated
 Replicas:      1
Placement:
 Constraints:   [node.role == worker]
UpdateConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:         lyrasis/blazegraph:2.1.5@sha256:e9fb46c9d7b2fc23202945a3d71b99ad8df2d7a18dcbcccc04cfc4f791b569e9
Resources:
Networks: news-hunter
Endpoint Mode:  vip
Ports:
 PublishedPort = 8089
  Protocol = tcp
  TargetPort = 8080
  PublishMode = ingress
</code></pre>

<p><strong>Additional environment details (AWS, VirtualBox, physical, etc.):</strong></p>

<p>We are working on a OpenStack IaaS cloud provider.
Out workload can expect more than 1000 http request per minute from external sources and more than 5000 requests between nodes.</p>

<p><em>Cross-posted:</em></p>

<p><a href=""https://forums.docker.com/t/swarm-mode-routing-mesh-not-working-instead-is-using-host-mode-by-default/89731"" rel=""nofollow noreferrer"">https://forums.docker.com/t/swarm-mode-routing-mesh-not-working-instead-is-using-host-mode-by-default/89731</a>
<a href=""https://github.com/moby/moby/issues/40590"" rel=""nofollow noreferrer"">https://github.com/moby/moby/issues/40590</a></p>
","<p>I have a docker swarm overlay network with 2 nodes, my localhost and a VM on my PC.</p>
<p><code>docker network list</code> output:</p>
<pre><code>NETWORK ID          NAME                   DRIVER              SCOPE
26ca272f0118        bridge                 bridge              local
48f94e0d47b8        docker_gwbridge        bridge              local
6ffc94e5474c        host                   host                local
o4ydpduwyfc9        ingress                overlay             swarm
3a177ebcdaae        none                   null                local
a3807498a383        source_default         bridge              local
igtysi5waw6s        test-overlay-network   overlay             swarm
</code></pre>
<p><code>docker node list</code> output:</p>
<pre><code>ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGE  STATUS      ENGINE VERSION
qex25m1yhjm28ojb18yrd3my3 *   node1              Ready               Active              Leader              19.03.13
64lsy02tvk2h6gqjdhejwvcvp     node2              Ready               Active              Reachable           19.03.13
</code></pre>
<p>Outputs are same for both hosts. I have 2 containers running on localhost and 2 containers running on the VM. What I am trying to do is to ping one of them from another.</p>
<p><code>netstat -tuplen</code> command shows me that following ports which are necessary for docker, are running for both hosts.</p>
<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name    
tcp6       0      0 :::2377                 :::*                    LISTEN      0          616456     -                   
tcp6       0      0 :::7946                 :::*                    LISTEN      0          612282     -                   
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          613172     -                   
udp6       0      0 :::7946                 :::*                                0          612283 
</code></pre>
<p>So I run a <code>ifconfig</code> in all the containers and get their ips, then try to ping each other. List of IPs on eth1 interface:</p>
<ul>
<li>10.0.3.102 -&gt; localhost</li>
<li>10.0.3.105 -&gt; localhost</li>
<li>10.0.3.107 -&gt; VM</li>
<li>10.0.3.110 -&gt; VM</li>
</ul>
<p>I can ping VM from the localhost, and also containers running on the same host. But I can not ping the containers from different hosts. What can be the reason?</p>
"
"52148404","Is there a Docker-ce version compatible for Ubuntu 16.04(i386 arch)? I find nothing on Web, only amd64, arm64 and other archs etc","<docker>","64428557","Docker and mysql problems?","<mysql><docker><mariadb>","<p>According to the official tutorial, it says ""Go to <a href=""https://download.docker.com/linux/ubuntu/dists/"" rel=""nofollow noreferrer"">https://download.docker.com/linux/ubuntu/dists/</a>, choose your Ubuntu version, browse to pool/stable/ and choose amd64, armhf, ppc64el, or s390x. Download the .deb file for the Docker version you want to install."". I did check that url, no deb for i386.</p>
","<p>I like to set up a LAMP on a docker.
I work on an i386 architecture.
I have some issues regarding the sql part.</p>
<p>I tried to do my own docker-compose.yml, and tried serveral other source but i encounter always the following problems :
When I do</p>
<blockquote>
<p>docker-compose up -d</p>
</blockquote>
<p>I always have an ERROR on the sql part :</p>
<ul>
<li><p>MARIADB<br />
Building database Step 1/1 : FROM mariadb:10.3<br />
10.3: Pulling from library/mariadb ERROR: Service 'database' failed to<br />
<strong>build: no matching manifest for unknown in the manifest list entries</strong></p>
</li>
<li><p>MYSQL<br />
8.0: Pulling from library/mysql<br />
<strong>ERROR: no matching manifest for unknown in the manifest list entries</strong></p>
</li>
</ul>
<p>I understand that there is no mariadb/sql on manifst...</p>
<p>Can you please tell me what goes wrong, or/and how can i setup a php-mysql-phpmyadmin on docker with an i386 under Debian?</p>
<p>EXEMPLE :<br />
The last source code i've tried is
<a href=""https://github.com/jcavat/docker-lamp"" rel=""nofollow noreferrer"">https://github.com/jcavat/docker-lamp</a></p>
<p>I also tried to use</p>
<pre><code>Example stack.yml for mariadb:

# Use root/example as user/password credentials
version: '3.1'

services:

  db:
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
</code></pre>
<p>then</p>
<pre><code>docker-compose -f stack.yml up
</code></pre>
<p>It result by the same error</p>
<pre><code>Pulling db (mariadb:)...
latest: Pulling from library/mariadb
ERROR: no matching manifest for unknown in the manifest list entries
</code></pre>
"
"62260186","pip install google-cloud-pubsub fails install in docker container","<python><docker><google-cloud-pubsub>","64461910","While running Dockerfile , python: can't open file 'setup.py': [Errno 2] No such file or directory","<python><docker><flask>","<p>I am trying to use a pupsub emulator. It starts but when I try to use my python script I get the following error</p>

<pre><code>ModuleNotFoundError: No module named 'google'
</code></pre>

<p>So i try to install the module.</p>

<pre><code>RUN pip install google-cloud-pubsub
</code></pre>

<p>error</p>

<pre><code>ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3.6 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-2hyoy1ly/grpcio/setup.py'""'""'; __file__='""'""'/tmp/pip-install-2hyoy1ly/grpcio/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-m25l52fe
         cwd: /tmp/pip-install-2hyoy1ly/grpcio/
    Complete output (11 lines):
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-2hyoy1ly/grpcio/setup.py"", line 191, in &lt;module&gt;
        if check_linker_need_libatomic():
      File ""/tmp/pip-install-2hyoy1ly/grpcio/setup.py"", line 152, in check_linker_need_libatomic
        stderr=PIPE)
      File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__
        restore_signals, start_new_session)
      File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'cc': 'cc'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
ERROR: Service 'praise-pubsub' failed to build: The command '/bin/sh -c pip install google-cloud-pubsub==0.24.0' returned a non-zero code: 1
</code></pre>

<p>full Dockerfile</p>

<pre><code>FROM google/cloud-sdk:alpine
RUN gcloud components install pubsub-emulator

FROM openjdk:jre-alpine


ENV PYTHONUNBUFFERED=1

RUN echo ""**** install Python ****"" &amp;&amp; \
    apk add --no-cache python3 &amp;&amp; \
    if [ ! -e /usr/bin/python ]; then ln -sf python3 /usr/bin/python ; fi &amp;&amp; \
    \
    echo ""**** install pip ****"" &amp;&amp; \
    python3 -m ensurepip &amp;&amp; \
    rm -r /usr/lib/python*/ensurepip &amp;&amp; \
    pip3 install --no-cache --upgrade pip setuptools wheel &amp;&amp; \
    if [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi

#RUN pip install google-cloud &lt;--- still fails when this is here
#RUN pip install Cython --install-option=""--no-cython-compile"" &lt;--- still fails
RUN pip install google-cloud-pubsub
COPY --from=0 /google-cloud-sdk/platform/pubsub-emulator /pubsub-emulator
</code></pre>
","<p>I am trying to build an image using Dockerfile using : <strong>sudo docker build . -f Dockerfile</strong></p>
<p>Dockerfile i am using :</p>
<pre><code>FROM python:3.6.8-alpine
WORKDIR /usr/app

COPY . ./

RUN pip install --upgrade pip
RUN pip install --upgrade setuptools
RUN pip install -r requirements.txt


RUN export GOOGLE_APPLICATION_CREDENTIALS=&quot;/app/static/*******.json&quot;
EXPOSE 5000
ENTRYPOINT [ &quot;python&quot; ]
CMD [ &quot;app.py&quot; ]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>Flask-RESTful==0.3.8
Flask==1.1.2
google-cloud-documentai==0.3.0
</code></pre>
<p><strong>Facing issues as:</strong></p>
<pre><code>  ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-v7prcez8/grpcio/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-v7prcez8/grpcio/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-x0mxi6mn
         cwd: /tmp/pip-install-v7prcez8/grpcio/
    Complete output (11 lines):
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-install-v7prcez8/grpcio/setup.py&quot;, line 212, in &lt;module&gt;
        if check_linker_need_libatomic():
      File &quot;/tmp/pip-install-v7prcez8/grpcio/setup.py&quot;, line 172, in check_linker_need_libatomic
        stderr=PIPE)
      File &quot;/usr/local/lib/python3.6/subprocess.py&quot;, line 729, in __init__
        restore_signals, start_new_session)
      File &quot;/usr/local/lib/python3.6/subprocess.py&quot;, line 1364, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'cc': 'cc'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 
</code></pre>
<p><strong>Apart from that :  python setup.py egg_info  giving me bellow log</strong></p>
<pre><code>python: can't open file 'setup.py': [Errno 2] No such file or directory
</code></pre>
<p>Any fix or clue will be helpful for me.</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","64490948","What non-heap memory allocations does the JVM makes?","<java><memory><jvm>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I know the JVM is famous for leaking memory outside of the maximum heap size, and I know of at least some of the places it leaks. But, we have an application that's using more memory than I think it should, and I worry that I've missed some of them.</p>
<p>The ones I know about:</p>
<ol>
<li><p>Heap (obviously): Stores the objects that user code allocates while running the process. Capped with <code>-xmx</code>, and shows up in jcmd's <code>VM.native_memory</code> tracking as &quot;Java Heap&quot;.</p>
</li>
<li><p>Metaspace: I gather this is the new instantiation of Permgen, and stores constants, loaded classes, and other misc. Capped with <code>-XX:MaxMetaspaceSize=XYZ</code>. I'm unsure exactly which of jcmd's <code>VM.native_memory</code> categories fall into this bucket?</p>
</li>
<li><p>Code Cache: The JIT caches its compiled code into this space. Capped with <code>-XX:ReservedCodeCacheSize=XYZ</code>. I believe this is a separate allocation budget from Metaspace. I'm also unsure exactly which of jcmd's <code>VM.native_memory</code> categories fall into this bucket?</p>
</li>
<li><p>Stack: Every running thread has a stack allocated. Capped with <code>-Xss</code> on a per-thread basis, but otherwise depends on the number of threads running. In jcmd's <code>VM.native_memory</code> tracking this shows up under &quot;Thread&quot;.</p>
</li>
</ol>
<p>I believe, then, that a cap on the total memory usage for a JVM process would be:</p>
<pre><code>(-mx) + (-XX:MaxMetaspaceSize) + (-XX:ReservedCodeCacheSize) + (num_threads * (-Xss))
</code></pre>
<p>Folks with more JVM tuning expertise than I have: (1) are my assumptions above correct, and (2) are there other places the JVM could leak memory? E.g., I/O buffers or the like?</p>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","64504751","What is wrong with the Docker command to remove all containers?","<docker><command-prompt><cmder>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I tried removing all the container that have stopped in Docker using</p>
<pre><code>docker rm $(docker ps -aq)
</code></pre>
<p>It gives an error saying <em>unknown shorthand flag: 'a' in -aq)</em></p>
<p>Using <code>docker rm Container_id</code> works correctly .</p>
<p>Is there anything else I should use?</p>
<p>I am using Docker version :</p>
<p>Client: Docker Engine - Community
Cloud integration  0.1.18
Version:           19.03.13
API version:       1.40
Go version:        go1.13.15
Git commit:        4484c46d9d
Built:             Wed Sep 16 17:00:27 2020
OS/Arch:           windows/amd64
Experimental:      false</p>
<p>Server: Docker Engine - Community
Engine:
Version:          19.03.13
API version:      1.40 (minimum version 1.12)
Go version:       go1.13.15
Git commit:       4484c46d9d
Built:            Wed Sep 16 17:07:04 2020
OS/Arch:          linux/amd64
Experimental:     false
containerd:
Version:          v1.3.7
GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175
runc:
Version:          1.0.0-rc10
GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
docker-init:
Version:          0.18.0
GitCommit:        fec3683</p>
<p>I use <strong>cmder</strong> mainly but the problem persists in <strong>windows command prompt</strong> too.</p>
<p>EDIT:
I tried most of the methods.</p>
<p>First -</p>
<pre><code>λ docker rm $(docker ps -aq)
unknown shorthand flag: 'a' in -aq)
See 'docker rm --help'.
</code></pre>
<p>Second -</p>
<pre><code>λ docker rm $(docker ps -a -q)
unknown shorthand flag: 'a' in -a
See 'docker rm --help'.
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","64512218","Docker and VS Code Remote Containers : How to connect remote Python dev container with Kafka container","<python><docker><visual-studio-code><apache-kafka><remote-access>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I use VS Code with the remote-containers extension.</p>
<p>I have 2 project folders for with each has it's own docker-compose file.</p>
<ol>
<li>zookeeper/kafka (listening to port 9092)</li>
<li>python dev container (running as a remote-container)</li>
</ol>
<p>In the python container I have a producer that sends messages to the kafka container. Unfortunately I cannot reach the kafka container from within the python container. I get the following error:<br />
<code>kafka.errors.NoBrokersAvailable: NoBrokersAvailable</code><br />
(If I run the producer directly from the host then it works like a charm!)</p>
<p><strong>How can I connect from the python remote-container to the kafka container?</strong></p>
<p>I tried port forwarding and publishing in the devcontainer.json without success:<br />
&quot;forwardPorts&quot;: [9092]<br />
&quot;appPort&quot;: [9092]
I even tried temporary port forwarding with
Forward a port &gt;&gt; 9092</p>
<p>I'm a docker beginner so I guess I have a misconception here, I would highly appreciate if someone could help me out.</p>
<p>Thanks!<br />
carlo</p>
<p><em><strong>devcontainer.json</strong></em></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;python&quot;,
  &quot;dockerComposeFile&quot;: [&quot;../docker-compose.yml&quot;],
  &quot;service&quot;: &quot;python&quot;,
  &quot;workspaceFolder&quot;: &quot;/workspace&quot;,
  &quot;settings&quot;: {
    &quot;terminal.integrated.shell.linux&quot;: &quot;/bin/bash&quot;,
    &quot;python.pythonPath&quot;: &quot;/usr/local/bin/python&quot;,
    &quot;python.linting.enabled&quot;: true,
    &quot;python.linting.pylintEnabled&quot;: true,
    &quot;python.linting.pylintPath&quot;: &quot;/usr/local/bin/pylint&quot;,
    &quot;python.formatting.autopep8Path&quot;: &quot;/usr/local/bin/autopep8&quot;,
    &quot;python.formatting.blackPath&quot;: &quot;/usr/local/bin/black&quot;
  },
  &quot;extensions&quot;: [
    &quot;ms-python.python&quot;,
    &quot;coenraads.bracket-pair-colorizer&quot;,
    &quot;wayou.vscode-todo-highlight&quot;
  ],
  // forward ports
  &quot;forwardPorts&quot;: [9092],
  // publish ports to the host
  &quot;appPort&quot;: [9092]
}
</code></pre>
<p><em><strong>producer.py</strong></em></p>
<pre class=""lang-py prettyprint-override""><code>from kafka import KafkaProducer
import json
from data import get_registered_user
import time

def json_serializer(data):
    return json.dumps(data).encode('utf-8')

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=json_serializer,
)

if __name__ == &quot;__main__&quot;:
    while True:
        registered_user = get_registered_user()
        producer.send('registered_user', registered_user)
        print(registered_user)
        time.sleep(3)
</code></pre>
"
"61234588","Docker: Multistage builds result in multiple images","<docker><multistage>","64524104","Docker multi-stage building saves intermediate image","<docker>","<p>Given this small example of a multistage build</p>

<pre><code>FROM node:10 AS ui-build
WORKDIR /usr/src/app

FROM node:10 AS server-build
WORKDIR /root/

EXPOSE 3070

ENTRYPOINT [""node""]
CMD [""index.js""]
</code></pre>

<p>why does this result in 3 images on my local file system?</p>

<pre><code>""&lt;none&gt;"";""&lt;none&gt;"";""58d63982fbef"";""2020-04-15 17:53:14"";""912MB""
""node"";""10"";""bd83fcefc19d"";""2020-04-14 01:32:21"";""912MB""
""test"";""latest"";""3913dd4d03b6"";""2020-04-15 17:53:15"";""912MB""
</code></pre>

<p>I expected two images, the base image and the server-build image. I used the standard docker build command, i.e. </p>

<pre><code>docker build -t test . 
</code></pre>

<p>so which of the parts of the image is none and which is test?</p>

<p>I am confused</p>
","<p>I've recently started to learn Docker and have been trying to learn how to use multi-stage build.
The documentation here: <a href=""https://docs.docker.com/develop/develop-images/multistage-build/"" rel=""nofollow noreferrer"">https://docs.docker.com/develop/develop-images/multistage-build/</a>
Says</p>
<blockquote>
<p>The end result is the same tiny production image as before, with a
significant reduction in complexity. You don’t need to create any
intermediate images and you don’t need to extract any artifacts to
your local system at all.</p>
</blockquote>
<p>There's also an example I don't think is necessary to copy here, however I've also started a small Spring Boot project with Gradle that I want to containerize. Here's my Dockerfile:</p>
<pre><code># using multi-stage to avoid manual ./gradlew build
FROM openjdk:11 as build

COPY gradle gradle
COPY build.gradle build.gradle
COPY gradlew gradlew
COPY gradlew.bat gradlew.bat
COPY settings.gradle settings.gradle
COPY src src

RUN ./gradlew build

# final image
FROM openjdk:11
WORKDIR /service
COPY --from=build /build/libs/*.jar /service/app.jar
ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/service/app.jar&quot;]
</code></pre>
<p>This is <code>docker images</code> command output before the build:</p>
<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
openjdk             11                  5c6e71a989bc        12 days ago         627MB
</code></pre>
<p>Build command: <code>docker build . -t sdemo:1.2.1</code><br/>
And the <code>docker images</code> after it's finished:</p>
<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
sdemo               1.2.1               24ea50770b2a        13 seconds ago      672MB
&lt;none&gt;              &lt;none&gt;              a1f8e74e1d81        20 seconds ago      960MB
openjdk             11                  5c6e71a989bc        12 days ago         627MB
</code></pre>
<p>As you can clearly see, I've got 2 images more instead of one, and the unnamed one is of ridiculous size of 960MB. What is causing this problem? Also, if it's relevant, I'm on a virtual box machine running Ubuntu.</p>
<p>The whole project is located under <a href=""https://github.com/vincent2704/demoSpringBootApp"" rel=""nofollow noreferrer"">https://github.com/vincent2704/demoSpringBootApp</a>, version 1.2.1 at the moment of posting this question.</p>
"
"53539807","Why docker in docker (dind) containers mount volumes with host path?","<docker><dind>","64605789","Docker inside Docker via -v /var/run/docker.sock:/var/run/docker.sock and volume inside the Docker","<docker><docker-compose>","<p>I have a setup with docker in docker and try to mount folders.</p>

<p>Let's say I have those folders that I wish to share with his parent. On the host, I created a file in /tmp/dind called <code>foo</code>. Host starts container 1, which starts container 2. This is the result I want to have.</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;     &lt;-------&gt;
</code></pre>

<p>Instead, I get</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;
      &lt;-----------------------&gt;
</code></pre>

<p>Code here:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind2:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>This outputs nothing, while the next command gives foo as result. I changed the mounted volume:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>The question is, what do I need to do in order to use Container 1 path and not host? Or do I misunderstand something about docker here?</p>
","<p>I have a docker container which image is based on <code>docker/compose</code> which I run with an option <code>-v /var/run/docker.sock:/var/run/docker.sock</code> on a Ubuntu 18.04 docker host. Inside that docker image I start <code>docker-compose up --abort-on-container-exit --build</code>. In my docker-compose.yml one of the services is specified this way</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3'
services:
  test:
    build: .
    depends_on:
      - hub
      - chrome
    volumes:
      - './TestResults:/tests/TestResults'      
    networks:
      - selenium-test 
</code></pre>
<p>What happens is that docker image runs the tests and test results from the run go into <code>/tests/TestResults/results.trx</code> file. When I was running <code>docker-compose up --abort-on-container-exit --build</code> on a Ubuntu 18.04 docker host all was working perfectly fine, after tests completed I have tests results in <code>TestResults/results.trx</code> on a host, because volume worked fine.
Now I need to run <code>docker-compose up --abort-on-container-exit --build</code> in Azure DevOps agent which is runnning inside the docker container on the same Ubuntu 18.04 docker host. All works fine, but I have a problem with volume which is in docker-compose specified:</p>
<pre class=""lang-yaml prettyprint-override""><code>    volumes:
      - './TestResults:/tests/TestResults'      
</code></pre>
<p>As I specify <code>-v /var/run/docker.sock:/var/run/docker.sock</code> when I run agent in a docker, so all docker containers which created inside of a agent container in fact are created on docker host of a host machine, but not as docker inside of a docker. The problem appears with volume. After <code>docker-compose up --abort-on-container-exit --build</code> finishes its work agent which runs in a docker wants to get test results from <code>./TestResults</code> folder, but for some reason nothing is in there as it seems it mounted host machine folder and I found the results on a host machine in a folder <code>/azp/agent/_work/1/s/TestResults</code>, while agent which runs in docker needs those result inside the agent container in the same path <code>/azp/agent/_work/1/s/TestResults</code>.</p>
<p>What do I do wrong and how do I fix that a container which I start inside of a container works with volumes correctly.</p>
"
"53276839","use docker's remote API in a secure manner","<docker><authentication><encryption><tls1.2>","64648259","How to add an authentication layer to Docker REST API?","<docker><rest><oauth-2.0><docker-compose><docker-machine>","<p>I am trying to find an effective way to use the docker remote API in a secure way.
I have a docker daemon running in a remote host, and a docker client on a different machine. I need my solution to not be client/server OS dependent, so that it would be relevant to any machine with a docker client/daemon etc. </p>

<p>So far, the only way I found to do such a thing is to create certs on a Linux machine with openssl and copy the certs to the client/server manually, as in this example:</p>

<p><a href=""https://docs.docker.com/engine/security/https/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/security/https/</a></p>

<p>and then configure docker on both sides to use the certificates for encryption and authentication.</p>

<p>This method is rather clunky in my opinion, because some times it's a problem to copy files and put them on each machine I want to use remote API from.</p>

<p>I am looking for something more elegant. </p>

<p>Another solution I've found is using a proxy for basic HTTP authentication, but in this method the traffic is not encrypted and it is not really secure that way. </p>

<p>Does anyone have a suggestion for a different solution or for a way to improve one of the above?</p>
","<p>I am trying to connect to docker daemon through a tcp port using REST API remotely. Currently anyone can access it through the endpoint. How can I make it secured with Oauth or ID/Password?</p>
"
"64644587","Containers not restarted after update with restart always in docker-compose.yml","<docker><docker-compose><coreos><flatcar-linux>","64686964","Can a docker container 'go to sleep'?","<docker><docker-compose><dockerfile>","<p>I have some containers which all of them have the always restart value in the docker-compose file like this:</p>
<pre><code>version: &quot;3.7&quot;
services:

  container:
    image: ghost:latest
    container_name: some_container
    restart: always
    depends_on:
       - ...
    ports:
       - ...
...
</code></pre>
<p>As soon as the OS (Flatcar Linux / CoreOS) has updated itself none of the containers restart. But if I just do <code>$ sudo docker ps</code> all of the containers starts at once. Whats up with that and how do I fix it so my containers automatically restarts after an update?</p>
<p>EDIT:</p>
<p>Not sure what is unclear about my question, <code>restart: always</code> is turned on. Unless I'm missing some vital thing in the documentation, this command should restart the container even if the docker daemon is restarted (after an os reboot).</p>
<p>Copy of one my comments from below:</p>
<blockquote>
<p>Ok, so help me out here. As you can see in my question, I have
restart: always turned on. All these containers are started
successfully and are running well. Then the OS updates itself
automatically and restarts itself. After this restart the docker
daemon is restarted. But for some reasons the containers I had running
WITH <code>RESTART: ALWAYS</code> turned on DOES NOT START. If I enter my server
at this moment, type <code>sudo docker ps</code> to list my running containers,
suddenly all containers are booted up and I see the list. So why
wasn't the containers started, even though the daemon is running?</p>
</blockquote>
","<p>I am running a server inside a docker container. I start it by using <code>docker-compose up -d</code> which works fine, i can access the Server. After some time however I receive a 503 - service unavailable error.</p>
<p>When I then perform any docker command e. g. <code>docker ps</code>  or <code>docker logs</code> the server runs perfectly again. <code>docker ps</code> shows that the container was created days ago, but  Status says <code>Up 1 second</code>. Is there any way for me to know what state my docker was in before? How does <code>docker ps</code> change the status?</p>
<p>docker-compose.yaml</p>
<pre><code>version: '3'
services:
  postgres:
    restart: always
    image: postgres:12.1
    environment:
      POSTGRES_PASSWORD: pass
      POSTGRES_USER: user
    networks:
      - mynetwork
    volumes:
      - /home/user/data/postgres:/var/lib/postgresql/data
    ports:
      - 5432:5432
  web:
    restart: always
    build: .
    environment:
      JPDA_ADDRESS: 8001
      JPDA_TRANSPORT: dt_socket
    networks:
      - mynetwork
    depends_on:
      - postgres
    ports:
      - 80:8080
      - 8001:8001
    volumes:
    - /home/user/data/images:/data/images
networks:
  mynetwork:
    driver: bridge
</code></pre>
<p>Dockerfile</p>
<pre><code>
FROM maven:3.6.3-jdk-8-slim as java-build-stage

WORKDIR /app/build
COPY . .

RUN mvn clean package -am -DskipTests -Dmaven.javadoc.skip=true -P deployment

# create the image to run the tomcat
FROM tomcat:9.0.30-jdk8-openjdk-slim as myname-build

ADD docker-resources/tomcat-users.xml /usr/local/tomcat/conf/tomcat-users.xml
ADD docker-resources/index.html /usr/local/tomcat/webapps/ROOT/index.jsp
COPY --from=java-build-stage /app/build/deployment/target/myname.war /usr/local/tomcat/webapps/myname.war

CMD [&quot;catalina.sh&quot;, &quot;jpda&quot; ,&quot;run&quot;]
</code></pre>
"
"63400807","Docker input file and save in output","<python><docker><dockerfile>","64698272","Ho to run python script inside docker and save results locally?","<python><docker>","<p>I built a docker image that inputs a local file, does some stuff to it, and returns an output file saved locally, but it does not work. How do I allow local, user input for files and then saving the output on the local machine?</p>
<p>My Dockerfile looks like this:</p>
<pre><code>FROM python:3
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
EXPOSE 5000
CMD [ &quot;python&quot;, &quot;process.py&quot; ]
</code></pre>
<p>Ideally, the terminal command would be something like this:</p>
<pre><code>docker run -p 5000:5000 [name of docker] [local path to input file] [local path to save output file]
</code></pre>
<p>When I run, I get this error:</p>
<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;../test.flac\&quot;: stat ../test.flac: no such file or directory&quot;: unknown.
</code></pre>
<p>How can I do this?</p>
","<p>I'm trying to run a python script but before my script I have to do <code>docker run -it --rm psycopg2 python</code>.
Inside my script I want to calculate some results and then store them locally in files.</p>
<p>I currenty do all the above by hand. First I run <code>docker run -it --rm psycopg2 python</code> and then I type one by one the python commands to the shell but I still cannot save anything to a file locally.</p>
<p>Any ideas?</p>
"
"63400807","Docker input file and save in output","<python><docker><dockerfile>","64698860","Get files output after Docker container running","<docker><dockerfile><containers>","<p>I built a docker image that inputs a local file, does some stuff to it, and returns an output file saved locally, but it does not work. How do I allow local, user input for files and then saving the output on the local machine?</p>
<p>My Dockerfile looks like this:</p>
<pre><code>FROM python:3
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
EXPOSE 5000
CMD [ &quot;python&quot;, &quot;process.py&quot; ]
</code></pre>
<p>Ideally, the terminal command would be something like this:</p>
<pre><code>docker run -p 5000:5000 [name of docker] [local path to input file] [local path to save output file]
</code></pre>
<p>When I run, I get this error:</p>
<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;../test.flac\&quot;: stat ../test.flac: no such file or directory&quot;: unknown.
</code></pre>
<p>How can I do this?</p>
","<p>I have a Python program that generates some output files.</p>
<p>I have successfully created an image using Dockerfile:</p>
<pre><code>FROM python:3.8.3
WORKDIR /main
COPY main.py /main
COPY /res /main/res/
RUN pip install --upgrade pip
RUN pip install elementpath
RUN pip install loguru
CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p>How can I find my output files after Docker container running?</p>
<p>I have tried <code>docker exec -t -i mycontainer /bin/bash</code>, but this command shows image's files, not generated by my Python script.</p>
"
"53010064","Pass environment variable into a Vue App at runtime","<docker><vue.js><kubernetes>","64709283","Is there a way for a built Vue.js app to read an environment variable defined in Kubernetes deployment?","<javascript><vue.js><kubernetes><vuejs2><kubernetes-deployment>","<p>How can I access environment variables in Vue, that are passed to the container at runtime and not during the build?</p>
<p>Stack is as follows:</p>
<ul>
<li>VueCLI 3.0.5</li>
<li>Docker</li>
<li>Kubernetes</li>
</ul>
<p>There are suggested solutions on stackoverflow and elsewhere to use .env file to pass variables (and using mode) but that's at build-time and gets baked into the docker image.</p>
<p>I would like to pass the variable into Vue at run-time as follows:</p>
<ul>
<li>Create Kubernetes ConfigMap (I get this right)</li>
<li>Pass ConfigMap value into K8s pod env variable when running deployment yaml file (I get this right)</li>
<li>Read from env variable created above eg. VUE_APP_MyURL and do something with that value in my Vue App (I DO NOT get this right)</li>
</ul>
<p>I've tried the following in helloworld.vue:</p>
<pre><code>&lt;template&gt;
&lt;div&gt;{{displayURL}}
  &lt;p&gt;Hello World&lt;/p&gt;
&lt;/div&gt;
&lt;/template&gt;
&lt;script&gt;
export default {  
    data() {
        return {
            displayURL: &quot;&quot;
        }
    },
    mounted() {
        console.log(&quot;check 1&quot;)
        this.displayURL=process.env.VUE_APP_ENV_MyURL
        console.log(process.env.VUE_APP_ENV_MyURL)
        console.log(&quot;check 3&quot;)
    }
}
&lt;/script&gt;
</code></pre>
<p>I get back &quot;undefined&quot; in the console log and nothing showing on the helloworld page.</p>
<p>I've also tried passing the value into a vue.config file and reading it from there. Same &quot;undefined&quot; result in console.log</p>
<pre><code>&lt;template&gt;
&lt;div&gt;{{displayURL}}
  &lt;p&gt;Hello World&lt;/p&gt;
&lt;/div&gt;
&lt;/template&gt;
&lt;script&gt;
const vueconfig = require('../../vue.config');
export default {  
    data() {
        return {
            displayURL: &quot;&quot;
        }
    },
    mounted() {
        console.log(&quot;check 1&quot;)
        this.displayURL=vueconfig.VUE_APP_MyURL
        console.log(vueconfig.VUE_APP_MyURL)
        console.log(&quot;check 3&quot;)
    }
}
&lt;/script&gt;
</code></pre>
<p>With vue.config looking like this:</p>
<pre><code>module.exports = {
    VUE_APP_MyURL: process.env.VUE_APP_ENV_MyURL
}
</code></pre>
<p>If I hardcode a value into VUE_APP_MyURL in the vue.config file it shows successfully on the helloworld page.</p>
<p>VUE_APP_ENV_MyURL is successfully populated with the correct value when I interrogate it: kubectl describe pod </p>
<p>process.env.VUE_APP_MyURL doesn't seem to successfully retrieve the value.</p>
<p>For what it is worth... I am able to use process.env.VUE_APP_3rdURL successfully to pass values into a Node.js app at runtime.</p>
","<p>So I have a project which builds 3 kubernetes pods connecting to each other, namely <code>server</code>, <code>manager</code> and <code>browser</code>, the browser part is implemented using <code>Vue.js</code>, others are using <code>Python Flask</code>.</p>
<p>I have a configMap storing the address of their services, which is supposed to be shared by the three:</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: env-config
data:
  SERVER_ADDRESS: http://ai-recommender-service:3518
  MANAGER_ADDRESS: http://manager-service:2354
</code></pre>
<p>with the <code>browser</code>, the deployment part is as follows:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: browser-deployment
  labels:
    app: browser
spec:
  replicas: 1
  selector:
    matchLabels:
      app: browser
  template:
    metadata:
      labels:
        app: browser
    spec:
      containers:
        - name: browser
          image: browser:latest
          imagePullPolicy: Never
          ports:
            - containerPort: 80
          env:
            - name: MANAGER_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: env-config
                  key: MANAGER_ADDRESS
</code></pre>
<p>In the Vue.js project <code>browser</code>, I have a <code>.js</code> file which tries to get the env variable through:</p>
<pre><code>const ServiceManagerUrl = process.env.MANAGER_ADDRESS;
</code></pre>
<p>But this doesn't work.</p>
<p>I know that there are other ways, such as creating a <code>.env</code> file with the variable with prefix <code>VUE_APP_</code> in the root folder, and then load it using something like <code>process.env.VUE_APP_MANAGER_ADDRESS</code>, but are there any ways for Vue.js to directly get the environment variables passed through the Kubernetes deployment (as these are shared by other parts of the system), without having to resort to creating extra files?</p>
<p>--- added ---</p>
<p><code>Dockerfile of browser</code></p>
<pre><code># build stage
FROM node:14-alpine as build-stage
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

# production stage
FROM nginx:stable-alpine as production-stage
COPY --from=build-stage /app/dist /usr/share/nginx/html
EXPOSE 80
CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]
</code></pre>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","64710185","java process consumes more memory over time but no memory leak","<java><memory-leaks><jvm>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>my java service is running on a 16 GB RAM host with -Xms and -Xmx set to 8GB.
The host is running a few other processes.</p>
<p>I noticed that my service consuming more memory over time.
I ran this command <code>ps aux  | awk '{print $6/1024 &quot; MB\t\t&quot; $11}'  | sort -n</code> on the host and recorded the memory usage by my java service.</p>
<p>When the service started, it used about 8GB memory (as -Xms and -Xmx set to 8GB) but after a week, it used about 9GB+ memory. It consumed about 100MB more memory per day.</p>
<p>I took a heap dump. I restarted my service and took another heap dump. I compared those two dumps but there were not much difference in the heap usage. The dump shows that the service used about 1.3GB before restart and used about 1.1 GB after restart.</p>
<p>From the process memory usage, my service is consuming more memory over time but that's not reported in the heap dump. How do I identify the increase in the memory usage in my service?</p>
<p>I set the -Xms and -Xmx to 8GB. Host has 16GB RAM. Do I set the min/max heap too high (50% of the total memory on the host)? would that cause any issues?</p>
"
"53509236","Mongo authentication inside Docker","<mongodb><docker><docker-compose>","64734574","Docker-compose caches my password for mongodb","<mongodb><docker><caching><docker-compose>","<p>I am trying to run the mongo docker image with authentication. Following the most simple example from the <a href=""https://docs.docker.com/samples/library/mongo/#start-a-mongo-server-instance"" rel=""noreferrer"">documentation</a> I ran the mongo and the mongo-express images by the <code>docker-compose up</code> command. My <code>docker-compose.yml</code> at this stage:</p>

<pre><code>version: '3.1'

services:

  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example

  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
</code></pre>

<p>This runs, both containers start ok and I can browse the contents of mongo from the mongo-express website. However, whenever I change the username or the password in the <code>docker-compose.yml</code> file, for example to this:</p>

<pre><code>version: '3.1'

services:

  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example123

  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example123
</code></pre>

<p>the mongo-express throws an unauthotrized error message:</p>

<pre><code>mongo-express_1  | Admin Database connected
mongo-express_1  | { MongoError: Authentication failed.
mongo-express_1  |     at Function.MongoError.create (/node_modules/mongodb-core/lib/error.js:31:11)
mongo-express_1  |     at /node_modules/mongodb-core/lib/connection/pool.js:483:72
mongo-express_1  |     at authenticateStragglers (/node_modules/mongodb-core/lib/connection/pool.js:429:16)
mongo-express_1  |     at Connection.messageHandler (/node_modules/mongodb-core/lib/connection/pool.js:463:5)
mongo-express_1  |     at Socket.&lt;anonymous&gt; (/node_modules/mongodb-core/lib/connection/connection.js:319:22)
mongo-express_1  |     at emitOne (events.js:116:13)
mongo-express_1  |     at Socket.emit (events.js:211:7)
mongo-express_1  |     at addChunk (_stream_readable.js:263:12)
mongo-express_1  |     at readableAddChunk (_stream_readable.js:250:11)
mongo-express_1  |     at Socket.Readable.push (_stream_readable.js:208:10)
mongo-express_1  |   name: 'MongoError',
mongo-express_1  |   message: 'Authentication failed.',
mongo-express_1  |   ok: 0,
mongo-express_1  |   errmsg: 'Authentication failed.',
mongo-express_1  |   code: 18,
mongo-express_1  |   codeName: 'AuthenticationFailed' }
mongo-express_1  | unable to list databases
mongo-express_1  | { MongoError: command listDatabases requires authentication
mongo-express_1  |     at Function.MongoError.create (/node_modules/mongodb-core/lib/error.js:31:11)
mongo-express_1  |     at /node_modules/mongodb-core/lib/connection/pool.js:483:72
mongo-express_1  |     at authenticateStragglers (/node_modules/mongodb-core/lib/connection/pool.js:429:16)
mongo-express_1  |     at Connection.messageHandler (/node_modules/mongodb-core/lib/connection/pool.js:463:5)
mongo-express_1  |     at Socket.&lt;anonymous&gt; (/node_modules/mongodb-core/lib/connection/connection.js:319:22)
mongo-express_1  |     at emitOne (events.js:116:13)
mongo-express_1  |     at Socket.emit (events.js:211:7)
mongo-express_1  |     at addChunk (_stream_readable.js:263:12)
mongo-express_1  |     at readableAddChunk (_stream_readable.js:250:11)
mongo-express_1  |     at Socket.Readable.push (_stream_readable.js:208:10)
mongo-express_1  |   name: 'MongoError',
mongo-express_1  |   message: 'command listDatabases requires authentication',
mongo-express_1  |   ok: 0,
mongo-express_1  |   errmsg: 'command listDatabases requires authentication',
mongo-express_1  |   code: 13,
mongo-express_1  |   codeName: 'Unauthorized' }
</code></pre>

<p>No matter what username or password I enter in <code>docker-compose.yml</code>, I cannot make mongo-express connect to mongo, only if I use the original <code>root</code> and <code>example</code> pair. </p>

<p>Note, that I am not getting the username and password as environment variables, but they are directly typed into the <code>docker-compose.yml</code> file as you can see here.</p>

<p>Also note, that when I change the <code>MONGO_INITDB_ROOT_USERNAME</code> and <code>MONGO_INITDB_ROOT_PASSWORD</code> (mongo's) variables to anything, they don't seem to have an effect, I can still connect with mongo-express using the original root and example credentials.</p>

<p>What causes this behaviour? How can I make this work? </p>
","<p>I'm trying to setup my mongodb and with docker. I created a docker-compose file</p>
<pre><code>services:
  mongo:
    image: mongo:latest
    container_name: mongo
    ports: [&quot;27017:27017&quot;]
    env_file: [&quot;.env&quot;]
    environment:
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    networks: [&quot;some_network&quot;]
    volumes: [./mongo-volume:/data/db]

volumes: 
  mongo-volume:

networks:
  some_network:
    driver: bridge
</code></pre>
<p>In my .env I did define my variables:</p>
<pre><code>MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=hMSZ7!dM!@HA&amp;EF6
</code></pre>
<p>Because I'd like to use MongoCompass and had some problems with the '@' I had to change the password, simply removed the '@'.</p>
<p>But I still can not got a connection. Instead I try to use mongo-client and used the old password, well it worked. But I'll like to change my password. I already used docker-compose down and remove the volume and the image. But it is still cached somewhere. Also I logged in to the container and echo the password, this will be the new one.</p>
<p>Additional information:
I use docker on windows with wsl (Ubuntu)
With this I watched through the directory /var/lib/docker/volumes/ what I found was nothing. I guess. on wsl the volumes are mount somewhere else...
So I changed the mount point, and my new password would be accepted.
But where are my volume mounted?</p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","64750349","JVM in pod high memory usage","<scala><kubernetes><jvm>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I am running a pod with alpine-jdk 8, code in scala, not something with high demand or processing. simple akka http server, mysql connection and redis connection. I have allocated to my pod 2GB memory limit and I saw it is taking almost all of it. other pods that also have mysql, http server and redis are taking around 800MiB
i took a dump using jattach and dumpheap, the size of the dump was 200MB. Analysing it using visualvm and Eclipse Memory Analyzer show the 200MB memory with no special things or leaks - took several dumps</p>
<p>however the Prometheus graph showing huge memory usage and also when i open the fc memory files it showed the same thing</p>
<p>what am I missing?</p>
"
"64733707","front-end Vue.js app in Kubernetes docker container cannot connect to back-end","<docker><vue.js><kubernetes><axios>","64778030","Docker: VueJS can not resolve service name backend","<docker><vue.js><docker-networking>","<p>I have built a front-end Vue.js application, running on a docker container under kubernetes environment. the backend is also in the same kubernetes cluster (I am using Minikube for the project). When running it gets error <code>net::ERR_NAME_NOT_RESOLVED</code> when connecting to back-end containers:
<a href=""https://i.stack.imgur.com/RlAsQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RlAsQ.png"" alt=""enter image description here"" /></a></p>
<p>while inside the container, there is no problem connect to the back-end using curl:</p>
<pre><code>$ kubectl exec -it deployment/hpl-browser-deployment -- sh
/ # curl http://hpl-manager-service:2354
{
  &quot;message&quot;: &quot;Manager status&quot;, 
  &quot;state&quot;: &quot;IDLE&quot;
}
</code></pre>
<p>I used <code>axios</code> for the api service:</p>
<pre><code>import axios from 'axios';

export default class APIService {
  API_URL = '';

  constructor(apiAddress) {
    this.API_URL = apiAddress;
  }

  async get() {
    console.log('ApiService: get()');
    try {
      const response = await axios.get(this.API_URL);
      console.log(`ApiService: get result: ${response.data}`);
      return response.data;
    } catch (error) {
      console.error(error);
      return error;
    }
  }

  async postPlainText(data) {
    console.log(`ApiService: post() - data: ${data}`);
    try {
      const response = await axios.post(this.API_URL,
        data,
        {
          headers: {
            'Content-Type': 'text/plain',
            Accept: '*/*',
          },
        });
      console.log(`ApiService: post result: ${response.data}`);
      return response.data;
    } catch (error) {
      console.error(error);
      return error;
    }
  }
}
</code></pre>
<p>The application has no problem running on development environment, when I port-forward the back-end service, and connect to <code>http://localhost:2354</code>.</p>
<p>I would like to know what may cause this problem?</p>
","<p>I created two container <code>boleto_api</code> and <code>boleto_web</code> the first is written in ExpressJS and the last is a VueJS application.</p>
<p>Created docker network named boleto and from that I started the containers.. If I ping <code>:ping boleto_api</code> inside container boleto_web I have successful</p>
<pre><code>PING boleto_api (172.21.0.2) 56(84) bytes of data.
64 bytes from boleto_api.boleto (172.21.0.2): icmp_seq=1 ttl=64 time=0.105 ms
64 bytes from boleto_api.boleto (172.21.0.2): icmp_seq=2 ttl=64 time=0.091 ms
</code></pre>
<p>But in Fetch API inside VueJS</p>
<pre><code>GET http://boleto_api:3030/pessoas/buscar/11532324723 net::ERR_NAME_NOT_RESOLVED
</code></pre>
<p>Part of code:</p>
<pre><code>fetch('http://boleto_api:3030/pessoas/buscar/'+this.$store.state.cpf)
             .then(response =&gt; response.json())
             .then(data =&gt; {
                this.dadosCliente = data
                this.show = false;
             });
</code></pre>
<p>I dont have sure if the frontend (VueJs) will be successful to do it, if not, how to proceeded ?</p>
"
"52787329","PHP extension unavailable to composer container in docker-compose","<php><docker><docker-compose><composer-php>","64783325","ext-gd missing in docker container with composer","<php><docker><composer-php><gd>","<p>I use docker-compose and have a number of containers in one project: Nginx, PHP, Composer and nginx. All works well except for one thing: composer does not work. I am trying to install a composer project that uses the GD extension, which is installed in PHP (confirmed using php -m inside the PHP container). However, the composer container does not ""see"" this extension and complaints it does not exist. How can I link those two?</p>

<p>docker-compose.yml:</p>

<pre><code>version: '2'
services:
    web:
        image: nginx:1.15.1
        volumes:
            - ./.docker/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf
            - ./html:/var/www/html
        ports:
            - 8888:80
        depends_on:
            - php
            - db
    php:
        build: .docker
        volumes:
            - ./.docker/conf/php/php.ini:/usr/local/etc/php/conf.d/php.ini
            - ./.docker/conf/php/xdebug.ini:/usr/local/etc/php/conf.d/xdebug.ini
            - ./html:/var/www/html
    composer:
        image: composer
        volumes:
            - ./html:/app
        command: install
        depends_on:
            - php
    db:
        image: postgres:10.4
        environment:
            - POSTGRES_DB=test
            - POSTGRES_USER=test
            - POSTGRES_PASSWORD=test
        ports:
            - 5432:5432
        volumes:
            - ./.docker/conf/postgres/:/docker-entrypoint-initdb.d/

    adminer:
        image: adminer
        ports:
        - 8080:8080
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM php:7.2-fpm

RUN apt-get update &amp;&amp; apt-get install -y \
        libfreetype6-dev \
        libjpeg62-turbo-dev \
        libmcrypt-dev \
        libpng-dev \
        libicu-dev \
        libpq-dev \
        libxpm-dev \
libvpx-dev \
    &amp;&amp; pecl install xdebug \
    &amp;&amp; docker-php-ext-enable xdebug \
    &amp;&amp; docker-php-ext-install -j$(nproc) gd \
    &amp;&amp; docker-php-ext-install -j$(nproc) intl \
    &amp;&amp; docker-php-ext-install -j$(nproc) zip \
    &amp;&amp; docker-php-ext-install -j$(nproc) pgsql \
    &amp;&amp; docker-php-ext-install -j$(nproc) pdo_pgsql \
    &amp;&amp; docker-php-ext-install -j$(nproc) exif \
    &amp;&amp; docker-php-ext-configure gd \
        --with-freetype-dir=/usr/include/ \
        --with-jpeg-dir=/usr/include/ \
        --with-xpm-dir=/usr/lib/x86_64-linux-gnu/ \
        --with-vpx-dir=/usr/lib/x86_64-linux-gnu/ \
</code></pre>

<p>Error:</p>

<pre><code>Starting test_app_db_1      ... done
Starting test_app_php_1     ... done
Starting test_app_adminer_1 ... done
Recreating test_app_composer_1 ... done
Starting test_app_web_1        ... done
Attaching to test_app_adminer_1, test_app_php_1, test_app_db_1, test_app_web_1, test_app_composer_1
php_1       | [12-Oct-2018 21:26:47] NOTICE: fpm is running, pid 1
php_1       | [12-Oct-2018 21:26:47] NOTICE: ready to handle connections
adminer_1   | PHP 7.2.1 Development Server started at Fri Oct 12 21:26:47 2018
db_1        | 2018-10-12 21:26:47.716 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
db_1        | 2018-10-12 21:26:47.716 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
db_1        | 2018-10-12 21:26:47.736 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
db_1        | 2018-10-12 21:26:47.827 UTC [21] LOG:  database system was shut down at 2018-10-12 21:11:42 UTC
db_1        | 2018-10-12 21:26:47.845 UTC [1] LOG:  database system is ready to accept connections
composer_1  | Loading composer repositories with package information
composer_1  | Updating dependencies (including require-dev)
composer_1  | Your requirements could not be resolved to an installable set of packages.
composer_1  | 
composer_1  |   Problem 1
composer_1  |     - gumlet/php-image-resize 1.9.1 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
composer_1  |     - gumlet/php-image-resize 1.9.0 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
composer_1  |     - Installation request for gumlet/php-image-resize 1.9.* -&gt; satisfiable by gumlet/php-image-resize[1.9.0, 1.9.1].
composer_1  | 
composer_1  |   To enable extensions, verify that they are enabled in your .ini files:
composer_1  |     - 
composer_1  |     - /usr/local/etc/php/conf.d/date_timezone.ini
composer_1  |     - /usr/local/etc/php/conf.d/docker-php-ext-sodium.ini
composer_1  |     - /usr/local/etc/php/conf.d/docker-php-ext-zip.ini
composer_1  |     - /usr/local/etc/php/conf.d/memory-limit.ini
composer_1  |   You can also run `php --ini` inside terminal to see which files are used by PHP in CLI mode.
test_app_composer_1 exited with code 2
</code></pre>
","<p>ive a docker container where i installed composer. My docker-composer.yml looks like this:</p>
<pre><code>webserver:
  build: ./docker/php
  ports:
    - 80:80
  links:
    - mysql
  volumes_from:
    - app

composer:
    image: composer:2.0
    volumes_from:
      - webserver
    working_dir: /var/www/html

mysql:
    image: mysql:5.7
    environment:
      MYSQL_DATABASE: &quot;${DB_NAME}&quot;
      MYSQL_USER: &quot;${DB_USER}&quot;
      MYSQL_ROOT_PASSWORD: &quot;${DB_ROOT_PW}&quot;
      MYSQL_PASSWORD: &quot;${DB_PW}&quot;
    volumes_from:
      - data

data:
  image: mysql:5.7.31
  volumes:
    - /var/lib/mysql
  command: &quot;true&quot;
    
phpmyadmin:
  image: phpmyadmin/phpmyadmin
  ports:
    - 8080:80
  links:
    - mysql
  environment:
    PMA_HOST: mysql
  volumes:
    - ./docker/php/uploads.ini:/usr/local/etc/php/conf.d/uploads.ini

app:
  image: tianon/true
  volumes:
    - ./src/oxid:/var/www/html:delegated
</code></pre>
<p>My dockerfile:</p>
<pre><code>FROM php:7.4.8-apache

COPY php.ini /usr/local/etc/php/
COPY 000-default.conf /etc/apache2/sites-available/
COPY ./ssl/ssl.crt /etc/apache2/ssl/ssl.crt
COPY ./ssl/ssl.key /etc/apache2/ssl/ssl.key

RUN a2enmod rewrite
RUN a2enmod expires
RUN a2enmod headers


RUN apt-get update
RUN apt-get install -y zlib1g-dev libxml2-dev libfreetype6-dev libjpeg62-turbo-dev libmcrypt-dev libpng12.0 imagemagick
RUN docker-php-ext-install mysqli soap pdo_mysql bcmath pdo
RUN docker-php-ext-configure gd --with-freetype --with-jpeg
RUN docker-php-ext-install gd
RUN docker-php-ext-configure intl
RUN docker-php-ext-install intl

# Install opcache
RUN docker-php-ext-install opcache

RUN a2enmod ssl
EXPOSE 80
EXPOSE 443

# Install APCu
RUN pecl install apcu
RUN echo &quot;extension=apcu.so&quot; &gt; /usr/local/etc/php/conf.d/apcu.ini
</code></pre>
<p>Now when i open the <code>index.php</code> with <code>phpinfo();</code> it says, that GD is installed and enabled.</p>
<p>But <code>docker-compose run composer install</code> gives me this error and i'm not really understanding why.</p>
<blockquote>
<ul>
<li>shopware/core[6.3.0.0, ..., 6.3.3.1] require ext-gd * -&gt; it is missing from your system. Install or enable PHP's gd extension.</li>
</ul>
</blockquote>
<p>Can someone please tell me how i can enable GD for this composer process.</p>
<p>Thank you!</p>
"
"58879058","Remove containers by image name","<bash><docker><unix>","64797449","docker remove containers by image","<docker>","<p>I mistakenly created a bunch of containers which I now want to remove. I can list them with:</p>

<pre><code>docker container ls -aq -f ""ancestor=portainer/portainer""
</code></pre>

<p>How can I ""pipe"" these container IDs to <code>docker container rm</code>?</p>

<p>What doesn't work:</p>

<ul>
<li><code>docker ls -aq -f ""ancestor=portainer/portainer"" | docker container rm</code></li>
<li><code>docker container rm $(docker ls -aq -f ""ancestor=portainer/portainer"")</code></li>
<li><code>docker rm `docker ls -aq -f ""ancestor=portainer/portainer""`</code></li>
</ul>
","<p>I need the smallest code possible to remove all containers that were run from a specific image. For example, to run the container:</p>
<pre><code>docker run -p 8888:80 fakeuser/my-image-name
</code></pre>
<p>And to remove the container (or containers):</p>
<pre><code>docker rm -f &quot;all containers that were run from the image fakeuser/my-image-name&quot;
</code></pre>
<p>Is this possible? If so, whats the syntax? A one liner would be great if possible.</p>
<p>Thanks in advance</p>
"
"59017082","How to use ""dotnet watch run"" with .Net Core 3, Visual Studio 2019 and docker","<c#><visual-studio><entity-framework><docker><asp.net-core>","64806616","How to tell dotnet watch to run a different build configuration?","<docker-compose><dotnet-watch>","<p>I'm playing with docker and .NET Core 3 using Visual Studio 2019. I containerize my application by adding the Dockerfile to my project (right click on the project -> Add -> Docker Support) and I was able to launch it, but now I want to use <code>dotnet watch run</code> inside the container.</p>

<p>This is the generated Dockerfile:</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/aspnet:3.0-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/core/sdk:3.0-buster AS build
WORKDIR /src
COPY [""DockerTestApp/DockerTestApp.csproj"", ""DockerTestApp/""]
RUN dotnet restore ""DockerTestApp/DockerTestApp.csproj""
COPY . .
WORKDIR ""/src/DockerTestApp""
RUN dotnet build ""DockerTestApp.csproj"" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish ""DockerTestApp.csproj"" -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [""dotnet"", ""DockerTestApp.dll""]

</code></pre>

<p>and I modified it like so:</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:3.0-buster AS build
ENV DOTNET_USE_POLLING_FILE_WATCHER 1
WORKDIR /src
EXPOSE 80
EXPOSE 443
COPY [""DockerTestApp/DockerTestApp.csproj"", ""DockerTestApp/""]
RUN dotnet restore ""DockerTestApp/DockerTestApp.csproj""
ENTRYPOINT [""dotnet"", ""watch"", ""run""] 
</code></pre>

<p>The container started with <code>dotnet watch run</code> but any file change isn't detected and the rebuild is not triggered.</p>

<p>Should I have to mount a volume from my code directory to the container in order to make it work?</p>

<p>Thanks.</p>

<p><strong>UPDATE</strong></p>

<p>with this Dokerfile</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:3.0

ENV DOTNET_USE_POLLING_FILE_WATCHER 1

WORKDIR /app
COPY . .
ENTRYPOINT dotnet watch run  --urls=https://+:5001 --project DocketTestApp.csproj

</code></pre>

<p>and this docker-compose.yml</p>

<pre><code>version: '3.4'

services:
  dotnet-watch-docker-example:
    container_name: dotnet_watch_docker_example
    image: giuseppeterrasi/dotnet-watch-docker-example
    build:
      context: ./DocketTestApp/
    ports:
      - 5001:5001
    volumes:
      - './DocketTestApp/:/app/'
    depends_on: 
      - db
  db:
    image: mysql
    restart: always
    ports:
        - ""3306:3306""
    environment:
      MYSQL_ROOT_PASSWORD: testPassword
      MYSQL_DATABASE: testDB
      MYSQL_USER: testUser
      MYSQL_PASSWORD: test
</code></pre>

<p>It works, but if I add a DbContext, Visual Studio misses the Entity Framework reference if the container is started. If I stop the container and reload Visual Studio everything is ok.</p>

<p>Why?</p>
","<p>Today I have a container running <code>dotnet watch -v --project &quot;.\\MyProject&quot; run --no-launch-profile</code> and the &quot;MyProject&quot; folder is mapped transparently to a .NET Core project of my solution. So whenever I alter a file in my Visual Studio it's picked up immediately inside the container and recompiled to allow quick alteration and testing. However I've noticed that this causes conflict issues when I build on my host machine causing a conflict over the <code>bin\Debug\netcoreapp3.1\MyProject.exe</code></p>
<p>To solve this situation I wanted to see if I can pass an argument/parameter/configuration/... anything to <code>dotnet watch</code> that would tell it to use a different build configuration (copy of Debug really but called Container so that it'd create a separate folder and not conflict with the host Visual Studio)</p>
<p>Sadly I have not been able to locate an example or documentation on how to do this with <code>dotnet watch</code>. Is this possible? Is there a better solution?</p>
"
"60183313","""The connection was reset"" in localhost:8000 using django and docker","<django><docker><docker-compose>","64841205","Django not running on localhost when using with Docker","<javascript><python><django><docker><web>","<p>After running <code>docker-compose up</code>,</p>
<blockquote>
<p>Starting docker_django ... done</p>
<p>Attaching to docker_django</p>
<p>docker_django | Watching for file changes with StatReloader</p>
<p>docker_django | Performing system checks...</p>
<p>docker_django | System check identified no issues (0 silenced).</p>
<p>docker_django | February 12, 2020 - 07:26:35</p>
<p>docker_django | Django version 3.0.3, using settings 'backend.settings'</p>
<p>docker_django | Starting development server at <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a></p>
<p>docker_django | Quit the server with CONTROL-C.</p>
</blockquote>
<p>when i connect to <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a> it shows this
<a href=""https://i.stack.imgur.com/7Jidk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Jidk.png"" alt=""enter image description here"" /></a>
There is no problem / error running the docker-compose command but only when visiting the site.</p>
<p>Im using ubuntu 19.10 and the project has django v3.</p>
<p>Dockerfile</p>
<pre><code>FROM python:3

ENV PYTHONUNBUFFERED 1

RUN mkdir /code
WORKDIR /code

COPY . /code/

RUN pip install -r requirements.txt

</code></pre>
<p>docker-compose.yml</p>
<pre><code>version: '3'

services:
    web:
        build: .
        container_name: docker_django
        command: python manage.py runserver
        volumes:
            - .:/code
        ports:
            - &quot;8000:8000&quot;
</code></pre>
","<p>I am a beginner to web development with Django. I have been trying to use docker-compose as a part of the lectures but due to some issues, I am not able to run the server.</p>
<p>This is my docker-compose.yml file:</p>
<pre><code>version: '3'



services:

db:

image: postgres

environment:

- POSTGRES_DB=postgres

- POSTGRES_USER=postgres

- POSTGRES_PASSWORD=1029384756Vithayathil


ports:

- &quot;5432:5432&quot;

migration:

build: .

command: python3 manage.py migrate

volumes:

- .:/usr/src/app

depends_on:

- db

web:

build: .

command: python3 manage.py runserver

volumes:

- .:/usr/src/app

ports:

- &quot;8000:8000&quot;

depends_on:

- db

- migration
</code></pre>
<p>This is my DockerFile:</p>
<pre><code>FROM python:3

WORKDIR /usr/src/app

ADD requirements.txt /usr/src/app

RUN pip install -r requirements.txt

ADD . /usr/src/app
</code></pre>
<p>This is my database in settings.py:</p>
<pre><code>DATABASES = {

'default': {

'ENGINE': 'django.db.backends.postgresql',

'NAME': 'postgres',

'USER': 'postgres',

'HOST': 'db',

'PASSWORD':'**********',

'PORT': 5432,

}

}
</code></pre>
<p>On doing docker-compose up command, these things happen:</p>
<pre><code>Starting airline4_db_1 ... done Starting airline4_migration_1 ... done Starting airline4_web_1 ... done Attaching to airline4_db_1, airline4_migration_1, airline4_web_1

db_1 |

db_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization

db_1 |

db_1 | 2020-11-07 15:37:00.770 UTC [1] LOG: starting PostgreSQL 13.0 (Debian 13.0-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit

db_1 | 2020-11-07 15:37:00.789 UTC [1] LOG: listening on IPv4 address &quot;0.0.0.0&quot;, port 5432

db_1 | 2020-11-07 15:37:00.790 UTC [1] LOG: listening on IPv6 address &quot;::&quot;, port 5432

db_1 | 2020-11-07 15:37:01.179 UTC [1] LOG: listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;

db_1 | 2020-11-07 15:37:01.647 UTC [26] LOG: database system was shut down at 2020-11-07 10:55:06 UTC

db_1 | 2020-11-07 15:37:02.449 UTC [1] LOG: database system is ready to accept connections

web_1 | Watching for file changes with StatReloader

migration_1 | Operations to perform:

migration_1 | Apply all migrations: admin, auth, contenttypes, flights, sessions

migration_1 | Running migrations:

migration_1 | No migrations to apply.

airline4_migration_1 exited with code 0
</code></pre>
<p>But when I try going to 127.0.0.1:8000 on my browser, I see a message, 'This page isn't working'.</p>
<p>Any help will be great! Thanks in advance</p>
<pre><code>When I tried 'docker-compose ps' command, it says like this

    Name Command State Ports

---------------------------------------------------------------------

airline4_db_1 docker-entrypoint.sh postgres Exit 0

airline4_migration_1 python3 manage.py migrate Exit 0

airline4_web_1 python3 manage.py runserver Exit 0

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
88d8655af03a        airline4_web        &quot;python3 manage.py r…&quot;   24 seconds ago      Up 18 seconds       0.0.0.0:8000-&gt;8000/tcp   airline4_web_1
bec29de59ee1        postgres            &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 32 seconds       5432/tcp                 airline4_db_1
</code></pre>
<p>This is what I get now on docker ps command</p>
"
"59877561","selenium.common.exceptions.InvalidCookieDomainException: Message: invalid cookie domain while executing tests in Django with Selenium","<django><selenium><docker><docker-compose><selenium-grid>","64843228","Selenium Error Load Cookies may only be set for the current domain","<python><selenium><web-scraping>","<p>I am setting up tests for both chrome and firefox using seleniumgrid.I am using docker images selenium-hub and selenium node-chrome and node-firefox as below.</p>

<pre><code>  app:
    build: .
    command: gunicorn --reload --capture-output --log-level debug --access-logfile - -w 3 -b 0.0.0.0 app.wsgi
    restart: always
    volumes_from:
        - initialize
    ports:
      - ""8000:8000""
    links:
      - db
      - rabbitmq
      - selenium_hub
    env_file: secrets.env
    volumes:
        - ./app/:/code/

  selenium_hub:
    image: selenium/hub
    ports:
      - 4444:4444
    expose:
      - 4444
    tty: true
    environment:
      - GRID_MAX_SESSION=20
      - GRID_NEW_SESSION_WAIT_TIMEOUT=60000
      - GRID_BROWSER_TIMEOUT=300
      - GRID_TIMEOUT=300
      - TIMEOUT=300
  node_1:
    image: selenium/node-chrome
    depends_on:
      - selenium_hub
    environment:
      - HUB_HOST=selenium_hub
      - HUB_PORT=4444
      - NODE_MAX_SESSION=3
      - NODE_MAX_INSTANCES=2
    shm_size: 2GB
  node_2:
    image: selenium/node-firefox
    environment:
      - HUB_HOST=selenium_hub
      - HUB_PORT=4444
      - NODE_MAX_SESSION=3
      - NODE_MAX_INSTANCES=2
    shm_size: 2GB
    depends_on:
      - selenium_hub
</code></pre>

<p>When I try to run the tests I am always running into this error <code>InvalidCookieDomainException: Message: invalid cookie domain</code>. I have already set domain to <code>self.live_server_url</code>.
below is the full traceback with the test setup.</p>

<pre><code>  Traceback (most recent call last):
    File ""/code/frontend/tests/test_user_auth.py"", line 75, in setUp
        ""port"": ""8082"",
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py"", line 894, in add_cookie
        self.execute(Command.ADD_COOKIE, {'cookie': cookie_dict})
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
        self.error_handler.check_response(response)
    File ""/usr/local/lib/python3.5/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
        raise exception_class(message, screen, stacktrace)
    selenium.common.exceptions.InvalidCookieDomainException: Message: invalid cookie domain
    (Session info: chrome=77.0.3865.75)
</code></pre>

<p>Test <a href=""https://elennion.wordpress.com/2018/11/23/django-automated-testing-with-selenium/"" rel=""noreferrer"">reference tutorial</a>.</p>

<pre><code>class TestUserCreate(StaticLiveServerTestCase):
    fixtures = [""test.json""]
    port = 8082

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        caps = {
            ""browserName"": os.getenv(""BROWSER"", ""chrome""),
            ""javascriptEnabled"": True,
         }
        cls.driver = webdriver.Remote(
            command_executor=""http://selenium_hub:4444/wd/hub"",
            desired_capabilities=caps,
         )
        cls.driver.implicitly_wait(10)

    @classmethod
    def tearDownClass(cls):
        cls.driver.quit()
        super().tearDownClass()

    def setUp(self):
        # Login the user
        self.assertTrue(self.client.login(username=""james"", password=""changemequick""))

        # Add cookie to log in the browser
        cookie = self.client.cookies[""sessionid""]
        self.driver.get(self.live_server_url + reverse(""find_patient""))
        self.driver.add_cookie(
            {
                ""name"": ""sessionid"",
                ""value"": cookie.value,
                ""domain"": ""localhost""
            }
         )
        super().setUp()

    def test_form_loader(self):
        # test forms loader is functioning properly

        driver = self.driver
        driver.get(self.live_server_url + ""/accounts/login/"")

        driver.find_element_by_xpath(""//input[@type='submit']"").click()
        driver.get_screenshot_as_file(""login.png"")
        assert len(driver.find_elements_by_css_selector("".loading"")) == 0
</code></pre>
","<p>i'm tired search solution load cookies in my case.
I've tried several ways, but still get an error.</p>
<p>i use cookies for login example.com</p>
<p>i have create new profile in mozilla firefox.</p>
<p>my code get cookie json:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
import time, json, pickle, pprint

cookies_location = &quot;cookies.json&quot;

firefox_options = FirefoxOptions()
firefox_options.add_argument('user-data-dir=C:\\Users\\xxx\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\sin8yczx.SeleniumUser')
driver = webdriver.Firefox(executable_path=&quot;./geckodriver.exe&quot;, options = firefox_options)

def save_cookies(driver, location):
    cookies = driver.get_cookies()
    with open('cookies_location', 'w', newline='') as outputdata:
        json.dump(cookies, outputdata)

driver.get(f&quot;https://www.example.com/&quot;)
save_cookies(driver, cookies_location)
driver.quit()

</code></pre>
<p>and i get cookie.json like this:</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;ks_sx&quot;,
    &quot;value&quot;: &quot;fsajfafjash8789sadadadasddassadasdasdada&quot;,
    &quot;path&quot;: &quot;/&quot;,
    &quot;domain&quot;: &quot;.example.com&quot;,
    &quot;secure&quot;: false,
    &quot;httpOnly&quot;: true,
    &quot;expiry&quot;: 1605446619,
    &quot;sameSite&quot;: &quot;None&quot;
  },
  {
    &quot;name&quot;: &quot;pu_sa&quot;,
    &quot;value&quot;: &quot;blablablabla&quot;,
    &quot;path&quot;: &quot;/&quot;,
    &quot;domain&quot;: &quot;.example.com&quot;,
    &quot;secure&quot;: false,
    &quot;httpOnly&quot;: true,
    &quot;expiry&quot;: 1605439425,
    &quot;sameSite&quot;: &quot;None&quot;
  },
bla bla bla bla
]
</code></pre>
<p>and then i load use in code:</p>
<pre><code>with open(cookies_location, 'r', newline='') as inputdata:
    cookies = json.load(inputdata)
for cookie in cookies:
    driver.add_cookie(cookie)

driver.get(f&quot;https://www.example.com/&quot;)
</code></pre>
<p>but, i get error like this.</p>
<pre><code>Traceback (most recent call last):
  File &quot;getcookies.py&quot;, line 84, in &lt;module&gt;
    driver.add_cookie(cookie)
  File &quot;C:\miniconda3\envs\xxx\lib\site-packages\selenium\webdriver\remote\webdriver.py&quot;, line 894, in add_cookie
    self.execute(Command.ADD_COOKIE, {'cookie': cookie_dict})
  File &quot;C:\miniconda3\envs\xxx\lib\site-packages\selenium\webdriver\remote\webdriver.py&quot;, line 321, in execute
    self.error_handler.check_response(response)
  File &quot;C:\miniconda3\envs\xxx\lib\site-packages\selenium\webdriver\remote\errorhandler.py&quot;, line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidCookieDomainException: Message: Cookies may only be set for the current domain (user-data-dir=c)
</code></pre>
"
"59035543","How to execute command from one docker container to another","<docker><docker-container><docker-networking><docker-command>","64893034","Calling code in one docker container from another, using docker-compose (without using ports)?","<docker><docker-compose>","<p>I'm creating an application that will allow users to upload video files that will then be put through some processing.</p>

<p>I have two containers.</p>

<ol>
<li><code>Nginx</code> container that serves the website where users can upload their video files. </li>
<li>Video processing container that has <code>FFmpeg</code> and some other processing stuff installed.</li>
</ol>

<p>What I want to achieve. I need container 1 to be able to run a bash script on container 2.</p>

<p>One possibility as far as I can see is to make them communicate over HTTP via an API. But then I would need to install a web server in container 2 and write an API which seems a bit overkill. 
I just want to execute a bash script. </p>

<p>Any suggestions?  </p>
","<p>Trying to get one docker-compose service to run a shell script that's in another container. Is there a way of doing this without having to install docker in docker?</p>
<p>In the example below, ideally I'd like to be able to run something like <code>shell-svc /bin/bash helloworld.sh</code> from <code>helloworld.py</code> in the <code>mgr-svc</code>  python container.</p>
<p>I can run <code>helloworld.sh</code> independently using docker-compose like this:</p>
<pre class=""lang-sh prettyprint-override""><code>docker-compose run shell-svc /bin/bash helloworld.sh
</code></pre>
<p>and <code>helloworld.py</code> using</p>
<pre><code>docker-compose run mgr-svc python helloworld.py
</code></pre>
<p>Example files:</p>
<p><code>docker-compose.yml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>services :

  shell-svc:
    build:
      context: .
      dockerfile: Dockerfile-shell
    image: shell-svc:1.0
    networks:
      - test-net
    expose:
      - &quot;22&quot;
      - &quot;8088&quot;
      - &quot;50070&quot;     

  mgr-svc:
    build:
       context: .
       dockerfile: Dockerfile-python
    image: mrg-svc:1.0
    networks:
      - test-net

networks:
 test-net:
version: &quot;3.5&quot;

</code></pre>
<p><code>Dockerfile-python</code></p>
<pre><code>FROM python:3.7
RUN pip install paramiko

COPY helloworld.py helloworld.py
</code></pre>
<p><code>Dockerfile-shell</code></p>
<pre><code>FROM debian:stretch-slim

USER root

RUN apt-get update &amp;&amp; \
    apt-get install -y openssh-server


RUN eval `ssh-agent -s` &amp;&amp; \
    mkdir ~/.ssh &amp;&amp; \
    echo &quot;StrictHostKeyChecking no&quot; &gt;&gt; /etc/ssh/ssh_config &amp;&amp; \
    cat /etc/ssh/ssh_config &amp;&amp; \
    chmod go-w /root &amp;&amp; \
    chmod 700 /root/.ssh 


EXPOSE 22 8088 50070

COPY helloworld.sh ./helloworld.sh

CMD service ssh start &amp;&amp; while true; do sleep 3000; done
</code></pre>
<p><code>helloworld.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import paramiko

if __name__ == '__main__':
    print (&quot;hello from python&quot;)

    ssh = paramiko.SSHClient()
    ssh.connect(&quot;shell-svc&quot;, username=&quot;root&quot;, password=&quot;password&quot;)
    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(helloworld.sh)
</code></pre>
<p><code>helloworld.sh</code></p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh

echo &quot;hello from shell&quot;
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","64956537","Spring Boot connect to Kafka Brokers","<spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have a docker-compose file that creates 3 Kafka nodes and 1 topic:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - &quot;2181:2181&quot;
  kafka:
    build: .
    ports:
      - &quot;9092&quot;
    environment:
      HOSTNAME_COMMAND: &quot;docker info | grep ^Name: | cut -d' ' -f 2&quot; # Normal instances
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_HOST_NAME: ${IPADDRESS}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE  
      KAFKA_CREATE_TOPICS: &quot;Upload_Kafka_Topic:1:3&quot; # 1 partition and 3 replicas
    volumes:
      - /docker-volumes/run/docker.sock:/var/run/docker.sock
</code></pre>
<p>Now I have an application.yml file as part of my Spring Project that gets the Broker IP Adresses injected:</p>
<pre><code>spring:
  kafka:
    consumer:
      group-id: upload-group
      auto-offset-reset: earliest
    bootstrap-servers: ${BROKERS_IP_ADDRESSES}
</code></pre>
<p>when I inject BROKERS_IP_ADDRESSES with for instance localhost:9092 or anything alike I get a connection error saying:</p>
<blockquote>
<p>[Consumer clientId=json-0, groupId=upload-group] Connection to node -1 (/localhost:9092) could not be established. Broker may not be available</p>
</blockquote>
<p>But when I use a custom script to insert each individual broker (or even just manually a single one!) into the BROKERS_IP_ADDRESSES,
I also get an error, but a java.net.UnknownHostException with exactly 3 different hashes something like: 8eedde00f315 (matching the count of my broker addresses)</p>
<p>I am <em><strong>assuming</strong></em> my second approach works and is delegated to Kafka, which uses a different internal handling of the brokers, but exposes these hashes to my application, which fails to make the correct look up of brokers in return.</p>
<p>Is there some additional configuration or a configuration of my Kafka environment that would resolve this look up error?</p>
"
"52640304","standard_init_linux.go:190: exec user process caused ""no such file or directory"" Docker with go basic web app","<docker><go><dockerfile>","65123256","standard_init_linux.go:211: exec user process caused ""no such file or directory"" from using net/http package","<docker><go>","<p>The very basic web app is created in Go</p>
<pre><code>package main

import(
   &quot;fmt&quot;
   &quot;net/http&quot;
   &quot;os&quot;
)

func hostHandler(w http.ResponseWriter, r *http.Request){
    name, err :=os.Hostname()

    if err != nil {
           panic(err)
        }

        fmt.Fprintf(w, &quot;&lt;h1&gt;HOSTNAME: %s&lt;/h1&gt;&lt;br&gt;&quot;,name)
        fmt.Fprintf(w, &quot;&lt;h1&gt;ENVIRONMENT VARS: &lt;/h1&gt;&lt;br&gt;&quot;)
        fmt.Fprintf(w, &quot;&lt;ul&gt;&quot;)

        for _, evar := range os.Environ(){
            fmt.Fprintf(w, &quot;&lt;li&gt;%s&lt;/li&gt;&quot;,evar)
        }
        fmt.Fprintf(w, &quot;&lt;/ul&gt;&quot;)

}

func rootHandler(w http.ResponseWriter, r *http.Request){

    fmt.Fprintf(w, &quot;&lt;h1&gt;Awesome site in Go!&lt;/h1&gt;&lt;br&gt;&quot;)
    fmt.Fprintf(w, &quot;&lt;a href='/host/'&gt;Host info&lt;/a&gt;&lt;br&gt;&quot;)

}

func main() {

        http.HandleFunc(&quot;/&quot;, rootHandler)
        http.HandleFunc(&quot;/host/&quot;, hostHandler)
        http.ListenAndServe(&quot;:8080&quot;, nil)


}
</code></pre>
<p>Docker File for it</p>
<pre><code>FROM scratch
WORKDIR /home/ubuntu/go
COPY webapp /
EXPOSE 8080
CMD [&quot;/webapp&quot;]
</code></pre>
<p>The image is built successfully</p>
<pre><code>ubuntu@ip-172-31-32-125:~/go/src/hello$ docker build -t &quot;webapp&quot; .
Sending build context to Docker daemon  6.152MB
Step 1/5 : FROM scratch
 ---&gt;
Step 2/5 : WORKDIR /home/ubuntu/go
 ---&gt; Using cache
 ---&gt; 8810a06c58c7
Step 3/5 : COPY webapp /
 ---&gt; Using cache
 ---&gt; d75222363d3a
Step 4/5 : EXPOSE 8080
 ---&gt; Using cache
 ---&gt; 45de0853de8e
Step 5/5 : CMD [&quot;/webapp&quot;]
 ---&gt; Using cache
 ---&gt; e9f9031f3632
Successfully built e9f9031f3632
Successfully tagged webapp:latest
</code></pre>
<p>But when i run the docker its show error.</p>
<pre><code>ubuntu@ip:~/go/src/hello$ docker run webapp
standard_init_linux.go:190: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Please guide what is the issue, I am new to docker and go.</p>
<p>Environment-related information</p>
<pre><code>ubuntu@ip:~/go/src/hello$ ls
Dockerfile  webapp
ubuntu@ip:~/go/src/hello$ echo $GOPATH
/home/ubuntu/go
</code></pre>
<p>Code was compiled with <strong>go build webapp.go</strong> command</p>
","<p>Steps to reproduce the problem:</p>
<ol>
<li><code>main.go</code> is using <code>net/http</code>.</li>
</ol>
<pre><code>docker build -o . main.go   // This generates the main executable.
</code></pre>
<ol start=""2"">
<li>Now preparing one Dockerfile</li>
</ol>
<pre><code> FROM alpine:3.6
 ENTRYPOINT [&quot;/home/myhome/main&quot;]
</code></pre>
<ol start=""3"">
<li>Then, <code>docker build -t myimag3</code></li>
<li>Now when I run this image</li>
</ol>
<pre><code>docker run -it -v /home/myhome:/home/myhome myimage
</code></pre>
<p>it results in the following error:</p>
<pre><code>getting : standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>I am facing this problem because of the main executable from <code>main.go</code>. If i remove all content and simply putting <code>fmt.Println(&quot;Hello world&quot;)</code> then it is working properly.
Only thing is when I try to use <code>net/http</code> packages at that time it is giving this error.
I am new to this go lang. Can anybody please provide some suggestions?</p>
<p>Code to <code>main.go</code></p>
<pre><code>https://tutorialedge.net/golang/go-docker-tutorial/
</code></pre>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","65196724","Cannot find the file. ERROR during connect","<docker><dockerfile>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>C:\Users\maggi&gt;docker images
error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/images/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</p>
"
"55603421","Accessing Docker Volume content on MacOS","<macos><docker><docker-volume>","65201369","Accessing Volume data directly on Docker Desktop","<docker><docker-compose>","<p>When I do a <code>docker volume inspect &lt;dockerid&gt;</code> on a Mac, I can see the path to the data, this appears as a <code>/var/lib/docker/volumes/&lt;volume name&gt;</code></p>

<p>On a Mac, this link does not exist, because docker runs on inside a very tiny VM.</p>

<p>I can use <code>screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty</code> to get into the vm and then navigate to the folder to see the volumes.</p>

<p>So got all that, but my question is: <strong>How do I link what is in these volumes on my host machine?</strong></p>

<p>I have tried this: <code>docker run -it --volume hello:/hello2 --name access_volumes busybox:latest /bin/sh</code> Where hello is the name of a volume I have created.</p>

<p>I can link a folder on my host machine to the container, but I want to backup the content or edit the content of the Volume from my host machine.</p>

<p>How do I do that?</p>
","<p>Reading about <a href=""https://github.com/docker/getting-started/blob/master/docs/tutorial/persisting-our-data/index.md"" rel=""nofollow noreferrer"">persisting DB data</a> with Docker. It says:</p>
<blockquote>
<p>While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM.</p>
</blockquote>
<p>What does the part about &quot;you would need to first get inside of the VM&quot; actually mean in this context?</p>
<p>What I've done so far:</p>
<ul>
<li>Created a volume using <code>docker volume create todo-db</code></li>
<li>Run <code>docker volume inspect todo-db</code></li>
</ul>
<p>The <code>inspect</code> command output gives me this:</p>
<pre><code>[
    {
        &quot;CreatedAt&quot;: &quot;2020-12-08T14:43:23Z&quot;,
        &quot;Driver&quot;: &quot;local&quot;,
        &quot;Labels&quot;: {},
        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/todo-db/_data&quot;,
        &quot;Name&quot;: &quot;todo-db&quot;,
        &quot;Options&quot;: {},
        &quot;Scope&quot;: &quot;local&quot;
    }
]
</code></pre>
<p>The Mountpoint is specified as <code>/var/lib/docker/volumes/todo-db/_data</code>.</p>
<p>This does not appear to exist on either my local machine (where I'm running Docker) or when opening the CLI through Docker Desktop for my image.</p>
<p>It mentions root access so I've done <code>sudo su</code> on my local machine then tried</p>
<pre><code>cd /var/lib/docker/volumes/todo-db/_data
</code></pre>
<p>This errors with:</p>
<pre><code>cd: no such file or directory: /var/lib/docker/volumes/todo-db/_data
</code></pre>
<p>Because the documentation says &quot;you would need to first get inside of the VM&quot; I've tried opening the CLI using Docker Desktop for my image then trying</p>
<pre><code>cd /var/lib/docker/volumes/todo-db
</code></pre>
<p>This errors with:</p>
<pre><code>/bin/sh: cd: can't cd to /var/lib/docker/volumes/todo-db: No such file or directory
</code></pre>
<p>Where is that directory? And what does &quot;inside of the VM&quot; actually refer to here?</p>
<p>Running macOS 10.15.6 and Docker Desktop version 2.5.0.1 with Engine 19.03.13</p>
"
"59615266","Super Slow Docker Build","<php><docker><docker-compose><dockerfile>","65231110","Docker Takes Ages Before Starting Build","<performance><docker><docker-compose><docker-desktop><wsl-2>","<p>I think I'm going to go crazy.  I've searched all over and can't seem to find a working solution both here on Stack, GitHub and other far reaches of the interwebs.</p>

<p>On this particular project, running <code>docker-compose build</code> is taking <strong><em>FOREVER</em></strong>.  It didn't use to be this way, and on other projects that use Docker, it's not an issue at all.  And by forever... I'm talking around 10-15 minute build times when it used to only take around 2 minutes tops.  I had two separate coworkers DL the <em>same repo</em> (one on Ubuntu 18, and the other on macOS 14.x).  When they ran the <code>build</code> command, the entire process took ~2 minutes.  Both of these people had never built this project before, so they were starting from complete scratch.</p>

<p>I've uninstalled/reinstalled Docker, ran a complete <code>docker system prune -a</code>, connected via wifi, connected via Ethernet, tried a different wifi network, tweaked my compose file, tweaked my docker file -- nothing.  </p>

<p>My machine is a 2018 MacBook Pro with a quad-core 2.7GHz i7, running macOS 10.14.6 with 16gb of installed RAM with Docker Desktop 2.1.0.5.</p>

<p>I've allowed Docker Desktop to have up to 12gb or RAM.  During the build process, my machine cpu usage spikes on average from 110% up to 270% running the <code>com.docker.hyperkit</code> process.</p>

<p>To be clear, it's hanging on the ""Building php"" (or ""Building web"") status message(s) before anything really even starts. After that, the actual build process runs smoothly and quick.</p>

<p>Here is my <strong>docker-compose.yaml</strong> file:</p>

<pre><code>version: '3.1'

services:
  db:
    container_name: clientsname.db
    hostname: db
    image: mariadb:10.4.1-bionic
    volumes:
      - ./db-data:/var/lib/mysql:delegated
    ports:
      - 3307:3306
    environment:
      MYSQL_DATABASE: my_database
      MYSQL_USER: my_user
      MYSQL_PASSWORD: my_pass
      MYSQL_ROOT_PASSWORD: my_pass

  php:
    container_name: clientsname.php
    hostname: php
    build:
      dockerfile: php/php.dockerfile
      context: ./
    environment:
      XDEBUG_CONFIG: remote_host=${REMOTE_HOST}

    volumes:
      - ../web:/var/www/web
      - ../moodle:/var/www/moodle
      - ../moodledata:/var/www/moodledata
      - ./php/custom.ini:/usr/local/etc/php/conf.d/zzz-custom.ini
      - ./php/z-errors.ini:/usr/local/etc/php/conf.d/z-errors.ini:delegated
      - ./php/z-upload.ini:/usr/local/etc/php/conf.d/z-upload.ini:delegated
      - ./php/z-xdebug.ini:/usr/local/etc/php/conf.d/z-xdebug.ini:delegated
    depends_on:
      - db

  web:
    container_name: clientsname.web
    hostname: web
    build:
      dockerfile: nginx/nginx.dockerfile
      context: ./
    volumes:
      - ../web:/var/www/web
      - ../moodle:/var/www/moodle
      - ../moodledata:/var/www/moodledata
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs:/var/log/nginx
    ports:
      - 80:80
      - 443:443
    depends_on:
      - php
      - db

</code></pre>

<p>Here is the referenced <strong>php.dockerfile</strong> file:</p>

<pre><code>FROM php:7.2.26-fpm
LABEL maintainer=""My Clients Name""

# Environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV COMPOSER_ALLOW_SUPERUSER=1
ENV COMPOSER_NO_INTERACTION=1
ENV COMPOSER_HOME=/usr/local/share/composer

# Working Directory
WORKDIR /var/www/web
WORKDIR /var/www/moodle
WORKDIR /var/www/moodledata


RUN rm /etc/apt/preferences.d/no-debian-php &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends apt-utils \
        build-essential     \
        php-soap            \
        libzip-dev          \
        libmagickcore-dev   \
        libmagickwand-dev   \
        libmagic-dev        \
        libpng-dev          \
        libfreetype6-dev    \
        libjpeg62-turbo-dev \
        libmcrypt-dev       \
        libmemcached-dev    \
        zlib1g-dev          \
        nano                \
        sudo                \
        gnupg               \
        curl                \
        unzip &amp;&amp;            \
    docker-php-ext-install soap pdo_mysql mysqli &amp;&amp; \
    docker-php-ext-install -j$(nproc) gd iconv &amp;&amp; \
    docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ &amp;&amp; \
    pecl install zip-1.15.2 imagick memcached-3.0.4 xdebug &amp;&amp; \
    docker-php-ext-enable memcached imagick zip xdebug

# Install Composer, Node, Gulp, and SASS
RUN curl -s https://getcomposer.org/installer | php &amp;&amp; mv composer.phar /usr/local/bin/composer

RUN curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - &amp;&amp; apt-get install -y nodejs &amp;&amp; npm install npm@latest -g &amp;&amp; npm install --global gulp-cli &amp;&amp; npm config set unsafe-perm=true


# Export composer vendor path
RUN echo """" &gt;&gt; ~/.bashrc &amp;&amp; echo 'export PATH=""$HOME/.composer/vendor/bin:$PATH""' &gt;&gt; ~/.bashrc

</code></pre>

<p>And the referenced <strong>nginx.dockerfile</strong></p>

<pre><code>FROM nginx:stable-alpine

RUN apk add --update bash &amp;&amp; rm -rf /var/cache/apk/*

WORKDIR /var/www/web

</code></pre>

<p>It's driving me batty... what in the bloody hell could I be doing wrong?  If there is anything I've left out that you'd all like to know, please let me know and I'll update the post.</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Thanks to @BMitch and you all who have commented thus far.  I took my entire /docker build directory and moved it into a test folder, and then created empty /web, /moodle, and /moodledata directories before running the <code>build</code> command.  It started to compile immediately.</p>

<p>What's curious to me is that the other coworkers DL'd the same Git repo that I have and did not have any of the same issues.  Oooh... come to think of it... I bet I know what the issue is.</p>
","<p>Like many people, I was happy to hear about WSL 2, and about the improvements that it brings, including the ability to virtualize from it, meaning I could now use Docker directly from Linux. But when I build any image, the following happens.</p>
<h3>Symptoms</h3>
<ul>
<li><code>vmmem</code> grows to about 3.5GB (this is not a problem).</li>
<li>It says <code>building &lt;myservice&gt;</code>, as usual;</li>
<li>Nothing else happens for <strong>2-5 minutes</strong>, while <code>vmmem</code> consumes about 7-8% of my CPU.</li>
<li>Only after this the actual building starts, and output begins to appear. At this point the speed is OK.</li>
</ul>
<h3>Specs</h3>
<ul>
<li>Razer Blade Pro 17.</li>
<li>i7-9750H</li>
<li>32GB DDR4</li>
</ul>
<h3>.wslconfig</h3>
<pre><code>[wsl2] 
memory=10000MB
processors=4
swap=0
localhostForwarding=true
</code></pre>
<h3>Remarks</h3>
<p>It's probably related to <a href=""https://stackoverflow.com/questions/62154016/docker-on-wsl2-very-slow"">this question</a>, but I'm talking about build performance, not performance inside the container.</p>
"
"55034727","What does . mean in docker? Does it mean the current working directory of the image or the local machine?","<docker>","65343923","COPY . . in Dockerfile doing?","<visual-studio><docker><.net-core>","<p>I am confused about whether <code>.</code> means that it's a shortened abbreviation of the current directory of the image or if it's the current working directory on the local machine. Or is it the same meaning of <code>.</code> in most console commands like essentially selecting all in the current directory.</p>

<pre><code>COPY somecode.java .

#copy the rest of the code

COPY . .
</code></pre>

<p>The <code>.</code> also seems to mean find the docker file in the current directory. </p>

<pre><code>docker build -t image-tag .
</code></pre>
","<p>I used Visual Studio to add Dockerfile to my Dotnet Core Web API project.</p>
<pre><code>FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build
WORKDIR /src
COPY App.WebAPI/App.WebAPI.csproj App.WebAPI/
COPY App.Data/App.Data.csproj App.Data/
RUN dotnet restore &quot;App.WebAPI/App.WebAPI.csproj&quot;
COPY . .
WORKDIR &quot;/src/App.WebAPI&quot;
RUN dotnet build &quot;App.WebAPI.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;App.WebAPI.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;App.WebAPI.dll&quot;]
</code></pre>
<p>What does the following line actually do?</p>
<pre><code>COPY . .
</code></pre>
"
"54269442","Why does docker create empty node_modules and how to avoid it?","<node.js><docker>","65395132","Use of anonymous volume to prevent Docker container to create folder locally not working","<docker><docker-compose><docker-volume>","<p>There are some <a href=""https://stackoverflow.com/questions/39651908/why-node-modules-is-empty-after-docker-build"">similar questions</a> but they haven't answered why docker creates the empty <code>node_modules</code> directory in the repo even though the dockerfile is setup to hold node_modules in the container?</p>

<p>I'm interested to know why directory is created on the host empty, give that yarn already installs packages inside the container within <code>node_modules</code> and how to avoid it.</p>

<pre><code>## Dockerfile

FROM node:8.11.4-alpine
RUN apk update &amp;&amp; apk add yarn
RUN yarn global add nodemon
WORKDIR /usr/app

COPY package.json yarn.lock /usr/app/
RUN yarn

EXPOSE 3000

</code></pre>

<pre><code>## docker-compose.yml

version: ""3.2""

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    command: nodemon index.js
    volumes:
      - .:/usr/app
      - /usr/app/node_modules
    ports:
      - ""3000:3000""

</code></pre>
","<p>As I read in this tutorial <a href=""https://mherman.org/blog/dockerizing-a-react-app/"" rel=""nofollow noreferrer"">https://mherman.org/blog/dockerizing-a-react-app/</a> it's possible to mount an anonymous volume into a subpath of a mounted volume in order to prevent the changes in that subpath to be reflected back to the actual volume. Is that correct? Because it's not working for me.</p>
<p>I'm trying to dockerize a react app by mounting the <code>src</code> folder into the container and building everything their. But I don't want the <code>node_modules</code> folder that is created during the build step to be reflected back in the mounted <code>src</code>.</p>
<p>My <code>Dockerfile.dev</code> looks like this:</p>
<pre><code>FROM node:15.4.0-alpine3.12

# set working directory
ARG APP_PATH=/app
WORKDIR $APP_PATH

# add `/app/node_modules/.bin` to $PATH
ENV PATH $APP_PATH/node_modules/.bin:$PATH

# install app dependencies
COPY package*.json ./
RUN npm install --silent

# add app
COPY . ./

# start app
EXPOSE 3000
CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;start&quot; ]
</code></pre>
<p>This is my <code>docker-compose.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.8&quot;
services:
    frontend:
        build: 
            context: ./frontend
            dockerfile: ../docker/Dockerfile.dev
        image: frontend_react
        ports: 
            - 4000:3000
        volumes:
            - ./frontend:/app
            - /app/node_modules
        environment: 
            - NODE_ENV=development
</code></pre>
<p>How can I prevent that the <code>node_modules</code> folder shows up on my local machine?
Should I even prevent the <code>node_modules</code> folder from being reflected back to my machine?</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","65459273","Laravel Sail / Docker is too slow on Mac: optimization possible?","<laravel><performance><docker><local><laravel-sail>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>Just installed Laravel Sail in an existing project. I really love the easy setup, but it seems way too slow for development. I'm running the latests Docker (20.10) on a Macbook Pro 2019. I gave Docker 6 cores and 8GB of RAM in the resource-settings.
The response times I'm getting are 700-800ms, sometimes even higher, for simple API calls. Coming from Laravel Valet this feels like a huge step back.</p>
"
"63540237","How to create docker image with nodejs 12, java, gcc, g++,python3, monocs","<java><node.js><docker>","65466700","Docker image install both nodejs and Java","<java><dockerfile>","<p>I am trying to dockerize my NodeJs &amp; Express API . In my API iam using <a href=""https://www.npmjs.com/package/compile-run"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/compile-run</a> package to compile and run C,
Cpp,Java,JavaScript(Node.js env), Python languages.
This package requires all 5 compilers(gcc,g++,nodejs,python3,javac) installed on the server. If any compiler misses, it throws error.</p>
<p>In my local(undockerized) the API is working completely fine on both windows &amp; ubuntu(As I have installed compilers on them).</p>
<p>I am trying to replicate the same on my docker image. But I am stuck.</p>
<p>Look at my
<strong>Dockerfile</strong>:</p>
<pre><code>FROM node:12
WORKDIR /app
COPY package.json /app
RUN npm install
COPY . /app
CMD [&quot;npm&quot;,&quot;start&quot;]
</code></pre>
<p>I think node-12 image comes along with gcc, g++,python3 and nodejs(obviously).
But the issue is with <strong>java</strong>. I am not able to compile java code.</p>
<p>so I tried it this way</p>
<pre><code>FROM node:12
RUN apt-get -y install default-jre
# RUN java -version
RUN apt -y install default-jre
RUN apt install openjdk-11-jre-headless
RUN java -version

WORKDIR /app


COPY package.json /app
RUN npm install
COPY . /app
CMD [&quot;npm&quot;,&quot;start&quot;]
</code></pre>
<p>But I am not able to install open-jdk or open-jre with apt/apt-get.
What is the right way to configure docker?</p>
<p>This is my nodeJS API repository <a href=""https://github.com/yogendramaarisetty/online-compiler-api"" rel=""nofollow noreferrer"">https://github.com/yogendramaarisetty/online-compiler-api</a></p>
","<p>My main application is used with <code>node.js</code>, but there is some code I also need to run <code>Java</code> in the container. In order to do so, I need to install <code>Java</code>. My try is:</p>
<pre><code>FROM node

WORKDIR /app

COPY package.json /app

RUN npm install

COPY . /app

EXPOSE 3000

FROM openjdk:8-jre-alpine

ENV JAVA_HOME=/usr/bin/java

CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;start&quot; ]
</code></pre>
<p>I've read that it is quite complicated to get multi base images due to operation system conflicts.
Then, how could I install both <code>node.js</code> and <code>Java</code> in the <code>Dockerfile</code>?</p>
"
"53681522","Share variable in multi-stage Dockerfile: ARG before FROM not substituted","<docker><dockerfile><docker-multi-stage-build>","65520250","'cannot normalize nothing' error on Docker Hub multi-stage build with shared ARG","<dockerfile><dockerhub>","<p>I'm writing a multi-stage Dockerfile for the <a href=""https://www.mcs.anl.gov/research/projects/darshan/"" rel=""noreferrer"">darshan utils</a>:</p>

<pre><code>ARG DARSHAN_VER=3.1.6

FROM fedora:29 as build
RUN dnf install -y \
        gcc \
        make \
        bzip2 bzip2-devel zlib zlib-devel
RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz"" \
    &amp;&amp; tar ...


FROM fedora:29
COPY --from=build ""/usr/local/darshan-${DARSHAN_VER}"" ""/usr/local/darshan-${DARSHAN_VER}""
...
</code></pre>

<p>I build it with <code>docker build -t darshan-util:3.6.1 .</code> and the error I get is:</p>

<pre><code>Step 5/10 : RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...

 ---&gt; Running in 9943cce1669c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
...
curl: (78) RETR response: 550
The command '/bin/sh -c curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...' returned a non-zero code: 78
</code></pre>

<p>I'd like to reuse the same ARG in both stages, so that I can define a default build variable just once.
If I duplicate ARG in both stages, just below the two FROMs, it builds correctly.</p>

<p>What is the correct way to define a ""global"" multi-stage ARG variable with a default?</p>
","<p>I have a multi-stage build sharing single <code>ARG</code> (to DRY out path) between stages. Something like this:</p>
<pre><code>ARG BUILD_DIR=/build

FROM alpine:latest as build
RUN apk add --no-cache gcc musl-dev
WORKDIR $BUILD_DIR
RUN echo -e '\n\
#include &lt;stdio.h&gt;\n\
int main (void)\n\
{ puts (&quot;Hello, World!&quot;);}'\
&gt;&gt; hello.c
RUN cc -o hello hello.c

FROM alpine:latest
COPY --from=build $BUILD_DIR/hello /app
CMD [&quot;/app&quot;]
</code></pre>
<p>It gets built with no problem locally, but automated Docker Hub build fails with the following error:</p>
<blockquote>
<p>Step 4/9 : WORKDIR $BUILD_DIR
cannot normalize nothing</p>
</blockquote>
<p>I am curious, what can cause the problem (too old build engine on Docker Hub?). And is there a better workaround than just hard-coding path in Dockerfile?</p>
"
"53836493","Remote debug container in kubernetes using intellij","<docker><intellij-idea><kubernetes>","65629590","How to do Java Remote debugging with kubernetes pod?","<java><kubernetes><kubernetes-helm><remote-debugging>","<p>I try to remote debug the application in attached mode with host: <code>192.168.99.100</code> and port <code>5005</code>, but it tells me that it is <code>unable to open the debugger port</code>. The IP is <code>192.268.99.100</code> (the cluster is hosted locally via minikube).</p>

<p>Output of <code>kubectl describe service catalogservice</code></p>

<pre><code>Name:                     catalogservice
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 app=catalogservice
Type:                     NodePort
IP:                       10.98.238.198
Port:                     web  31003/TCP
TargetPort:               8080/TCP
NodePort:                 web  31003/TCP
Endpoints:                172.17.0.6:8080
Port:                     debug  5005/TCP
TargetPort:               5005/TCP
NodePort:                 debug  32003/TCP
Endpoints:                172.17.0.6:5005
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>

<p>This is the pods service.yml:</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: catalogservice
spec:
  type: NodePort
  selector:
    app: catalogservice
  ports:
  - name: web
    protocol: TCP
    port: 31003
    nodePort: 31003
    targetPort: 8080
  - name: debug
    protocol: TCP 
    port: 5005
    nodePort: 32003
    targetPort: 5005
</code></pre>

<p>And in here I expose the containers port</p>

<pre><code>spec:
  containers:
  - name: catalogservice
    image: elps/myimage
    ports:
    - containerPort: 8080
      name: app
    - containerPort: 5005
      name: debug
</code></pre>

<p>The way I build the image:</p>

<pre><code>FROM openjdk:11
VOLUME /tmp
EXPOSE 8082
ADD /target/catalogservice-0.0.1-SNAPSHOT.jar catalogservice-0.0.1-SNAPSHOT.jar
ENTRYPOINT [""java"", ""-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=n"", ""-jar"", ""catalogservice-0.0.1-SNAPSHOT.jar""]
</code></pre>

<p>When I execute <code>nmap -p 5005 192.168.99.100</code> I receive</p>

<pre><code>PORT     STATE  SERVICE
5005/tcp closed avt-profile-2
</code></pre>

<p>When I execute <code>nmap -p 32003 192.168.99.100</code> I receive</p>

<pre><code>PORT     STATE  SERVICE
32003/tcp closed unknown
</code></pre>

<p>When I execute <code>nmap -p 31003 192.168.99.100</code> I receive</p>

<pre><code>PORT     STATE  SERVICE
31003/tcp open unknown
</code></pre>

<p>When I execute <code>kubectl get services</code> I receive</p>

<pre><code>NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                          AGE
catalogservice    NodePort    10.108.195.102   &lt;none&gt;        31003:31003/TCP,5005:32003/TCP   14m
</code></pre>

<p><code>minikube service customerservice --url</code> returns </p>

<pre><code>http://192.168.99.100:32004
</code></pre>
","<p>We have java (spring boot application) running in <strong>3</strong> Node kubernetes cluster.</p>
<p>I want to implement remote debugging with <strong>intelliJ</strong> IDE?</p>
<p><strong>Note:</strong>
if i want remote debugging only be available in QA/DEV environment is it possible?
We are using <strong>helm</strong> for deploying application</p>
"
"63455621","COPY failed: stat /var/lib/docker/tmp/docker-xxx : no such file or directory","<docker><dockerfile>","65676514","wrong path in copy command in dockerfile in azure pipeline","<docker><azure-pipelines>","<p>I have a github actions workflow to build a docker image:</p>
<pre><code>name: Backend-Demo Docker Image CI
on:
  push:
    branches: [ master ]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Login to Azure Container Registry
        run: echo ${{ secrets.REGISTRY_PASSWORD }} | docker login ${{ secrets.LOGIN_SERVER_URL }} -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin
      - name: Get the version
        id: vars
        run: echo ::set-output name=tag::$(echo ${GITHUB_REF:10})
      - name: Build the tagged Docker image
        run: docker build . --file backend/Dockerfile --tag backend-demo/spring-boot:v1.0
</code></pre>
<p>The Dockerfile is:</p>
<pre><code>FROM openjdk:14-alpine
MAINTAINER example.com
RUN mkdir -p /opt/demo-0.0.1/lib
# Setting application source code working directory
WORKDIR /opt/demo-0.0.1/
COPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar
# ADD target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/

RUN sh -c 'touch demo-0.0.1-SNAPSHOT.jar'
ENTRYPOINT [&quot;java&quot;]
CMD [&quot;-jar&quot;, &quot;/opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar&quot;]
</code></pre>
<p>But when I execute this workflow I got this error at the <code>COPY</code> instruction:</p>
<pre><code>Step 5/8 : COPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar
COPY failed: stat /var/lib/docker/tmp/docker-builder851513197/target/demo-0.0.1-SNAPSHOT.jar: no such file or directory
##[error]Process completed with exit code 1.
</code></pre>
<p>I have been checking and it looks a typical error when the file we have the <code>Dockerfile</code> in a different directory like my instruction:</p>
<p><code>docker build . --file backend/Dockerfile --tag backend-demo/spring-boot:v1.0</code></p>
<p>I also don't have <code>.dockerignore</code> file and my Dockerfile is called <code>Dockerfile</code> precisely.</p>
<p>The <code>target/demo-0.0.1-SNAPSHOT.jar</code> file I am trying to copy is present in my github repository
Not sure what could be happening with the context, but probably <a href=""https://github.com/docker/for-linux/issues/90#issuecomment-509810005"" rel=""noreferrer"">this answer</a> could be a good hint?</p>
","<p>I use azure devops pipeline to build jar and put it to docker image.<br />
I use maven to build jar and put jar and dependencies to folder /componentA/target/container<br />
Pom file - /componemtA/pom.xml<br />
Docker file - /componentA/docker/Dockerfile</p>
<p>My pipeline yuml file is</p>
<pre><code>......
stages:
 - stage: BUILD
    jobs:
      - job: BUILD_JAR_AND_IMAGE
        steps:
          - task: MavenAuthenticate@0
....
          - task: Maven@3
......
          - task: Docker@2
             inputs:
              command: login
              containerRegistry: xxxx
  
          - task: Docker@2
            displayName: Build an image
            inputs:
              containerRegistry: 'xxx'
              repository: '$(image_name)'
              command: 'build'
              Dockerfile: '$(component_dir)/docker/Dockerfile'
              tags: '$(Build.BuildNumber)'
 
</code></pre>
<p>My docker file is:</p>
<pre><code>.....
COPY /componentA/target/container /opt
....
</code></pre>
<p>And I get error<br />
2021-01-12T00:04:43.9376526Z Step 2/19 : COPY /componentA/target/container /opt<br />
2021-01-12T00:04:43.9431565Z COPY failed: stat /var/lib/docker/tmp/docker-builder630817503/componentA/target/container: no such file or directory<br />
2021-01-12T00:04:43.9501516Z ##[error]COPY failed: stat /var/lib/docker/tmp/docker-builder630817503/besReportService/target/container: no such file or directory<br />
2021-01-12T00:04:43.9717162Z ##[error]The process '/usr/bin/docker' failed with exit code 1</p>
<p>copy command starts in <strong>var/lib/docker/tmp/docker-builder630817503</strong> instead of <strong>/home/vsts/work/1/s</strong></p>
<p>How to fix this problem ?</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","65757920","Docker container failed to spin up","<node.js><docker><docker-compose>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I am trying to run this <a href=""https://github.com/tadhglewis/issue-status"" rel=""nofollow noreferrer"">project</a> in <code>Github</code> in a <code>Docker</code> container. It is a simple reactjs status application.</p>
<p>I tried to run this in my local environment with following commands after cloning:</p>
<pre><code>// install dependencies
npm install

// and then
npm start
</code></pre>
<p>And works totally fine.</p>
<p>Then I did the same thing with a <code>Dockerfile</code>, as below:</p>
<pre><code># Use whatever version you are running locally (see node -v)
FROM node:12.13

WORKDIR /

COPY package.json /


RUN npm install

COPY . /


EXPOSE 3000

CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<p>And when I am using <code>docker-compose.yml</code> to build this as below using <code>docker-compose up --build -d</code>:</p>
<pre><code>version: &quot;3.2&quot;

services:
  frontend_common:
    build: .
    ports:
      - 80:3000
       
</code></pre>
<p>But the container EXITS and when I try to see the logs, it gives no error but following output:</p>
<pre><code>
&gt; issue-status@0.1.0 start /
&gt; react-scripts start

ℹ ｢wds｣: Project is running at http://&lt;Redacted&gt;/
ℹ ｢wds｣: webpack output is served from
ℹ ｢wds｣: Content not from webpack is served from /public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...

</code></pre>
<p>Can someone help me where I am being gone wrong?</p>
"
"54954187","Docker images - types. Slim vs slim-stretch vs stretch vs alpine","<java><docker><dockerfile>","65776474","Difference between the docker images for aspnet (aspnet version, host os codename, ect.)","<docker><.net-core><dockerfile>","<p>I am looking to pick up a docker image to build a java app and looking at the variants of the OpenJDK images available.
I am looking here <a href=""https://github.com/docker-library/openjdk/tree/master/8/jdk"" rel=""noreferrer"">https://github.com/docker-library/openjdk/tree/master/8/jdk</a> and see alpine, slim and windows.
What are the differences between these and what does each variant give?</p>
","<p>What is the difference between the docker images below?</p>
<ul>
<li>mcr.microsoft.com/dotnet/aspnet:3.1</li>
<li>mcr.microsoft.com/dotnet/aspnet:3.1-buster</li>
<li>mcr.microsoft.com/dotnet/aspnet:3.1-buster-slim</li>
</ul>
<p>I see all these tags listed with the same docker file:
<em>3.1.11-buster-slim, 3.1-buster-slim,  3.1.11, 3.1</em></p>
<p>Reference: <a href=""https://hub.docker.com/_/microsoft-dotnet-aspnet/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/microsoft-dotnet-aspnet/</a></p>
<p>I've read some details for the java images, so i guess the format is</p>
<blockquote>
<p>[aspnet version]-[host os codename]-[features included]</p>
</blockquote>
<p>but then it's not clear to me why event the <em>[features included]</em> tag points to the same docker file description or if the naming concept is the same as the JDK one.</p>
"
"55756372","When using BuildKit with Docker, how do I see the output of RUN commands?","<docker><docker-buildkit>","65936425","Dockerfile: RUN ls -l","<docker><dockerfile>","<p>When building Docker images with <code>DOCKER_BUILDKIT=1</code>, there is a very cool progress indicator but no command output. How do I see the command output to debug my build?</p>
","<p>I have this simple Dockerfile</p>
<pre><code>FROM adoptopenjdk/openjdk8
RUN ls -l
RUN pwd
</code></pre>
<p>When I build it <code>docker build . --no-cache</code> I'm expecting to see the result of the <code>ls -l</code> and <code>pwd</code> but nothing is shown.</p>
<pre><code> =&gt; CACHED [1/3] FROM docker.io/adoptopenjdk/openjdk8@sha256:e8de192088d82fa93e1...
 =&gt; [2/3] RUN ls -l
 =&gt; [3/3] RUN pwd
 =&gt; exporting to image
 =&gt; =&gt; exporting layers
 =&gt; =&gt; writing image sha256:ea9d3d5490cea3d73321654a6... 
</code></pre>
<p>I'm using Big Sur with Docker 3.1.0 (just freshly reinstalled). My colleagues are able to see the result of the command.</p>
"
"53681522","Share variable in multi-stage Dockerfile: ARG before FROM not substituted","<docker><dockerfile><docker-multi-stage-build>","65940714","Not Able to use ARGUMENT for ADD command in Dockerfile","<java><docker><jar><dockerfile><build-automation>","<p>I'm writing a multi-stage Dockerfile for the <a href=""https://www.mcs.anl.gov/research/projects/darshan/"" rel=""noreferrer"">darshan utils</a>:</p>

<pre><code>ARG DARSHAN_VER=3.1.6

FROM fedora:29 as build
RUN dnf install -y \
        gcc \
        make \
        bzip2 bzip2-devel zlib zlib-devel
RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz"" \
    &amp;&amp; tar ...


FROM fedora:29
COPY --from=build ""/usr/local/darshan-${DARSHAN_VER}"" ""/usr/local/darshan-${DARSHAN_VER}""
...
</code></pre>

<p>I build it with <code>docker build -t darshan-util:3.6.1 .</code> and the error I get is:</p>

<pre><code>Step 5/10 : RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...

 ---&gt; Running in 9943cce1669c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
...
curl: (78) RETR response: 550
The command '/bin/sh -c curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...' returned a non-zero code: 78
</code></pre>

<p>I'd like to reuse the same ARG in both stages, so that I can define a default build variable just once.
If I duplicate ARG in both stages, just below the two FROMs, it builds correctly.</p>

<p>What is the correct way to define a ""global"" multi-stage ARG variable with a default?</p>
","<p>i have a Dockerfile in which i need to provide build argument before running Dockerfile.</p>
<pre><code>ARG JAR_VERSION=${JAR_VERSION}
FROM openjdk:8-jre-slim
ADD App1-${JAR_VERSION}-SNAPSHOT.jar /opt/myapp/examples/App1-${JAR_VERSION}-SNAPSHOT.jar
</code></pre>
<p><strong>Not able to figure out why ADD command is not working as required, i have tried multiple ways, but no luck</strong></p>
<p>i am supplying build arg on while building image as
docker build -t myapp:1.0.20 --build-arg JAR_VERSION=1.0.20 .</p>
<p>but in logs it shows same error below
Step 16/20 : ADD App1-${JAR_VERSION}-SNAPSHOT.jar /opt/myapp/examples/App1-${JAR_VERSION}-SNAPSHOT.jar
20:55:12  ADD failed: stat /var/lib/docker/tmp/docker-builder164635490/App1-SNAPSHOT.jar: no such file or directory</p>
"
"63602750","How to create and add user with password in alpine Dockerfile?","<docker><alpine>","65985089","/bin/sh: useradd: not found when running RUN useradd app with Alpine","<node.js><docker><dockerfile><alpine>","<p>The following <code>Dockerfile</code> works fine for Ubuntu:</p>
<pre><code>FROM ubuntu:20.04
SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]
ARG user=hakond
ARG home=/home/$user
RUN useradd --create-home -s /bin/bash $user \
        &amp;&amp; echo $user:ubuntu | chpasswd \
        &amp;&amp; adduser $user sudo

WORKDIR $home
USER $user
COPY --chown=$user entrypoint.sh .
RUN chmod +x entrypoint.sh
ENTRYPOINT [&quot;./entrypoint.sh&quot;]
</code></pre>
<p>where <code>entrypoint.sh</code> is</p>
<pre><code>#! /bin/bash
exec bash
</code></pre>
<p>How can I do the same in Alpine? I tried:</p>
<pre><code>FROM alpine:3.12
SHELL [&quot;/bin/sh&quot;, &quot;-c&quot;]
RUN apk add --no-cache bash
ARG user=hakond
ARG home=/home/$user
RUN addgroup -S docker
RUN adduser \
    --disabled-password \
    --gecos &quot;&quot; \
    --home $home \
    --ingroup docker \
    $user
WORKDIR $home
USER $user
COPY chown=$user entrypoint.sh .
RUN chmod +x entrypoint.sh
ENTRYPOINT [&quot;./entrypoint.sh&quot;]
</code></pre>
<p>But this fails to build:</p>
<pre><code>$ docker build -t alpine-user .
Sending build context to Docker daemon   5.12kB
Step 1/12 : FROM alpine:3.12
 ---&gt; a24bb4013296
Step 2/12 : SHELL [&quot;/bin/sh&quot;, &quot;-c&quot;]
 ---&gt; Using cache
 ---&gt; ce9a303c96c8
Step 3/12 : RUN apk add --no-cache bash
 ---&gt; Running in e451a2481846
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
(1/4) Installing ncurses-terminfo-base (6.2_p20200523-r0)
(2/4) Installing ncurses-libs (6.2_p20200523-r0)
(3/4) Installing readline (8.0.4-r0)
(4/4) Installing bash (5.0.17-r0)
Executing bash-5.0.17-r0.post-install
Executing busybox-1.31.1-r16.trigger
OK: 8 MiB in 18 packages
Removing intermediate container e451a2481846
 ---&gt; 7b5f7f87bdf6
Step 4/12 : ARG user=hakond
 ---&gt; Running in 846b4b12856e
Removing intermediate container 846b4b12856e
 ---&gt; a0453cb6706e
Step 5/12 : ARG home=/home/$user
 ---&gt; Running in 06550ad3f550
Removing intermediate container 06550ad3f550
 ---&gt; 994d71fb0281
Step 6/12 : RUN addgroup -S docker
 ---&gt; Running in 70aaec6f40e0
Removing intermediate container 70aaec6f40e0
 ---&gt; 5188ed7b234c
Step 7/12 : RUN adduser     --disabled-password     --gecos &quot;&quot;     --home $home     --ingroup docker     $user
 ---&gt; Running in ff36a7f7e99b
Removing intermediate container ff36a7f7e99b
 ---&gt; 97f481916feb
Step 8/12 : WORKDIR $home
 ---&gt; Running in 8d7f0411d6e3
Removing intermediate container 8d7f0411d6e3
 ---&gt; 5de66f4b5d4e
Step 9/12 : USER $user
 ---&gt; Running in ac4abac7c3a8
Removing intermediate container ac4abac7c3a8
 ---&gt; dffd2185df1f
Step 10/12 : COPY chown=$user entrypoint.sh .
COPY failed: stat /var/snap/docker/common/var-lib-docker/tmp/docker-builder615220199/chown=hakond: no such file or directory
</code></pre>
","<p>Hey guys can you check what is wrong with my Dockerfile? I am getting</p>
<pre><code>/bin/sh: useradd: not found
The command '/bin/sh -c useradd app' returned a non-zero code: 127

</code></pre>
<pre><code># Dockerfile

FROM node:12.13.0-alpine
RUN mkdir -p /opt/app
WORKDIR /opt/app
RUN useradd app
COPY addressbook/ .
RUN npm install
RUN chown -R app:app /opt/app
USER app
EXPOSE 3000
CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;pm2&quot; ]
</code></pre>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","66002476","Docker with WordPress Page Load is Painfully slow","<wordpress><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I am using Docker to run acceptance tests. The website in production is fast and loads almost immediately. On my docker container, the a lot of time is just spent waiting:</p>
<p>The docker container Waiting (TTFB) time is about 5 seconds:</p>
<p><a href=""https://i.stack.imgur.com/KhNlr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KhNlr.jpg"" alt=""Docker Container Network"" /></a></p>
<p>The production website's (TFFB) time is 200 ms.</p>
<p><a href=""https://i.stack.imgur.com/ESMln.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ESMln.jpg"" alt=""Production Website Network"" /></a></p>
<p>I have tried to delegate the volumes in my DockerComposer file:</p>
<pre><code>    volumes:
      - ./config/php.conf.ini:/usr/local/etc/php/php.ini:delegated
      # Docker WordPress files will be placed at the root.
      - .:/var/www/html:delegated
</code></pre>
<p>And the speed is still awful. An acceptance test that should take 3 seconds to run is taking about 20 seconds with Docker.</p>
<p>Why is there such a long waiting time with Docker? I love it but if this continues I am better off running MAMP. I LOVE Docker, if there is a way to improve this massive wait time, please pitch in.</p>
"
"53610385","Docker - Postgres and pgAdmin 4 : Connection refused","<postgresql><docker>","66006126","Docker - cannot connect to posgres from pgadmin","<postgresql><docker><docker-compose><pgadmin-4>","<p>Newbie with docker, I am trying to connect throught localhost my pgAdmin container to the postgres one.</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
0b00555238ba        dpage/pgadmin4      ""/entrypoint.sh""         43 minutes ago      Up 43 minutes       0.0.0.0:80-&gt;80/tcp, 443/tcp   pedantic_turing
e79fb6440a95        postgres            ""docker-entrypoint.s…""   About an hour ago   Up About an hour    0.0.0.0:5432-&gt;5432/tcp        pg-docker
</code></pre>

<p>I succeed connecting with psql command.</p>

<pre><code>psql -h localhost -U postgres -d postgres
</code></pre>

<p>But when I create the server on pgAdmin with the same parameters as psql I got the following error.</p>

<blockquote>
  <p>Unable to connect to server:</p>
  
  <p>could not connect to server: Connection refused Is the server running
  on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on
  port 5432? could not connect to server: Address not available Is the
  server running on host ""localhost"" (::1) and accepting TCP/IP
  connections on port 5432?</p>
</blockquote>

<p>I succeed to connect throught the IPAddress given by <code>docker inspect</code> on the container.</p>

<p>By the way, I checked postgresql.conf and assert that <code>listen_addresses = '*'</code> and also that pg_hba.conf contain <code>host all all all md5</code>.</p>

<p>But I don't get it, why shouldn't I be able to use the localhost address ? And why does docker even give me an address that is not local ?</p>
","<p>I ran a docker-compose to run Postgres and pgadmin, with the following docker-compose file.
I can login to pgadmin but pgadmin cannot connect to the Postgres. However, Postgres is running smoothly and accepting requests.</p>
<p>docker-compose.yaml</p>
<pre><code>version: &quot;3&quot;
services:
  postgres:
    image: postgres
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres-data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - 5050:80
    environment:
      - PGADMIN_DEFAULT_EMAIL=user@hotmail.com
      - PGADMIN_DEFAULT_PASSWORD=password

volumes:
  postgres-data:
    driver: local
</code></pre>
<p>This is the error while connecting the Postgres from pgadmin view page.</p>
<blockquote>
<p>Unable to connect to server:</p>
<p>could not connect to server: Connection refused Is the server running
on host &quot;localhost&quot; (127.0.0.1) and accepting TCP/IP connections on
port 5432? could not connect to server: Address not available Is the
server running on host &quot;localhost&quot; (::1) and accepting TCP/IP
connections on port 5432?</p>
</blockquote>
<p>or</p>
<p><a href=""https://i.stack.imgur.com/lBcaW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lBcaW.png"" alt=""enter image description here"" /></a></p>
<p>This is the docker-compose log.</p>
<pre><code>db_1            | PostgreSQL init process complete; ready for start up.
db_1            | 
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  starting PostgreSQL 12.5 (Debian 12.5-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 5432
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  listening on IPv6 address &quot;::&quot;, port 5432
db_1            | 2021-02-02 04:57:59.842 UTC [1] LOG:  listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
db_1            | 2021-02-02 04:57:59.867 UTC [76] LOG:  database system was shut down at 2021-02-02 04:57:59 UTC
db_1            | 2021-02-02 04:57:59.877 UTC [1] LOG:  database system is ready to accept connections
pgadmin_1       | NOTE: Configuring authentication for SERVER mode.
pgadmin_1       | 
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Starting gunicorn 19.9.0
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Listening at: http://[::]:5050 (1)
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Using worker: threads
pgadmin_1       | /usr/local/lib/python3.9/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
pgadmin_1       |   return io.open(fd, *args, **kwargs)
pgadmin_1       | [2021-02-02 04:58:10 +0000] [87] [INFO] Booting worker with pid: 87
pgadmin_1       | ::ffff:192.168.80.1 - - [02/Feb/2021:05:34:52 +0000] &quot;GET /browser/ HTTP/1.1&quot; 302 257 &quot;http://localhost:5050/login?next=%2Fbrowser%2F&quot; &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&quot;
pgadmin_1       | ::ffff:192.168.80.1 - - [02/Feb/2021:05:34:52 +0000] &quot;GET /login?next=%2Fbrowser%2F HTTP/1.1&quot; 200 1707 &quot;http://localhost:5050/login?next=%2Fbrowser%2F&quot; &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&quot;
pgadmin_1       | 2021-02-02 05:34:52,568: ERROR    flask.app:  404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.
pgadmin_1       | Traceback (most recent call last):
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1813, in full_dispatch_request
pgadmin_1       |     rv = self.dispatch_request()
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1791, in dispatch_request
pgadmin_1       |     self.raise_routing_exception(req)
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1774, in raise_routing_exception
pgadmin_1       |     raise request.routing_exception
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/ctx.py&quot;, line 336, in match_request
pgadmin_1       |     self.url_adapter.match(return_rule=True)
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/werkzeug/routing.py&quot;, line 1945, in match
pgadmin_1       |     raise NotFound()
pgadmin_1       | werkzeug.exceptions.NotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.
</code></pre>
<p>I don't know what could have been gone wrong.</p>
"
"55123637","Activate conda environment in docker","<python><docker><anaconda><conda>","66020955","How do you create a Docker container that includes a conda environment and all its dependencies?","<windows><docker><dockerfile><conda><environment>","<p>I need to activate environment in docker and run a command in this environment.
I create the environment, but then I try to activate this environment and run the command in this way:</p>

<pre><code>CMD [ ""source activate mro_env &amp;&amp; ipython kernel install --user --name=mro_env"" ]
</code></pre>

<p>but when I ran docker I get an error:</p>

<pre><code>[FATAL tini (8)] exec source activate mro_env &amp;&amp; ipython kernel install 
--user --name=mro_env failed: No such file or directory
</code></pre>

<p>This is the whole Dockerfile:</p>

<pre><code>FROM continuumio/miniconda3

ADD /src/mro_env.yml /src/mro_env.yml
RUN conda env create -f /src/mro_env.yml

# Pull the environment name out of the mro_env.yml
RUN echo ""source activate $(head -1 /src/mro_env.yml | cut -d' ' -f2)"" &gt; ~/.bashrc
ENV PATH /opt/conda/envs/$(head -1 /src/mro_env.yml | cut -d' ' -f2)/bin:$PATH

CMD [ ""source activate mro_env &amp;&amp; ipython kernel install --user --name=mro_env"" ]
</code></pre>
","<p>I'm attempting to create a Docker container for a data science project. The project uses Anaconda and conda environments and I'm working in Windows.</p>
<p>I'm completely new to Docker and have seen tutorials for creating simple containers, but nothing for conda environments that fully explains how the Dockerfile should be constructed. It seems like conda can be tricky due to the need to have conda on the path in order to activate the environment.</p>
<p>This is what I have attempted so far, but Docker doesn't even seem to recognize it as a legitimate Dockerfile:</p>
<pre><code>FROM continuumio/miniconda3

WORKDIR /app

COPY environment.yml ./
COPY . . 

RUN conda env create -f environment.yml


CMD [&quot;conda&quot;, &quot;activate&quot;, &quot;project-environment&quot;]
</code></pre>
<p>What is needed in the Dockerfile to ensure that the container will have all the environment attributes and dependencies and that it will activate the environment when it is run?</p>
"
"56518032","Windows 10 and Docker container logs / Docker Logging driver","<docker><windows-10>","66059094","Docker: ""Cannot access '/var/lib/docker/containers': No such file or directory"" error","<windows><docker><powershell><dockerfile>","<p>I'm using Windows 10 with native docker installation. </p>

<p>I'm looking for the location where docker save the containers logs.<br>
In Linux, the Docker containers log files are in this location:</p>

<pre><code>/var/lib/docker/containers/container-id/container-id-json.log
</code></pre>

<p>But where can I find it in windows 10 ?</p>
","<p>I am trying to copy a file from container to local drive, but I encountered  several errors during this procedure. Later, I realized that the problem is not related to the commands I use, it is related to permission. Because I cannot list the containers directory that I use when trying to copy.</p>
<p>I try to list the <code>/var/lib/docker/containers</code> using PowerShell on Windows:</p>
<pre><code>PS C:\Windows\system32&gt; docker exec 144625ac6917 ls /var/lib/docker/containers
</code></pre>
<p>Gives this error: <strong>ls: cannot access '/var/lib/docker/containers': No such file or directory</strong></p>
<p>How can I fix this problem?</p>
"
"51948084","docker-compose.yml content - How can i avoid ""must be a mapping not a string"" error message?","<docker><docker-compose>","66075689","ERROR: In file './docker-compose.yml', network must be a mapping, not a string","<docker><jenkins>","<p>Problem: the below content returns ""service 'image' must be a mapping not a string."" 
I tried using YAML Parser(<a href=""http://yaml-online-parser.appspot.com/"" rel=""nofollow noreferrer"">http://yaml-online-parser.appspot.com/</a>), but it returned no error. </p>

<pre><code>version: 
     ""2.0""

services:

 blog:

 image: 
  abc/defg
 environment:
  APPLICATION_SECRET:
   82xxxxxxx

  ports: -""9000:9000""
</code></pre>

<p>working version:</p>

<pre><code>version: ""2.1""
services:
 blog:
  image:  abc/defg
  environment:
   APPLICATION_SECRET:
    82xxx
  ports: 
   - ""9000:9000""
networks:
  default:
    external:
      name: nat
</code></pre>
","<p>Geeting below error upon running the docker compose file/jenkins container.</p>
<pre><code>ERROR: In file './docker-compose.yml', network must be a mapping, not a string.
</code></pre>
<p>Below is the yml file:</p>
<pre><code>version: '3'
services:
  jenkins:
    container_name: jenkins
    image: jenkins/jenkins
    ports:
      -&quot;8080&quot;:&quot;8080&quot;
    volume:
      -&quot;$PWD/jenkins_home:/var/jenkins_home&quot;
  networks:
      -net
networks:
   net
</code></pre>
<p>Docker compose verion:</p>
<pre><code>[jenkins@localhost ~]$ docker compose -v
Docker version 20.10.3, build 48d30b5
</code></pre>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","66125189","Java memory leak in non-heap space","<java><jvm><heap-memory><metaspace>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I have a Java application (running under OpenJDK 1.8.0_181) that has memory issues. Specifically after some time all physical memory appears consumed and the app fails to allocate more memory. The difficult part is that it's not the heap memory that causes problems. I'm struggling to understand how to investigate it further as it's not the heap memory.</p>
<p>Here are types of memory crashes I got:</p>
<pre><code>Exception in thread &quot;eXistThread-22&quot; java.lang.OutOfMemoryError: unable to create new native thread
</code></pre>
<p>OR</p>
<pre><code>OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000773200000, 142606336, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 142606336 bytes for committing reserved memory.
</code></pre>
<p>In all such cases the physical memory of the server is completely consumed. Then the app is unable to allocate memory or extend the heap so it crashes.</p>
<p>To start with here are the results of the <code>top</code> command:</p>
<pre><code>KiB Mem :  7867456 total,   454608 free,  6696500 used,   716348 buff/cache
KiB Swap:        0 total,        0 free,        0 used.   933392 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
   18 root      20   0 8827048 6.180g  28028 S   0.0 82.4  24:41.10 java
</code></pre>
<p>The Java app is running with the following memory settings: <code>java -Xms512m -Xmx3072m</code></p>
<p>So the app can use max 3GB of the heap space. However you can see that the java process (RES memory) already uses over 6GB RAM.</p>
<p>At this point it's clear I need to look at non-heap space. There is so-called Metaspace which I thought faces memory leaks. I've done the following checks:</p>
<pre><code># jcmd 18 GC.heap_info
18:
 PSYoungGen      total 984576K, used 647154K [0x0000000780000000, 0x00000007c0000000, 0x00000007c0000000)
  eden space 925696K, 63% used [0x0000000780000000,0x00000007a3e83970,0x00000007b8800000)
  from space 58880K, 99% used [0x00000007b8800000,0x00000007bc179030,0x00000007bc180000)
  to   space 64000K, 0% used [0x00000007bc180000,0x00000007bc180000,0x00000007c0000000)
 ParOldGen       total 1671168K, used 1048981K [0x0000000700000000, 0x0000000766000000, 0x0000000780000000)
  object space 1671168K, 62% used [0x0000000700000000,0x0000000740065488,0x0000000766000000)
 Metaspace       used 2235431K, capacity 2246406K, committed 2252780K, reserved 2981888K
  class space    used 317765K, capacity 320038K, committed 321024K, reserved 1048576K
</code></pre>
<p>Here I see that the used memory is 647154K + 1048981K of heap and 2235431K of metaspace (non-heap). The total is 3931566K = 3840M = 3.75 GB. But what is the rest of memory consumed by the Java process then? It's a lot: 6.18 - 3.75 = 2.43 GB of RAM.</p>
<p>I've also checked more stats with jstat:</p>
<pre><code># jstat -gccapacity 18
 NGCMN    NGCMX     NGC     S0C   S1C       EC      OGCMN      OGCMX       OGC         OC       MCMN     MCMX      MC     CCSMN    CCSMX     CCSC    YGC    FGC
174592.0 1048576.0 1048576.0 64000.0 58880.0 925696.0   349696.0  2097152.0  1671168.0  1671168.0      0.0 2981888.0 2252780.0      0.0 1048576.0 321024.0    289    13
</code></pre>
<p>Here I calculate the entire Java memory using this formula:</p>
<pre><code>NGCMN + S0C + S1C + EC + OGCMN + MC + CCSC = 174592 + 64000 + 58880 + 925696 + 349696 + 2252780 + 321024 = 4146668K = 3.95 GB
</code></pre>
<p>It's slightly more than 3.75 GB calculated from <code>GC.heap_info</code> but still much less that 6.18 GB consumed by the Java process.</p>
<p>So I'm a bit lost and I need an advice:</p>
<ul>
<li>How can I identify which type of Java memory is taking 2.43 GB?</li>
<li>How can I limit that memory or make sure it's cleared timely to avoid OutOfMemory crashes?</li>
<li>Should I set extra parameters like -XX:MaxMetaspaceSize -XX:MaxMetaspaceFreeRatio? If so, what will be the reasonable values for those?</li>
</ul>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","66230719","Laravel Docker For Mac Very Slowly","<php><laravel><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I am following this tutorial (<a href=""https://tighten.co/blog/converting-a-legacy-app-to-laravel/"" rel=""nofollow noreferrer"">https://tighten.co/blog/converting-a-legacy-app-to-laravel/</a>) to migrate a legacy app to Laravel and have made it as far as the &quot;Spring Cleaning&quot; section. My legacy code is in a <code>legacy</code> directory inside my Laravel build.</p>
<p>Our development environment uses Docker Compose to build a container on the host machine (which in my case is a Mac but can be either Windows or Linux as well depending on the developer). The source code is in a volume mounted to the container so that the developer's updates can be seen in the browser as soon as the developer reloads the page.</p>
<p>When I go to get Laravel to load it's default route (the basic route that Laravel builds with) I get page load times in excess of 6 minutes.</p>
<p>I have tried using cached volumes like so: <code>./:/var/www/portal/:cached</code></p>
<p>I have also tried following this tutorial (<a href=""https://www.jeffgeerling.com/blog/2020/revisiting-docker-macs-performance-nfs-volumes"" rel=""nofollow noreferrer"">https://www.jeffgeerling.com/blog/2020/revisiting-docker-macs-performance-nfs-volumes</a>) for using an NFS mount. I got it working but the page load size was still over 6 minutes for the default page.</p>
<p>What is causing this extremely slow page-load time? How are Dockerized Laravel Development Environments supposed to be structured to avoid this issue with Docker's VM on Mac and Windows?</p>
<p>I didn't have this problem when developing just in the legacy app. I'm not inclined to think that Laravel itself is causing so large a slowdown. My thought is that it is the Docker VM running on Mac but I haven't been able to prove that yet.</p>
"
"53010064","Pass environment variable into a Vue App at runtime","<docker><vue.js><kubernetes>","66302043","Vue js base url is not changing after build","<vue.js><webpack><axios><vuejs3>","<p>How can I access environment variables in Vue, that are passed to the container at runtime and not during the build?</p>
<p>Stack is as follows:</p>
<ul>
<li>VueCLI 3.0.5</li>
<li>Docker</li>
<li>Kubernetes</li>
</ul>
<p>There are suggested solutions on stackoverflow and elsewhere to use .env file to pass variables (and using mode) but that's at build-time and gets baked into the docker image.</p>
<p>I would like to pass the variable into Vue at run-time as follows:</p>
<ul>
<li>Create Kubernetes ConfigMap (I get this right)</li>
<li>Pass ConfigMap value into K8s pod env variable when running deployment yaml file (I get this right)</li>
<li>Read from env variable created above eg. VUE_APP_MyURL and do something with that value in my Vue App (I DO NOT get this right)</li>
</ul>
<p>I've tried the following in helloworld.vue:</p>
<pre><code>&lt;template&gt;
&lt;div&gt;{{displayURL}}
  &lt;p&gt;Hello World&lt;/p&gt;
&lt;/div&gt;
&lt;/template&gt;
&lt;script&gt;
export default {  
    data() {
        return {
            displayURL: &quot;&quot;
        }
    },
    mounted() {
        console.log(&quot;check 1&quot;)
        this.displayURL=process.env.VUE_APP_ENV_MyURL
        console.log(process.env.VUE_APP_ENV_MyURL)
        console.log(&quot;check 3&quot;)
    }
}
&lt;/script&gt;
</code></pre>
<p>I get back &quot;undefined&quot; in the console log and nothing showing on the helloworld page.</p>
<p>I've also tried passing the value into a vue.config file and reading it from there. Same &quot;undefined&quot; result in console.log</p>
<pre><code>&lt;template&gt;
&lt;div&gt;{{displayURL}}
  &lt;p&gt;Hello World&lt;/p&gt;
&lt;/div&gt;
&lt;/template&gt;
&lt;script&gt;
const vueconfig = require('../../vue.config');
export default {  
    data() {
        return {
            displayURL: &quot;&quot;
        }
    },
    mounted() {
        console.log(&quot;check 1&quot;)
        this.displayURL=vueconfig.VUE_APP_MyURL
        console.log(vueconfig.VUE_APP_MyURL)
        console.log(&quot;check 3&quot;)
    }
}
&lt;/script&gt;
</code></pre>
<p>With vue.config looking like this:</p>
<pre><code>module.exports = {
    VUE_APP_MyURL: process.env.VUE_APP_ENV_MyURL
}
</code></pre>
<p>If I hardcode a value into VUE_APP_MyURL in the vue.config file it shows successfully on the helloworld page.</p>
<p>VUE_APP_ENV_MyURL is successfully populated with the correct value when I interrogate it: kubectl describe pod </p>
<p>process.env.VUE_APP_MyURL doesn't seem to successfully retrieve the value.</p>
<p>For what it is worth... I am able to use process.env.VUE_APP_3rdURL successfully to pass values into a Node.js app at runtime.</p>
","<p>I am using vue for my frontend and PHP rest api server from my backend.The problem is i cannot be able to change the server address after my vue app build.I tried to create a separate json file in my  public folder and picked the server address from there but after build when i am trying to  change the server address its not taking that address instead its using the same at the time of build.</p>
<p>Here is my json file from public folder ServerConfig.json</p>
<pre><code>{
    &quot;ServerIP&quot;: &quot;http://localhost/vue/backend/&quot;
}
</code></pre>
<p>//HERE I AM TRYING TO PICK THE SERVER ADDRESS IN MY AXOIS REQUESTS</p>
<pre><code>import axios from 'axios';
import router from &quot;../router&quot;;
import Toaster from '../helpers/Toaster'
import ServerConfig from '../../public/ServerConfig.json'

export default () =&gt; {

    const token = localStorage.getItem(&quot;store_token&quot;);
    if (token) {
        axios.defaults.headers.common[&quot;Authorization&quot;] = token;
    }

    // axios.defaults.withCredentials = true;
    const options = {};
    options.baseURL = ServerConfig.ServerIP;
    options.timeout = 15000;
    options.params = {}; // do not remove this, its added to add params later in the config
    const instance = axios.create(options)

    instance.interceptors.request.use(
        config =&gt; {
            return config;
        },
        error =&gt; {
            const toaster = new Toaster()
            toaster.showError('Request Failed! Please try to restart the api server.');
            return Promise.reject(error.request);
        }
    );


    //on successful response
    instance.interceptors.response.use(
        response =&gt; {
            if (response.status === 200 || response.status === 201) {
                return Promise.resolve(response);
            } else {
                return Promise.reject(response);
            }
        },
        error =&gt; {
            if (error.response.status) {
                const toaster = new Toaster();

                switch (error.response.status) {
                    case 400:
                        toaster.showError('Bad Request! Server is rejected the request.');
                        break;
                    case 401:
                        toaster.showError('Unauthorized! Session expired Please try to login again.');
                        break;
                    case 403:
                        toaster.showError('Forbidden! Invalid access.');
                        router.replace({ path: &quot;/login&quot;, params: {} });
                        break;
                    case 404:
                        toaster.showError('Not Found! Page not found on server');
                        break;
                    case 502:
                        toaster.showError('Bad gateway! Got an invalid response');
                        router.replace({ path: &quot;/&quot;, params: {} });
                }
                return Promise.reject(error.response);
            }
            else {
                const toaster = new Toaster();
                toaster.showError('Got an invalid response from server');
            }
        }
    );

    return instance;
}
</code></pre>
<p>I just want to change the server address after build because i have multiple servers and i dont wanna build frontend from every server.I just want to change the ip address from a output file. Thanks</p>
"
"58398715","Docker-Compose can't connect to MySQL","<mysql><django><python-3.x><docker><docker-compose>","66408689","Dockerize Django project with MySQL container","<mysql><django><docker><docker-compose>","<p>I'm trying to connect Django project with MySQL using docker.</p>

<p>I have the problem when upping docker-compose. It says the next error:</p>

<blockquote>
  <p>super(Connection, self).<strong>init</strong>(*args, **kwargs2)
  django.db.utils.OperationalError: (2002, ""Can't connect to MySQL server on 'db' (115)"")</p>
</blockquote>

<p>I'm using port 3307, should i create first new database schema in my local? Or how can I do it because my default port is 3306 but if i try to use it, it says that is busy.</p>

<p>My code here: </p>

<p>Dockerfile</p>

<pre><code>
FROM python:3.7
ENV PYTHONUNBUFFERED 1
ENV LANG=C.UTF-8
RUN mkdir /code
WORKDIR /code
ADD requirements.txt /code/
RUN apt-get update
RUN pip install -r requirements.txt
ADD . /code/


</code></pre>

<p>Docker-compose</p>

<pre><code>
    version: '2'

services:
  app:
    container_name: container_app
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    command: bash -c ""python3 manage.py migrate &amp;&amp; python manage.py shell &lt; backend/scripts/setup.py &amp;&amp; python3 manage.py runserver 0.0.0.0:8000""
    links:
      - db
    depends_on:
      - db
    ports:
      - ""8000:8000""
  db:
    container_name: container_database
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_HOST: 'host.docker.internal'
      MYSQL_DATABASE: 'container_develop'
      MYSQL_USER: 'root'
      MYSQL_PASSWORD: 'password'
      MYSQL_ROOT_PASSWORD: 'password'
    ports:
      - ""3307:3307""

</code></pre>

<p>settings.py:</p>

<pre><code>
DATABASES = {
    'default' : {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'database_develop',
        'USER': 'root',
        'PASSWORD': 'password',
        'HOST': 'db',
        'PORT': 3307,
        'CHARSET': 'utf8',
        'COLLATION': 'utf8_bin',
        'OPTIONS': {
            'use_unicode' : True,
            'init_command': 'SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED',
        },
    }
}

</code></pre>

<p>The thing is that I do not have the database anywhere so I'm using it locally, do I have to upload the db to a server and then use the port that server provides to me?
Is there any way to use it locally from docker? Or installing it to docker? So I can share that docker to a friend and he can use the same DB?</p>
","<p>I have a very simple Django project (v.2.2) with Python (v.3.6.9), Docker (v.20.10.3), and docker-compose (v. 1.28.4) running on Ubuntu 18.04.</p>
<p>I'm trying to dockerize Django project with MySQL container. I did all the migrations and have the .sql file of my database. The project works fine without Docker, however, when I run <em>docker-compose up</em> I got the following error:</p>
<pre><code>django.db.utils.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
</code></pre>
<p>Here's my full configuration: <a href=""https://github.com/aziele/docker-django-mysql"" rel=""nofollow noreferrer"">https://github.com/aziele/docker-django-mysql</a></p>
<p>Briefly, this is my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.9
ENV PYTHONUNBUFFERED 1
COPY ./requirements.txt /requirements.txt
RUN pip install -r /requirements.txt
WORKDIR /app
</code></pre>
<p>And these my <code>requirements.txt</code>:</p>
<pre><code>Django==2.2
mysqlclient==2.0.3
django-mysql
</code></pre>
<p>The <code>docker-compose.yml</code> file:</p>
<pre><code>version: '3'
services:
  db:
    image: mysql:5.7.33
    restart: unless-stopped
    environment:
      MYSQL_DATABASE: 'projectdb1'
      MYSQL_USER: 'user'
      MYSQL_PASSWORD: 'password'
      MYSQL_ROOT_PASSWORD: 'password'
    volumes:
      - ./db/projectdb1.sql:/docker-entrypoint-initdb.d/projectdb1.sql
    ports:
      - '3307:3306'

  django:
    build:
      context: .
      dockerfile: ./Dockerfile-dev
    volumes:
      - ./src:/app
    command: python manage.py runserver 0.0.0.0:8000
    ports:
     - 8080:8000
    restart: always
    depends_on:
      - db
</code></pre>
<p>Also I have change the database configurations in <code>settings.py</code> for this:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'projectdb1',
        'USER': 'root',
        'PASSWORD': 'password',
        'HOST': 'db',
        'PORT': '3307',   
    }
}
</code></pre>
"
"59868898","""kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection"" ONLY DURING LISTING TOPICS","<docker><apache-kafka><docker-compose><apache-zookeeper>","66458973","Exception in thread ""main"" kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING","<apache-kafka>","<p>I found few questions with similar topic but different context: I can connect to create a Topic but I can't list the topics because I got the error mentioned below (as far as I could see, people were facing issues for basic connecting while I am getting only for listing the topic list).</p>
<p>In case it matters, here is my <code>docker-compose.yml</code>:</p>
<pre><code>version: &quot;3&quot;
services:
    zookeeper:
        image: wurstmeister/zookeeper

    kafka:
        image: wurstmeister/kafka
        ports:
            - &quot;9092:9092&quot;
        environment:
            KAFKA_ADVERTISED_HOST_NAME: &quot;localhost&quot;
            KAFKA_ZOOKEEPER_CONNECT: &quot;zookeeper:2181&quot;
</code></pre>
<p>My console:</p>
<pre><code>bash-4.4# kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test2
Created topic test2.
bash-4.4# kafka-topics.sh --list --zookeeper localhost:2181
Exception in thread &quot;main&quot; kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
        at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:259)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
        at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:255)
        at kafka.zookeeper.ZooKeeperClient.&lt;init&gt;(ZooKeeperClient.scala:113)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1858)
        at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:321)
        at kafka.admin.TopicCommand$.main(TopicCommand.scala:54)
        at kafka.admin.TopicCommand.main(TopicCommand.scala)
bash-4.4# kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test3
Created topic test3.
bash-4.4# kafka-topics.sh --list --zookeeper localhost:2181
Exception in thread &quot;main&quot; kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
        at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:259)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
        at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:255)
        at kafka.zookeeper.ZooKeeperClient.&lt;init&gt;(ZooKeeperClient.scala:113)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1858)
        at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:321)
        at kafka.admin.TopicCommand$.main(TopicCommand.scala:54)
        at kafka.admin.TopicCommand.main(TopicCommand.scala)
bash-4.4#
</code></pre>
<h2>Edited</h2>
<p>Future readers may be found useful how I could list all topics straight from my Docker Kafka Container without logging in my Docker Zookeper Container (<a href=""https://stackoverflow.com/a/56595227/4148175"">https://stackoverflow.com/a/56595227/4148175</a>)</p>
<pre><code>C:\Users\&gt;docker exec -it test1_kafka_1 bash

bash-4.4# kafka-topics.sh --list --bootstrap-server localhost:9092
__consumer_offsets
test2
test3
test_topic
bash-4.4# 
</code></pre>
","<p>I'm reiterating the question here - <a href=""https://stackoverflow.com/questions/59868898/kafka-zookeeper-zookeeperclienttimeoutexception-timed-out-waiting-for-connecti"">&quot;kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection&quot; ONLY DURING LISTING TOPICS</a>.</p>
<pre><code>C:\Softwares\Elastic_Kafka\kafka\bin\windows&gt;kafka-topics.bat --zookeeper localhost:9092 --alter --topic quickstart-events --partitions 5
[2021-03-03 19:52:20,897] WARN Client session timed out, have not heard from server in 30001ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)
Exception in thread &quot;main&quot; kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
        at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:262)
        at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:258)
        at kafka.zookeeper.ZooKeeperClient.&lt;init&gt;(ZooKeeperClient.scala:119)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1865)
        at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:360)
        at kafka.admin.TopicCommand$.main(TopicCommand.scala:55)
        at kafka.admin.TopicCommand.main(TopicCommand.scala)
</code></pre>
<p>Here is my error:</p>
<pre><code>C:\Softwares\Elastic_Kafka\kafka\bin\windows&gt;kafka-topics.bat --create --topic quickstart-events --bootstrap-server localhost:9092
Created topic quickstart-events.

C:\Softwares\Elastic_Kafka\kafka\bin\windows&gt;kafka-topics.bat --zookeeper localhost:9092 --alter --topic quickstart-events --partitions 5
[2021-03-03 19:52:20,897] WARN Client session timed out, have not heard from server in 30001ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)
Exception in thread &quot;main&quot; kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
        at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:262)
        at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:258)
        at kafka.zookeeper.ZooKeeperClient.&lt;init&gt;(ZooKeeperClient.scala:119)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1865)
        at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:360)
        at kafka.admin.TopicCommand$.main(TopicCommand.scala:55)
        at kafka.admin.TopicCommand.main(TopicCommand.scala)

C:\Softwares\Elastic_Kafka\kafka\bin\windows&gt;
</code></pre>
"
"65036374","are VOLUME in Dockerfile persistent in kubernetes","<docker><kubernetes><dockerfile><docker-volume><persistent-volumes>","65074472","List Kubernetes containers that use docker local volumes","<docker><kubernetes><docker-volume><kubernetes-statefulset>","<p>Some Dockerfile have a <a href=""https://docs.docker.com/engine/reference/builder/#volume"" rel=""nofollow noreferrer"">VOLUME</a> command.</p>
<p>What happens when such containers are deployed in Kubernetes, but no kubernetes volume are provided: no persistent volume (PV), nor persistent volume claim (PVC) ?</p>
<p>Where are the file stored ?</p>
<p>Is the volume persistent ?</p>
<hr />
<p>For exemple, Dockerfile image for Docker's <a href=""https://github.com/docker-library/postgres/blob/master/13/Dockerfile"" rel=""nofollow noreferrer"">library/postgreSQL</a> container image has:</p>
<pre><code>    VOLUME /var/lib/postgresql/data
</code></pre>
<p>The <a href=""https://github.com/helm/charts/blob/master/stable/postgresql/templates/statefulset.yaml"" rel=""nofollow noreferrer"">stable/postgresql</a> helm charts won't always create a PV:</p>
<pre class=""lang-yaml prettyprint-override""><code>kind: StatefulSet
### SNIP SNIP ###
      containers:
        - name: {{ template &quot;postgresql.fullname&quot; . }}
          image: {{ template &quot;postgresql.image&quot; . }}
### SNIP SNIP ###
          volumeMounts:
            {{ if .Values.persistence.enabled }}
            - name: data
              mountPath: {{ .Values.persistence.mountPath }}
              subPath: {{ .Values.persistence.subPath }}
{{- end }}
### SNIP SNIP ###
{{- if and .Values.persistence.enabled .Values.persistence.existingClaim }}
        - name: data
          persistentVolumeClaim:
{{- with .Values.persistence.existingClaim }}
            claimName: {{ tpl . $ }}
{{- end }}
{{- else if not .Values.persistence.enabled }}
        - name: data
          emptyDir: {}
{{- else if and .Values.persistence.enabled (not .Values.persistence.existingClaim) }}
  volumeClaimTemplates:
    - metadata:
        name: data
      {{- with .Values.persistence.annotations }}
        annotations:
        {{- range $key, $value := . }}
          {{ $key }}: {{ $value }}
        {{- end }}
      {{- end }}
      spec:
        accessModes:
        {{- range .Values.persistence.accessModes }}
          - {{ . | quote }}
        {{- end }}
        resources:
          requests:
            storage: {{ .Values.persistence.size | quote }}
        {{ include &quot;postgresql.storageClass&quot; . }}
{{- end }}
</code></pre>
","<p>When deploying a workload that has a <code>VOLUME</code> in a Dockerfile, that volume may not be mapped to a persistent volume (PV/PVC) in Kubernetes.</p>
<p>Actually, unless a Kubernetes volume is attached to that workload, The docker daemon container will temporarily create a docker-volume when starting the container (<a href=""https://docs.docker.com/storage/volumes/#use-a-volume-driver"" rel=""nofollow noreferrer"">driver</a> type: <code>local</code>). Kubernetes won't be aware of it. See: <a href=""https://stackoverflow.com/q/65036374/1260896"">are VOLUME in Dockerfile persistent in kubernetes</a>. This docker volume will be destroyed when the pod is removed or redeployed.</p>
<p>It is certainly good practice to use a kubernetes volume, even <a href=""https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/"" rel=""nofollow noreferrer"">ephermeal volumes</a> (or <a href=""https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes"" rel=""nofollow noreferrer"">generic ephemeral volumes</a>... still in alpha in 1.19)</p>
<p><strong>Q:</strong> How to list pods/containers that use such local volumes?</p>
<p>This is really important since restarting the workload/deployment/stateful-set will cause disruption (lost of ephemeral volume).</p>
"
"65090971","docker daemon unexpectedly exits","<docker>","65092848","Docker daemon stops on multiple systems at the same time","<docker><crash><daemon>","<p>Docker exited for no apparent reason and I'm trying to understand what happened.</p>
<p>Right now the docker daemon is a loaded service, inactive with an exit status 0 SUCCESS.</p>
<pre><code>$ systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
   Active: inactive (dead) since Tue 2020-12-01 06:25:16 UTC;
     Docs: https://docs.docker.com
 Main PID: 2218 (code=exited, status=0/SUCCESS)
</code></pre>
<p>Looking at the docker logs, this happened because it processed signal 'terminated':</p>
<pre><code>$ journalctl -u docker.service | tail -25
Nov 30 18:30:21 ip-10-38-4-210 dockerd[2218]: time=&quot;2020-11-30T18:30:21.728694550Z&quot; &lt;redacted irrelevant&gt;
Dec 01 06:25:05 ip-10-38-4-210 systemd[1]: Stopping Docker Application Container Engine...
Dec 01 06:25:05 ip-10-38-4-210 dockerd[2218]: time=&quot;2020-12-01T06:25:05.867748396Z&quot; level=info msg=&quot;Processing signal 'terminated'&quot;
Dec 01 06:25:16 ip-10-38-4-210 systemd[1]: Stopped Docker Application Container Engine.
</code></pre>
<p>No user was logged in on that host at that time, no user explicitly terminated the docker daemon.</p>
<ul>
<li>Why is this happening, what logs could provide a clue?</li>
<li>Can I configure dockerd to restart automatically upon exits?</li>
</ul>
<p>This is Docker version 18.09.7 (build 2d0083d) on Ubuntu 16.04.6 LTS on x86-64.</p>
<p>The relationship with systemd is being asked on Unix.stackexchange <a href=""https://unix.stackexchange.com/questions/622355/systemd-stopped-docker-daemon-for-no-obvious-reason"">here</a></p>
","<p>I have a situation where multiple docker daemons stopped on separate AWS instances in relatively close time frame (all in the same 45 minutes, two in the same 10 minutes).</p>
<p>There appears</p>
<p>Stopping Docker Application Container Engine...</p>
<p>in the logs without any previous event.</p>
<p>It is followed by</p>
<p>level=info msg=&quot;Processing signal 'terminated'&quot;
level=info msg=&quot;ignoring event&quot; module=libcontainerd namespace=moby topic=/tasks/delete type=&quot;*events.TaskDelete&quot;</p>
<p>and finally</p>
<p>Stopped Docker Application Container Engine.</p>
<p>after what Docker Daemon has to be started manually.</p>
<p>It happened before in one system but now it appeared in 3 systems simultaneously.</p>
<p>Application logs do not indicate anything suspicious.</p>
<p>Does anybody have similar experience? I am wondering at the moment what steps I should take to have more information during next such event.</p>
"
"65825025","Docker and Kafka Connection Issue Kafka-Python","<docker><apache-kafka>","65902677","BrokerConnect Issue","<python><docker><apache-kafka>","<p>I have a kafka cluster running in docker which I create with the .yml file below:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: &quot;confluentinc/cp-zookeeper:5.4.0&quot;
    hostname: zookeeper
    ports:
      - '32181:32181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafka:
    image: &quot;confluentinc/cp-enterprise-kafka:5.4.0&quot;
    hostname: kafka
    ports:
      - '9092:9092'
      - '29092:29092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  schema-registry:
    image: &quot;confluentinc/cp-schema-registry:latest&quot;
    hostname: schema-registry
    depends_on:
      - zookeeper
      - kafka
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafdrop:
    image: &quot;obsidiandynamics/kafdrop&quot;
    ports:
      - '9000:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: &quot;-Xms32M -Xmx64M&quot;
      SERVER_SERVLET_CONTEXTPATH: &quot;/&quot;
    depends_on:
      - kafka

</code></pre>
<p>I then have a simple producer not in Docker just on my machine that I am trying to produce messages to the cluster. The producer file is:</p>
<pre><code>from kafka import KafkaProducer
import time


topicName = 'myTestTopic'

producer = KafkaProducer(bootstrap_servers=['localhost:9092', 'kafka:29092'])


while True:
    message = producer.send(topicName, value=b'Hello World')

    metadata = message.get()
    print(metadata.topic)
    print(metadata.partition)
    time.sleep(5)
</code></pre>
<p>When I run the file and the cluster is running on docker all I get is this error</p>
<pre><code>    raise Errors.NoBrokersAvailable()
kafka.errors.NoBrokersAvailable: NoBrokersAvailable
</code></pre>
<p>Any help to resolve this issue would be great</p>
","<p>I have a kafka producer written in python that I have added to docker-compose.yml</p>
<p>Producer:</p>
<pre><code>import os, csv, avro.schema, json
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter
from kafka import KafkaProducer
from collections import namedtuple

output_loc = '{}/avro.avro'.format(os.path.dirname(__file__))
CSV = '{}/oscar_age_male.csv'.format(os.path.dirname(__file__))
fields = (&quot;Index&quot;,&quot;Year&quot;, &quot;Age&quot;, &quot;Name&quot;, &quot;Movie&quot;)
csv_record = namedtuple('csv_record', fields)

p = KafkaProducer(bootstrap_servers = ['localhost:9092', 'kafka:9092'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))

def read_csv(path):
    with open(path, 'rU') as data:
        data.readline()
        reader = csv.reader(data, delimiter=&quot;,&quot;)
        for row in map(csv_record._make, reader):
            yield row

def parse_schema(path='{}/schema.avsc'.format(os.path.dirname(__file__))):
    with open(path, 'r') as data:
        return avro.schema.parse(data.read())

def serilialise_records_and_send(records, outpath=output_loc):
    schema = parse_schema()
    with open(outpath, 'w') as out:
        writer = DataFileWriter(out, DatumWriter(), schema)
        for record in records:
            record = dict((f, getattr(record, f)) for f in record._fields)
            writer.append(record)
            msg = p.send(topic='test', value=record)
            metadt = msg.get()
            print(metadt.topic)
            print(metadt.partition)

serilialise_records_and_send(read_csv(CSV))
</code></pre>
<p>when I run the docker-compose my producer image fails due to no brokers available.</p>
<p>can anyone enlighten me as to why the brokers aren't available?</p>
<p>when I run the producer locally from IDE I can connect so unsure what's missing</p>
<p>docker-compose.yml</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: &quot;confluentinc/cp-zookeeper:5.4.0&quot;
    hostname: zookeeper
    ports:
      - '32181:32181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafka:
    image: &quot;confluentinc/cp-enterprise-kafka:5.4.0&quot;
    hostname: kafka
    ports:
      - '9092:9092'
      - '29092:29092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  schema-registry:
    image: &quot;confluentinc/cp-schema-registry:latest&quot;
    hostname: schema-registry
    depends_on:
      - zookeeper
      - kafka
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*' 
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

      
  kafdrop:
    image: &quot;obsidiandynamics/kafdrop&quot;
    ports:
      - '9000:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: &quot;-Xms32M -Xmx64M&quot;
      SERVER_SERVLET_CONTEXTPATH: &quot;/&quot;
    depends_on:
      - kafka
  
  producer:
    image: &quot;producer&quot;
    ports: 
      - '5000:5000'
    environment: 
      KAFKA_BROKERCONNECT: kafka:29092
    depends_on: 
      - kafka
</code></pre>
"
"65932112","The Name of Hyperledger Fabric Test Network is not detected by an Application given in the samples","<docker><hyperledger-fabric><hyperledger><docker-network>","66042192","Error: endorsement failure during invoke. response: status:500 message:""error in simulation: failed to execute transaction","<hyperledger>","<p>I just reinstalled Fabric Samples v2.2.0 from Hyperledger Fabric repository according to the <a href=""https://hyperledger-fabric.readthedocs.io/en/release-2.3/install.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>But when I try to run <code>asset-transfer-basic</code> application located in <code>fabric-samples/asset-transfer-basic/application-javascript</code> directory by running <code>node app.js</code> the wallet is created and an admin and user is registered. But then it tries to invoke the function as given in <code>app.js</code> and shows this error</p>
<pre><code>error: [Transaction]: Error: No valid responses from any peers. Errors:
    peer=peer0.org1.example.com:7051, status=500, message=error in simulation: failed to execute transaction 
aa705c10403cb65cecbd360c13337d03aac97a8f233a466975773586fe1086f6: could not launch chaincode basic_1.0:b359a077730d7
f44d6a437ad49d1da951f6a01c6d1eed4f85b8b1f5a08617fe7: error starting container: error starting container:
 API error (404): network _test not found
</code></pre>
<p><a href=""https://i.stack.imgur.com/ApusC.png"" rel=""nofollow noreferrer"">Response of a transaction to invoke a function</a></p>
<p>This error never occured before. But somehow after reinstalling docker and Hyperledger Fabric <code>fabric-samples</code> it never seems to find the network <code>_test</code>.</p>
<p>N.B. : Before reinstalling name of the network was <code>net_test</code>. But now when I try <code>docker network ls</code> it shows a network called <code>docker_test</code>. I am using Windows Subsystem for Linux (WSL) version 1.</p>
<pre><code>NETWORK ID     NAME          DRIVER    SCOPE
b7ac05456f46   bridge        bridge    local
acaa5856b871   docker_test   bridge    local
866f58b9078d   host          host      local
4812f94efb15   none          null      local
</code></pre>
<p>How can I fix the issue occurring when I try to run the application?</p>
","<pre><code>Error: endorsement failure during invoke. response: status:500 message:&quot;error in simulation: failed to execute transaction 4a5ed9609bd32b32c59d3208f93cb746db489f850040fa6e6f2853c21a18084e: 
could not launch chaincode basic_1.0:4ec191e793b27e953ff2ede5a8bcc63152cecb1e4c3f301a26e22692c61967ad: error starting container: error starting container: API error (404): network _test not found&quot;
</code></pre>
<p>When i try to initialize the ledger with assets i got above error. I'm new to blockchain, i have no idea why i received such error.</p>
"
"66456627","Docker image run in m1 processor","<mysql><macos><docker><docker-compose><apple-m1>","66468991","What is the correct Docker Baseimage for a manylinux1_x86_64.whl?","<python><linux><docker><pip><gurobi>","<p>I can only play in my macbook air m1 with docker preview and i can't run an image of mysql with version 8.0.22 through a docker-compose file.</p>
<p><a href=""https://i.stack.imgur.com/z20rc.png"" rel=""nofollow noreferrer"">docker-compose set</a></p>
<p>The command i run is : <code>docker-compose up -d mysql</code></p>
<p>How can I solve this problem?</p>
","<p>I'm trying to pip install gurobipy in my docker. When I used pip install with the given index <code>pip install -i https://pypi.gurobi.com gurobipy</code>, i got a &quot;no matching distribution found&quot; error when building the image. So I decided to use the correct wheel file directly. This left me with <code>ERROR: gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl is not a supported wheel on this platform.</code></p>
<ul>
<li><p>I tried using <code>python:3.6</code>, <code>python:3.8</code> and ubuntu latest with python3 as base images with the corresponding .whl files</p>
<ul>
<li>Besides <code>pip install -i *url*</code>, I used the following .whl files for the corresponding python versions(all can be found here: <a href=""https://pypi.gurobi.com/gurobipy/"" rel=""nofollow noreferrer"">https://pypi.gurobi.com/gurobipy/</a>):
<ul>
<li>python:3.8: gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl</li>
<li>python:3.6: gurobipy-9.1.1-cp36-cp36m-manylinux1_x86_64.whl</li>
</ul>
</li>
</ul>
</li>
<li><p>I checked the python architecture - it's 64. Bit. When I used <code>pip install -i *url*</code> in my pycharm Python 3.8 venv (64-Bit), everything worked out perfectly fine.</p>
</li>
<li><p>The directorys inside the .whl file can be found in the attached screenshot.<a href=""https://i.stack.imgur.com/sQn1Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sQn1Y.png"" alt=""file structure inside whl file"" /></a></p>
</li>
</ul>
<p>A similar issue was adressed at <a href=""https://stackoverflow.com/questions/54945866/montagepy-1-0-1-cp36-cp36m-manylinux1-x86-64-whl-is-not-a-supported-wheel-on-thi"">MontagePy-1.0.1-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform</a> without a sufficient anwser.</p>
<p>Gurobi is widely used and I couldn't find others with similar problems regarding gurobi, so I'm pretty sure i messed something up in my dockerfile. Am I using the wrong baseimage, Python version or missing any requirements?</p>
<p>I'm using the following dockerfile and requirements.</p>
<p>Dockerfile:
The outcommented lines i usually run in the terminal of the running container.</p>
<pre><code>FROM python:3.8

WORKDIR /

COPY . ./Wueexam_optimization
RUN pip install --upgrade pip
RUN pip install -r Wueexam_optimization/optimization/requirements.txt
#RUN pip install --extra-index-url https://pypi.gurobi.com gurobipy
#RUN pip install https://pypi.gurobi.com/gurobipy/gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl

ENV PYTHONPATH /

WORKDIR /Wueexam_optimization/optimization
CMD [&quot;python3&quot;,&quot;-u&quot;,&quot;app.py&quot;]
</code></pre>
<p>Requirements:</p>
<pre><code>flask
flask-cors
flask-sqlalchemy
Flask-SQLAlchemy
numpy == 1.19.3
pandas
mysqlclient
openpyxl
pulp
pymysql
cryptography

</code></pre>
"