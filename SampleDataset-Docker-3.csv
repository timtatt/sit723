ParentPostId,ParentPostTitle,ParentTags,Id,Title,Tags,ParentBody,Body
"6850500","Postgis installation: type ""geometry"" does not exist","<postgresql><geometry><postgis>","28417409","Type ""geometry"" does not exists","<postgresql><docker><gis><geospatial><postgis>","<p>I am trying to create table with Postgis. I do it by this <a href=""http://postgis.refractions.net/documentation/manual-1.5/ch02.html#id2619431"" rel=""noreferrer"" title=""official documentation"">page</a>. But when I import postgis.sql file, I get a lot of errors:</p>

<pre><code>ERROR:  type ""geometry"" does not exist
</code></pre>

<p>Does anybody know how can I fix it?</p>
","<p>I'm trying to construct a Docker image with a PostgreSQL and PostGIS database for one of our products. Although constructing the image (and container) isn't that hard and works, based on off <a href=""https://github.com/helmi03/docker-postgis"" rel=""nofollow"">another PostGIS image</a>. However when adding the ddl for constructing the database I'm running into issues.</p>

<p>I've constantly get the message: <code>ERROR:  type ""geometry"" does not exist</code>. Because of this I've read and read on the internet and the main root seemed to be that I didn't properly initialize PostGIS Extensions. So I did that on a template table and running <code>SELECT postgis_full_version()</code> returns <code>POSTGIS=""2.1.3 r12547"" GEOS=""3.4.2-CAPI-1.8.2 r3921"" PROJ=""Rel. 4.8.0, 6 March 2012"" GDAL=""GDAL 1.10.1, released 2013/08/26"" LIBXML=""2.9.1"" LIBJSON=""UNKNOWN"" TOPOLOGY RASTER</code> which seems to be alright. However still no dice...</p>

<p>Also after checking the available datatypes in the public schema via <code>\dT</code> I get that <code>geometry</code> type is available. That's why I'm a bit clueless here, since everything seems fine.</p>

<p>Anybody have any ideas how I can resolve this issue, or what I could look for?</p>

<p>Used versions:</p>

<ul>
<li>Ubuntu 14.04 (Trusty)</li>
<li>PostgreSQL version: 9.3</li>
<li>PostGIS version: 2.1</li>
</ul>

<p>Edit: I'm trying again and tried to creating the extensions, than stopping PostgreSQL, running <code>ldconfig</code> (to reload lib's) and starting PostgreSQL again, but the same issue still persists.</p>
"
"27937185","Assign static IP to Docker container","<docker>","33545247","Docker – fix service IP addresses","<docker><docker-compose>","<p>I'm now trying to assign a static IP 172.17.0.1 when a Docker container be started up. </p>

<p>I use port 2122 as the ssh port of this container so that I let this container listen port 2122.</p>

<pre><code>sudo docker run -i -t -p 2122:2122 ubuntu
</code></pre>

<p>This command will run a Docker container with a random IP like 172.17.0.5, but I need to assign a specific IP to the container.</p>

<p>The following shell script is what I reference Docker documentation in advanced network settings.</p>

<pre><code>pid=$(sudo docker inspect -f '{{.State.Pid}}' &lt;container_name&gt; 2&gt;/dev/null)
sudo rm -rf /var/run/netns/*
sudo ln -s /proc/$pid/ns/net /var/run/netns/$pid
sudo ip link add A type veth peer name B
sudo brctl addif docker0 A
sudo ip link set A up
sudo ip link set B netns $pid
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link delete eth0
sudo ip netns exec $pid ip link set dev B name eth0
sudo ip netns exec $pid ip link set eth0 address 12:34:56:78:9a:bc
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link set eth0 up
sudo ip netns exec $pid ip addr add 172.17.0.1/16 dev eth0
sudo ip netns exec $pid ip route add default via 172.17.42.1
</code></pre>

<p>This shell script will assign a static IP 172.17.0.1 and link to the world fine. But whenever I try to ssh to this container from my local, it didn't work. What's the problem possibly I met?</p>
","<p>I have a docker-compose setup with a bunch of backend services (postgres, redis, ...), a few apps (rails, node, ...) and an nginx on top of it. </p>

<p>The apps are connected to the databases using docker env variables (e.g. <code>DOCKERCOMPOSEDEMO_POSTGRES_1_PORT_5432_TCP_ADDR</code>), and the nginx is connected to the apps using the docker generated <code>/etc/hosts</code>: (e.g. <code>upstream nodeapp1-upstream { server dockercomposedemo_node_app1_1:3000; }</code>)</p>

<p>The problem is that each time I restart some service it gets a new IP address, and thus everything on top of it can't connect to it any more, so restarting a rails app requires to restart nginx, and restarting a database requires to restart the apps and the nginx.</p>

<p>Am I doing somethings wrong, or is it the intended behaviour? Always restarting all that stuff doesn't look like a good solution.</p>

<p>Thank you</p>
"
"9689793","Can't execute jar- file: ""no main manifest attribute""","<java><jar><manifest><main>","31079375","Unable to run docker image - no main manifest attribute, in app.jar","<java><maven><docker>","<p>I have installed an application, when I try to run it (it's an executable jar) nothing happens. When I run it from the commandline with: </p>

<blockquote>
  <p>java -jar ""app.jar""</p>
</blockquote>

<p>I get the following message:</p>

<blockquote>
  <p>no main manifest attribute, in ""app.jar""</p>
</blockquote>

<p>Normally, if I had created the program myself, I would have added a main class attribute to the manifest file. But in this case, since the file is from an application, i cannot do that. I also tried extracting the jar to see if I could find the main class, but there are to many classes and none of them has the word ""main"" in it's name. There must be a way to fix this because the program runs fine on other systems.</p>
","<p>Unable to run a docker image built. When I run with the command</p>

<pre><code>docker run -p 8080:8080 -t {image prefix name}/{image name}
</code></pre>

<p>I get a message <code>""no main manifest attribute, in app.jar""</code></p>

<p>Using docker-maven-plugin and tried maven-assembly-plugin with main class called out explicitly. </p>

<p><code>docker version Boot2Docker 1.7.0</code></p>
"
"27721178","Is it possible to create a docker container that contains one or more containers?","<docker>","38541167","Docker inside Docker (without command line)","<bash><docker><dockerfile><docker-image>","<p>I want to create a docker container which contains one or more containers.
Is it possible with Docker?</p>
","<p>I want to run docker daemon inside my docker image. I know I can easily achieve this by exposing outer docker process during running my docker image, but unfortunately I have no possibility to control how my image is run. I CAN NOT use DIND.</p>

<p>Therefore I would like to start docker service inside either Dockerfile or some script that is my entrypoint. I tried invoking command:</p>

<pre><code>CMD service docker start
</code></pre>

<p>after downloading and installing docker in the image and also including this command in some script <code>start.sh</code> and calling it:</p>

<pre><code>ENRYPOINT [""path/to/script/start.sh""]
</code></pre>

<p>None of this worked. Is it at all possible to run docker as I want or am I doing something incorrectly or it is just not possible in my execution environment?</p>
"
"29746304","ProcessBuilder and Process.waitFor(), how long does it wait?","<java><process><console><wait><processbuilder>","38678080","No Process output from Java ProcessBuilder on Docker","<java><docker><docker-compose>","<p>I am executing an .exe-file from java, using the <a href=""https://docs.oracle.com/javase/8/docs/api/java/lang/ProcessBuilder.html"" rel=""nofollow noreferrer"">ProcessBuilder</a> class and the <a href=""https://docs.oracle.com/javase/8/docs/api/java/lang/Process.html"" rel=""nofollow noreferrer"">Process</a> class. To explain what I am doing:</p>

<pre><code> builder = new ProcessBuilder(commands);
 builder.redirectErrorStream(true);
 Process process = builder.start();
 process.waitFor();
</code></pre>

<p>I just wanted to know, for how long is ""waitFor()"" waiting? Is it waiting until my .exe is executed, or is it waiting till its execution is finished?</p>

<p>My .exe is a compiled AutoIt-script. That means, that there could be interactions like mouse movements, which take some time. 
So I need to know if my Java-code execution goes on after calling the .exe or if it is really waiting for it. </p>

<p>What I want to achieve is the rotational execution of two scripts, but I'm afraid, that my Java code is executing the second script while the first one is still running.
Has anyone a workaround for this? I am glad for any ideas.</p>
","<p>I'm trying to start a process (dstat system-tool) from inside a Java application running on Docker.
Running the app on my local machine, the following code works:</p>

<pre><code>LOG.debug(""Start DSTAT data collecting..."");
try {
    final ProcessBuilder processBuilder = new ProcessBuilder(DSTAT_COMMAND);
    processBuilder.redirectErrorStream(true);
    final Process process = processBuilder.start();
    try (BufferedReader processOutputReader = new BufferedReader(new InputStreamReader(process.getInputStream()))) {
        final String dstatResult = processOutputReader.lines()
                    .map(String::toString)
                    .collect(Collectors.joining(System.lineSeparator()));
        final boolean exitCode = process.waitFor(10, TimeUnit.SECONDS);
        LOG.debug(""Dstat exit: {}"", exitCode);
        LOG.debug(""Dstat output: {}"", dstatResult);
}
</code></pre>

<p>But starting the same with Docker there is no output from the dstat-process, my log is:</p>

<p>2016-07-31 08:31:56.984 DEBUG 15 --- [lector-thread-1] i.thesis.collector.dstat.DstatCollector  : Start DSTAT data collecting...
2016-07-31 08:31:57.190 DEBUG 15 --- [lector-thread-1] i.thesis.collector.dstat.DstatCollector  : Dstat exit: true
2016-07-31 08:31:57.190 DEBUG 15 --- [lector-thread-1] i.thesis.collector.dstat.DstatCollector  : Dstat output: </p>

<p>I also tried Runtime.getRuntime().exec(""dstat""), what causes hanging at</p>

<pre><code>2016-07-30 19:18:41.199 DEBUG 11 --- [lector-thread-1] i.thesis.collector.dstat.DstatCollector  : Start DSTAT data collecting...
</code></pre>

<p>The dstat tool is correctly installed on the container, I can docker exec into, run the dstat command and get the expected output.</p>

<p>For completeness the Dockerfile:</p>

<pre><code>FROM java:8-jre-alpine

RUN apk add --no-cache bash snappy

ARG FLINK_VERSION=1.0.3
ARG HADOOP_VERSION=27
ARG SCALA_VERSION=2.11

RUN set -x &amp;&amp; \
  apk --update add --virtual build-dependencies curl &amp;&amp; \
  # install dstat as collector source
  apk add dstat --update-cache --repository http://dl-3.alpinelinux.org/alpine/edge/testing/ --allow-untrusted &amp;&amp; \
  curl -s $(curl -s https://www.apache.org/dyn/closer.cgi\?as_json\=1 | \
  awk '/preferred/ {gsub(/""/,""""); print $2}')flink/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala_${SCALA_VERSION}.tgz | \
  tar xvz -C /usr/local/ &amp;&amp; \
  ln -s /usr/local/flink-$FLINK_VERSION /usr/local/flink &amp;&amp; \
  sed -i -e ""s/echo \$mypid &gt;&gt; \$pid/echo \$mypid &gt;&gt; \$pid \&amp;\&amp; wait/g"" /usr/local/flink/bin/flink-daemon.sh &amp;&amp; \
  apk del build-dependencies &amp;&amp; \
  rm -rf /var/cache/apk/*

  ADD collector-client-app.jar /usr/local/collector/collector-client-app.jar
  EXPOSE 9091

  ENV FLINK_HOME /usr/local/flink
  ENV PATH $PATH:$FLINK_HOME/bin
  ENV FLINK_JMX_PORT 9999

  ADD docker-entrypoint.sh $FLINK_HOME/bin/
  ENTRYPOINT [""docker-entrypoint.sh""]
  CMD [""sh"", ""-c""]
</code></pre>

<p>docker-entrypoint.sh:</p>

<pre><code>if [ ""$1"" = ""jobmanager"" ]; then
    echo ""Starting Job Manager""
    sed -i -e ""s/jobmanager.rpc.address: localhost/jobmanager.rpc.address: `hostname -f`/g"" $FLINK_HOME/conf/flink-conf.yaml
    sed -i -e ""s/taskmanager.numberOfTaskSlots: 1/taskmanager.numberOfTaskSlots: `grep -c ^processor /proc/cpuinfo`/g"" $FLINK_HOME/conf/flink-conf.yaml
    echo ""env.java.opts: \""-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=$FLINK_JMX_PORT -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false \"""" &gt;&gt; $FLINK_HOME/conf/flink-conf.yaml
    echo ""config file: "" &amp;&amp; grep '^[^\n#]' $FLINK_HOME/conf/flink-conf.yaml
    $FLINK_HOME/bin/jobmanager.sh start cluster &amp;
elif [ ""$1"" = ""taskmanager"" ]; then
    echo ""Starting Task Manager""
    echo ""config file: "" &amp;&amp; grep '^[^\n#]' $FLINK_HOME/conf/flink-conf.yaml
    $FLINK_HOME/bin/taskmanager.sh start &amp;
else
    $@
fi
echo ""Start collector client...""
java -Djava.security.egd=file:/dev/./urandom -jar /usr/local/collector/collector-client-app.jar
</code></pre>

<p>and docker-compose.yml</p>

<pre><code>version: '2'
services:
    flink-jobmanager:
    image: flink
    container_name: flink-jobmanager
    ports:
      - ""8081:8081""
      - ""9999:9999""
      - ""9095:9091""
    command: jobmanager
</code></pre>

<p>It probably has something to do with docker's process-stdout-or-whatever handling, but I couldn't find anything helpful yet.</p>

<p>So thanks in advance for any ideas or suggestions!</p>
"
"31881904","Docker follow symlink outside context","<docker><symlink><dockerfile><symlink-traversal>","38600604","Follow Symbolic link when creating dockerfile","<docker><symlink><dockerfile>","<p>Yet another Docker symlink question. I have a bunch of files that I want to copy over to all my Docker builds. My dir structure is:</p>

<pre><code>parent_dir
    - common_files
        - file.txt
    - dir1
        - Dockerfile  
        - symlink -&gt; ../common_files
</code></pre>

<p>In above example,  I want file.txt to be copied over when I docker build inside dir1. But I don't want to maintain multiple copies of file.txt. 
Per <a href=""https://github.com/docker/docker/blob/master/CHANGELOG.md#0100-2014-04-08"">this link, as of docker version 0.10</a>, docker build must </p>

<blockquote>
  <p>Follow symlinks inside container's root for ADD build instructions.</p>
</blockquote>

<p>But I get <strong>no such file or directory</strong> when I build with either of these lines in my Dockerfile: </p>

<p><code>ADD symlink /path/dirname</code> or
<code>ADD symlink/file.txt /path/file.txt</code></p>

<p>mount option will NOT solve it for me (cross platform...).
I tried <code>tar -czh . | docker build -t</code> without success.</p>

<p>Is there a way to make Docker follow the symlink and copy the common_files/file.txt into the built container?</p>
","<p>I have wrote a Dockerfile to create a new node.js container with given code:</p>

<pre><code>FROM node:latest
RUN mkdir -p //usr/src/app
WORKDIR /usr/src/app
COPY . /usr/src/app
RUN npm install
EXPOSE 8080
EXPOSE 5858
CMD [ ""npm"", ""start"" ]
</code></pre>

<p>nothing complicated. But now I add a symbolic link to the folder, which should added to container. But if I try to run the container I get a error message that a folder is not found. It is same I added as symbolic link.</p>

<p>Is there a possibility to keep link and tell docker to follow link and copy files to docker container?</p>
"
"33884870","How can I get a Docker image's label if the label name has a ""."" in it?","<templates><go><docker>","36439800","get label value from docker inspect","<docker><docker-compose><jq>","<p>The docker inspect command can be very useful for getting the labels on a Docker image:</p>

<pre><code># -*- Dockerfile -*-
FROM busybox
LABEL foo=""bar""
LABEL com.wherever.foo=""bang""
</code></pre>

<p>For simple label names, the inspect command has a <code>--format</code> option (which uses Go templates) that works nicely.</p>

<pre><code>$ docker build -t foo .
$ docker inspect -f '{{ .Config.Labels.foo }}' foo
bar
</code></pre>

<p>But how do I access labels that have a dot in their name?</p>

<pre><code>$ docker inspect -f '{{ .Config.Labels.com.wherever.foo }}' foo
&lt;no value&gt;
</code></pre>

<p>I'm writing this in a bash script, where I'd like to avoid re-parsing the JSON output from <code>docker inspect</code>, if possible.</p>
","<p>I had problem to get the value from the map list due to the key has ""."" inside.</p>

<pre>
docker inspect jenkins
...
  Config: {
  ..
      ""Labels"": {
            ""com.docker.compose.config-hash"": ""85bcf1e0bcd708120185a303e2a8d8e65543c1ec77ec0c6762fc057dc10320aa"",
            ""com.docker.compose.container-number"": ""1"",
            ""com.docker.compose.oneoff"": ""False"",
            ""com.docker.compose.project"": ""new"",
            ""com.docker.compose.service"": ""sc2"",
            ""com.docker.compose.version"": ""1.5.2""
        }
    }
}
</pre>

<p>I can get map list </p>

<pre>
docker inspect -f {{.Config.Labels}} new_sc2_1
map[com.docker.compose.config-hash:85bcf1e0bcd708120185a303e2a8d8e65543c1ec77ec0c6762fc057dc10320aa com.docker.compose.container-number:1 com.docker.compose.oneoff:False com.docker.compose.project:new com.docker.compose.service:sc2 com.docker.compose.version:1.5.2]
</pre>

<p>But how can I get the project name <code>new</code> from key <code>com.docker.compose.project</code></p>

<pre><code>docker inspect -f {{.Config.Labels.com.docker.compose.project}} new_sc2_1
&lt;no value&gt;
</code></pre>
"
"16047306","How is Docker different from a virtual machine?","<docker><containers><virtual-machine><virtualization>","29096967","What are the differences between a VM image and a Docker image?","<cloud><docker><virtual-machine><openstack>","<p>I keep rereading <a href=""https://docs.docker.com/"" rel=""noreferrer"">the Docker documentation</a> to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?</p>

<p>Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?</p>
","<p>Are there any differences in images of Docker and Virtual Machine?
Except the image formats, I couldn't find any info on this anywhere.
Please comment out on the things like image size, instance creation time, capture time, etc.
Thanks!</p>
"
"29314665","systemtap:while resolving probe point: identifier 'process' at source: probe process().function no match","<nginx><process><docker><systemtap>","29327180","systemtap: nothing for ""sudo stap -L 'process(""/data1/nginx/sbin/nginx"").function(""*"")'","<nginx><lua><docker><systemtap>","<p>I had installed nginx and lua in my docker image, but I didn't install nginx on my physical CentOS system. I ""docker run my image"" and start nginx on my physical CentOS system. So the nginx master and worker process are working. I run an example of nginx-systemtap-toolkit. I run:</p>

<pre><code>sudo ./ngx-active-reqs -p 24945
</code></pre>

<p>24945 is worker process id. errors as following:</p>

<pre><code>semantic error: while resolving probe point: identifier 'process' at &lt;input&gt;:6:7
    source: probe   process(""/data1/nginx/sbin/nginx"").function(""ngx_process_events_and_timers""),
                    ^
semantic error: no match
Pass 2: analysis failed.  [man error::pass2]
Number of similar error messages suppressed: 1
</code></pre>

<p>In fact, ""/data1/nginx/sbin/nginx"" is the nginx path in docker image, nginx is not installed on my physical system. So whether I must install nginx on my physical system , or is there other method to use nginx's process function? I don't know how to solve the problem.</p>
","<p>I had installed nginx and lua in my docker image, but I didn't install them on my physical CentOS system. I installed nginx-debuginfo on both physical CentOS system and docker image. I docker run and start nginx:</p>

<pre><code>docker run -it -p 2000:20  9cdf8f425fd9 /bin/bash /home/startnginx.sh
</code></pre>

<p>then I run ""stap -L"" on my physical CentOS system to see the functions of nginx,but nothing is returned:</p>

<pre><code>sudo stap  -L 'process(""/data1/nginx/sbin/nginx"").function(""*"")'
sudo stap -d 0.0.0.0:2000:/data1/nginx/sbin/nginx --ldd -L 'process(""/data1/nginx/sbin/nginx"").function(""*"")'
</code></pre>

<p>/data1/nginx/sbin/nginx is the path of nginx on docker image. 
I don't know how to solve the problem?</p>
"
"24309526","How to change the docker image installation directory?","<docker>","32070113","How do I change the default docker container location?","<docker>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>When I run docker, downloaded docker images (seem to be) stored in <code>/var/lib/docker</code> somewhere.</p>

<p>Since disk space is limited on this directory, and I'm provisioning docker to multiple machines at once; is there a way to change this default location to i.e. <code>/mnt/hugedrive/docker/</code>?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","32227982","How to start flask server with docker run and use it on host?","<python><flask><docker>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a flask script which I try to execute it via docker run command. Following command i am doing</p>

<pre><code>docker run -dit -v /media/sf_MY_WINDOWS/GitRepo/:/ext/GitRepo -p 5000:5000 ""isbhatt/prefixman:v1"" /ext/docker/vm_scripts/db_loader.sh
</code></pre>

<p>and db_loader.sh file contains</p>

<pre><code>/usr/local/bin/python2.7 /ext/SDSNG/src/prefix_manager/manage.py runserver --host 0.0.0.0 &amp;
</code></pre>

<p>but when I do curl localhost:5000 Connection refused.
If I go into container and run the stuff and do curl localhost:5000 it works in container..
What is wrong here?</p>

<p>Output of netstat -tln on container</p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address               Foreign Address             State      
tcp        0      0 0.0.0.0:5000                0.0.0.0:*                   LISTEN  
</code></pre>
"
"25829296","Exposing multiple ports from Docker within Elastic Beanstalk","<docker><amazon-elastic-beanstalk>","27848735","AWS Elastic Beanstalk, Dockerrun.aws.json and multiple ports on docker run","<amazon-web-services><nginx><amazon-ec2><docker><amazon-elastic-beanstalk>","<p>From reading the AWS documentation, it appears that when using Docker as the platform on Elastic Beanstalk (EB) (as opposed to Tomcat, etc.), only a single port can be exposed. I'm trying to understand why Amazon created this restriction -- seems that you now can't even serve both HTTP and HTTPS.</p>

<p>I'd like to use Docker as the container since it allows me to run several interconnected server processes within the same container, some of which require multiple ports (e.g. RTSP). Are there any workarounds for this kind of application, where say an RTSP and HTTP server can both be running within the same Docker container on EB?</p>
","<p>I wish to run a docker in a EC2 instance with AWS API, and I have a <a href=""http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_image.html"" rel=""nofollow noreferrer"">Dockerrun.aws.json</a> like this:</p>

<pre class=""lang-json prettyprint-override""><code>{
  ""AWSEBDockerrunVersion"": ""1"",
  ""Authentication"": {
    ""Bucket"": ""&lt;BUCKET&gt;"",
    ""Key"": "".dockercfg""
  },
  ""Image"": {
    ""Name"": ""&lt;NAME&gt;:&lt;TAG&gt;"",
    ""Update"": ""true""
  },
  ""Ports"": [
    {
      ""ContainerPort"": ""80""
    },
    {
      ""ContainerPort"": ""443""
    }
  ]
}
</code></pre>

<p>Like you can see, I have multiple ports to expose, but elastic beanstalk expose only the first of they.</p>

<p>I found this sentence in the documentation: You can specify multiple container ports, but AWS Elastic Beanstalk uses only the first one to connect your container to the host's reverse proxy and route requests from the public Internet.</p>

<p>My question is why ?</p>

<p>I have an authentication which use Oauth2 protocol, and I must use HTTPS protocol for obvious security reasons.
With this restriction, I can only choose HTTP or HTTPS, because I can only expose port 80 or 443.</p>

<p>I tried to tinker ebextensions to make nginx redirections with ports at the level of EC2 instances, but i've failed.
How can I do ?</p>

<p>This stackoverflow user has the same problem.
<a href=""https://stackoverflow.com/questions/25829296/exposing-multiple-ports-from-docker-within-elastic-beanstalk"">Exposing multiple ports from Docker within Elastic Beanstalk</a></p>

<p>Thanking you in advance</p>
"
"24241292","Dockerized nginx is not starting","<nginx><docker>","36795164","nginx not starting inside Docker","<bash><nginx><docker><dockerfile>","<p>I have tried following some tutorials and documentation on dockerizing my web server, but I am having trouble getting the service to run via the docker run command.</p>

<p>This is my Dockerfile:</p>

<pre><code>FROM ubuntu:trusty

#Update and install stuff
RUN apt-get update
RUN apt-get install -y python-software-properties aptitude screen htop nano nmap nginx

#Add files
ADD src/main/resources/ /usr/share/nginx/html

EXPOSE 80
CMD service nginx start
</code></pre>

<p>I create my image: </p>

<pre><code>docker build -t myImage .
</code></pre>

<p>And when I run it:</p>

<pre><code>docker run -p 81:80 myImage
</code></pre>

<p>it seems to just stop:</p>

<pre><code>docker ps -a

CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
90e54a254efa        pms-gui:latest      /bin/sh -c service n   3 seconds ago       Exit 0                                  prickly_bohr
</code></pre>

<p>I would expect this to be running with port 81->80 but it is not. Running </p>

<pre><code>docker start 90e
</code></pre>

<p>does not seem to do anything.</p>

<p>I also tried entering it directly</p>

<pre><code>docker run -t -i -p 81:80 myImage /bin/bash
</code></pre>

<p>and from here I can start the service</p>

<pre><code>service nginx start
</code></pre>

<p>and from another tab I can see it is working as intended (also in my browser):</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                NAMES
408237a5e10b        myImage:latest      /bin/bash           12 seconds ago      Up 11 seconds       0.0.0.0:81-&gt;80/tcp   mad_turing 
</code></pre>

<p>So I assume it is something I am doing wrong with my Dockerfile? Could anyone help me out with this, I am quite new to Docker. Thank you!</p>

<p><strong>SOLUTION:</strong> Based on the answer from Ivant I found another way to start nginx in the foreground. My Dockerfile CMD now looks like:</p>

<pre><code>CMD /usr/sbin/nginx -g ""daemon off;""
</code></pre>
","<p>Here is my Dockerfile:</p>

<pre><code>FROM ubuntu:14.04.4
ENV DEBIAN_FRONTEND noninteractive
RUN apt-get update
RUN apt-get install -y software-properties-common
RUN add-apt-repository ppa:nginx/stable

RUN apt-get update
RUN apt-get upgrade -y

RUN apt-get install -y nginx

ADD configurations/nginx.conf /etc/nginx/nginx.conf

ADD configurations/app.conf /etc/nginx/sites-available/default.conf
RUN ln -sf /etc/nginx/sites-available/default.conf /etc/nginx/sites-enabled/default.conf

RUN chown -Rf www-data.www-data /var/www/

ADD scripts/start.sh /start.sh
RUN chmod 755 /start.sh

EXPOSE 443
EXPOSE 80

CMD [""/bin/bash"", ""/start.sh""]
</code></pre>

<p>The start.sh script:</p>

<pre><code>cat scripts/start.sh 

service nginx start
echo ""test"" &gt; /tmp/test
</code></pre>

<p>When I log to the container:</p>

<pre><code>docker exec --interactive --tty my_container bash
</code></pre>

<p>neither the test file exists nor nginx is running. There are no errors on the nginx log.</p>
"
"35315996","How do I mount a Docker volume while using a Windows host?","<windows><docker><containers><volume><boot2docker>","36690353","Why volume mapping does not work?","<docker>","<p>Mounting a Docker volume while being under a Windows host, has been a huge pain for me, and I could not get it to work.</p>

<p>Currently I got the following simple Dockerfile:</p>

<pre><code>FROM php:5-apache
RUN apt-get update
</code></pre>

<p>When I build an image from it, and start a container</p>

<pre><code>docker build -t phisch:dev .
docker run phisch:dev
</code></pre>

<p>the container starts properly.</p>

<p>But when I am trying to mount a volume,</p>

<pre><code>docker run -v /c/Users/phisch/dev/htdocs:/var/www phisch:dev
</code></pre>

<p>the following message will be displayed:</p>

<pre><code>C:\Users\phisch\dev&gt;docker run -v /c/Users/phisch/dev/htdocs:/var/www phisch:dev
no such file or directory
docker: Error response from daemon: Container command not found or does not exist..
</code></pre>

<p>The <code>/var/www</code> directory definitely exists in the container, and trying other directories does not change the result. Prepending a trailing slash to the host-side directory does not help either. (<code>//c/Users/phisch/dev/htdocs</code>)</p>

<p>How do I mount for example <code>/var/www</code> to <code>C:/Users/phisch/dev/htdocs</code>?</p>

<pre><code>phisch@DESKTOP-UC1LB9J MINGW64 ~/dev (master)
$ docker inspect phisch:dev
[
    {
        ""Id"": ""sha256:73c1533222a905a378f12505ccbd9e9b34cde5a4b34ed008c39e23d5d58a9c91"",
        ""RepoTags"": [
            ""dev_web:latest"",
            ""phisch:dev"",
            ""phisch:dev3""
        ],
        ""RepoDigests"": [],
        ""Parent"": ""sha256:d2c4149d86c4dfceaff0e9c4eb5a5d42ca7815f81dd08baad4dc8bda6db2fb10"",
        ""Comment"": """",
        ""Created"": ""2016-02-10T12:16:37.667236134Z"",
        ""Container"": ""dad811f51ef3b94d9845d13a0e43ad07ccd5684ea2747b3846accdc71abeb628"",
        ""ContainerConfig"": {
            ""Hostname"": ""e06f5a03fe1f"",
            ""Domainname"": """",
            ""User"": """",
            ""AttachStdin"": false,
            ""AttachStdout"": false,
            ""AttachStderr"": false,
            ""ExposedPorts"": {
                ""80/tcp"": {}
            },
            ""Tty"": false,
            ""OpenStdin"": false,
            ""StdinOnce"": false,
            ""Env"": [
                ""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",
                ""PHP_INI_DIR=/usr/local/etc/php"",
                ""PHP_EXTRA_BUILD_DEPS=apache2-dev"",
                ""PHP_EXTRA_CONFIGURE_ARGS=--with-apxs2"",
                ""GPG_KEYS=0BD78B5F97500D450838F95DFE857D9A90D90EC1 6E4F6AB321FDC07F2C332E3AC2BF0BC433CFC8B3"",
                ""PHP_VERSION=5.6.18"",
                ""PHP_FILENAME=php-5.6.18.tar.xz"",
                ""PHP_SHA256=54dd9106c3469bc7028644d72ac140af00655420bbaaf4a742a64e9ed02ec1b0""
            ],
            ""Cmd"": [
                ""/bin/sh"",
                ""-c"",
                ""apt-get update""
            ],
            ""ArgsEscaped"": true,
            ""Image"": ""sha256:d2c4149d86c4dfceaff0e9c4eb5a5d42ca7815f81dd08baad4dc8bda6db2fb10"",
            ""Volumes"": null,
            ""WorkingDir"": ""/var/www/html"",
            ""Entrypoint"": null,
            ""OnBuild"": [],
            ""Labels"": {}
        },
        ""DockerVersion"": ""1.10.0"",
        ""Author"": """",
        ""Config"": {
            ""Hostname"": ""e06f5a03fe1f"",
            ""Domainname"": """",
            ""User"": """",
            ""AttachStdin"": false,
            ""AttachStdout"": false,
            ""AttachStderr"": false,
            ""ExposedPorts"": {
                ""80/tcp"": {}
            },
            ""Tty"": false,
            ""OpenStdin"": false,
            ""StdinOnce"": false,
            ""Env"": [
                ""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",
                ""PHP_INI_DIR=/usr/local/etc/php"",
                ""PHP_EXTRA_BUILD_DEPS=apache2-dev"",
                ""PHP_EXTRA_CONFIGURE_ARGS=--with-apxs2"",
                ""GPG_KEYS=0BD78B5F97500D450838F95DFE857D9A90D90EC1 6E4F6AB321FDC07F2C332E3AC2BF0BC433CFC8B3"",
                ""PHP_VERSION=5.6.18"",
                ""PHP_FILENAME=php-5.6.18.tar.xz"",
                ""PHP_SHA256=54dd9106c3469bc7028644d72ac140af00655420bbaaf4a742a64e9ed02ec1b0""
            ],
            ""Cmd"": [
                ""apache2-foreground""
            ],
            ""ArgsEscaped"": true,
            ""Image"": ""sha256:d2c4149d86c4dfceaff0e9c4eb5a5d42ca7815f81dd08baad4dc8bda6db2fb10"",
            ""Volumes"": null,
            ""WorkingDir"": ""/var/www/html"",
            ""Entrypoint"": null,
            ""OnBuild"": [],
            ""Labels"": {}
        },
        ""Architecture"": ""amd64"",
        ""Os"": ""linux"",
        ""Size"": 491287784,
        ""VirtualSize"": 491287784,
        ""GraphDriver"": {
            ""Name"": ""aufs"",
            ""Data"": null
        }
    }
]
</code></pre>

<p><strong>It turns out the container-side directory <code>/var/www</code> needs to be empty, since it had a sub-directory <code>www</code>. Docker was not able to use it as a mounting point.</strong></p>
","<p>I need to run a npm commands via a dockerized npm as:</p>

<pre><code>$ docker run -it -v /C/Users/sam/Documents/Workspace/project:/workspace lynxsolutions/docker-npm-bower-gulp /bin/bash
</code></pre>

<p>The image name is: lynxsolutions/docker-npm-bower-gulp</p>

<p>I will have the prompt:</p>

<pre><code>root@892f74a14e2d:/workspace#
</code></pre>

<p>however, if I run ls, it will return nothing, so, why the volume mapping is not working?</p>
"
"28490874","docker run <IMAGE> <MULTIPLE COMMANDS>","<docker><docker-image>","38942484","execute 2 commands within container","<docker>","<p>I'm trying to run MULTIPLE commands like this.</p>

<pre><code>docker run image cd /path/to/somewhere &amp;&amp; python a.py
</code></pre>

<p>But this gives me ""No such file or directory"" error because it is interpreted as...</p>

<pre><code>""docker run image cd /path/to/somewhere"" &amp;&amp; ""python a.py""
</code></pre>

<p>It seems that some ESCAPE characters like """" or () are needed.</p>

<p>So I also tried </p>

<pre><code>docker run image ""cd /path/to/somewhere &amp;&amp; python a.py""
docker run image (cd /path/to/somewhere &amp;&amp; python a.py)
</code></pre>

<p>but these didn't work.</p>

<p>I have searched for <a href=""https://docs.docker.com/reference/run/"">Docker Run Reference</a> but have not find any hints about ESCAPE characters.</p>
","<p>When I run this, the first command works but the command after ""&amp;&amp;"" is executed on host instead of within container.</p>

<pre><code>docker run -p 7778:7778 -t continuumio/miniconda conda  install -c conda-forge -y jupyter &amp;&amp; ipython notebook --ip=* --port=7778
</code></pre>

<p>Is there any way to run both the commands within container?
I am looking for a way to to do this at command prompt without using script file.</p>
"
"33784295","Setting DNS for Docker daemon on OS with systemd","<dns><docker><systemd>","39473687","How to change dockerd parameters with systemd?","<docker><systemd>","<p>The default DNS for Docker (e.g. 8.8.8.8) is blocked where I work, so I want to change the default. I've been able to do this using</p>

<pre><code>$ docker daemon --dns &lt;mydnsaddress&gt;
</code></pre>

<p>but I want to do this using a systemd drop-in instead, since the official Docker docs recommend this way. I've made a <code>/etc/systemd/system/docker.service.d/dns.conf</code> file, and used things like this:</p>

<pre><code>[Service]
DNS=&lt;mydnsaddress&gt;
</code></pre>

<p>But I just have no idea what the variable name is supposed to be. How do I set this? More importantly, is there a page that documents all config variables that can be used in systemd drop-ins for Docker?</p>

<p>(btw, this is Docker 1.9 on Ubuntu 15.10, although I don't suspect any bugs)</p>
","<p>Since 16.04 release Ubuntu stopped using Upstart and switch to Systemd for its init system.</p>

<p>How can I change default DOCKER_OPTS parameters?</p>
"
"38396139","Docker: change folder where to store docker volumes","<mongodb><amazon-ec2><docker><ubuntu-14.04><volumes>","39496564","docker volume custom mount point","<docker><docker-compose>","<p>On my <code>Ubuntu EC2</code> I host an application using docker containers. <code>db</code> data and <code>upload</code> data is being stored in volumes <code>CaseBook-data-db</code> and <code>CaseBook-data-uploads</code> which are being created with this commands:</p>

<pre><code>docker volume create --name=CaseBook-data-db
docker volume create --name=CaseBook-data-uploads
</code></pre>

<p>Volumes being attached through <code>docker-compose</code> file:</p>

<pre><code>version: '2'
services:
    mongo:
        container_name: ""CaseBook-db""
        restart: always
        image: mongo:3.2.7
        ports:
            - ""27017""
        volumes:
            - data_db:/data/db
        labels:
            - ""ENVIRONMENT_TYPE=meteor""

    app:
        container_name: ""CaseBook-app""
        restart: always
        image: ""meteor/casebook""
        build: .
        depends_on:
            - mongo
        environment:
            - MONGO_URL=mongodb://mongo:27017/CaseBook
        ports:
            - ""80:3000""
        volumes:
            - data_uploads:/Meteor-CaseBook-Container/.uploads
        labels:
            - ""ENVIRONMENT_TYPE=meteor""
volumes:
     data_db:
        external:
            name: CaseBook-data-db
     data_uploads:
        external:
            name: CaseBook-data-uploads
</code></pre>

<p>I need to store those docker volumes in different folder(for example <code>/home/ubuntu/data/</code>) of the host machine. How to change docker storage folder for volumes? Or there is a better way in doing this? Thank you in advance.</p>
","<p>I'm new to Docker and I was playing around with <code>docker volume</code>. I wanted to specify the location where <code>docker volume</code> stores the data. Much like when we provide the <code>-v</code> option when we execute <code>docker run</code>. <strong><code>Ex : -v /somefolder/:/var/somefolder</code></strong></p>

<p>How can we set a custom <strong>Mountpoint</strong> when we create a <code>docker volume</code>. I didn't find any options on docs.</p>

<p>When I inspect the volume</p>

<pre><code>[                                                                                        
    {                                                                                    
        ""Name"": ""sampleproject_mysql_data"",                                              
        ""Driver"": ""local"",                                                               
        ""Mountpoint"": ""/mnt/sda1/var/lib/docker/volumes/sampleproject_mysql_data/_data"", 
        ""Labels"": null,                                                                  
        ""Scope"": ""local""                                                                 
    }                                                                                    
]   
</code></pre>

<p>I get something like above.</p>

<h2>Is there a way we can set custom Mountpoint. Through <code>docker volume</code> command or through <code>docker-compose.yml</code>?</h2>
"
"38986057","How to set image name in Dockerfile?","<docker><tags><dockerfile>","38993182","How to tag an image in a Dockerfile?","<docker><tags><dockerfile><docker-image>","<p>You can set image name when building a custom image, like this:</p>

<pre><code>docker build -t dude/man:v2 . # Will be named dude/man:v2
</code></pre>

<p>Is there a way to define the name of the image in Dockerfile, so I don't have to mention it in the <code>docker build</code> command?</p>
","<p>I have been digging the documentation, but I did not find an instruction to define the tag name of an image in a <code>Dockerfile</code>. There is one available for the command line though.</p>

<p>Say I create an image <code>FROM</code> another image, I don't want it to bear the same name. How should I proceed?</p>
"
"23513045","How to check if a process is running inside docker container?","<shell><docker><containers>","29618621","use shell script to detect inside docker container","<shell><docker>","<p>[Updated1] I have a shell which will change TCP kernel parameters in some functions, but now I need to make this shell run in Docker container, that means, the shell need to know it is running inside a container and stop configuring the kernel. </p>

<p>Now I'm not sure how to achieve that, here is the contents of <code>/proc/self/cgroup</code> inside the container: </p>

<pre><code>9:hugetlb:/
8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>Any flags above can I use to figure out if this process is running inside a container?</p>

<p>[Updated2]: I have also noticed <a href=""https://stackoverflow.com/questions/20010199/determining-if-a-process-runs-inside-lxc-docker"">Determining if a process runs inside lxc/Docker</a>, but it seems not working in this case, the content in <code>/proc/1/cgroup</code> of my container is:</p>

<pre><code>8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>No /lxc/containerid</p>
","<p>If use <code>docker run -t -i --env IN_DOCKER=1 --name cc centos /bin/bash</code> to enter docker container. We could check it by <code>IN_DOCKER</code> variable.</p>

<p>If use <code>docker exec -t -i cc /bin/bash</code> to enter the same container, <code>IN_DOCKER</code> variable works, too.</p>

<p>If I run <code>sshd</code> inside the container, and use ssh to login the container.
I can not found proper way to detect, it is inside a docker container.</p>
"
"24586573","Docker error: client and server don't have same version","<docker><boot2docker><docker-machine>","29714326","After installing docker in mac with boot2docker, The below error appears. How can i workaround it?","<macos><docker><boot2docker>","<p>Since I just updated Docker to 1.1.0 I get:</p>

<blockquote>
  <p>Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)</p>
</blockquote>

<p>Do you know how to fix this?</p>

<p>I switched back to 1.0.1 and everything works again.</p>
","<p><strong>Error message:</strong></p>

<blockquote>
  <p>Error response from daemon: client and server don't have same version (client : 1.18, server: 1.15)</p>
</blockquote>

<p><strong>My client docker version details are below:</strong> </p>

<pre><code>$ docker version

Client version: 1.6.0

Client API version: 1.18

Go version (client): go1.4.2

Git commit (client): 4749651

OS/Arch (client): darwin/amd64

FATA[0000] Error response from daemon: client and server don't have same version (client : 1.18, server: 1.15)
</code></pre>

<p><strong>And my boot2docker version details are :</strong></p>

<pre><code>$ boot2docker version

Boot2Docker-cli version: v1.6.0
</code></pre>

<p>How can I workaround this error?</p>
"
"27925006","Share files between host system and docker container using specific UID","<docker>","30052019","Docker creates files as root in mounted volume","<docker><file-ownership><volumes>","<p>I'm trying to share files within a Docker guest using the volume sharing.  In order to get the same UID, and therefore interoperability with those files, I would like to create a user in the Docker guest with the same UID as my own user.</p>

<p>In order to test out the idea, I wrote the following simplistic Dockerfile:</p>

<pre><code>FROM phusion/baseimage

RUN touch /root/uid-$UID
</code></pre>

<p>Testing it with <code>docker build -t=docktest .</code> and then <code>docker run docktest ls -al /root</code> reveals that the file is simply named <code>uid-</code>.</p>

<p>Is there a means to share host environment variables with Docker during the guest build process?</p>
","<p>I'm using Docker (1.3.1) to build RPMs inside a container:</p>

<pre><code>docker run -v /home/matt/build:/build build-rpm /build/build-pkg.sh
</code></pre>

<p>This works fine (my user is in the <code>docker</code> group, so I don't need to sudo) and drops a completed <code>.rpm</code> file in the current directory. The problem is that the file is created as owned by root. </p>

<p>How can I arrange it so that the file is created owned by the same user as I run docker with?</p>
"
"38858515","Can Jprofile connect to JVM running in docker","<docker><jvm><jprofiler>","29429985","How to connect JProfiler with application running on multiple docker container?","<docker><boot2docker><jprofiler>","<p>I'm new to JProfiler. I came into a problem recently. My Java app is running in docker which means the JVM is runnning in docker. But my jprofile is installed in the host machine. I know the jprofiler must connect to a JVM. So, is there anyway that the jprofiler can connect to jvm running in docker?</p>
","<p>I would like to connect to applications running on docker containers through JProfiler agents and check the performance of application, but I am struggling to connect JProfiler to applicatios processes running on docker. </p>

<p>I am using boot2docker on mac ox. Anyone had any experience with such connections ?</p>

<p>Will be grateful for any tips.</p>
"
"40796494","Docker swarm run tasks only in workers","<docker><service><docker-swarm>","40999899","Docker Swarm that uses just nodes?","<docker><docker-swarm>","<p>Say that we are working in swarm mode and we have three nodes: </p>

<ul>
<li>manager1</li>
<li>worker1</li>
<li>worker2</li>
</ul>

<p>Is it possible to create a service and specify that the tasks only has to run in the workers (worker1 and worker2) and not in the managers (manager1)</p>

<p>I am running the following command to create the service:</p>

<pre><code>docker-machine ssh manager1 ""docker service create --network dognet --name dog-db redis""
</code></pre>

<p>and when I ps the service:</p>

<pre><code>docker-machine ssh manager1 ""docker service ps dog-db""
</code></pre>

<p>I get:</p>

<pre><code>ID                         NAME      IMAGE  NODE      DESIRED STATE  CURRENT STATE            ERROR
3kvfpbhl6fj0qwtglc5k7sbkw  dog-db.1  redis  manager1  Running        Preparing 4 seconds ago  
</code></pre>
","<p>If I create a Docker 1.12 Swarm(kit) with e.g. 1 manager and 2 nodes, Swarm will use all 3 hosts to spread containers onto.</p>

<p>I would like to prevent Swarm from using my manager hosts for creating containers. How?</p>
"
"32451748","how to bind ports with docker-py","<python><docker><dockerpy>","43462339","How to bind a port when using docker-py to create a service?","<python><docker><dockerpy>","<p>I try to start a docker container with docker-py (Version 1.3.1). I want to map the container internal ports to different ports but fail to expose them properly.</p>

<p>I do this like so:</p>

<pre><code>def start_container(client, host_config, image_tagged_name, command):
    print (""create_host_config"", host_config.binds, host_config.port_bindings)
    the_host_config = create_host_config(binds         = host_config.binds,
                                         port_bindings = host_config.port_bindings);
    the_ports = host_config.port_bindings.values();
    print (""create_container"", image_tagged_name, command, the_ports, the_host_config)
    cont_id = client.create_container(image=image_tagged_name, command=command, ports=the_ports, host_config=the_host_config)[""Id""]
</code></pre>

<p>In the case at hand the output is as follows:</p>

<pre><code>create_host_config ['/dbfiles/test:/opt/db'] {3001: 3000, 2425: 2424, 2481: 2480}
create_container test:test ./initdb.sh [3000, 2424, 2480] {'Binds': ['/dbfiles/test:/opt/db'], 'PortBindings': {'3001/tcp': [{'HostPort': '3000', 'HostIp': ''}], '2425/tcp': [{'HostPort': '2424', 'HostIp': ''}], '2481/tcp': [{'HostPort': '2480', 'HostIp': ''}]}}
</code></pre>

<p>docker ps tells me:</p>

<pre><code>$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                    NAMES
169ad3ae0f63        test:test           ""./initdb.sh""            5 minutes ago       Up 5 minutes        2424/tcp, 2480/tcp, 3000/tcp                                             silly_pasteur
</code></pre>

<p>However if I give it mappings 3000 -> 3000, 2424 -> 2424 and 2480 -> 2480 it gives</p>

<pre><code>$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                    NAMES
cba483673bdd        test:test           ""./initdb.sh""            53 minutes ago      Up 5 minutes        0.0.0.0:2424-&gt;2424/tcp, 0.0.0.0:2480-&gt;2480/tcp, 0.0.0.0:3000-&gt;3000/tcp   stupefied_ptolemy
</code></pre>

<p>The point is that from the commandline I can start the container with proper port mappings. That is</p>

<p>docker run -d -p 3001:3000 -p 2425:2424 -p 2481:2480 -v /dbfiles/test:/opt/db localhost:5000/test:test /initdb.sh</p>

<p>gives the desired result.</p>

<pre><code>CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                                                                    NAMES
7c1580e0ace9        localhost:5000/test:test         ""/initdb.sh""             8 seconds ago       Up 6 seconds        0.0.0.0:2425-&gt;2424/tcp,     0.0.0.0:2481-&gt;2480/tcp, 0.0.0.0:3001-&gt;3000/tcp   backstabbing_brahmagupta
</code></pre>

<p>However with docker-py I just can not figure out how to map the ports to different port numbers. What am I missing?</p>
","<p>I want to manage the docker service automating. And I must bind a port to the service. But the docker service creating function in the docker SDK for python cannot bind a port. <a href=""https://docker-py.readthedocs.io/en/stable/services.html"" rel=""nofollow noreferrer"">https://docker-py.readthedocs.io/en/stable/services.html</a> Is there any method to figure out this problem?</p>
"
"28212380","Why docker container exits immediately","<docker>","43717282","Issue when I try to create a docker image","<mysql><database><bash><docker><containers>","<p>I run a container in the background using</p>

<pre><code> docker run -d --name hadoop h_Service
</code></pre>

<p>it exits quickly. But if I run in the foreground, it works fine. I checked logs using</p>

<pre><code>docker logs hadoop
</code></pre>

<p>there was no error. Any ideas?</p>

<p><strong>DOCKERFILE</strong></p>

<pre><code> FROM java_ubuntu_new
 RUN wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
 RUN dpkg -i cdh4-repository_1.0_all.deb
 RUN curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | apt-key add -
 RUN  apt-get update
 RUN apt-get install -y hadoop-0.20-conf-pseudo
 RUN dpkg -L hadoop-0.20-conf-pseudo
 USER hdfs
 RUN hdfs namenode -format
 USER root
 RUN apt-get install -y sudo
 ADD . /usr/local/
 RUN chmod 777 /usr/local/start-all.sh
 CMD [""/usr/local/start-all.sh""]
</code></pre>

<p>start-all.sh</p>

<pre><code> #!/usr/bin/env bash
 /etc/init.d/hadoop-hdfs-namenode start
 /etc/init.d/hadoop-hdfs-datanode start
 /etc/init.d/hadoop-hdfs-secondarynamenode start
 /etc/init.d/hadoop-0.20-mapreduce-tasktracker start
 sudo -u hdfs hadoop fs -chmod 777 /
 /etc/init.d/hadoop-0.20-mapreduce-jobtracker start
 /bin/bash
</code></pre>
","<p>I'm trying to create a mysql image with Docker but it doesn't work correctly...</p>

<p>My goal is to create a custom image of MYSQL from Ubuntu Trusty. The image should execute the typical script to configure the database depending of the variables passed through command line but when I run <strong>'docker run -d -e MYSQL_ROOT=docker -e MYSQL_DATABASE=wp ... '</strong> the script works right but when it finished, the container stop. </p>

<p>I tried to 'nohup /usr/sbin/mysqld &amp;' , exec '/usr/sbin/mysqld &amp;' but nothing, the daemon die.</p>

<p><strong>My dockerfile is the following:</strong></p>

<pre><code>FROM ubuntu:trusty

ENV DEBIAN_FRONTEND noninteractive

RUN \
  apt-get update &amp;&amp; \
  apt-get -y install mysql-server-5.6 supervisor --no-install-recommends &amp;&amp; \
   apt-get -y clean &amp;&amp; \
   apt-get -y autoclean &amp;&amp; \
   rm -rf /var/lib/apt/lists/* 

RUN \
   ln -sf /dev/stderr /var/log/mysql/error.log &amp;&amp; \
   sed -i 's/127.0.0.1/0.0.0.0/' /etc/mysql/my.cnf

COPY config.sh /

VOLUME [""/var/lib/mysql""]

EXPOSE 3306 

ENTRYPOINT [""/config.sh""]
</code></pre>

<p><strong>And the script 'config.sh' :</strong></p>

<pre><code>#!/bin/bash -x

/usr/sbin/mysqld &amp;

sleep 5

if [ $MYSQL_ROOT_PASSWORD ]
  then
     mysql -u root -e ""SET PASSWORD FOR 'root'@'localhost' = PASSWORD('${MYSQL_ROOT_PASSWORD}') ;""
  else
    echo 'Error al establecer la contraseña de root.'
    exit 1
fi

if [ $MYSQL_DATABASE ]
  then
    mysql -u root -p${MYSQL_ROOT_PASSWORD} -e ""CREATE DATABASE IF NOT EXISTS ${MYSQL_DATABASE} ;""
  else
   echo 'Error al crear la base de datos.'
fi

if [ $MYSQL_USER ] &amp;&amp; [ $MYSQL_PASSWORD ]
  then
    mysql -u root -p${MYSQL_ROOT_PASSWORD} -e ""GRANT ALL ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_PASSWORD}' ; FLUSH PRIVILEGES ;""
   elif [ $MYSQL_USER ] 
      then
        mysql -u root -p${MYSQL_ROOT_PASSWORD} -e ""GRANT ALL ON ${MYSQL_DATABASE}.* TO '${MYSQL_USER}'@'%' IDENTIFIED BY '${MYSQL_USER}' ; FLUSH PRIVILEGES ;""
   else
        echo 'No se pudo crear el usuario.'
fi
</code></pre>
"
"33649192","How do I set ulimit for containers in Kubernetes?","<kubernetes><google-kubernetes-engine>","43656788","Change process limit of open file descriptors inside a container","<docker><kubernetes><kops>","<p>How do I set ulimit for containers in Kubernetes? (specifically ulimit -u)</p>
","<p>Is there any way to change the limits, open file descriptors in my case, both soft and hard, for a running process inside a pod?</p>

<p>I'm running a memcached deployment using helm <code>stable/memcached</code>, but the <code>1024</code> open file limit is really short for the intended concurrency.</p>

<p>If it is not possible to do so, what is the right way to change the limits for a deployment or globaly on a kubernetes cluster (running on AWS and setup with <code>kops</code>)?</p>
"
"3068139","How can I sandbox Python in pure Python?","<python><scripting>","35322452","Is there a way to sandbox test execution with pytest, especially filesystem access?","<python><unit-testing><testing><docker><pytest>","<p>I'm developing a web game in pure Python, and want some simple scripting available to allow for more dynamic game content. Game content can be added live by privileged users.</p>

<p>It would be nice if the scripting language could be Python. However, it can't run with access to the environment the game runs on since a malicious user could wreak havoc which would be bad. Is it possible to run sandboxed Python in pure Python?</p>

<p><em>Update</em>: In fact, since true Python support would be way overkill, a simple scripting language with Pythonic syntax would be perfect.</p>

<p>If there aren't any Pythonic script interpreters, are there any other open source script interpreters written in pure Python that I could use? The requirements are support for variables, basic conditionals and function calls (not definitions).</p>
","<p>I'm interested in executing potentially untrusted tests with pytest in some kind of sandbox, like docker, similarly to what continuous integration services do.</p>

<p>I understand that to properly sandbox a python process you need OS-level isolation, like running the tests in a disposable chroot/container, but in my use case I don't need to protect against intentionally malicious code, only from dangerous behaviour of pairing ""randomly"" functions with arguments. So lesser strict sandboxing may still be acceptable. But I didn't find any plugin that enables any form of sandboxing.</p>

<p>What is the best way to sandbox tests execution in pytest?</p>

<p><strong>Update</strong>: This question is not about <a href=""https://stackoverflow.com/questions/3068139/how-can-i-sandbox-python-in-pure-python"">python sandboxing in general</a> as the tests' code is run by pytest and I can't change the way it is executed to use <code>exec</code> or <code>ast</code> or whatever. Also using pypy-sandbox is not an option unfortunately as it is ""a prototype only"" as per the <a href=""http://pypy.org/features.html"" rel=""nofollow noreferrer"">PyPy feature page</a>.</p>

<p><strong>Update 2</strong>: Hoger Krekel on the pytest-dev mailing list <a href=""https://mail.python.org/pipermail/pytest-dev/2016-February/003394.html"" rel=""nofollow noreferrer"">suggests using a dedicated testuser via pytest-xdist</a> for user-level isolation:</p>

<pre><code>py.test --tx ssh=OTHERUSER@localhost --dist=each
</code></pre>

<p>which <a href=""https://mail.python.org/pipermail/pytest-dev/2016-February/003399.html"" rel=""nofollow noreferrer"">made me realise</a> that for my CI-like use case:</p>

<blockquote>
  <p>having a ""disposable"" environment is as important as having a isolated
  one, so that every test or every session runs from the same initial
  state and it is not influenced by what older sessions might have left
  on folders writable by the <em>testuser</em> (/home/testuser, /tmp, /var/tmp,
  etc).</p>
</blockquote>

<p>So the testuser+xdist is close to a solution, but not quite there.</p>

<p>Just for context I need isolation to run <a href=""https://pytest-nodev.readthedocs.org"" rel=""nofollow noreferrer"">pytest-nodev</a>.</p>
"
"33133630","Automating an entry for terminal","<python><bash><python-2.7>","33593703","how to create images interactive from Dockerfile?","<docker><pyqt><dockerfile>","<p>I have a problem which do not really know how to ask. I will try to explain as best as possible. </p>

<p>I have to automate an installation process using a bash script. In the middle of the process I make a call to a Python script called configure.py. This script expects input terminal.</p>

<p>Is there any way to call the script by passing terminal arguments waiting for?</p>

<pre><code>Determining the layout of your Qt installation...
This is the GPL version of PyQt 4.11.4 (licensed under the GNU General Public
License) for Python 2.7.9 on linux2.

Type 'L' to view the license.
Type 'yes' to accept the terms of the license.
Type 'no' to decline the terms of the license.

Do you accept the terms of the license? yes
</code></pre>

<p>Thank you so much.</p>
","<p>i want to create an image about PyQt5 from Dockerfile .
the following is part code in Dockerfile.</p>

<pre><code>RUN cd PyQt-gpl-5.5.1 &amp;&amp;\ 
        python3  configure.py
</code></pre>

<p>when the code executes,it will ask me <strong>""Do you accept the terms of the license?""</strong>,but i can't type any word.</p>

<p>And i don't want to use the command 'commit'.
so how to create an image <strong>interactive</strong> from Dockerfile?</p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","46369275","default postgresql.conf file from docker","<postgresql><docker>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>how can i get to my host the postgresql.conf from a container? or where can i see the default one? one coworker gave me his config file to compare it to my own to see some changes i would need to make. </p>

<p>the file path on the container is:</p>

<pre><code>postgres=# show config_file;
               config_file                
------------------------------------------
 /var/lib/postgresql/data/postgresql.conf
(1 row)
</code></pre>

<p>but i dont know how to get the file from the container to my host</p>
"
"29563183","Connecting to a USB Android device in a Docker container via ADB","<android><docker><adb>","46413545","Mounting USB Android device to Docker container from a Windows 10 host","<android><docker><adb><appium>","<p>I have created a Docker image which contains the Android SDK and am trying to expose my Android phone in a container running this image. So I used the <code>--privileged</code> flag and mounted the USB devices as follows:</p>

<pre><code>$ docker run --privileged -v /dev/bus/usb:/dev/bus/usb -d -P my-android:0.0.1
</code></pre>

<p>However, when I run ADB devices, it does not show me the USB device:</p>

<pre><code>ubuntu@d56b666be455:~/Android/Sdk/platform-tools$ ./adb devices
* daemon not running. starting it now on port 5037 *
* daemon started successfully *
List of devices attached

ubuntu@d56b666be455:~/Android/Sdk/platform-tools$
</code></pre>

<p>lsusb inside the container lists the device:</p>

<pre><code>ubuntu@d56b666be455:~$ lsusb
...
Bus 002 Device 017: ID 04e8:6866 Samsung Electronics Co., Ltd GT-I9300 Phone [Galaxy S III] (debugging mode)
</code></pre>

<p>The device is however visible on the host:</p>

<pre><code>⇒  ./adb devices
List of devices attached
4d11abcd65b74045    device
</code></pre>

<p>Host OS</p>

<pre><code>$ uname -a
Linux ananya 3.16.0-33-generic #44~14.04.1-Ubuntu SMP Fri Mar 13 10:33:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<p>Docker version</p>

<pre><code>$ docker --version
Docker version 1.5.0, build a8a31ef
</code></pre>

<p>What could be the issue?</p>
","<p>I have exactly the same problem as <a href=""https://stackoverflow.com/questions/29563183/connecting-to-a-usb-android-device-in-a-docker-container-via-adb"">Connecting to a USB Android device in a Docker container via ADB</a> but my host is Windows 10.</p>

<p><code>adb devices</code> command on the host shows the connected android phone, but the same command inside the container does not show any devices.</p>

<p>This is how I run my container:</p>

<pre><code>docker run -d -p 4723:4723 --privileged -v /dev/bus/usb:/dev/bus/usb -e APPIUM_ARGS="""" --name appium0922_1 softsam/appium
</code></pre>

<p><code>appium0922_1</code> is the name of my container. 
<code>softsam/appium</code> is the name of the image the container is running.</p>

<p>Any thoughts?</p>
"
"9942594","UnicodeEncodeError: 'ascii' codec can't encode character u'\xa0' in position 20: ordinal not in range(128)","<python><unicode><beautifulsoup><python-2.x><python-unicode>","52065842","Python, Docker - 'ascii' codec can't encode character","<python><python-3.x><csv><docker>","<p>I'm having problems dealing with unicode characters from text fetched from different web pages (on different sites). I am using BeautifulSoup. </p>

<p>The problem is that the error is not always reproducible; it sometimes works with some pages, and sometimes, it barfs by throwing a <code>UnicodeEncodeError</code>. I have tried just about everything I can think of, and yet I have not found anything that works consistently without throwing some kind of Unicode-related error.</p>

<p>One of the sections of code that is causing problems is shown below:</p>

<pre><code>agent_telno = agent.find('div', 'agent_contact_number')
agent_telno = '' if agent_telno is None else agent_telno.contents[0]
p.agent_info = str(agent_contact + ' ' + agent_telno).strip()
</code></pre>

<p>Here is a stack trace produced on SOME strings when the snippet above is run:</p>

<pre><code>Traceback (most recent call last):
  File ""foobar.py"", line 792, in &lt;module&gt;
    p.agent_info = str(agent_contact + ' ' + agent_telno).strip()
UnicodeEncodeError: 'ascii' codec can't encode character u'\xa0' in position 20: ordinal not in range(128)
</code></pre>

<p>I suspect that this is because some pages (or more specifically, pages from some of the sites) may be encoded, whilst others may be unencoded. All the sites are based in the UK and provide data meant for UK consumption - so there are no issues relating to internalization or dealing with text written in anything other than English.</p>

<p>Does anyone have any ideas as to how to solve this so that I can CONSISTENTLY fix this problem?</p>
","<p>I wrote a python3 script that does some web scraping and stores some information on a <code>CSV</code> file. The script works fine on my computer. The problem happens when I try to run the script on a docker container. The error seems to be on this part of my code (simplified further for the purposes of this question).</p>
<pre><code># default CSV module
import csv

# this is how an ACTUAL row looks like in my program, included it in case it was important
row = {'title': 'Electrochemical sensor for the determination of dopamine in presence of high concentration of ascorbic acid using a Fullerene-C60 coated gold electrode', 'url': 'https://onlinelibrary.wiley.com/doi/abs/10.1002/elan.200704073', 'author': 'Goyal, Rajendra Nath and Gupta, Vinod Kumar and Bachheti, Neeta and Sharma, Ram Avatar', 'abstract': 'A fullerene‐C60‐modified gold electrode is employed for the determination of dopamine in the excess of ascorbic acid using square‐wave voltammetry. Based on its strong catalytic function towards the oxidation of dopamine and ascorbic acid, the overlapping voltammetric …', 'eprint': 'http://www.academia.edu/download/3909892/Dopamene.pdf', 'publisher': 'Wiley Online Library', 'year': '2008', 'pages': '757--764', 'number': '7', 'volume': '20', 'journal': 'Electroanalysis: An International Journal Devoted to Fundamental and Practical Aspects of Electroanalysis', 'ENTRYTYPE': 'article', 'ID': 'goyal2008electrochemical'}

# the CSV writer object
writer = csv.DictWriter(&quot;file.csv&quot;, fieldnames=[a, b, c],  dialect='toMYSQL')

# this is the source of the problem!
writer.writerow(row)
</code></pre>
<p>I understand the containers have only the bare bones and that means that maybe the encoding the script uses is not supported. Thus, I added this to the start of my script: (bellow the usual she-bang)</p>
<pre><code># coding=utf-8
</code></pre>
<p>These are the locales on my docker:</p>
<pre><code>$ locale -a

C
C.UTF-8
POSIX
en_US.utf8
es_CR.utf8
</code></pre>
<p>I have way more on my PC, but that shouldn't change much since en_US.utf8 covers all English stuff and es_CR.utf8 covers all Spanish stuff. (most, if not all, of my results are in English.)</p>
<p>I'm using python3, so I know all strings are unicode characters, maybe thats related to the problem?</p>
<pre><code>$ python3 --version
Python 3.6.5
</code></pre>
<p>Despite all that, when I run my program, I get the following error message as soon as the script tries to print the row on console:</p>
<pre><code>Exception in thread Thread-6:
Traceback (most recent call last):
  File &quot;/usr/lib/python3.6/threading.py&quot;, line 916, in _bootstrap_inner
    self.run()
  File &quot;/usr/lib/python3.6/threading.py&quot;, line 864, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/home/Systematic-Mapping-Engine/sysmapengine/scraper.py&quot;, line 100, in build_csv
    writer.writerow(clean_row)
  File &quot;/usr/lib/python3.6/csv.py&quot;, line 155, in writerow
    return self.writer.writerow(self._dict_to_list(rowdict))
UnicodeEncodeError: 'ascii' codec can't encode character '\u2010' in position 262: ordinal not in range(128)
</code></pre>
"
"21871479","Docker can't connect to docker daemon","<linux><docker>","35711821","ps command cannot connect to the docker daemon even after adding user to docker group","<docker>","<p>After I update my Docker version to <code>0.8.0</code>, I get an error message while entering <code>sudo docker version</code>:</p>

<pre><code>Client version: 0.8.0
Go version (client): go1.2
Git commit (client): cc3a8c8
2014/02/19 12:54:16 Can't connect to docker daemon. Is 'docker -d' running on this host?
</code></pre>

<p>And I've followed the instructions and entered command <code>sudo docker -d</code>, and I got this:</p>

<pre><code>[/var/lib/docker|2462000b] +job initserver()
[/var/lib/docker|2462000b.initserver()] Creating server
open /var/lib/docker/aufs/layers/cf2414da53f9bcfaa48bc3d58360d7f1cfd3784e4fe51fbef95197709dfc285d: no such file or directory[/var/lib/docker|2462000b] -job initserver() = ERR (1)
2014/02/19 12:55:57 initserver: open /var/lib/docker/aufs/layers/cf2414da53f9bcfaa48bc3d58360d7f1cfd3784e4fe51fbef95197709dfc285d: no such file or directory
</code></pre>

<p>How do I solve the problem?</p>
","<p>I have started a docker container using the following command</p>

<pre><code>     vagrant@vagrant-ubuntu-trusty-64:~$ docker run -t -i ubuntu /bin/bash
     root@abb8ef669ab6:/# cd 
</code></pre>

<p>I am able to see the container shell. However, I am not able to list the docker processes in the new shell on the host using docker ps. am I missing something here?</p>

<pre><code>     vagrant@vagrant-ubuntu-trusty-64:~$ sudo usermod -aG docker vagrant
     vagrant@vagrant-ubuntu-trusty-64:~$ docker ps -a
     Cannot connect to the Docker daemon. Is the docker daemon running on this host?
</code></pre>
"
"27879713","Is it ok to run docker from inside docker?","<jenkins><docker><docker-dind>","52064183","Docker in Docker","<node.js><docker><docker-compose><docker-machine>","<p>I'm running Jenkins inside a Docker container. I wonder if it's ok for the Jenkins container to also be a Docker host? What I'm thinking about is to start a new docker container for each integration test build from inside Jenkins (to start databases, message brokers etc). The containers should thus be shutdown after the integration tests are completed. Is there a reason to avoid running docker containers from inside another docker container in this way?</p>
","<p>We have app and which will spin the short term (short span) docker containers. Right now, it runs in Ubunut16.04 server (VM) and we installed docker, and nodejs in same server. We have nodejs app which runs in same server so whenever a request comes in, then the nodejs app will spin up a docker container and execute a user input inside the docker container. Once after the docker finish its job or if it runs out of admin defined resources then the docker container will be forcefully killed (docker kill) and removed (docker rm). </p>

<p>Now my question is, is it best practices to run the Ubunte16.04 docker container and run nodjes app and the short term docker containers inside the Ubunuter16.04 docker container. </p>

<p>In short run a docker inside other docker container.</p>

<p><a href=""https://i.stack.imgur.com/qCxL5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qCxL5.jpg"" alt=""enter image description here""></a></p>
"
"39397831","Docker cp error: Path not specified","<docker><cp>","52135393","Copy data from ssh server to ssh server docker","<docker><ssh><server>","<p>I want to copy a file from my Ubuntu host to Docker container. </p>

<p>I use Docker 1.6.2 from 14.04 LTS repository. </p>

<p>When I try to copy I get error:</p>

<pre><code>sudo docker cp 1.JPG evil_sammet:/root/openface/training-images/misha/1.jpg
FATA[0000] Error: Path not specified
</code></pre>

<p>With equal success, I can type gibberish instead of file name and path:</p>

<pre><code>sudo docker cp sdfsdfasd dfsdffd 
FATA[0000] Error: Path not specified 
</code></pre>
","<p>How can data be copied from ssh server and paste into docker's folder which is also in ssh server?</p>

<p>The docker cp command</p>

<pre><code>docker cp filename container_id:destination
</code></pre>

<p>is not working for server and shows</p>

<pre><code>Error: Path not specified.
</code></pre>

<p>but it works the other-way-round to copy data from docker to server:</p>

<pre><code>docker cp docker_id:file name host_destination_folder
</code></pre>

<p>TIA</p>
"
"592620","How can I check if a program exists from a Bash script?","<bash>","46575136","How to check if Docker is installed in a Unix shell script?","<shell><ubuntu><docker>","<p>How would I validate that a program exists, in a way that will either return an error and exit, or continue with the script?</p>

<p>It seems like it should be easy, but it's been stumping me.</p>
","<p>I need to check in a shell script if Docker is installed (Ubuntu server).</p>

<p>I came up with this, but the syntax is not correct.</p>

<pre><code>if [[ which docker &amp;&amp; docker --version ]]; then
  echo ""Update docker""
  # command
else
  echo ""Install docker""
  # command
fi
</code></pre>

<p>I also tried <code>if [ which docker ] &amp;&amp; [ docker --version ]; then</code></p>
"
"24370156","Docker image size not matching the container size after a commit","<docker>","46659831","why docker image size is so big after I run docker commit","<docker>","<p>Recently I created a docker container from a centos base image which ran a jboss. Originally I had installed the jdk (and committed) which made the container bulky (about 850M). Later, I uninstalled jdk and installed jre. From inside the container a </p>

<pre><code>du -xsh /
</code></pre>

<p>shows only 440M. But after committing the changes to the image it still shows 711M. Should the image size not match (or be close to) the container's <em>du</em>? Or while committing, does docker keep adding to the old ones (like an SCM)?</p>

<p>Thanks</p>
","<p>I did a test on docker about the image size.</p>

<pre><code># docker images
REPOSITORY                          TAG                 IMAGE ID            
CREATED             SIZE
img_anaconda_installed              latest              5bbdedd62fd3        
21 seconds ago      2.79GB
img_anaconda                        latest              5d9dbd3c4a63        
14 minutes ago      794MB
centos                              latest              196e0ce0c9fb        
3 weeks ago         197MB
</code></pre>

<p>centos is the image I run 'docker pull centos', the size of it is 197M.</p>

<p>Then, I run a container of centos, and in the container I did yum install wget, yum install bzip2, download anaconda.sh file, and stop the container.</p>

<p>Then I did 'docker commit my_container img_anaconda' to make a new image. The image size is 794M. It is a little bit big size than what I thought.</p>

<p>Finally, I once again entered my_container to install anaconda. After I finished anaconda, I stopped the container and docker commit a new image whose name is img_anaconda_installed. The size of it is 2.79GB.</p>

<p>So my question is that the reason of the big size is only that anaconda is big, or docker commit does some other things causing it?</p>

<p>PS: anaconda.sh file's size is 103M.</p>
"
"41167349","docker executable file not found in $PATH","<ruby-on-rails><docker><docker-compose>","46603267","Unable to run shell in my container","<docker><docker-container><docker-image>","<p>Trying to run a rails migration on a running docker compose container throws this error:</p>

<pre><code>$ docker-compose run webapp rails db:migrate
</code></pre>

<blockquote>
  <p>ERROR: Cannot start service webapp: invalid header field value ""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\""rails\\"": executable file not found in $PATH\""\n""</p>
</blockquote>

<p>However, I can access rails from inside the container:</p>

<pre><code>$ docker-compose run webapp bash
root@3fd3a87275a1:/home/app/webapp# which rails
/usr/local/rvm/gems/ruby-2.3.1/bin/rails
</code></pre>

<p>My container is already running and I can GET the page:</p>

<pre><code>$ curl http://localhost -i
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
Transfer-Encoding: chunked
Connection: keep-alive
Status: 200 OK
Cache-Control: max-age=0, private, must-revalidate
ETag: W/""b62d4f67b7b823c017534cd9727752cd""
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
X-Runtime: 0.019881
X-Request-Id: eb32ec21-3a8f-4caa-a27b-2e42f0b2bce9
Date: Thu, 15 Dec 2016 15:15:59 GMT
X-Powered-By: Phusion Passenger 5.0.29
Server: nginx/1.10.1 + Phusion Passenger 5.0.29
</code></pre>

<p>How do I <code>run rails db:migrate</code> in my container?</p>

<h2>Dockerfile</h2>

<pre><code>FROM phusion/passenger-ruby23

# Set correct environment variables.
ENV HOME /root

# Use baseimage-docker's init process.
CMD [""/sbin/my_init""]

# additional packages
RUN apt-get update

# Active nginx
RUN rm -f /etc/service/nginx/down

# Install bundle of gems
WORKDIR /tmp
ADD Gemfile /tmp/
ADD Gemfile.lock /tmp/
RUN bundle install

# Copy the nginx template for configuration and preserve environment variables
RUN rm /etc/nginx/sites-enabled/default

# Add the nginx site and config
ADD webapp.conf /etc/nginx/sites-enabled/webapp.conf

RUN mkdir /home/app/webapp
COPY . /home/app/webapp
RUN usermod -u 1000 app
RUN chown -R app:app /home/app/webapp
WORKDIR /home/app/webapp

# Clean up APT when done.
RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
EXPOSE 80
</code></pre>

<h2>Docker Compose</h2>

<pre><code>version: '2'
services:
  webapp:
    build: .
    container_name: myapp
    working_dir: /home/app/webapp
    ports:
      - ""80:80""
    environment:
      - PASSENGER_APP_ENV=development
    volumes:
      - .:/home/app/webapp
    networks:
      - back-end
  db:
    container_name: db
    image: postgres:9.4
    networks:
      - back-end
networks:
  back-end:
    driver: bridge
</code></pre>
","<p>I want to dockerize my application and I want to run and enter my container in order to see whether packages were installed properly, files were copied, etc.</p>

<p>Here's my Dockerfile:</p>

<pre><code>FROM node:8.6
RUN mkdir /app
WORKDIR /app
COPY .*.json .
COPY src/ .
USER node
RUN yarn global add @angular/cli
EXPOSE 4200
</code></pre>

<p>The problem is, I can't run my container via <code>docker run</code>:</p>

<pre><code>docker run my-notes -it --rm ash
</code></pre>

<p>I see errors:</p>

<pre><code>container_linux.go:262: starting container process caused ""exec: \""-it\"": executable file not found in $PATH""
docker: Error response from daemon: oci runtime error: container_linux.go:262: starting container process caused ""exec: \""-it\"": executable file not found in $PATH"".
ERRO[0000] error waiting for container: context canceled 
</code></pre>

<p>What am I doing wrong?</p>
"
"46406525",".NET Core 2.0 BasePath Error","<c#><asp.net-core><.net-core>","46749411","System.InvalidOperationException: 'A path base can only be configured using IApplicationBuilder.UsePathBase().'","<c#><docker><asp.net-core-2.0>","<p>Just started a fresh .NET Core 2.0 app, but getting weird sudden behaviour, can't seem to find anything.</p>

<p>The following error pops up when hitting the <code>.Run()</code> of my <code>BuildWebHost().</code>:</p>

<pre><code>System.InvalidOperationException: A path base can only be configured using IApplicationBuilder.UsePathBase().
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAddressAsync&gt;d__7.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.AddressesStrategy.&lt;BindAsync&gt;d__2.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAsync&gt;d__0.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServer.&lt;StartAsync&gt;d__21`1.MoveNext()
'dotnet.exe' (CoreCLR: clrhost): Loaded 'C:\Program Files\dotnet\shared\Microsoft.NETCore.App\2.0.0\System.Diagnostics.Tools.dll'. Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled.
Application Insights Telemetry (unconfigured): {""name"":""Microsoft.ApplicationInsights.Dev.Exception"",""time"":""2017-09-25T13:26:29.6923656Z"",""tags"":{""ai.application.ver"":""1.0.0.0"",""ai.internal.sdkVersion"":""aspnet5c:2.1.1""},""data"":{""baseType"":""ExceptionData"",""baseData"":{""ver"":2,""properties"":{""{OriginalFormat}"":""Unable to start Kestrel."",""CategoryName"":""Microsoft.AspNetCore.Server.Kestrel"",""Exception"":""System.InvalidOperationException: A path base can only be configured using IApplicationBuilder.UsePathBase().\r\n   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAddressAsync&gt;d__7.MoveNext()\r\n--- End of stack trace from previous location where exception was thrown ---\r\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\r\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.AddressesStrategy.&lt;BindAsync&gt;d__2.MoveNext()\r\n--- End of stack trace from previous location where exception was thrown ---\r\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\r\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAsync&gt;d__0.MoveNext()\r\n--- End of stack trace from previous location where exception was thrown ---\r\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\r\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n   at Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServer.&lt;StartAsync&gt;d__21`1.MoveNext()"",""AspNetCoreEnvironment"":""Development"",""DeveloperMode"":""true""},""exceptions"":[{""id"":47609378,""typeName"":""System.InvalidOperationException"",""message"":""Unable to start Kestrel."",""hasFullStack"":true,""parsedStack"":[{""level"":0,""method"":""Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder+&lt;BindAddressAsync&gt;d__7.MoveNext"",""assembly"":""Microsoft.AspNetCore.Server.Kestrel.Core, Version=2.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60""},{""level"":1,""method"":""System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":2,""method"":""System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":3,""method"":""Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder+AddressesStrategy+&lt;BindAsync&gt;d__2.MoveNext"",""assembly"":""Microsoft.AspNetCore.Server.Kestrel.Core, Version=2.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60""},{""level"":4,""method"":""System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":5,""method"":""System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":6,""method"":""Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder+&lt;BindAsync&gt;d__0.MoveNext"",""assembly"":""Microsoft.AspNetCore.Server.Kestrel.Core, Version=2.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60""},{""level"":7,""method"":""System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":8,""method"":""System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification"",""assembly"":""System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e""},{""level"":9,""method"":""Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServer+&lt;StartAsync&gt;d__21`1.MoveNext"",""assembly"":""Microsoft.AspNetCore.Server.Kestrel.Core, Version=2.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60""}]}],""severityLevel"":""Critical""}}}
The thread 0x1d7c has exited with code 0 (0x0).
The thread 0x25c0 has exited with code 0 (0x0).
Exception thrown: 'System.InvalidOperationException' in System.Private.CoreLib.dll
An unhandled exception of type 'System.InvalidOperationException' occurred in System.Private.CoreLib.dll
A path base can only be configured using IApplicationBuilder.UsePathBase().
</code></pre>

<p>Here's my Program.cs</p>

<pre><code>public class Program
{
    public static void Main(string[] args)
    {
        BuildWebHost(args).Run();
    }

    public static IWebHost BuildWebHost(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .UseStartup&lt;Startup&gt;()
            .Build();
}
</code></pre>

<p>Here's my Startup.cs</p>

<pre><code>public class Startup
{
    public Startup(IConfiguration configuration)
    {
        Configuration = configuration;
    }

    public IConfiguration Configuration { get; set; }

    public void ConfigureServices(IServiceCollection services)
    {
        services.AddMvc();

        //Swagger
        services.AddSwaggerGen(c =&gt;
        {
            c.SwaggerDoc(""v1"", new Info
            {
                Title = ""2Commit Blogpost API"",
                Version = ""v1""
            });
        });

        //Mediatr
        services.AddScoped&lt;IMediator, Mediator&gt;();
        services.AddTransient&lt;SingleInstanceFactory&gt;(sp =&gt; sp.GetService);
        services.AddTransient&lt;MultiInstanceFactory&gt;(sp =&gt; sp.GetServices);
        services.AddMediatorHandlers(typeof(Startup).Assembly);

        //MongoDB
        services.Configure&lt;MongoSettings&gt;(s =&gt;
        {
            s.Database = Configuration.GetSection(""MongoConnection:Database"").Value;
        });
        services.AddSingleton&lt;IMongoClient, MongoClient&gt;(client =&gt; new MongoClient(Configuration.GetSection(""MongoConnection:ConnectionString"").Value));

        //BL
        services.AddTransient&lt;IUserService, UserService&gt;();

        //DAL
        services.AddTransient&lt;IRepository, MongoRepository&gt;();
    }

    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {
        //Swagger
        app.UseSwagger();
        app.UseSwaggerUI(c =&gt;
        {
            c.SwaggerEndpoint(""/swagger/v1/swagger.json"", ""BlogPost API"");
        });

        app.UseMvc();
    }
}
</code></pre>

<p>Can't seem to wrap my head around this one!</p>

<p>UPDATE 1 : </p>

<p>The error only pops up when running my project from the command line, when running it trough IIS Express, no errors. </p>

<p>Here are my launchSettings</p>

<pre><code>{
  ""iisSettings"": {
    ""windowsAuthentication"": false,
    ""anonymousAuthentication"": true,
    ""iisExpress"": {
      ""applicationUrl"": ""http://localhost:51592/swagger"",
      ""sslPort"": 0
    }
  },
  ""profiles"": {
    ""IIS Express"": {
      ""commandName"": ""IISExpress"",
      ""launchBrowser"": true,
      ""environmentVariables"": {
        ""ASPNETCORE_ENVIRONMENT"": ""Development""
      }
    },
    ""API"": {
      ""commandName"": ""Project"",
      ""launchBrowser"": true,
      ""environmentVariables"": {
        ""ASPNETCORE_ENVIRONMENT"": ""Development""
      },
      ""applicationUrl"": ""http://localhost:5000/swagger""
    }
  }
}
</code></pre>
","<p>I have an ASP.Net Core 2 Solution running in Docker which is working fine on 1 machine the is running VS 2017 Professional but on another machine running VS 2017 Community I am getting the following error</p>

<blockquote>
  <p>""System.InvalidOperationException: 'A path base can only be
  configured using IApplicationBuilder.UsePathBase().'""</p>
</blockquote>

<p>when I try and start or debug the solution using docker. If I start the solution with IIS Express it works fine. </p>

<p>There is nothing special about my project:</p>

<pre><code>public class Program
{
    public static void Main(string[] args)
    {
        BuildWebHost(args).Run(); // &lt;- Exception happens here
    }

    public static IWebHost BuildWebHost(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .CaptureStartupErrors(true)
            .UseStartup&lt;Startup&gt;()
            .Build();
}

public class Startup
{
    public Startup(IHostingEnvironment env)
    {

    }

    public IConfiguration Configuration { get; }

    // This method gets called by the runtime. Use this method to add services to the container.
    public void ConfigureServices(IServiceCollection services)
    {
        services.AddMvc();
    }

    // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {

        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
        }

        app.UseMvc();
    }
}
</code></pre>

<p>I also get this pop up in a new window:</p>

<pre><code>crit: Microsoft.AspNetCore.Server.Kestrel[0]
      Unable to start Kestrel.
System.InvalidOperationException: A path base can only be configured using IApplicationBuilder.UsePathBase().
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAddressAsync&gt;d__7.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.AddressesStrategy.&lt;BindAsync&gt;d__2.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.&lt;BindAsync&gt;d__0.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.AspNetCore.Server.Kestrel.Core.KestrelServer.&lt;StartAsync&gt;d__21`1.MoveNext()
</code></pre>

<p>Everything i've read on this issue don't seem related. Any ideas?</p>
"
"3224878","What is the purpose of the : (colon) GNU Bash builtin?","<bash><shell><built-in>","32342841","Colon at the beginning of line in docker entrypoint bash script","<bash><docker>","<p>What is the purpose of a command that does nothing, being little more than a comment leader, but is actually a shell builtin in and of itself?</p>

<p>It's slower than inserting a comment into your scripts by about 40% per call, which probably varies greatly depending on the size of the comment. The only possible reasons I can see for it are these:</p>

<pre><code># poor man's delay function
for ((x=0;x&lt;100000;++x)) ; do : ; done

# inserting comments into string of commands
command ; command ; : we need a comment in here for some reason ; command

# an alias for `true' (lazy programming)
while : ; do command ; done
</code></pre>

<p>I guess what I'm really looking for is what historical application it might have had.</p>
","<p>I was looking through <a href=""https://github.com/docker-library/postgres/blob/cd294bf8dfdf4a74b2077aa6413fa579f9bf07de/9.4/docker-entrypoint.sh"" rel=""noreferrer"">an entrypoint script for a docker image</a> and they had the following lines (53-54)</p>

<pre><code>: ${POSTGRES_USER:=postgres}
: ${POSTGRES_DB:=$POSTGRES_USER}
</code></pre>

<p>I saw this thread <a href=""https://stackoverflow.com/questions/3224878/what-is-the-purpose-of-the-colon-gnu-bash-builtin"">What is the purpose of the : (colon) GNU Bash builtin?</a></p>

<p>and was able to figure out <code>:</code> meant true and that the <code>:=</code> was used to assign default values, but it doesn't really touch upon what does <code>:</code> at the beginning of the line at least for this specific case.</p>
"
"20845056","How can I expose more than 1 port with Docker?","<docker><docker-networking>","33309402","Cannot get more than one port to be exposed with docker","<docker>","<p>So I have 3 ports that should be exposed to the machine's interface. Is it possible to do this with a Docker container?</p>
","<p>Im trying to get more than one port exposed with the docker run command.  The command I am running is 
<code>docker run -p --detach --publish 8055:80 8455:443 cptactionhank/atlassian-jira:latest</code>, but it gives me the error <code>docker: Invalid containerPort: --detach.</code></p>

<p>If I then move the -p right before the container ports, I get <code>docker run --detach --publish -p 8055:80 8455:443 cptactionhank/atlassian-jira:latest</code> and get the same error - <code>docker: Invalid containerPort: -p.</code></p>

<p>Am I missing something obvious here?  How can I get this command to work?</p>

<p>Trying <code>docker run -p 8055:80 -p 8455:443 --detach --publish cptactionhank/atlassian-jira:latest</code> gives me <code>docker: ""run"" requires a minimum of 1 argument.</code></p>
"
"32919515","Advantages of a Dockerfile","<docker><dockerfile>","33192102","docker basics: what is the difference between using a Dockerfile and commit?","<docker>","<p>We can create Docker images and all push them to Hub without a Dockerfile. Why is it useful, to have a Dockerfile? What are advantages of it? Dockerfile creation is a process with high consumption of time and can made only by a human.
I would like to know what is the main difference between a base image based, committed image and a Dockerfile based image.</p>
","<p>If i'm not wrong, a <code>Dockerfile</code> allows you to run a series of commands to setup a container.</p>

<p>What is the difference between create a <code>Dockerfile</code> and launch a generic image, setup everything on that container using bash and then do a <code>docker commit</code> to generate a new image. Is it the same?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","32302444","Interact with kafka docker container from outside of docker host","<docker><apache-kafka><producer>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have built a kafka docker container and orchestrate it using docker-compose. </p>

<p>Calling <code>docker ps</code> I get the following putput:</p>

<pre><code>CONTAINER ID        IMAGE                          COMMAND                CREATED             STATUS              PORTS                                         NAMES
    5bde6f76246e        hieutrtr/docker-kafka:0.0.1    ""/start.sh""            About an hour ago   Up About an hour    7203/tcp, 0.0.0.0:32884-&gt;9092/tcp             dockerkafka_kafka_3
    be354f1b8cc0        hieutrtr/docker-ubuntu:devel   ""/usr/bin/supervisor   About an hour ago   Up About an hour    22/tcp                                        producer1
    50d3203af90e        hieutrtr/docker-kafka:0.0.1    ""/start.sh""            About an hour ago   Up About an hour    7203/tcp, 0.0.0.0:32883-&gt;9092/tcp             dockerkafka_kafka_2
    61b285f39615        hieutrtr/docker-kafka:0.0.1    ""/start.sh""            2 hours ago         Up 2 hours          7203/tcp, 0.0.0.0:32882-&gt;9092/tcp             dockerkafka_kafka_1
    20c9c5ccec05        jplock/zookeeper:3.4.6         ""/opt/zookeeper/bin/   2 hours ago         Up 2 hours          2888/tcp, 3888/tcp, 0.0.0.0:32881-&gt;2181/tcp   dockerkafka_zookeeper_1
</code></pre>

<p>I can run a producer and a consumer from inside the docker container, but it is not working from outside the docker network.</p>

<p><strong>For example</strong>: </p>

<p>I run a kafka producer on my localhost and the following error appears:</p>

<pre><code>$ kafka_2.9.1-0.8.2.1: bin/kafka-console-producer.sh --topic test --broker-list $DOCKER_HOST:32884
[2015-08-31 06:55:15,450] WARN Property topic is not valid (kafka.utils.VerifiableProperties)
to
[2015-08-31 06:55:20,214] WARN Failed to send producer request with correlation id 2 to broker 1 with data for partitions [test,0] (kafka.producer.async.DefaultEventHandler)
java.nio.channels.ClosedChannelException
    at kafka.network.BlockingChannel.send(BlockingChannel.scala:100)
    at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73)
    at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:72)
    at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:103)
    at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:103)
    at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:103)
</code></pre>

<p><a href=""https://github.com/hieutrtr/docker-kafka"" rel=""noreferrer"">This is my kafka docker example on github</a> that includes the mentioned problem. </p>

<p>So, is anyone experiencing the same problems and can help me in any way? </p>

<p><strong>Additional info</strong>:</p>

<p>(Just fork from ches/kafka and modify something for docker-compose) :</p>
"
"18460016","Connect from one Docker container to another","<rabbitmq><celery><docker>","52214713","How to connect multiple docker container using link?","<php><mysql><docker>","<p>I want to run rabbitmq-server in one docker container and connect to it from another container using celery (<a href=""http://celeryproject.org/"" rel=""noreferrer"">http://celeryproject.org/</a>)</p>

<p>I have rabbitmq running using the below command...</p>

<pre><code>sudo docker run -d -p :5672 markellul/rabbitmq /usr/sbin/rabbitmq-server
</code></pre>

<p>and running the celery via</p>

<pre><code>sudo docker run -i -t markellul/celery /bin/bash
</code></pre>

<p>When I am trying to do the very basic tutorial to validate the connection on <a href=""http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html"" rel=""noreferrer"">http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html</a></p>

<p>I am getting a connection refused error:</p>

<blockquote>
  <p>consumer: Cannot connect to amqp://guest@127.0.0.1:5672//: [Errno 111]
  Connection refused.</p>
</blockquote>

<p>When I install rabbitmq on the same container as celery it works fine.</p>

<p>What do I need to do to have container interacting with each other?</p>
","<p>I am trying to <code>link</code> mysql and php/nginx docker containers.
Here is the command I am running to connect the containers:</p>

<pre><code>docker run -p 127.0.0.1:3005:80 --name nginxTest --link mysqlTest ubuntu
</code></pre>

<p>The php container contains this php file:</p>

<pre><code>&lt;?php
    $servername = ""mysqlTest"";
    $username = ""root"";
    $password = """";

    // Create connection
    $conn = new mysqli($servername, $username, $password);

    // Check connection
    if ($conn-&gt;connect_error) {
        die(""Connection failed: "" . $conn-&gt;connect_error);
    } 
    echo ""Connected successfully"";
?&gt;
</code></pre>

<p>But I am getting this error on accessing <code>127.0.0.1:3005</code>:</p>

<pre><code>Connection failed: MySQL server has gone away
</code></pre>

<p>What am I doing wrong?</p>
"
"24958140","What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?","<docker><dockerfile>","52285134","What is the difference between the ADD and COPY Dockerfile instructions?","<docker><copy><add>","<p>What is the difference between the <code>COPY</code> and <code>ADD</code> commands in a Dockerfile, and when would I use one over the other?</p>

<pre><code>COPY &lt;src&gt; &lt;dest&gt;
</code></pre>

<blockquote>
  <p>The COPY instruction will copy new files from <code>&lt;src&gt;</code> and add them to the container's filesystem at path <code>&lt;dest&gt;</code></p>
</blockquote>

<pre><code>ADD &lt;src&gt; &lt;dest&gt;
</code></pre>

<blockquote>
  <p>The ADD instruction will copy new files from <code>&lt;src&gt;</code> and add them to the container's filesystem at path <code>&lt;dest&gt;</code>.</p>
</blockquote>
","<p>A - ADD supports remote URL handing while COPY does not might be 
 B - ADD supports regular expression handling while COPY does not
 C - ADD supports compression format handling while COPY does not
 D - COPY supports regular expression handling while ADD does not </p>

<p>I appreciate you help guys</p>
"
"27572453","MySQL Docker container is not saving data to new image","<mysql><database><save><docker>","52188493","Is Using A Commited Container Image In A Dockerfile/Docker-Compose Possible?","<mysql><docker><docker-compose>","<p>I'm trying to create a MySQL Docker container that is preset with a certain schema and seed data so that I can have other containers connect to it as a db. I'm using the <a href=""https://registry.hub.docker.com/u/dockerfile/mysql/"" rel=""noreferrer"">trusted dockerfile/mysql image</a> as a base, and I wrote a Dockerfile to create a new image from that base and add my schema.sql into it. After building that image (mysql:base), I've been trying to run bash in new container, then go into mysql and create my database and then import the schema. I then exit the container and try to commit the container to a new Docker image.  However, the resulting image does not persist any of the changes I made to the MySQL db. It does persist other files that I wrote in the container, but not the db.</p>

<p>Here is the Dockerfile I use to build the initial image (myorg/mysql:base).</p>

<pre><code>FROM dockerfile/mysql:latest
MAINTAINER (me)

ADD schema.sql /data/schema.sql

EXPOSE 3306

# Define working directory.
WORKDIR /data

CMD [""mysqld_safe""]
</code></pre>

<p>After building that, I go into the image:</p>

<pre><code>docker run -i -t myorg/mysql:base bash
</code></pre>

<p>And run MySQL to import the schema:</p>

<pre><code>myslqd_safe &amp;
141218 00:15:56 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql

mysql -u root

mysql&gt; CREATE DATABASE mydb;
exit;

mysql -u root -D mydb &lt; schema.sql
</code></pre>

<p>I can go into mysql and verify the schema has been imported successfully:</p>

<pre><code> mysql -u root -D mydb -e ""SELECT * from tokens;""
</code></pre>

<p>Also, if I go into <code>/var/lib/mysql</code> I can see that there is a <code>mydb</code> directory that contains .frm files corresponding to the db.</p>

<p>But when I exit and try to commit that container to a new image:</p>

<pre><code>docker commit -m=""import schema.sql"" -a=""Me"" 72c2ff39dd65 myorg/mysql:seed
</code></pre>

<p>And then go into the new image:</p>

<pre><code>docker run -i -t --rm myorg/mysql:seed bash
</code></pre>

<p>The db files are no longer in /var/lib/mysql, and running <code>mysql -u root -e ""SHOW DATABASES""</code> does not show the <code>mydb</code> database, only the default <code>mysql</code>, <code>information_schema</code>, and <code>performance_schema</code> dbs. I found that if I created a new textfile in the container (<code>echo 'test' &gt; newfile</code>), that file would be present in the committed image, but not the db.</p>

<p>I wonder if this has something to do with the fact that the trusted image Dockerfile has <code>VOLUME [""/etc/mysql"", ""/var/lib/mysql""]</code> so it mounts the db directory as a volume. My Dockerfile does not have this command, but I don't know if it's inherited anyway (I don't really understand how volumes work well enough to know how this might affect my build). I don't need the db mounted as a volume because I need another container to connect to it over a network connection (I'm going to use docker links for that).</p>

<p>FWIW, I am running boot2docker 1.3.2 on OS X 10.9.5.</p>
","<p>I have a container that ran and stopped it's processes after build. I'd like to use that state to build on top of it (like using its data from first run). I did it with a MySQL image (hoping the data will be stored) as follows (there are other containers in the compose as well, that is aimed to connect to this MySQL instance):</p>

<pre><code>mysql:
  image: mysql-custom
  command: mysqld --user=root
  environment:
      MYSQL_ROOT_PASSWORD: ""toor""
      MYSQL_ALLOW_EMPTY_PASSWORD: ""yes""
</code></pre>

<p>mysql-custom is the commit image that has the data during the first run. Is this possible, to have the data in and build that MySQL instance again with the same data, or am I doing something unacceptably wrong here?</p>
"
"46619397","docker version number confusion: 17.06 vs 1.13?","<docker>","52290896","How to determine Docker version","<docker>","<p>I am confused by the version number of <code>docker</code></p>

<p>Some documentation refers to version numbers like <code>17.06</code>  E.g. (from <a href=""https://docs.docker.com/engine/admin/volumes/volumes/"" rel=""noreferrer"">here</a>)</p>

<blockquote>
  <p>However, starting with Docker 17.06, you can also use <code>--mount</code> with standalone containers</p>
</blockquote>

<p>But some other documentation refers to version number like <code>1.13</code>  E.g. (from <a href=""https://docs.docker.com/get-started/#container-diagram"" rel=""noreferrer"">here</a>)</p>

<blockquote>
  <p>Note: version 1.13 or higher is required</p>
</blockquote>

<p>Was the version numbering convention changed at some point in time?</p>
","<p>When I run:</p>

<pre><code>$ docker -v
</code></pre>

<p>the output is:</p>

<pre><code>Docker version 18.06.1-ce, build e68fc7a
</code></pre>

<p>What version they mean when the write</p>

<blockquote>
  <p>If you use Docker 1.13 or higher, use --cpus instead.</p>
</blockquote>

<p>Asking because the difference between 18 and 1 is too big to me. Can the docu be so old? Or do the docker versions increase so fast?</p>
"
"49334897","Best practice for dockerfile maintain?","<docker><dockerfile>","52214664","What if the base image delete by others?","<docker><dockerhub><docker-image>","<p>I have a <code>Dockerfile</code> something like follows:</p>
<pre><code>FROM openjdk:8u151

# others here
</code></pre>
<p>I have 2 questions about the base image:</p>
<h3>1. How to get the tags?</h3>
<p>Usually, I get it from dockerhub, let's say <code>openjdk:8u151</code>, I can get it from <a href=""https://hub.docker.com/r/library/openjdk/tags/"" rel=""noreferrer"">dockerhub's openjdk repository</a>.</p>
<p>If I could get all tags from any local docker command, then I no need to visit web to get the tags, really a little low efficiency?</p>
<h3>2. Will the base image safe?</h3>
<p>I mean if my base image always there?</p>
<p>Look at the above openjdk repo, it is an offical repo.</p>
<p>I found there is only <code>8u151</code> left for me to choose. But I think there should be a lots of <code>jdk8</code> release during the process, so should also a lots of <code>jdk8</code> images there, something like <code>8u101</code>, <code>8u163</code> etc.</p>
<p>So can I guess the maintainer will delete some old images for <code>openjdk</code>?
Then if this happen, how my <code>Dockerfile</code> work? I should always change my base image if my upstream delete there image? Really terrible for me to maintain such kind of thing.</p>
<p>Even if the <code>openjdk</code> really just generate one release of <code>jdk8</code>. My puzzle still cannot be avoided, as <code>dockerhub</code> really afford the <code>delete</code> button for users.</p>
<p>What's the best practice, please suggest, thanks.</p>
","<p>For following dockerfile:</p>

<p><strong>My Dockerfile</strong></p>

<pre><code>FROM ubuntu:12.04
</code></pre>

<p>If the maintainer on dockerhub for ubuntu delete the 12.04 tag, any impact for me?
If any impact, how I can avoid?</p>
"
"49854767","docker php7.2-fpm can't install imap module","<php><docker><imap>","52314179","Errors when installing IMAP extension on official PHP docker image","<php><docker><imap>","<p>I'm having problems trying to get imap working with my docker-compose.</p>

<p>Here is what my php dockerfile looks like.</p>

<pre><code>FROM php:7.2-fpm

RUN apt-get update &amp;&amp; \
  apt-get install -y \
    curl \
    libmcrypt-dev \
    unzip \
    git 

# Install Composer
RUN curl -sS https://getcomposer.org/installer | php -- --install- 
dir=/usr/local/bin --filename=composer
RUN composer --version

# Set timezone to UTC
RUN rm /etc/localtime
RUN ln -s /usr/share/zoneinfo/UTC /etc/localtime
RUN ""date""

RUN apt-get -y install libmagickwand-dev --no-install-recommends \
    &amp;&amp; pecl install imagick \
    &amp;&amp; docker-php-ext-enable imagick \
    &amp;&amp; rm -r /var/lib/apt/lists/*

RUN /usr/local/bin/docker-php-ext-install pdo pdo_mysql

ADD ./scripts/entry.sh /root/init.sh

WORKDIR /var/www/insight
</code></pre>

<p>But I keep getting the error </p>

<pre><code>Call to undefined function imap_open()
</code></pre>

<p>I've been trying a lot of different ways of getting the imap to work, but nothing seems to be working for me. I need to keep using php7.2 so downgrading to php5 is not an option for me. </p>

<p>My ideal outcome is to keep the current php version of fpm and find a nice solution to get imap working with the current dockerfile.</p>

<p>Adding </p>

<pre><code>Docker-php-ext-install imap 
</code></pre>

<p>inside the dockerfile does not seem to work and result with the following error: </p>

<pre><code>configure: error: utf8_mime2text() has new signature, but U8T_CANONICAL is missing. This should not happen. Check config.log for additional information.
</code></pre>
","<p>If you're installing the IMAP extension in your PHP docker image, it's possible that you get some errors like:</p>

<blockquote>
  <p>configure: error: utf8_mime2text() has new signature, but
  U8T_CANONICAL is missing. This should not happen. Check config.log for
  additional information.</p>
</blockquote>

<p>and then:</p>

<blockquote>
  <p>configure: error: This c-client library is built with Kerberos
  support.</p>
</blockquote>
"
"51343767","How does Docker handle different kernel versions?","<docker>","52314446","Which docker version to use for app using linux kernel 2.6?","<docker>","<p>Let's say that I make an image for an OS that uses a kernel of version 10. What behavior does Docker exhibit if I run a container for that image on a host OS running a kernel of version 9? What about version 11?</p>

<p>Does the backward compatibility of the versions matter? I'm asking out of curiosity because the documentation only talks about ""minimum Linux kernel version"", etc. This sounds like it doesn't matter what kernel version the host is running beyond that minimum. Is this true? Are there caveats?</p>
","<h1>Background</h1>
<p>I have a very old C++ monolithic app running on Virtual Machines. The Virtual Machines OS is using linux kernel 2.6. Now I am assigned with the task of containerizing the same. The latest docker v18 needs kernel 3.10.</p>
<h1>Question1</h1>
<p>Can I run docker host with linux kernel 3.10 and the container image containing app built for kernel 2.6? Will my application see any problems? If yes, then what kind of problems? How do I go about finding and fixing them?</p>
<h1>Question2</h1>
<p>Is there a way I can run docker on linux kernel 2.6 and not change my app?</p>
<h1>Question3</h1>
<p>Can I use LXC or some other technology? What are the options other than docker for linux kernel 2.6 apps?</p>
"
"52195175","Strange file permission in docker container (question marks on permission bit and user bit)","<linux><docker><file-permissions>","52214178","File permission displayed a lot question marks in docker container","<linux><docker><file-permissions>","<p>I write a Dockerfile and docker-compose.yml to build a custom image which combines beakerx and cling, which contents are below.</p>

<pre><code>FROM beakerx/beakerx

MAINTAINER liudonghua123 &lt;liudonghua123@gmail.com&gt;

# not works
#RUN conda install xeus-cling notebook -c QuantStack -c conda-forge -y --quiet

USER root
# install gcc7
RUN add-apt-repository -y ppa:jonathonf/gcc-7.2
RUN apt-get update -y
RUN apt-get install -y gcc-7

# revert to beakerx user
USER beakerx
ARG CLING_FILENAME=cling_2018-09-04_ubuntu16
RUN echo ""use ${CLING_FILENAME} for build""
# add the prebuild cling packages
ADD ${CLING_FILENAME}.tar.bz2 /home/beakerx
ENV PATH=/home/beakerx/${CLING_FILENAME}/bin:$PATH
USER root
RUN pip install --upgrade pip
# install steps, https://github.com/root-project/cling/tree/master/tools/Jupyter
RUN cd /home/beakerx/${CLING_FILENAME}/share/cling/Jupyter/kernel &amp;&amp; pip install -e . &amp;&amp; jupyter-kernelspec install --user cling-cpp17 &amp;&amp; jupyter-kernelspec install --user cling-cpp1z &amp;&amp; jupyter-kernelspec install --user cling-cpp14 &amp;&amp; jupyter-kernelspec install --user cling-cpp11
RUN ln -s /usr/bin/gcc-7 /usr/bin/gcc

RUN echo ""root:root"" | chpasswd
RUN echo ""beakerx:beakerx"" | chpasswd

RUN chown -R beakerx:beakerx /home/beakerx/.local
RUN find /home/beakerx/.local -type d -exec chmod 755 {} \;
RUN find /home/beakerx/.local -type f -exec chmod 644 {} \;

RUN id
RUn ls -la /home/beakerx/.local
RUn ls -la /home/beakerx/.local/share

USER beakerx
</code></pre>

<p>and</p>

<pre><code>version: '2'

services:
    beakerx-cling-prebuild:
        build: .
        image: liudonghua123/beakerx-cling-prebuild:latest
        ports:
            - ""28888:8888""
        volumes:
            - ./work:/work
        restart: always
</code></pre>

<p>Some logs of <code>docker-compose build</code> were</p>

<pre><code>Step 22/24 : RUN ls -la /home/beakerx/.local
 ---&gt; Running in 95457585aed0
total 12
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 25 beakerx beakerx 4096 Sep  6 00:51 ..
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 00:51 share
 ---&gt; affcb9f1ca94
Removing intermediate container 95457585aed0
Step 23/24 : RUN ls -la /home/beakerx/.local/share
 ---&gt; Running in 15ea51bcc3bf
total 12
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ..
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 jupyter
 ---&gt; 6f2ca28d4d21
Removing intermediate container 15ea51bcc3bf
Step 24/24 : USER beakerx
 ---&gt; Running in 0ab6574079d7
 ---&gt; 9561902b99ef
Removing intermediate container 0ab6574079d7
Successfully built 9561902b99ef
</code></pre>

<p>But When I run <code>docker-compose up -d</code>, I got <code>PermissionError: [Errno 13] Permission denied: '/home/beakerx/.local/share'</code> error. So I drive into the docker image.</p>

<pre><code>ldh@ldh55:~/docker/cling/jupinger/beakerx-cling-prebuild$ docker run -it --entrypoint='' --rm liudonghua123/beakerx-cling-prebuild bash
beakerx@716d9a8334ca:~$ ls -la ~
total 108
drwxr-xr-x 25 beakerx beakerx  4096 Sep  6 00:51 .
drwxr-xr-x 11 root    root     4096 Sep  6 00:51 ..
-rw-r--r--  1 beakerx beakerx   220 Aug 31  2015 .bash_logout
-rw-r--r--  1 beakerx beakerx  3771 Aug 31  2015 .bashrc
drwxr-xr-x  3 root    root     4096 Mar 13 13:32 .config
-rw-r--r--  1 beakerx beakerx   938 Mar  8 14:03 .gitignore
-rw-r--r--  1 beakerx beakerx    53 Feb 27  2018 .jscsrc
drwxr-xr-x  6 beakerx beakerx  4096 Sep  6 00:51 .local
drwxr-xr-x  3 root    root     4096 Mar 13 13:32 .npm
-rw-r--r--  1 beakerx beakerx   655 May 16  2017 .profile
-rw-r--r--  1 beakerx beakerx  2285 Feb 27  2018 CONTRIBUTING.md
-rw-r--r--  1 beakerx beakerx 11325 Feb 27  2018 LICENSE
-rw-r--r--  1 beakerx beakerx   193 Feb 27  2018 NOTICE
-rw-r--r--  1 beakerx beakerx  8682 Mar 12 14:27 README.md
-rw-r--r--  1 beakerx beakerx  5821 Mar 12 20:04 StartHere.ipynb
-rw-r--r--  1 beakerx beakerx     6 Mar 13 13:32 VERSION
drwxr-xr-x 16 beakerx beakerx  4096 Mar 12 20:11 beakerx
drwxr-xr-x  8   14806    2735  4096 Sep  5 07:44 cling_2018-09-04_ubuntu16
drwxr-xr-x 22 beakerx beakerx  4096 Feb 27  2018 doc
-rw-r--r--  1 beakerx beakerx    81 Feb 27  2018 environment.yml
drwxr-xr-x  6 beakerx beakerx  4096 Feb 27  2018 js
-rwxr-xr-x  1 beakerx beakerx   927 Mar 13 12:42 setup.sh
beakerx@716d9a8334ca:~$ ls -la ~/.local/
ls: cannot access '/home/beakerx/.local/share': Permission denied
total 8
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 25 beakerx beakerx 4096 Sep  6 00:51 ..
d?????????  ? ?       ?          ?            ? share
beakerx@716d9a8334ca:~$ ls -la ~/.local/share
ls: cannot access '/home/beakerx/.local/share': Permission denied
beakerx@716d9a8334ca:~$ 
beakerx@716d9a8334ca:~$ su
Password: 
root@716d9a8334ca:/home/beakerx# ls -la ~
total 28
drwx------  4 root root 4096 Mar 13 13:27 .
drwxr-xr-x 77 root root 4096 Sep  6 01:06 ..
-rw-r--r--  1 root root 3106 Oct 22  2015 .bashrc
drwxr-xr-x  3 root root 4096 Mar 13 13:20 .conda
-rw-r--r--  1 root root   38 Mar 13 13:20 .condarc
drwxr-xr-x  5 root root 4096 Mar 13 13:28 .gradle
-rw-r--r--  1 root root  148 Aug 17  2015 .profile
root@716d9a8334ca:/home/beakerx# ls -la /home/beakerx/
total 108
drwxr-xr-x 25 beakerx beakerx  4096 Sep  6 00:51 .
drwxr-xr-x 11 root    root     4096 Sep  6 00:51 ..
-rw-r--r--  1 beakerx beakerx   220 Aug 31  2015 .bash_logout
-rw-r--r--  1 beakerx beakerx  3771 Aug 31  2015 .bashrc
drwxr-xr-x  3 root    root     4096 Mar 13 13:32 .config
-rw-r--r--  1 beakerx beakerx   938 Mar  8 14:03 .gitignore
-rw-r--r--  1 beakerx beakerx    53 Feb 27  2018 .jscsrc
drwxr-xr-x  6 beakerx beakerx  4096 Sep  6 00:51 .local
drwxr-xr-x  3 root    root     4096 Mar 13 13:32 .npm
-rw-r--r--  1 beakerx beakerx   655 May 16  2017 .profile
-rw-r--r--  1 beakerx beakerx  2285 Feb 27  2018 CONTRIBUTING.md
-rw-r--r--  1 beakerx beakerx 11325 Feb 27  2018 LICENSE
-rw-r--r--  1 beakerx beakerx   193 Feb 27  2018 NOTICE
-rw-r--r--  1 beakerx beakerx  8682 Mar 12 14:27 README.md
-rw-r--r--  1 beakerx beakerx  5821 Mar 12 20:04 StartHere.ipynb
-rw-r--r--  1 beakerx beakerx     6 Mar 13 13:32 VERSION
drwxr-xr-x 16 beakerx beakerx  4096 Mar 12 20:11 beakerx
drwxr-xr-x  8   14806    2735  4096 Sep  5 07:44 cling_2018-09-04_ubuntu16
drwxr-xr-x 22 beakerx beakerx  4096 Feb 27  2018 doc
-rw-r--r--  1 beakerx beakerx    81 Feb 27  2018 environment.yml
drwxr-xr-x  6 beakerx beakerx  4096 Feb 27  2018 js
-rwxr-xr-x  1 beakerx beakerx   927 Mar 13 12:42 setup.sh
root@716d9a8334ca:/home/beakerx# ls -la /home/beakerx/.local/
total 12
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 25 beakerx beakerx 4096 Sep  6 00:51 ..
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 00:51 share
root@716d9a8334ca:/home/beakerx# ls -la /home/beakerx/.local/share/
total 12
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ..
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 jupyter
root@716d9a8334ca:/home/beakerx# exit
exit
beakerx@716d9a8334ca:~$ ls -la ~/.local/share
total 12
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 .
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ..
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 jupyter
beakerx@716d9a8334ca:~$
</code></pre>

<p>Why the permission of <code>~/.local/share</code> was a lot of question marks at the first sight, and when I run as root or return to the original user, it was correct.</p>

<p>I tried run start-notebook.sh manually, the first time it was failed, then the second time it was successfully.</p>

<pre><code>beakerx@716d9a8334ca:~$ cd /usr/local/bin/
beakerx@716d9a8334ca:/usr/local/bin$ ls
start-notebook.sh  start-singleuser.sh  start.sh
beakerx@716d9a8334ca:/usr/local/bin$ 
beakerx@716d9a8334ca:/usr/local/bin$ 
beakerx@716d9a8334ca:/usr/local/bin$ start-notebook.sh 
Execute the command
Traceback (most recent call last):
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/traitlets/traitlets.py"", line 528, in get
    value = obj._trait_values[self.name]
KeyError: 'runtime_dir'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/beakerx/bin/jupyter-notebook"", line 6, in &lt;module&gt;
    sys.exit(notebook.notebookapp.main())
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/jupyter_core/application.py"", line 266, in launch_instance
    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/traitlets/config/application.py"", line 657, in launch_instance
    app.initialize(argv)
  File ""&lt;decorator-gen-7&gt;"", line 2, in initialize
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/traitlets/config/application.py"", line 87, in catch_config_error
    return method(app, *args, **kwargs)
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1505, in initialize
    self.init_configurables()
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1209, in init_configurables
    connection_dir=self.runtime_dir,
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/traitlets/traitlets.py"", line 556, in __get__
    return self.get(obj, cls)
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/traitlets/traitlets.py"", line 535, in get
    value = self._validate(obj, dynamic_default())
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/jupyter_core/application.py"", line 99, in _runtime_dir_default
    ensure_dir_exists(rd, mode=0o700)
  File ""/opt/conda/envs/beakerx/lib/python3.6/site-packages/jupyter_core/utils/__init__.py"", line 13, in ensure_dir_exists
    os.makedirs(path, mode=mode)
  File ""/opt/conda/envs/beakerx/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/home/beakerx/.local/share/jupyter/runtime'
beakerx@716d9a8334ca:/usr/local/bin$ ll /home/beakerx/.local/share/jupyter/runtime
ls: cannot access '/home/beakerx/.local/share/jupyter/runtime': Permission denied
beakerx@716d9a8334ca:/usr/local/bin$ ll /home/beakerx/.local/share/jupyter/       
ls: cannot access '/home/beakerx/.local/share/jupyter/kernels': Permission denied
total 8
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ./
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ../
d????????? ? ?       ?          ?            ? kernels/
beakerx@716d9a8334ca:/usr/local/bin$ ll /home/beakerx/.local/share/        
total 12
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ./
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 ../
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 jupyter/
beakerx@716d9a8334ca:/usr/local/bin$ su
Password: 
root@716d9a8334ca:/usr/local/bin# ls -l /home/beakerx/.local/share/jupyter/runtime
ls: cannot access '/home/beakerx/.local/share/jupyter/runtime': No such file or directory
root@716d9a8334ca:/usr/local/bin# ls -l /home/beakerx/.local/share/jupyter/       
total 4
drwxr-xr-x 18 beakerx beakerx 4096 Sep  6 00:51 kernels
root@716d9a8334ca:/usr/local/bin# ls -l /home/beakerx/.local/share/        
total 4
drwxr-xr-x 6 beakerx beakerx 4096 Sep  6 00:51 jupyter
root@716d9a8334ca:/usr/local/bin# ls -l /home/beakerx/.local/share/jupyter/kernels/
total 16
drwxr-xr-x 2 beakerx beakerx 4096 Sep  6 00:51 cling-cpp11
drwxr-xr-x 2 beakerx beakerx 4096 Sep  6 00:51 cling-cpp14
drwxr-xr-x 2 beakerx beakerx 4096 Sep  6 00:51 cling-cpp17
drwxr-xr-x 2 beakerx beakerx 4096 Sep  6 00:51 cling-cpp1z
root@716d9a8334ca:/usr/local/bin# exit
exit
beakerx@716d9a8334ca:/usr/local/bin$ id
uid=1000(beakerx) gid=1000(beakerx) groups=1000(beakerx)
beakerx@716d9a8334ca:/usr/local/bin$ start-notebook.sh 
Execute the command
[I 01:20:56.433 NotebookApp] Writing notebook server cookie secret to /home/beakerx/.local/share/jupyter/runtime/notebook_cookie_secret
[W 01:20:56.567 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 01:20:56.647 NotebookApp] [beakerx] enabled
[I 01:20:56.736 NotebookApp] JupyterLab beta preview extension loaded from /opt/conda/envs/beakerx/lib/python3.6/site-packages/jupyterlab
[I 01:20:56.736 NotebookApp] JupyterLab application directory is /opt/conda/envs/beakerx/share/jupyter/lab
[I 01:20:56.971 NotebookApp] Serving notebooks from local directory: /usr/local/bin
[I 01:20:56.971 NotebookApp] 0 active kernels
[I 01:20:56.971 NotebookApp] The Jupyter Notebook is running at:
[I 01:20:56.971 NotebookApp] http://[all ip addresses on your system]:8888/?token=1b94f5bf7e14e4ed5defece6870addc630d81eb8aae85990
[I 01:20:56.971 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 01:20:56.972 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=1b94f5bf7e14e4ed5defece6870addc630d81eb8aae85990
</code></pre>

<p>If I modified Dockerfile (move up USER beakerx)</p>

<pre><code>FROM beakerx/beakerx

MAINTAINER liudonghua123 &lt;liudonghua123@gmail.com&gt;

# not works
#RUN conda install xeus-cling notebook -c QuantStack -c conda-forge -y --quiet

USER root
# install gcc7
RUN add-apt-repository -y ppa:jonathonf/gcc-7.2
RUN apt-get update -y
RUN apt-get install -y gcc-7

# revert to beakerx user
USER beakerx
ARG CLING_FILENAME=cling_2018-09-04_ubuntu16
RUN echo ""use ${CLING_FILENAME} for build""
# add the prebuild cling packages
ADD ${CLING_FILENAME}.tar.bz2 /home/beakerx
ENV PATH=/home/beakerx/${CLING_FILENAME}/bin:$PATH
USER root
RUN pip install --upgrade pip
# install steps, https://github.com/root-project/cling/tree/master/tools/Jupyter
RUN cd /home/beakerx/${CLING_FILENAME}/share/cling/Jupyter/kernel &amp;&amp; pip install -e . &amp;&amp; jupyter-kernelspec install --user cling-cpp17 &amp;&amp; jupyter-kernelspec install --user cling-cpp1z &amp;&amp; jupyter-kernelspec install --user cling-cpp14 &amp;&amp; jupyter-kernelspec install --user cling-cpp11
RUN ln -s /usr/bin/gcc-7 /usr/bin/gcc

RUN echo ""root:root"" | chpasswd
RUN echo ""beakerx:beakerx"" | chpasswd
RUN usermod -aG sudo beakerx

RUN echo beakerx | sudo -S chown -R beakerx:beakerx /home/beakerx/
RUN echo beakerx | sudo -S find /home/beakerx/ -type d -exec chmod 755 {} \;
RUN echo beakerx | sudo -S find /home/beakerx/ -type f -exec chmod 644 {} \;

USER beakerx

RUN id
RUn ls -la /home/beakerx/.local
RUn ls -la /home/beakerx/.local/share
</code></pre>

<p>Then some errors occurred (<code>ls -la /home/beakerx/.local</code>).</p>

<pre><code>Step 18/25 : RUN usermod -aG sudo beakerx
 ---&gt; Using cache
 ---&gt; 2b341f8539b9
Step 19/25 : RUN echo beakerx | sudo -S chown -R beakerx:beakerx /home/beakerx/
 ---&gt; Running in 8adb3c593f96
 ---&gt; bd6237aa0196
Removing intermediate container 8adb3c593f96
Step 20/25 : RUN echo beakerx | sudo -S find /home/beakerx/ -type d -exec chmod 755 {} \;
 ---&gt; Running in 5dc9ff3d8d1b
 ---&gt; 98d3338124ce
Removing intermediate container 5dc9ff3d8d1b
Step 21/25 : RUN echo beakerx | sudo -S find /home/beakerx/ -type f -exec chmod 644 {} \;
 ---&gt; Running in 63a4ce864b75
 ---&gt; 0ca175cb1663
Removing intermediate container 63a4ce864b75
Step 22/25 : USER beakerx
 ---&gt; Running in 42f8c91f6930
 ---&gt; 94e9f6099aa3
Removing intermediate container 42f8c91f6930
Step 23/25 : RUN id
 ---&gt; Running in 6f24ee8ce894
uid=1000(beakerx) gid=1000(beakerx) groups=1000(beakerx),27(sudo)
 ---&gt; 58627cedc9de
Removing intermediate container 6f24ee8ce894
Step 24/25 : RUN ls -la /home/beakerx/.local
 ---&gt; Running in cfd7bdd70666
ls: cannot access '/home/beakerx/.local/share': Permission denied
total 8
drwxr-xr-x  6 beakerx beakerx 4096 Sep  6 03:08 .
drwxr-xr-x 43 beakerx beakerx 4096 Sep  6 03:09 ..
d?????????  ? ?       ?          ?            ? share
ERROR: Service 'beakerx-cling-prebuild' failed to build: The command '/bin/sh -c ls -la /home/beakerx/.local' returned a non-zero code: 1
ldh@ldh55:~/docker/cling/jupinger/beakerx-cling-prebuild$
</code></pre>
","<p>I wrote a Dockerfile, the last contents are</p>

<pre><code>RUN echo ""root:root"" | chpasswd
RUN echo ""beakerx:beakerx"" | chpasswd
RUN usermod -aG sudo beakerx

RUN echo beakerx | sudo -S chown -R beakerx:beakerx /home/beakerx/.local
RUN echo beakerx | sudo -S find /home/beakerx/.local -type d -exec chmod 755 {} \;
RUN echo beakerx | sudo -S find /home/beakerx/.local -type f -exec chmod 644 {} \;

RUN id
RUN ls -la /home/beakerx/.local
RUN ls -la /home/beakerx/.local/share

USER beakerx

RUN id
RUN ls -la /home/beakerx/.local
RUN ls -la /home/beakerx/.local/share
</code></pre>

<p>When I build this image, it gave me the following errors.</p>

<pre><code>Step 17/29 : RUN echo ""root:root"" | chpasswd
 ---&gt; Running in b07756b764ef
 ---&gt; 11a182191463
Removing intermediate container b07756b764ef
Step 18/29 : RUN echo ""beakerx:beakerx"" | chpasswd
 ---&gt; Running in 2f2bc836b1af
 ---&gt; dee6ebdf5b9c
Removing intermediate container 2f2bc836b1af
Step 19/29 : RUN usermod -aG sudo beakerx
 ---&gt; Running in 8a1ccfffd565
 ---&gt; d7815406e070
Removing intermediate container 8a1ccfffd565
Step 20/29 : RUN echo beakerx | sudo -S chown -R beakerx:beakerx /home/beakerx/.local
 ---&gt; Running in 19aebc73f517
 ---&gt; a8cb84a563c5
Removing intermediate container 19aebc73f517
Step 21/29 : RUN echo beakerx | sudo -S find /home/beakerx/.local -type d -exec chmod 755 {} \;
 ---&gt; Running in 7c2434fa279a
 ---&gt; 5ce4b0b0e859
Removing intermediate container 7c2434fa279a
Step 22/29 : RUN echo beakerx | sudo -S find /home/beakerx/.local -type f -exec chmod 644 {} \;
 ---&gt; Running in 5f57457f1fe5
 ---&gt; 1bb42b3ef8f3
Removing intermediate container 5f57457f1fe5
Step 23/29 : RUN id
 ---&gt; Running in 101209499f50
uid=0(root) gid=0(root) groups=0(root)
 ---&gt; e45945b090ab
Removing intermediate container 101209499f50
Step 24/29 : RUN ls -la /home/beakerx/.local
 ---&gt; Running in d337b58c1571
total 12
drwxr-xr-x  6 beakerx beakerx 4096 Sep  7 01:30 .
drwxr-xr-x 25 beakerx beakerx 4096 Sep  7 01:30 ..
drwxr-xr-x  6 beakerx beakerx 4096 Sep  7 01:30 share
 ---&gt; 7fd474369e15
Removing intermediate container d337b58c1571
Step 25/29 : RUN ls -la /home/beakerx/.local/share
 ---&gt; Running in e05cd55aaae6
total 12
drwxr-xr-x 6 beakerx beakerx 4096 Sep  7 01:30 .
drwxr-xr-x 6 beakerx beakerx 4096 Sep  7 01:30 ..
drwxr-xr-x 6 beakerx beakerx 4096 Sep  7 01:30 jupyter
 ---&gt; 03191c2d9fc8
Removing intermediate container e05cd55aaae6
Step 26/29 : USER beakerx
 ---&gt; Running in 40b2d522ea0f
 ---&gt; 604503b2152b
Removing intermediate container 40b2d522ea0f
Step 27/29 : RUN id
 ---&gt; Running in e7b8ed6a1165
uid=1000(beakerx) gid=1000(beakerx) groups=1000(beakerx),27(sudo)
 ---&gt; 5987e9d9f0bb
Removing intermediate container e7b8ed6a1165
Step 28/29 : RUN ls -la /home/beakerx/.local
 ---&gt; Running in 4c65bd4a383e
ls: cannot access '/home/beakerx/.local/share': Permission denied
total 8
drwxr-xr-x  6 beakerx beakerx 4096 Sep  7 01:30 .
drwxr-xr-x 25 beakerx beakerx 4096 Sep  7 01:30 ..
d?????????  ? ?       ?          ?            ? share
ERROR: Service 'beakerx-cling-prebuild' failed to build: The command '/bin/sh -c ls -la /home/beakerx/.local' returned a non-zero code: 1
</code></pre>

<p>That's quite strange, I can see the right permission using root, but a lot of question marks using other users. When I removed these debugging code and run this docker image, it gave me <code>PermissionError: [Errno 13] Permission denied: '/home/beakerx/.local/share/jupyter/runtime'</code> errors.</p>

<p>I have searched a lot on the Internet, but couldn't found some helpful info about this.</p>
"
"21736048","Log stdout and stderr from ssh command in the same order it was created","<java><ssh><jsch>","43914014","Run docker command with JSch","<java><docker><jsch>","<p><b>Short Version:</b>  Is it possible to log the stdout and stderr on the local side of a command executed remotely via ssh in the same order as it was output on the remote host?   If so, how?</p>

<p><b>Long Version:</b></p>

<p>I am attempting to log the standard and error output from a remotely exec'd SSH command (using Jsch) in the same order as it was output by the remote command.  In other words, if the remote command writes ""a"" to stdout, then ""b"" to stderr, then ""c"" to stdout, I want the log on the client (local) side to read:</p>

<p>a<br/>
b<br/>
c<br/></p>

<p>Below is what I have so far.  It comes relatively close to what I want, but I think it will be apparent that it does not guarantee the correct output order on the client side.</p>

<pre><code>public int exec(String strCommand) throws ExceptionUnableToExecCommand {
    JSch jsch = new JSch();
    Session session = null;
    ChannelExec channel = null;
    try {
          session = jsch.getSession(user, host, 22);
          UserInfo ui = new cyclOps.jsch.UserInfo(password);
          session.setUserInfo(ui);
          session.connect();
          channel = (ChannelExec) session.openChannel(""exec"");
          channel.setCommand(strCommand);
          channel.setInputStream(null);
          InputStream in = channel.getInputStream();
          InputStream err = channel.getErrStream();
          channel.connect();
          /* getOutput() defined below. */
          return this.getOutput(channel, in, err);
    } catch (JSchException | IOException e) {
        throw new ExceptionUnableToExecCommand(""Unable to execute "" + strCommand + "" "" + this.toString(), e);
    } finally {
        if (channel != null) channel.disconnect();
        if (session != null) session.disconnect();
    }
}

private int getOutput(ChannelExec channel, InputStream in, InputStream err) throws IOException { 
    byte[] tmp = new byte[1024];
    while(true){
        while(in.available() &gt; 0){
            int i=in.read(tmp, 0, 1024);
            if(i&lt;0)break;
            this.sshLogger.logOutputFromSSH(new String(tmp, 0, i));
        }
        while(err.available() &gt; 0){
            int i=err.read(tmp, 0, 1024);
            if(i&lt;0)break;
            this.sshLogger.logOutputFromSSH(new String(tmp, 0, i));
        }
        if(channel.isClosed()){
            return channel.getExitStatus();
        }
        try{Thread.sleep(1000);}catch(Exception ee){}
    }
}
</code></pre>

<p>I think I should point out that this is a modified version of Exec.java from the Jsch web site examples.</p>
","<p>I´m using JSch to run a docker command, but it´s not working, and it´s not returning anything.</p>

<pre><code>    ChannelExec channel = (ChannelExec) session.openChannel(""exec"");
    BufferedReader in = new BufferedReader(new InputStreamReader(channel.getInputStream()));
    channel.setCommand(""docker ps -a"");
    channel.connect();



 String msg = null;
        while ((msg = in.readLine()) != null) {
            System.out.println(msg);
        }
</code></pre>

<p>If I run a bash command as 'pwd' or 'ls -l' works fine.</p>

<p>I can expect that the docker command will work on ChannelExec and not ChannelShell or any other type?</p>

<pre><code>session.openChannel(""exec"")
</code></pre>

<p>Regards.</p>
"
"34398632","Docker how to run pip requirements.txt only if there was a change?","<python><docker><dockerfile>","43888438","How to prevent Docker from re-running pip installs every time I modify code","<docker><pip><dockerfile>","<p>In a Dockerfile I have a layer which installs <code>requirements.txt</code>:</p>

<pre><code>FROM python:2.7
RUN pip install -r requirements.txt
</code></pre>

<p>When I build the docker image it runs the whole process <strong>regardless</strong> of any changes made to this file.</p>

<p>How do I make sure Docker only runs <code>pip install -r requirements.txt</code> if there has been a change to the file?</p>

<pre><code>Removing intermediate container f98c845d0f05
Step 3 : RUN pip install -r requirements.txt
 ---&gt; Running in 8ceb63abaef6
Collecting https://github.com/tomchristie/django-rest-framework/archive/master.zip (from -r requirements.txt (line 30))
  Downloading https://github.com/tomchristie/django-rest-framework/archive/master.zip
Collecting Django==1.8.7 (from -r requirements.txt (line 1))
</code></pre>
","<p>I have a scraper with the following <code>Dockerfile</code>:</p>

<pre><code># Adapted from trcook/docker-scrapy
FROM python:alpine
RUN apk --update add libxml2-dev libxslt-dev libffi-dev gcc musl-dev libgcc openssl-dev
COPY . /scraper
RUN pip install -r /scraper/requirements.txt
WORKDIR /scraper/apkmirror_scraper
CMD [""scrapy"", ""crawl"", ""apkmirror""]
</code></pre>

<p>The code for the scraper is located in <code>/scraper/apkmirror_scraper</code>, and the requirements in <code>scraper/requirements.txt</code>. I've noticed that every time I modify the code and build the image, it re-runs the <code>pip install -r requirements.txt</code> rather than using the local cache.</p>

<p>How can I prevent this and make it use the local cache? </p>

<p>(One 'theory' about this is that whereas <code>/scraper/requirements.txt</code> itself hasn't changed, the <code>/scraper</code> directory has, which makes the <code>RUN</code> directive have to 're-run'; in this case it might help to move <code>requirements.txt</code> to a different directory. I wasn't able to verify whether this 'theory' is correct from <a href=""https://docs.docker.com/engine/reference/builder/#run"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/builder/#run</a>, however). </p>
"
"37706635","In Docker, apt-get install fails with ""Failed to fetch http://archive.ubuntu.com/ ... 404 Not Found"" errors. Why? How can we get past it?","<ubuntu><docker>","44293651","Docker build fail","<docker>","<p>My team uses Docker (with <code>ubuntu:14.04</code> base image) for local development and we often have to rebuild some or all of our images. But we often get failures downloading packages with <code>apt-get install</code>, even immediately after running <code>apt-get -y update</code>. For instance, today I see</p>

<pre><code>Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libxml2 amd64 2.9.1+dfsg1-3ubuntu4.7
  404  Not Found [IP: 91.189.88.161 80]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libxml2-dev amd64 2.9.1+dfsg1-3ubuntu4.7
  404  Not Found [IP: 91.189.88.161 80]
Fetched 84.7 MB in 1min 6s (1281 kB/s)
Unable to correct missing packages.
E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libx/libxml2/libxml2_2.9.1+dfsg1-3ubuntu4.7_amd64.deb  404  Not Found [IP: 91.189.88.161 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libx/libxml2/libxml2-dev_2.9.1+dfsg1-3ubuntu4.7_amd64.deb  404  Not Found [IP: 91.189.88.161 80]

E: Aborting install.
</code></pre>

<p>Apparently the specific version of a particular package has been deleted from the archive and replaced with a slightly differently named patch version. For instance, the above error is looking for <code>libxml2_2.9.1+dfsg1-3ubuntu4.7_amd64.deb</code> but the version on the server is <code>libxml2_2.9.1+dfsg1-3ubuntu4.8_amd64.deb</code>.</p>

<p>Often this is solvable by removing the base image (<code>docker rmi ubuntu:14.04</code>) and rebuilding; the newly downloaded ubuntu image has the correct patch number and finds the right archive file. But even this doesn't always work -- probably due to a delay between a new minor upgrade to Ubuntu's dependency db and the deployment of that new <code>ubuntu:14.04</code> image onto Docker Hub.</p>

<p>We've tried using <code>apt-get</code> flags <code>--fix-missing</code> and <code>--fix-broken</code> and those don't consistently work either.</p>

<p>Any other ideas?</p>

<p><a href=""https://stackoverflow.com/questions/34778262/apt-get-install-fails-with-not-found-error-because-package-removed-from-reposito"">apt-get install fails with Not Found error because package removed from repository</a> is a similar problem but the accepted answer is unacceptable because it's not possible to be automated. Our daily development process, including automatic build and deploy, is all scripted and using Docker and it's not practical to hack around inside a Dockerfile every time a particular archive goes missing (then remove the hack after a few hours or days).</p>

<hr>

<p>In response to @prateek05, here's the <code>/etc/apt/sources.list</code> from the official <code>ubuntu:14.04</code> docker image:</p>

<pre><code>root@72daa1942714:/# cat /etc/apt/sources.list
# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to
# newer versions of the distribution.

deb http://archive.ubuntu.com/ubuntu/ trusty main restricted
deb-src http://archive.ubuntu.com/ubuntu/ trusty main restricted

## Major bug fix updates produced after the final release of the
## distribution.
deb http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted
deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted

## Uncomment the following two lines to add software from the 'universe'
## repository.
## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu
## team. Also, please note that software in universe WILL NOT receive any
## review or updates from the Ubuntu security team.
deb http://archive.ubuntu.com/ubuntu/ trusty universe
deb-src http://archive.ubuntu.com/ubuntu/ trusty universe
deb http://archive.ubuntu.com/ubuntu/ trusty-updates universe
deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates universe

## N.B. software from this repository may not have been tested as
## extensively as that contained in the main release, although it includes
## newer versions of some applications which may provide useful features.
## Also, please note that software in backports WILL NOT receive any review
## or updates from the Ubuntu security team.
# deb http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted
# deb-src http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted

deb http://archive.ubuntu.com/ubuntu/ trusty-security main restricted
deb-src http://archive.ubuntu.com/ubuntu/ trusty-security main restricted
deb http://archive.ubuntu.com/ubuntu/ trusty-security universe
deb-src http://archive.ubuntu.com/ubuntu/ trusty-security universe
# deb http://archive.ubuntu.com/ubuntu/ trusty-security multiverse
# deb-src http://archive.ubuntu.com/ubuntu/ trusty-security multiverse
</code></pre>
","<p>My docker build command fails with the message:</p>

<pre><code>Step 3/23 : RUN apt-get install   vim -yqq   cron -yqq   python-software-properties -yqq   supervisor -yqq  python-dev -yqq   python -yqq   python3-dev -yqq   python-distribute -yqq   python-pip -yqq   python-numpy -yqq   libjpeg8-dev -yqq   libfreetype6-dev -yqq   libxft-dev -yqq   curl -yqq   unzip -yqq
 ---&gt; Running in 38d814924eaa
E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/u/unattended-upgrades/unattended-upgrades_0.90ubuntu0.5_all.deb  404  Not Found [IP: 91.189.88.162 80]

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
The command '/bin/sh -c apt-get install   vim -yqq   cron -yqq   python-software-properties -yqq   supervisor -yqq  python-dev -yqq   python -yqq   python3-dev -yqq   python-distribute -yqq   python-pip -yqq   python-numpy -yqq   libjpeg8-dev -yqq   libfreetype6-dev -yqq   libxft-dev -yqq   curl -yqq   unzip -yqq' returned a non-zero code: 100
</code></pre>

<p>My Dockerfile is (the relevant portion)</p>

<pre><code>FROM ubuntu:latest

RUN apt-get update
RUN apt-get install \
  vim -yqq \
  cron -yqq \
  software-properties-common -yqq \
  python-software-properties -yqq \
  supervisor -yqq\
  python-dev -yqq \
  python -yqq \
  python3-dev -yqq \
  python-distribute -yqq \
  python-pip -yqq \
  python-numpy -yqq \
  libjpeg8-dev -yqq \
  libfreetype6-dev -yqq \
  libxft-dev -yqq \
  curl -yqq \
  unzip -yqq
</code></pre>

<p>It was working perfectly fine last week but today I am getting the aforementioned error message. What is happening here?</p>
"
"44339237","Missing Carriage Return in Docker for Mac Containers","<bash><macos><docker>","44461438","Docker shell 'ls -l' indenting newlines","<bash><shell><docker>","<p>All of a sudden all of my <code>docker/docker-compose exec/run</code> commands are printing logs that are lacking a carriage return making command line impossible to read due to indentation (see photos below).</p>

<p>I re-installed docker to factory settings, but that didn't fix anything.</p>

<p>where else should I look to solve this sort of problem?</p>

<p><strong>Update</strong> 
This is an active issue in <a href=""https://github.com/docker/for-mac/issues/1681"" rel=""noreferrer"">docker-for-mac</a>.</p>

<p>I just updated to 17.06.0-rc1-ce-mac13 and that is when I started having the problems.</p>

<p>Also, can you leave a comment if you are voting to close?</p>

<p><a href=""https://i.stack.imgur.com/vkF2u.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vkF2u.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/fEw3A.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fEw3A.jpg"" alt=""enter image description here""></a></p>
","<p>How would I go about setting up the bash shell environment for a docker image, such as <code>ubuntu:latest</code>, so that when I run the <code>ls -l</code> the output is what I'm used to (straight columns)?</p>

<p>Even with <code>ls -1</code> I see every newline starting at the position of the last character of the previous line:</p>

<p>Run container:
<code>docker run --rm -it ubuntu:latest bash</code></p>

<pre><code>root@b82014f4ddbf:/# ls -1
                          bin
                             boot
                                 dev
                                    etc
                                       home
                                           lib
                                              lib64
                                                   media
                                                        mnt
                                                           opt
                                                              proc
                                                                  root
                                                                      run
                                                                         sbin
                                                                             srv
                                                                                sys
                                                                                   tmp
                                                                                      usr
                                                                                         var
                                                                                        root@b82014f4ddbf:/#
</code></pre>

<p>Perhaps I'm missing a .bashrc profile or something else common? TIA.</p>
"
"13194272","What does an underscore and interface name after keyword var mean?","<syntax><interface><go><underscores>","30367803","Why declare like ""var _ I = T{}"" and ""var _ I = &T{}"" continuously?","<go><registry><docker>","<p>From <a href=""http://golang.org/src/pkg/database/sql/driver/types.go"" rel=""noreferrer"">http://golang.org/src/pkg/database/sql/driver/types.go</a>:</p>

<pre><code>type ValueConverter interface {
    // ConvertValue converts a value to a driver Value.
    ConvertValue(v interface{}) (Value, error)
}

var Bool boolType

type boolType struct{}

var _ ValueConverter = boolType{} // line 58

func (boolType) String() string { return ""Bool"" }

func (boolType) ConvertValue(src interface{}) (Value, error) {....}
</code></pre>

<p>I known that ValueConverter is an interface name. Line 58 seems to declare that boolType implement interface ValueConverter, but is that necessary? I deleted line 58 and the code works well.</p>
","<p>When I read a copy of the docker/distribution source code, I find there are variables declared which make me quite confused.
<a href=""https://github.com/docker/distribution/blob/65b0d73cb749fd9757352f472e399024f1c597df/registry/storage/driver/fileinfo.go#L55-56"" rel=""noreferrer"">The code is</a>:</p>

<pre><code>var _ FileInfo = FileInfoInternal{}
var _ FileInfo = &amp;FileInfoInternal{}
</code></pre>

<p>I don't know what the declare mean, and hope to get some help.</p>
"
"27989751","Mount SMB/CIFS share within a Docker container","<linux><windows><docker><mount><cifs>","30493300","Docker - Mount Windows Network Share Inside Container","<python><linux><windows><docker><boot2docker>","<p>I have a web application running in a Docker container. This application needs to access some files on our corporate file server (Windows Server with an Active Directory domain controller). The files I'm trying to access are image files created for our clients and the web application displays them as part of the client's portfolio.</p>

<p>On my development machine I have the appropriate folders mounted via entries in <code>/etc/fstab</code> and the host mount points are mounted in the Docker container via the <code>--volume</code> argument. This works perfectly.</p>

<p>Now I'm trying to put together a production container which will be run on a different server and which doesn't rely on the CIFS share being mounted on the host. So I tried to add the appropriate entries to the <code>/etc/fstab</code> file in the container &amp; mounting them with <code>mount -a</code>. I get <code>mount error(13): Permission denied</code>.</p>

<p>A little research online led me to <a href=""http://opensource.com/business/14/9/security-for-docker"" rel=""noreferrer"">this article about Docker security</a>. If I'm reading this correctly, it appears that Docker explicitly denies the ability to mount filesystems within a container. I tried mounting the shares read-only, but this (unsurprisingly) also failed.</p>

<p>So, I have two questions:</p>

<ol>
<li><p>Am I correct in understanding that Docker prevents any use of <code>mount</code> inside containers?</p></li>
<li><p>Can anyone think of another way to accomplish this <strong>without</strong> mounting a CIFS share on the host and then mounting the host folder in the Docker container?</p></li>
</ol>
","<p>I have a small Python application that I'd like to run on Linux in Docker (using boot2docker for now). This application reads some data from my Windows network share, which works fine on Windows using the network path but fails on Linux. After doing some research I figured out how to mount a Windows share on Ubuntu. I'm attempting to implement the dockerfile so that it sets up the share for me but have been unsuccessful so far. Below is my current approach, which encounters operation not permitted at the mount command during the build process. </p>

<pre><code>#Sample Python functionality
import os
folders = os.listdir(r""\\myshare\folder name"")

#Dockerfile
RUN apt-get install cifs-utils -y
RUN mkdir -p ""//myshare/folder name""
RUN mount -t cifs ""//myshare/folder name"" ""//myshare/folder name"" -o username=MyUserName,password=MyPassword

#Error at mount during docker build
#""mount: error(1): Operation not permitted""
#Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)
</code></pre>

<p><strong>Edit</strong>
Not a duplicate of <a href=""https://stackoverflow.com/questions/27989751/mount-smb-cifs-share-within-a-docker-container"">Mount SMB/CIFS share within a Docker container</a>. The solution for that question references a fix during <code>docker run</code>. I can't run <code>--privileged</code> if the docker build process fails.</p>

<hr>

<p><strong>Q: What is the correct way to mount a Windows network share inside a Docker container?</strong></p>

<hr>
"
"30172605","How do I get into a Docker container's shell?","<docker><docker-container>","48989421","How to access a docker container through SSH?","<docker><ssh>","<p>I'm getting started working with Docker. I'm using the WordPress base image and docker-compose.</p>

<p>I'm trying to ssh into one of the containers to inspect the files/directories that were created during the initial build. I tried to run <code>docker-compose run containername ls -la</code>, but that didn't do anything. Even if it did, I'd rather have a console where I can traverse the directory structure, rather than run a single command. What is the right way to do this with Docker?</p>
","<p>I am currently <strong>thinking</strong> of building a docker image for my ipython parallel nodes. Because its a pain to configure each manually with commands. Will i be able to access this image (located on a different PC on my LAN) simply by typing ssh user@ip on my laptop (Master Node)? How do i get the ip of the docker image running on my Node?</p>
"
"30717347","docker-machine create node without tls verification","<docker><docker-machine>","30716874","docker-machine without TLS verification","<docker><boot2docker><docker-machine>","<p>When I create a node with <strong>docker-machine</strong></p>

<pre><code>docker-machine create -d virtualbox node1
</code></pre>

<p>it is created with tls verification enabled for docker deamon which made things a bit more of a hassle than normal for swarm.</p>

<p>I want to create a node with <strong>docker-machine</strong> without tls verification for testing purpose.</p>

<p>I tried with:</p>

<pre><code>docker-machine create -d virtualbox --engine-tls false node1
</code></pre>

<p>and</p>

<pre><code>docker-machine create -d virtualbox --engine-tls-verify false node1
</code></pre>

<p>and</p>

<pre><code>docker-machine create -d virtualbox --engine-opt-tls false node1
</code></pre>
","<p>I'm using docker-machine v0.3.0-RC-1 and I need create a host on virtualbox without TLS verification, but I don't know if it's posible to do it? Anybody know a ligth way to do it?</p>
"
"37904682","How do I use Docker environment variable in ENTRYPOINT array?","<docker><dockerfile>","49016801","Unable to pass variables to run docker container","<bash><shell><variables><docker><environment-variables>","<p>If I set an environment variable, say <code>ENV ADDRESSEE=world</code>, and I want to use it in the entry point script concatenated into a fixed string like:</p>

<pre><code>ENTRYPOINT [""./greeting"", ""--message"", ""Hello, world!""]
</code></pre>

<p>with <code>world</code> being the value of the environment varible, how do I do it? I tried using <code>""Hello, $ADDRESSEE""</code> but that doesn't seem to work, as it takes the <code>$ADDRESSEE</code> literally.</p>
","<p>I am trying to pass some variables to run my docker container. I have this in my dockerfile </p>

<pre><code>#base stuff here
CMD [""echo"", ""$ENV""]
</code></pre>

<p>The output I get is this</p>

<pre><code>$ENV
</code></pre>

<p>I am running my container like this </p>

<pre><code>docker run -e ENV='hello' imagename
</code></pre>

<p>Why is it printing out $ENV and not my variable? If I start an interactive bash session then it will print out my variable but it through this way. </p>
"
"38882654","docker entrypoint running bash script gets ""permission denied""","<bash><shell><docker>","49487552","Odd permission denied from docker executing script","<postgresql><docker><jenkins>","<p>I'm trying to dockerize my node.js app. When the container is built I want it to run a <code>git clone</code> and then start the node server. Therefore I put these operations in a .sh script. And run the script as a single command in the ENTRYPOINT:</p>

<pre><code>FROM ubuntu:14.04

RUN apt-get update &amp;&amp; apt-get install -y build-essential libssl-dev gcc curl npm git

#install gcc 4.9
RUN apt-get install -y software-properties-common python-software-properties
RUN add-apt-repository -y ppa:ubuntu-toolchain-r/test
RUN apt-get update
RUN apt-get install -y libstdc++-4.9-dev

#install newst nodejs
RUN curl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -
RUN apt-get install -y nodejs

RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

ADD package.json /usr/src/app/
RUN npm install

ADD docker-entrypoint.sh /usr/src/app/

EXPOSE 8080

ENTRYPOINT [""/usr/src/app/docker-entrypoint.sh""] 
</code></pre>

<p>My docker-entrypoint.sh looks like this:</p>

<pre><code>git clone git@&lt;repo&gt;.git
git add remote upstream git@&lt;upstream_repo&gt;.git

/usr/bin/node server.js
</code></pre>

<p>After building this image and run:</p>

<pre><code>docker run --env NODE_ENV=development -p 8080:8080 -t -i &lt;image&gt;
</code></pre>

<p>I'm getting: </p>

<pre><code>docker: Error response from daemon: oci runtime error: exec: ""/usr/src/app/docker-entrypoint.sh"": permission denied.
</code></pre>

<p>I shell into the container and the permission of docker-entrypoint.sh is:</p>

<pre><code>-rw-r--r-- 1 root root 292 Aug 10 18:41 docker-entrypoint.sh
</code></pre>

<p>three questions:</p>

<ol>
<li><p>Does my bash script have wrong syntax?</p></li>
<li><p>How do I change the permission of a bash file before adding it into an image?</p></li>
<li><p>What's the best way to run multiple git commands in entrypoint without using a bash script?</p></li>
</ol>

<p>Thanks.</p>
","<p>I have created my own docker image for postgres because I needed some customization that wasn't out of the box available on the normal postgres images.</p>

<p>A part of my image is adding a setup script that is executed each time the container is started. And when I run in on my local machine it works exactly as expected. However when executed on our jenkins server I get this error:</p>

<pre><code>docker: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused ""exec: \""/script/setup.sh\"": permission denied"".
</code></pre>

<p>Which I find really odd, especially since docker is supposed to remove the issues of ""it works on my machine""</p>

<p>The script is pretty much just executing some postgres commands and moving files around and I don't think that it is the problem here.
But this is how I add it:</p>

<pre><code>USER root

RUN mkdir /script
ADD script /script
RUN chown -R postgres:postgres /script

USER postgres
</code></pre>

<p>So I am a bit confused as to wether or not this is an image/container problem or could it be because the jenkins server is lacking permissions to run the docker image? which btw would be odd, since it has permissions to build and delete images</p>
"
"48507357","Access vagrant VMs from inside docker container","<docker><networking><vagrant>","49317065","Set Docker's containers on same network as VM","<docker><networking><vagrant><bridge>","<p>How do I setup a network between 2 centos VM's using Vagrant/Virtualbox and a docker container using docker for mac. They all need to be able to access each other. </p>

<p>Currently I'm not able to access the vm's from within a docker container.</p>

<p>Vagrant setup:</p>

<pre><code>Vagrant.configure(""2"") do |config|

   config.vm.define ""build"" do |build|
      build.vm.box = ""centos/7""
      build.vm.provider ""virtualbox""
      build.vm.hostname = ""server-a""
      build.vm.network ""private_network"", ip: ""192.168.50.4""
   end

   config.vm.define ""test"" do |test|
      test.vm.box = ""centos/7""
      test.vm.provider ""virtualbox""
      test.vm.hostname = ""server""
      test.vm.network ""private_network"", ip: ""192.168.50.5""
   end
end
</code></pre>

<p>The vm's can access each other but the docker container can't access the vm's</p>

<pre><code>docker network create -d bridge --gateway=192.168.50.1 --subnet=192.168.50.1/24 mybridge
docker run --network=mybridge alpine ping 192.168.50.4
=&gt; not able to connect
</code></pre>
","<p>I have a Laravel environment on a VM provided by Vagrant on the 192.168.33.0/24 network, and created some apps containers on the host for it. </p>

<p>For now, they're communicating using port forwarding and are on the default docker's 172 network, but I need to plug them on the 192.168.33.0/24 network in order to make Gearman PHP Workers share the same .env file values, but I can't make it work.</p>

<p>A bit of google took me here <a href=""https://qiita.com/kjtanaka/items/f16757c1f0cc86ff225b"" rel=""nofollow noreferrer"">https://qiita.com/kjtanaka/items/f16757c1f0cc86ff225b</a> but it's not exactly what I need, the bridge has to be plugged on the .33.1 interface in my case and from the host. I tried to build some bridges but I can't make it work:</p>

<ul>
<li><p>created a bridge on the host with 192.168.33.1 as gateway using <code>docker network create</code></p></li>
<li><p>plugged the ""vboxnet0"" .33.33 interface on it</p></li>
<li><p>took my testing redis container using this subnet as 33.101</p></li>
</ul>

<p>This way, my container is on .33.101, but my VM isn't even able to ping 33.1 (but still got ssh from the 10.X NAT interface).</p>

<p>Where am I doing it wrong ?</p>
"
"60235353","Docker is not running on Colab","<docker><google-colaboratory>","49503893","How to run docker images in google-colaboratory?","<docker><docker-swarm><google-colaboratory>","<p>I have tried  to install Docker on google Colab through the following ways:</p>

<p>(1)<a href=""https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04</a> </p>

<p>(2)<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04</a></p>

<p>(3)<a href=""https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP"" rel=""noreferrer"">https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP</a></p>

<p>I started the docker service and saw the status, but it showed 'Docker is not running'. Maybe the docker can not work on the Colab.
<a href=""https://i.stack.imgur.com/Xxs7O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xxs7O.png"" alt=""enter image description here""></a></p>

<p>I feel confused and want to know the reason.</p>

<p>Thanks</p>
","<p>How to run docker images in google-colaboratory ?
I tried to install docker service in google-colaboratory.</p>

<p>However, I got the error message about "" 
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"", when I executed the docker's commands.</p>

<p><a href=""https://i.stack.imgur.com/URNQ7.png"" rel=""noreferrer"">screen-snap</a></p>

<p>I wonder How to solve this problem?</p>

<p>THX</p>
"
"8026281","Parse X-Forwarded-For to get ip with werkzeug on Heroku","<python><heroku><ip><flask><werkzeug>","37083041","How to pass correct client ip from nginx running in one container to python-flask app running 2nd container?","<python><nginx><flask><docker><iptables>","<p>Heroku proxies requests from a client to server, so you have to parse the X-Forwarded-For to find the originating IP address.</p>

<p>The general format of the X-Forwarded-For is:</p>

<pre><code>X-Forwarded-For: client1, proxy1, proxy2
</code></pre>

<p>Using werkzeug on flask, I'm trying to come up with a solution in order to access the originating IP of the client.</p>

<p>Does anyone know a good way to do this?</p>

<p>Thank you!</p>
","<p>I have two docker container running, one is a nginx as reverse-proxy on 80,443 that accepts http and https requests and passes them to the another container which is a Python Flask App container running on port 8000.</p>

<p>If I run nginx on host machine instead of running it in a container, flask app can read the correct client ip address but </p>

<p>if I run nginx also in a container, it shows only docker0 network interface's ip address to the flask application running in 2nd container, no matter what the actual client ip is.</p>

<p>In Python Flask app I use the following code:</p>

<pre><code>ip = request.environ.get('HTTP_X_REAL_ip', request.remote_addr)
</code></pre>

<p>Docker-Compose File:</p>

<pre><code>mongo:
  restart: always
  image: mongo:latest
  ports:
    - '27017'
  volumes:
    - /opt/python/apps/myapp/PersistentDockerVolumes/mongo/data:/data/db/

redis:
  restart: always
  build: ./redis
  command: [""redis-server"", ""/usr/local/etc/redis/redis.rb""]
  ports:
    - '6379'

myapp:
  restart: always
  build: ./myapp
  command: [""gunicorn"", ""--config=gunicorn.py"", ""myapp:app""]
  environment:
    DEBIAN_FRONTEND: 'noninteractive'
    PYTHONUNBUFFERED: 'true'
  links:
    - mongo
    - redis
  volumes:
    - /opt/python/apps/myapp/logs/myapp/:/myapp/log/
  ports:
    - '8000'

nginx:
  restart: always
  build: ./nginx
  command: [""nginx"", ""-g"", ""daemon off;""]
  links:
    - myapp
  volumes:
    - /opt/python/apps/myapp/logs/nginx:/etc/nginx/logs
  volumes_from:
    - myapp
  ports:
    - '80:80'
    - '443:443'
</code></pre>

<p>In the nginx config, I have defined an proxy pass to myapp:</p>

<pre><code>proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
</code></pre>

<p>I tried looking for solution, but none of them is for python flask application. My scenario is similar to <a href=""https://stackoverflow.com/questions/34729849/passing-correct-ip-via-linked-docker-containers-from-nginx-to-jetty"">this</a>, but the difference is I have it for Flask instead of Jetty and the solution is very specific for Jetty.</p>

<p>Another thread which is partially similar is <a href=""https://github.com/jwilder/nginx-proxy/issues/130"" rel=""nofollow noreferrer"">here</a>, scenario discussed in this thread is nginx server behind nginx reverse proxy, which is again not the same scenario, and I am skeptical about running docker with iptables disabled.</p>
"
"21928691","How to continue a Docker container which has exited","<docker>","38067511","Start an existing docker ubuntu container","<docker>","<p>Consider:</p>

<pre><code>docker run -it centos /bin/bash
</code></pre>

<p>I pressed <kbd><strong>Ctrl</strong></kbd>+<kbd><strong>D</strong></kbd> to exit it.</p>

<p>I want to continue to run this container, but I found I can't.</p>

<p>The only method is</p>

<pre><code>docker commit `docker ps -q -l` my_image
docker run -it my_image /bin/bash
</code></pre>

<p>Am I right? Is there a better method? (I'm using docker 0.8.0.)</p>
","<p>A related question &amp; answer on  <a href=""https://stackoverflow.com/questions/25985741/how-to-start-a-docker-container-ubuntu-image"">How to start a docker container (ubuntu image)</a> suggest using <code>docker run -it ubuntu</code> to start a ubuntu container and connect to it. However the run command creates and starts a new ubuntu container.</p>

<p>How do we start an existing docker container (ubuntu image) given it's <code>CONTAINER_ID</code> without creating a new container?</p>

<p>Example:</p>

<p><code>docker ps -a</code></p>

<pre><code>CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                          PORTS               NAMES
9f297d02f419        ubuntu              ""/bin/bash""         3 seconds ago        Exited (0) 1 seconds ago                            cranky_wilson
</code></pre>

<p>How do we start <code>9f297d02f419</code> ?</p>
"
"25783324","docker: SSH access directly into container","<linux><ssh><virtualization><docker>","37965790","How do I SSH to a Docker in Mac container","<macos><ssh><docker><beta>","<p>Up to now we use several linux users:</p>

<ul>
<li>system_foo@server</li>
<li>system_bar@server</li>
<li>...</li>
</ul>

<p>We want to put the system users into docker container.</p>

<ul>
<li>linux user <code>system_foo</code> --> container <code>system_foo</code></li>
</ul>

<p>The changes inside the servers are not problem, but remote systems use these users to send us data.</p>

<p>We need to make <code>ssh system_foo@server</code> work. The remote systems can't be changed.</p>

<p>I would be very easy if there would be just one system per linux operating system (pass port 22 to the container). But there are several.</p>

<p>How can we change from the old scheme to docker containers and keep the service <code>ssh system_foo@server</code> available without changes at the remote site?</p>

<p>Please leave a comment if you don't understand the question. Thank you.</p>
","<p>I am running Docker for Mac (Version 1.12.0-rc2-beta16 (Build: 9493)).</p>

<p>I have pulled an image from my local repository, and used 'docker run -d' to create a container. Using 'docker ps' I obtained the 'CONTAINER ID', and then used 'docker inspect &lt;CONTAINER_ID&gt;| grep IPA' to obtain the IP address for the running container.</p>

<p>I now want to connect to the container using SSH with 'ssh root@&lt;IP address&gt;' but the command gives the following error: 'Operation timed out'.</p>

<p>Further investigation shows that I can not ping the &lt;IP address&gt; -> 'Request timeout for icmp_seq 0'</p>

<p>How can I connect to the container using SSH? What is the correct command?</p>

<p>UPDATE: This IS NOT a duplicate (as stated above). The entry that begins ""The scenario you described"" is the correct solution.</p>
"
"37904599","What is exactly happening when you're Docker Commiting a container?","<docker><dockerfile><docker-machine>","37948688","Docker Commit takes a lot of time","<python><docker><tensorflow>","<p>I know that when you use, for example, <code>docker commit abcdefgh ola/minhaimagem:1.0</code>  you are saving your changes from the <strong>abcdefgh</strong> container into a new image to use it again futurely. However, i noticed that sometimes the commit runs very slow and that fact arised the question in my mind, what does exactly is happening while the commit is running? I mean, what is happening under the hood? </p>
","<p>I am pursuing the following course :
<a href=""https://classroom.udacity.com/courses/ud730/lessons/6452084188/concepts/65798813760923"" rel=""nofollow"">https://classroom.udacity.com/courses/ud730/lessons/6452084188/concepts/65798813760923</a>
I have setup docker and installed TenorFlow
I have created a container and downloaded notMnist images
when i commit this, docker is taking a lot of time and not commiting
Please resolve
Edit : Please note it is running from hours. </p>
"
"26178654","bash command not found when setting a variable","<linux><macos><bash><shell><zsh>","41450156","Saving function output into a variable named in an argument","<bash><nginx><docker><sh>","<p>I am writing a shell script where I am setting few variables, whose value is the output of commands.</p>

<p>The errors I get are:</p>

<pre><code>$  $tag_name=""proddeploy-$(date +""%Y%m%d_%H%M"")""
-bash: =proddeploy-20141003_0500: command not found
</code></pre>

<p>now, I did read <a href=""https://stackoverflow.com/questions/12616623/command-not-found-on-bash-function-call"">other similar questions</a> and based on it, I tried various things:</p>

<h3>spliting command into two calls</h3>

<pre><code>$ $deploy_date=date +""%Y%m%d_%H%M""
bash: =date: command not found
$ $tag_name=""proddeploy-$deploy_date""
bash: proddeploy- command not found
</code></pre>

<h3>tried using backticks</h3>

<pre><code>$ $tag_name=`proddeploy-$(date +""%Y%m%d_%H%M"")`
bash: proddeploy-20141003_1734: command not found
bash: =: command not found
</code></pre>

<h3>tried using <code>$()</code></h3>

<pre><code>$ $tag_name=$(proddeploy-$(date +""%Y%m%d_%H%M""))
bash: proddeploy-20141003_1735: command not found
bash: =: command not found
</code></pre>

<p>But in every case the command output is getting executed. how do I make it to stop executing command output and just store as a variable? I need this to work on ZSH and BASH.</p>
","<p>I have an interesting problem that I can't seem to find the answer for. I am creating a simple app that will help my dev department auto launch docker containers with NginX and config files. My problem is, for some reason I can't get the bash script to store the name of a folder, while scanning the directory. Here is an extremely simple example of what I am talking about....</p>

<pre><code>#!/bin/bash

getFolder() {
    local __myResultFolder=$1
    local folder
    for d in */ ; do
        $folder=$d
    done
    __myResultFolder=$folder
    return $folder
}

getFolder FOLDER

echo ""Using folder: $FOLDER""
</code></pre>

<p>I then save that simple script as folder_test.sh and put it in a folder where there is only one folder, change owner to me, and give it correct permissions. However, when I run the script I keep getting the error...</p>

<pre><code>./folder_test.sh: 8 ./folder_test.sh: =test_folder/: not found
</code></pre>

<p>I have tried putting the <code>$folder=$d</code> part in different types of quotes, but nothing works. I have tried <code>$folder=""'""$d""'""</code>, <code>$folder=`$d`</code>, <code>$folder=""$d""</code> but none of it works. Driving me insane, any help would be greatly appreciated. Thank you.</p>
"
"30209776","Docker container will automatically stop after ""docker run -d""","<docker>","41159637","Why my docker exit after excute my shell script?","<shell><docker>","<p>According to tutorial I read so far, use ""<code>docker run -d</code>"" will start a container from image, and the container will run in background. This is how it looks like, we can see we already have container id.</p>

<pre><code>root@docker:/home/root# docker run -d centos
605e3928cdddb844526bab691af51d0c9262e0a1fc3d41de3f59be1a58e1bd1d
</code></pre>

<p>But if I ran ""<strong><code>docker ps</code></strong>"", nothing was returned.</p>

<p>So I tried ""<strong><code>docker ps -a</code></strong>"", I can see container already exited:</p>

<pre><code>root@docker:/home/root# docker ps -a
CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                         PORTS               NAMES
605e3928cddd        centos:latest         ""/bin/bash""         31 minutes ago      Exited (0) 31 minutes ago                          kickass_swartz
</code></pre>

<p>Anything I did wrong? How can I troubleshoot this issue?</p>
","<p>Why my docker exit after excute my shell script? Thanks.</p>

<p>Docker version:</p>

<pre><code>docker --version
Docker version 1.12.4, build 1564f02
</code></pre>

<p>My images:</p>

<pre><code>docker pull lw96/ubuntu-16.04-lnmp1.3
</code></pre>

<p>After I run:</p>

<pre><code>docker run -it -d --name test -p 8080:80 lw96/ubuntu-16.04-lnmp1.3 sh /root/run.sh
</code></pre>

<p>And I checked with:<code>docker ps -a</code></p>

<pre><code>root@ubuntu:/home/liwei# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                  NAMES
68c0ce4c59f7        a57b0c1a60cf        ""/bin/bash""         2 minutes ago       Up 2 minutes        0.0.0.0:8080-&gt;80/tcp   test
</code></pre>

<p>Here's run.sh:</p>

<pre><code>#! /bin/bash
lnmp restart
echo ""lnmp already start""
exit 0
</code></pre>

<p>And I got logs:</p>

<pre><code>root@ubuntu:/home/liwei# docker logs -f test
+-------------------------------------------+
|    Manager for LNMP, Written by Licess    |
+-------------------------------------------+
|              http://lnmp.org              |
+-------------------------------------------+
Stoping LNMP...
Stoping nginx... nginx is not running.
 * MySQL server PID file could not be found!
Gracefully shutting down php-fpm /etc/init.d/php-fpm: 82: kill: No such process

................................... failed. Use force-quit
Starting LNMP...
Starting nginx...  done
Starting MySQL
.. * 
Starting php-fpm  done
lnmp already start!
</code></pre>

<p>AFTER THAT, MY DOCKER CONTAINER EXIT:</p>

<pre><code>root@ubuntu:/home/liwei# docker ps -a
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS                     PORTS               NAMES
cb98d7427802        lw96/ubuntu-16.04-lnmp1.3   ""sh /root/run.sh""   8 minutes ago       Exited (0) 7 minutes ago                       test
</code></pre>

<p>But, when I use:</p>

<pre><code>docker pull lw96/ubuntu-16.04-lnmp1.3

docker run -it -d -p 80:80 --name test  lw96/ubuntu-16.04-lnmp1.3

docker exec -it test /bin/bash

cd ~ &amp;&amp; ./run.sh
</code></pre>

<p>My container works well. Why?????</p>
"
"6686261","What, at the bare minimum, is required for an HTTP request?","<http>","40900625","Accessing Docker daemon with Rust doesn't work","<sockets><unix><docker><rust><daemon>","<p>I'm trying to issue a GET command to my local server using <code>netcat</code> by doing the following:</p>

<pre><code>echo -e ""GET / HTTP/1.1\nHost: localhost"" | nc localhost 80
</code></pre>

<p>Unfortunately, I get a <code>HTTP/1.1 400 Bad Request</code> response for this. What, at the very minimum, is required for a HTTP request? </p>
","<p>I'd like to send a request to my Docker daemon with Rust but it doesn't work:</p>

<pre><code>use std::os::unix::net::UnixStream;
use std::io::prelude::*;

fn main() {
    let mut stream = UnixStream::connect(""/var/run/docker.sock"").unwrap();
    stream.write_all(b""GET /containers/json HTTP/1.1\r\n"").unwrap();
    let mut response = String::new();
    stream.read_to_string(&amp;mut response).unwrap();
    println!(""{}"", response);
}
</code></pre>

<p>I get no response, no error message. The program doesn't terminate.</p>
"
"13282189","MissingSectionHeaderError: File contains no section headers","<python><virtualenv><configparser>","58114728","Systemd config file in docker entrypoint: Error: File contains no section headers","<bash><docker><ubuntu>","<p>I am trying to build collective.simserver according to <a href=""http://plone.org/products/collective.simserver"" rel=""nofollow noreferrer"">this manual</a>, with some modifications:</p>
<pre><code>instead of: virtualenv --python=bin/python2.7 simserver/
I am using: virtualenv --python=myVirtualEnv/bin/python simserver
</code></pre>
<p>and I managed to come to this point:</p>
<pre><code>myVirtualEnv/bin/python bootstrap.py
</code></pre>
<p>and then it breaks apart with this error info:</p>
<pre><code>An internal error occurred due to a bug in either zc.buildout or in a
recipe being used:
Traceback (most recent call last):
  File &quot;/tmp/tmpLiHZgo/zc.buildout-1.6.3-py2.6.egg/zc/buildout/buildout.py&quot;, line 1851, in main
    command)
  File &quot;/tmp/tmpLiHZgo/zc.buildout-1.6.3-py2.6.egg/zc/buildout/buildout.py&quot;, line 203, in __init__
    data['buildout'].copy(), override, set()))
  File &quot;/tmp/tmpLiHZgo/zc.buildout-1.6.3-py2.6.egg/zc/buildout/buildout.py&quot;, line 1465, in _open
    parser.readfp(fp)
  File &quot;/usr/lib/python2.6/ConfigParser.py&quot;, line 305, in readfp
    self._read(fp, filename)
  File &quot;/usr/lib/python2.6/ConfigParser.py&quot;, line 482, in _read
    raise MissingSectionHeaderError(fpname, lineno, line)
MissingSectionHeaderError: File contains no section headers.
file: /home/nenad/buildout.cfg, line: 4
'&lt;!DOCTYPE html&gt;\n'
Mint-AMD64 nenad # 
</code></pre>
<p>What might be wrong?</p>
","<p>I am new to Docker and I am having some problems with it that I have not been able to solve. I have two docker container running, one is a MySQL server and the other one is a web app. The entry point script has this command at the end:</p>

<pre><code># Start supervisord
echo ""Starting supervisord""
cd /
exec /usr/bin/supervisord -c /etc/supervisor/conf.d/supervisord.conf

</code></pre>

<p>When the script runs I get the following error:</p>

<pre><code>misp_web | Starting supervisord
misp_db | [Entrypoint] MySQL Docker Image 5.7.26-1.1.11
misp_db | [Entrypoint] Starting MySQL 5.7.26-1.1.11
misp_web | Error: File contains no section headers.
misp_web | file: /etc/supervisor/conf.d/supervisord.conf, line: 1
misp_web | 'nodaemon = true &amp;\n'
misp_web | For help, use /usr/bin/supervisord -h
</code></pre>

<p>I'd be very grateful if someone could guide me to the right solution as I have been trying to solve this for a while without any success.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","40826147","Unable to connect to Flask application running in Docker Container","<python><docker><flask><containers>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>There are 2 issues I am facing while running this image :</p>

<p>Dockerfile :</p>

<pre><code># cat Dockerfile 
FROM ubuntu:14.04
MAINTAINER ""RAGHU"" &lt;raghavendralokineni@gmail.com&gt;
RUN apt-get update &amp;&amp; apt-get install -y python3.4 python-pip python-dev build-essential
RUN pip install flask
COPY welcome-page/ /root/
EXPOSE 5010
CMD [""python /root/mypage_bkp/login.py"", ""-D"", ""FOREGROUND""]
root@labadmin-VirtualBox:~/RAGHU/WEBPAGE# 
</code></pre>

<p>Running the image :</p>

<pre><code># docker run -d webpage:2.0 
6f32eb7bf8c9eb41aaece84b861e33055d8ed1066805af064e65d291944fdd04
docker: Error response from daemon: oci runtime error: exec: ""python /root/mypage_bkp/login.py"": stat python /root/mypage_bkp/login.py: no such file or directory.
</code></pre>

<p><strong>ISSUE 1 : Why is the above error shown, even though the file present ?</strong></p>

<p>Running the same image with a different CMD :  </p>

<pre><code> # docker run -itd webpage:2.0 /bin/bash
92950bf69de9f5696557a34eecf1d926b65a96aebdc86e529c208d4a2198534e

# docker exec -it 92950bf69de9 /bin/bash
root@92950bf69de9:/# cd /root/
root@92950bf69de9:~# ls
README.md  mypage_bkp
root@92950bf69de9:~# cd mypage_bkp/
root@92950bf69de9:~/mypage_bkp# ls
database.pyc  login.py  names.db  static  templates
root@92950bf69de9:~/mypage_bkp# pwd
/root/mypage_bkp
</code></pre>

<p>Able to run the same application in the same way as mentioned in Docker file: </p>

<pre><code>root@92950bf69de9:~/mypage_bkp# python /root/mypage_bkp/login.py 
* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
* Restarting with stat
* Debugger is active!
* Debugger pin code: 270-194-240
</code></pre>

<p><strong>ISSUE 2: I have run the application manually as above and unable to connect to the container from host.</strong></p>

<p>What I mean here is, the application should be available to the host machine with IP address allocated to the container and PORT exposed. When I open the IP:172.17.0.2:5010 in firefox on my host machine I don't see any output:</p>

<pre><code># docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
92950bf69de9        webpage:2.0         ""/bin/bash""         40 minutes ago      Up 40 minutes       5010/tcp            cranky_jang

# docker inspect 92950bf69de9 | grep -i ipaddr
     ""SecondaryIPAddresses"": null,
     ""IPAddress"": ""172.17.0.2"",
             ""IPAddress"": ""172.17.0.2"",
root@labadmin-VirtualBox:/home/labadmin# ^C
</code></pre>

<p>Code for application is copied at <a href=""https://github.com/Raghavendarlokineni/welcome-page/tree/master/mypage_bkp"" rel=""nofollow noreferrer"">https://github.com/Raghavendarlokineni/welcome-page/tree/master/mypage_bkp</a> </p>

<p>Please help me in understanding the issues I am facing.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","39646474","How to run Python Flask within a Docker container","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I'm trying to run a Python Flask webserver within a docker container, but I can't connect to the Flask server from the outside.  </p>

<p><strong>What I've done:</strong></p>

<p>I created <code>/temp/HelloFlask.py</code></p>

<pre><code>from flask import Flask
app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello World!""

if __name__ == ""__main__"":
    app.run()
</code></pre>

<p>I started the docker container with container port 5000 mapped to host port 5000 and with ~/temp mounted to /temp</p>

<pre><code>docker run -it -p 5000:5000 -v ~/temp:/temp --name tf gcr.io/tensorflow/tensorflow:latest-devel
</code></pre>

<p>Inside the running docker container, I installed Flask and ran <code>HelloFlask.py</code></p>

<pre><code>cd /temp
pip install Flask
python HelloFlask.py &amp;
</code></pre>

<p>I validated that the server was accessible within the container</p>

<pre><code>[root@de8b6996b540:/temp# curl localhost:5000
127.0.0.1 - - [22/Sep/2016 17:41:48] ""GET / HTTP/1.1"" 200 -
Hello World!
</code></pre>

<p>I'm using Docker Version 1.12.1 (build: 12133), which exposes the container ports on localhost, so I should be able to access <code>localhost:5000</code> on my Mac, outside the container, but I can't connect.</p>

<p>I tested to make sure that Docker was correctly binding container ports to localhost by running an nginx container as described in the <a href=""https://docs.docker.com/docker-for-mac/"" rel=""noreferrer"">docker for mac quickstart</a>, and I can access ports from the container via localhost just fine.</p>
"
"31726407","Mount directory in Container and share with Host","<docker>","40043887","How to mount a directory inside a docker container on Linux host","<linux><ubuntu><docker><docker-volume>","<p>I thought I understood the docs, but maybe I didn't. I was under the impression that the <code>-v /HOST/PATH:/CONTAINER/PATH</code> flag is bi-directional. If we have file or directories in the container, they would be mirrored on the host giving us a way to retain the directories and files even after removing a docker container.</p>

<p>In the official MySQL docker images, this works. The <code>/var/lib/mysql</code> can be bound to the host and survive restarts and replacement of container while maintaining the data on the host.</p>

<p>I wrote a docker file for sphinxsearch-2.2.9 just as a practice and for the sake of learning and understanding, here it is:</p>

<pre><code>FROM debian

ENV SPHINX_VERSION=2.2.9-release

RUN apt-get update -qq &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -yqq\
    build-essential\
    wget\
    curl\
    mysql-client\
    libmysql++-dev\
    libmysqlclient15-dev\
    checkinstall

RUN wget http://sphinxsearch.com/files/sphinx-${SPHINX_VERSION}.tar.gz &amp;&amp; tar xzvf sphinx-${SPHINX_VERSION}.tar.gz &amp;&amp; rm sphinx-${SPHINX_VERSION}.tar.gz

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; ./configure --prefix=/usr/local/sphinx

EXPOSE 9306 9312

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; make

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; make install

RUN rm -rf sphinx-${SPHINX_VERSION}

VOLUME /usr/local/sphinx/etc
VOLUME /usr/local/sphinx/var
</code></pre>

<p>Very simple and easy to get your head wrapped around while learning. I am assigning the /etc &amp; /var directories from the sphinx build to the VOLUME command thinking that it will allow me to do something like <code>-v ~/dev/sphinx/etc:/usr/local/sphinx/etc -v ~/dev/sphinx/var:/usr/local/sphinx/var</code>, but it's not, instead it's overwriting the directories inside the container and leaving them blank. When i remove the -v flags and create the container, the directories have the expected files and they are not overwritten.</p>

<p>This is what I run to create the docker file after navigating to the directory that it's in: <code>docker build -t sphinxsearch .</code></p>

<p>And once I have that created, I do the following to create a container based on that image: <code>docker run -it --hostname some-sphinx --name some-sphinx --volume ~/dev/docker/some-sphinx/etc:/usr/local/sphinx/etc -d sphinxsearch</code></p>

<p>I really would appreciate any help and insight on how to get this to work. I looked at the MySQL images and don't see anything magical that they did to make the directory bindable, they used VOLUME.</p>

<p>Thank you in advance.</p>
","<p>I would like to mount a directory from a docker container to the local filesystem. the directory is a website root and I need to be able to edit it my local machine using any editor. </p>

<p>I know I can run <code>docker run -v local_path:container_path</code> but doing that it is only creating an empty directory inside the container.</p>

<p>How can one mount a directory inside a docker container on linux host?</p>
"
"33887194","How to set multiple commands in one yaml file with Kubernetes?","<yaml><kubernetes>","58030761","Is there a sneaky way to run a command before the entrypoint (in a k8s deployment manifest) without having to modify the dockerfile/image?","<docker><kubernetes><dockerfile>","<p>In this official document, it can run command in a yaml config file:</p>

<blockquote>
  <p><a href=""https://kubernetes.io/docs/tasks/configure-pod-container/"" rel=""noreferrer"">https://kubernetes.io/docs/tasks/configure-pod-container/</a></p>
</blockquote>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: hello-world
spec:  # specification of the pod’s contents
  restartPolicy: Never
  containers:
  - name: hello
    image: ""ubuntu:14.04""
    env:
    - name: MESSAGE
      value: ""hello world""
    command: [""/bin/sh"",""-c""]
    args: [""/bin/echo \""${MESSAGE}\""""]
</code></pre>

<p>If I want to run more than one command, how to do?</p>
","<p>This is the dockerfile for the image im working with:
<a href=""https://github.com/runatlantis/atlantis/blob/master/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/runatlantis/atlantis/blob/master/Dockerfile</a></p>

<p>I want to install a utility in that image, but I don't want to have to keep my own dockerfile and image. If I can just add a command to the existing image it means I don't have to maintain my own copy and I'd get upstream changes, etc. (If I was making considerable changes I'd make my own copy but I was wondering if it is possible to sneak in a command.)</p>

<p>I'm deploying this with k8s.</p>

<p>Is there a way I can run an additional command in the entrypoint? For example this is the current entrypoint:</p>

<pre><code>ENTRYPOINT [""docker-entrypoint.sh""]
CMD [""server""]
</code></pre>

<p>I'd like to do something like this in my deployment manifest:</p>

<pre><code>command: [""go get -u github.com/dmlittle/scenery &amp;&amp; docker-entrypoint.sh""]
args: [""server""]
</code></pre>

<p>I don't believe the above will work correctly. Is there a supported way or an actual k8s feature to do this?</p>
"
"37526509","How to install pdo driver in php docker image?","<php><mysql><pdo><docker>","40949752","Docker - Install PDO Driver for PHP + Nginx","<php><docker><pdo><docker-compose><dockerfile>","<p>I am using as a base the <a href=""https://hub.docker.com/_/php/"" rel=""noreferrer"">php docker container</a> with the tag:</p>



<pre class=""lang-none prettyprint-override""><code>php:5.6-apache
</code></pre>

<p>I linked it with a basic <code>mysql:5.6</code> image which I can reach at the host <code>mysql</code>. I created a DB, and filled a table with basic values.</p>

<p>Yet trying to access my app, I get:</p>

<pre class=""lang-none prettyprint-override""><code>Fatal error: Uncaught exception 'PDOException' with message
could not find driver' in /var/www/html/index.php:30 
Stack trace: #0 [internal function]: 
PDO-&gt;__construct('mysql:host=mysq...', 'root', 'root', Array) 
#1 [internal function]: Phalcon\Db\Adapter\Pdo-&gt;connect(Array)
#2 /var/www/html/index.php(30): Phalcon\Db\Adapter\Pdo-__construct(Array)
#3 [internal function]: {closure}()
#4 [internal function]: Phalcon\Di\Service-&gt;resolve(NULL, Object(Phalcon\Di\FactoryDefault))
#5 [internal function]: Phalcon\Di-&gt;get('db', NULL)
#6 [internal function]: Phalcon\Di-&gt;getShared('db')
#7 [internal function]: Phalcon\Mvc\Model\Manager-&gt;_getConnection(Object(Reviews), NULL)
#8 [internal function]: Phalcon\Mvc\Model\Manager-&gt;getReadConnection(Object(Reviews))
#9 [internal function]: Phalcon\Mvc\Model-&gt;getReadConnection()
#10 [internal function]: Phalcon\Mvc\Model\MetaData\Strategy\Introspection-&gt;getMetaData(Object(Reviews), Object(Phalcon\Di\FactoryDefault))
#11 [internal function]: Phalcon\Mvc\Model\MetaData-&gt;_initialize(Object(Rev in /var/www/html/index.php on line 30
</code></pre>

<p>Hence, I thought that the php container was lacking the <code>php-mysql</code> component I installed via:</p>

<pre class=""lang-none prettyprint-override""><code>apt-get install php5-mysql
</code></pre>

<p>I also added a mysql.ini at:</p>

<pre class=""lang-none prettyprint-override""><code>cat /usr/local/etc/php/conf.d/mysql.ini
; configuration for php MySQL module
; priority=20
extension=pdo_mysql.so
</code></pre>

<p>If I <code>echo phpinfo();die</code> it tells me that:</p>

<pre class=""lang-none prettyprint-override""><code>Additional .ini files parsed:
    /usr/local/etc/php/conf.d/mysql.ini,
    /usr/local/etc/php/conf.d/phalcon.ini
</code></pre>

<p>Yet still, the error persists.</p>

<p>Furthermore, when running:</p>

<pre class=""lang-none prettyprint-override""><code>php -i|grep PDO
</code></pre>

<p>I get:</p>

<pre class=""lang-none prettyprint-override""><code>PHP Warning:  PHP Startup: Unable to load dynamic library '/usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so' - /usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so: cannot open shared object file: No such file or directory in Unknown on line 0
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite
PDO Driver for SQLite 3.x =&gt; enabled
</code></pre>

<p>so it seems the mysql extension isn't even activated.</p>

<p>What am I doing wrong?</p>
","<p>I have a Dockerfile:</p>

<pre><code>FROM php:7-fpm

RUN apt-get update \
  &amp;&amp; apt-get install -y --no-install-recommends libpq-dev \
  &amp;&amp; docker-php-ext-install mysqli pdo_pgsql pdo_mysql
</code></pre>

<p>Then I have in my docker-compose.yml file:</p>

<pre><code>web:
  image: nginx:latest
  ports:
    - ""80:80""
  volumes:
    - ./frontend:/var/www/html
    - ./api:/var/www/html/api
    - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
  links:
    - php
mysql:
  image: mariadb
  ports:
    - ""3306:3306""
  environment:
    - MYSQL_ROOT_PASSWORD=password
    - MYSQL_DATABASE=example
  volumes:
    - ./database:/var/lib/mysql
php:
  image: php:7-fpm
  volumes:
    - ./frontend:/var/www/html
    - ./api:/var/www/html/api
  links:
    - mysql
</code></pre>

<p>Then In my PHP Code I have:</p>

<pre><code>&lt;?php
$servername = ""localhost"";
$username = ""root"";
$password = ""password"";

try {
        $conn = new PDO(""mysql:host=$servername;dbname=example"", $username, $password);
        // set the PDO error mode to exception
        $conn-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
        echo ""Connected successfully"";
        }
    catch(PDOException $e)
        {
        echo ""Connection failed: "" . $e-&gt;getMessage();
        }
    ?&gt;
</code></pre>

<p>Which when I go to connect to my database I get:</p>

<blockquote>
  <p>Connection failed: could not find driver</p>
</blockquote>

<p>How would I download the PDO driver using this docker setup?</p>
"
"37599128","docker - how do you disable auto-restart on a container?","<docker>","40513545","How to prevent docker from starting a container automatically on system startup?","<docker><startup>","<p>I can enable auto-restart with <code>--restart=always</code>, but after I stop the container, how do I turn off that attribute?</p>

<p>I normally run a webserver and typically map port 80:</p>

<pre><code>docker run -d --restart=always -p 80:80 -i -t myuser/myproj /bin/bash
</code></pre>

<p>But there are times when I want to run a newer version of my image, but I want to keep the old container around.  The problem is that if there are multiple containers with <code>--restart=always</code>, only one of them (random?) starts because they're all contending for port 80 on the host.</p>
","<p>Docker starts a container on every system startup (debian) but I didn't create a service to do so. How can I prevent docker from doing that?</p>
"
"22944631","How to get the IP address of the docker host from inside a docker container","<docker><ip>","38177272","On Docker for Mac what would be the host ip as seen from the container?","<macos><docker>","<p>As the title says. I need to be able to retrieve the IP address the docker hosts and the portmaps from the host to the container, and doing that inside of the container. </p>
","<p>I need to setup a debugger on my container, that requires me to set the remote (docker host) IP. In different tutorials I find different IP addresses listed as possible solution. In my container I ran a webserver that showed me that connections made to the container from my host machine come from <code>172.19.0.1</code>. This is on Docker for Mac version <code>1.11.x</code>. </p>

<p>But how would I reliably find out the host IP as seen from the container on Docker for Mac? When would it be possible for this IP to change?</p>

<p><strong>Edit:
Duplicate of <a href=""https://stackoverflow.com/a/24716645/6309"">https://stackoverflow.com/a/24716645/6309</a></strong></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60923688","How to access localhost API's inside docker container?","<windows><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>We have two apps running in the system, one is from docker and another is directly running on the system, We need to access the system's app inside container app using <code>localhost:8082</code> API</p>

<p>We are running a <code>node JS</code> container which is port 5000 on <code>Windows 10</code> Home using <code>Docker toolbox</code>, and running an app outside container means directly from the machine which is 8042, We need to access this <code>localhost:8042</code> API inside the node container, how can We archive it?, using <code>Docker version 19.03.1</code></p>

<p>Run cmd:</p>

<pre><code>docker container run -p --net=host -it -v `pwd`:/usr/src/app --rm --name server server/windows:v1
</code></pre>

<p><code>Error: Connection refuesed 127.0.0.1:8042</code> while running docker container.</p>

<p>Complete Error:</p>

<pre><code>(node:80) UnhandledPromiseRejectionWarning: Error: connect ECONNREFUSED 127.0.0.1:8042
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1137:16)
(node:80) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection,
use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)
(node:80) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will
terminate the Node.js process with a non-zero exit code.
</code></pre>

<p>Dockerfile</p>

<pre><code>FROM node:12

WORKDIR /usr/src/app

COPY package*.json ./
RUN npm install
COPY . .

# Specify port app runs on
EXPOSE 5000

# Run the app
CMD [ ""npm"", ""run"", ""dev""]
</code></pre>
"
"26237044","Error starting node with forever in docker container","<node.js><docker><forever>","38297092","Forever monitor fails to run inside a docker container","<node.js><docker><sails.js><dockerfile><forever>","<p>i have a problem when start node with forever in docker container, if i launch manually works, instead the same command in Dockerfile, when build and start the container, exited. 
The command works in bash:</p>

<pre><code>docker run -it container_name bash forever start -c 'node --harmony' /my/path/app.js
</code></pre>

<p>I tried to put command in Dockerfile but the container don't start</p>

<pre><code>CMD forever start -c 'node --harmony' /my/path/app.js
</code></pre>
","<p>I am trying to deploy my <code>sails.js</code> based webapp in a docker container, in which I want to use <code>forever</code> to run the <code>app.js</code> instead of using <code>sails lift</code>. I have the docker container built based on the following <strong>Dockerfile</strong>:</p>

<pre><code>FROM node:latest
RUN npm install -g sails mongodb forever pm2
RUN  mkdir -p /host
WORKDIR /host
RUN npm install
EXPOSE 1337
#CMD sails lift
#CMD PORT=1337 forever start app.js --prod
CMD forever start app.js --prod
</code></pre>

<p>As soon as I start the docker container, it goes down almost immediately, however, if I change it to use <code>CMD sails lift</code>, the container stays up okay. </p>

<p>So I am wondering if it is possible to use forever to serve node.js app in a docker container, if so, how? or what might be wrong with my <strong>Dockerfile</strong>?</p>

<p><strong>Edit:</strong></p>

<p>Docker logs for the problematic docker container (not much information though):</p>

<pre><code>warn:    --minUptime not set. Defaulting to: 1000ms
warn:    --spinSleepTime not set. Your script will exit if it does not stay up for at least 1000ms
info:    Forever processing file: app.js
</code></pre>
"
"27701930","How to add users to Docker container?","<linux><ubuntu><dockerfile>","60925054","Add vsftpd user directory using single Dockerfile and set password","<docker>","<p>I have a docker container with some processes (uwsgi and celery) running inside. I want to create a celery user and a uwsgi user for these processes as well as a worker group that they will both belong to, in order to assign permissions. </p>

<p>I tried adding <code>RUN adduser uwsgi</code> and <code>RUN adduser celery</code> to my Dockerfile, but this is causing problems, since these commands prompt for input (I've posted the responses from the build below). </p>

<p>What is the best way to add users to a Docker container so as to set permissions for workers running in the container?</p>

<p>My Docker image is built from the official Ubuntu14.04 base.</p>

<p>Here is the output from the Dockerfile when the adduser commands are run:</p>

<pre><code>Adding user `uwsgi' ...
Adding new group `uwsgi' (1000) ... 
Adding new user `uwsgi' (1000) with group `uwsgi' ... 
Creating home directory `/home/uwsgi' ...
Copying files from `/etc/skel' ... 
[91mEnter new UNIX password: Retype new UNIX password: [0m 
[91mpasswd: Authentication token manipulation error
passwd: password unchanged
[0m 
[91mUse of uninitialized value $answer in chop at /usr/sbin/adduser line 563.
[0m 
[91mUse of uninitialized value $answer in pattern match (m//) at /usr/sbin/adduser line 564.
[0m 
Try again? [y/N] 
Changing the user information for uwsgi
Enter the new value, or press ENTER for the default
    Full Name []: 
Room Number []:     Work Phone []:  Home Phone []:  Other []: 
[91mUse of uninitialized value $answer in chop at /usr/sbin/adduser line 589.
[0m 
[91mUse of uninitialized value $answer in pattern match (m//) at /usr/sbin/adduser line 590.
[0m 
Is the information correct? [Y/n] 
---&gt; 258f2f2f13df 
Removing intermediate container 59948863162a 
Step 5 : RUN adduser celery 
---&gt; Running in be06f1e20f64 
Adding user `celery' ...
Adding new group `celery' (1001) ... 
Adding new user `celery' (1001) with group `celery' ... 
Creating home directory `/home/celery' ...
Copying files from `/etc/skel' ... 
[91mEnter new UNIX password: Retype new UNIX password: [0m 
[91mpasswd: Authentication token manipulation error
passwd: password unchanged
[0m 
[91mUse of uninitialized value $answer in chop at /usr/sbin/adduser line 563.
[0m 
[91mUse of uninitialized value $answer in pattern match (m//) at /usr/sbin/adduser line 564.
[0m 
Try again? [y/N] 
Changing the user information for celery
Enter the new value, or press ENTER for the default
    Full Name []:   Room Number []:     Work Phone []: 
Home Phone []:  Other []: 
[91mUse of uninitialized value $answer in chop at /usr/sbin/adduser line 589.
[0m 
[91mUse of uninitialized value $answer in pattern match (m//) at /usr/sbin/adduser line 590.
[0m 
Is the information correct? [Y/n] 
</code></pre>
","<p>Hello i am a beginner in Docker. 
I just wanted to add user for ftp server so i used command ""adduser lal"". As it is interactive console, it will ask for password. But how to follow this step in single Dockerfile. My docker looks like:</p>

<pre><code># getting base image debian
FROM debian


RUN apt-get update &amp;&amp; apt-get install -y s3fs

RUN apt-get install -y vsftpd
RUN apt-get update &amp;&amp; apt-get install -y ftp
RUN cp /etc/vsftpd.conf /etc/vsftpd.conf.orig
RUN adduser lal
RUN mkdir /home/lal/ftp
RUN chown nobody:nogroup /home/lal/ftp
RUN chmod a-w /home/lal/ftp
RUN mkdir /home/lal/ftp/files
RUN chown lal:lal /home/lal/ftp/files


CMD [ ""echo"",""Hello World...! from my first docker image"" ]
</code></pre>

<p>It would be helpful for me if you give any advise regarding my Dockerfile too. Thank you for your precious time.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60952484","Can't see my running Docker Container on localhost","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a simple python application that I want to run in a Docker Image. The application looks like this</p>

<pre><code>from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World!'

if __name__ == '__main__':
    app.run()
</code></pre>

<p>and a dockerfile that looks like this:</p>

<pre><code>FROM python:3
RUN pip install --upgrade pip
RUN pip install flask
CMD [""python"",""app.py""]
COPY app.py /app.py
</code></pre>

<p>In addition to these two files the rest of the folder structure looks like this:</p>

<pre><code>├───.idea
│   └───inspectionProfiles
├───static
├───templates
├───venv
│   ├───Include
│   ├───Lib
│   │   ├───site-packages
│   │   │   └───pip-19.0.3-py3.7.egg
│   │   │       ├───EGG-INFO
│   │   │       └───pip
│   │   │           ├───_internal
│   │   │           │   ├───cli
│   │   │           │   ├───commands
│   │   │           │   ├───models
│   │   │           │   ├───operations
│   │   │           │   ├───req
│   │   │           │   ├───utils
│   │   │           │   └───vcs
│   │   │           └───_vendor
│   │   │               ├───cachecontrol
│   │   │               │   └───caches
│   │   │               ├───certifi
│   │   │               ├───chardet
│   │   │               │   └───cli
│   │   │               ├───colorama
│   │   │               ├───distlib
│   │   │               │   └───_backport
│   │   │               ├───html5lib
│   │   │               │   ├───filters
│   │   │               │   ├───treeadapters
│   │   │               │   ├───treebuilders
│   │   │               │   ├───treewalkers
│   │   │               │   └───_trie
│   │   │               ├───idna
│   │   │               ├───lockfile
│   │   │               ├───msgpack
│   │   │               ├───packaging
│   │   │               ├───pep517
│   │   │               ├───pkg_resources
│   │   │               ├───progress
│   │   │               ├───pytoml
│   │   │               ├───requests
│   │   │               ├───urllib3
│   │   │               │   ├───contrib
│   │   │               │   │   └───_securetransport
│   │   │               │   ├───packages
│   │   │               │   │   ├───backports
│   │   │               │   │   └───ssl_match_hostname
│   │   │               │   └───util
│   │   │               └───webencodings
│   │   └───tcl8.6
│   └───Scripts
└───__pycache__

</code></pre>

<p>From Powershell I then build the Docker image by writing the command:</p>

<pre><code>docker build . -t myusername/flaskapp
</code></pre>

<pre><code>PS C:\Users\mypcuser\projects\flask_docker_test&gt; docker build . -t myusername/flaskapp
Sending build context to Docker daemon  19.49MB
Step 1/5 : FROM python:3
 ---&gt; f88b2f81f83a
Step 2/5 : RUN pip install --upgrade pip
 ---&gt; Running in 56dc287d7501
Requirement already up-to-date: pip in /usr/local/lib/python3.8/site-packages (20.0.2)
Removing intermediate container 56dc287d7501
 ---&gt; 2dff8ebf09c6
Step 3/5 : RUN pip install flask
 ---&gt; Running in 5b59f8968a63
Collecting flask
  Downloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)
Collecting Werkzeug&gt;=0.15
  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)
Collecting click&gt;=5.1
  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)
Collecting Jinja2&gt;=2.10.1
  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)
Collecting itsdangerous&gt;=0.24
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting MarkupSafe&gt;=0.23
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Installing collected packages: Werkzeug, click, MarkupSafe, Jinja2, itsdangerous, flask
Successfully installed Jinja2-2.11.1 MarkupSafe-1.1.1 Werkzeug-1.0.0 click-7.1.1 flask-1.1.1 itsdangerous-1.1.0
Removing intermediate container 5b59f8968a63
 ---&gt; 7583bc2d8be6
Step 4/5 : CMD [""python"",""app.py""]
 ---&gt; Running in 9394be530612
Removing intermediate container 9394be530612
 ---&gt; 53e72fb77552
Step 5/5 : COPY app.py /app.py
 ---&gt; 5925b08ae09e
Successfully built 5925b08ae09e
Successfully tagged myusername/flaskapp:latest
SECURITY WARNING: You are building a Docker image from Windows against a non-Windows Docker host. All files and directories added to build context will have '-rwxr-xr-x' permissions. It is recommended to double check and reset permissions for sensitive files and directories.
PS C:\Users\mypcuser\projects\flask_docker_test&gt;
</code></pre>

<p>I then go ahead and run my app using this command:</p>

<pre><code>docker run -p 5001:5000 -t myusername/flaskapp
</code></pre>

<p>And get this output:</p>

<pre><code> * Serving Flask app ""app"" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
</code></pre>

<p>But when I go to this URL in both Firefox, Google Chrome and Postman I get this:</p>

<p><a href=""https://i.stack.imgur.com/MYGkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MYGkZ.png"" alt=""enter image description here""></a></p>
"
"30401448","Docker vs Vagrant","<docker><vagrant>","60915782","Docker-machine vs Vagrant?","<docker><vagrant><docker-machine>","<p>Every Docker image, as I understand, is based on <strong>base image</strong> - for example, Ubuntu.</p>

<p>And if I want to isolate any process I should deploy ubuntu docker base image (<strong>where is difference with Vagrant here?</strong>), and create a necessary subimage after it installing on ubuntu image?</p>

<p>So, if Ubuntu is launched on Vagrant and on Docker, where is practice difference?
And if to use docker provider in Vagrant - where here is difference between Vagrant and Docker?</p>

<p>And, in Docker is it possible to isolate processes on some PC without base image without it's sharing to another PC?</p>
","<p>I am reading about docker-machine and Vagrant. From what I am reading its looks like docker-machine is light version of Vagrant, but Vagrant gives you more flexibility and control. Am I right?</p>

<p>Today I am using docker on my local machine with MacOS. I use Docker for simple stuff like run database server and etc. So Docker is great to have in many ways. </p>

<p>But if I want to seperate my dev environments?</p>

<p>Can anyone who have experience with both tell me which one I should prefer if I want to seperate my dev environments and pros/cons with both?</p>

<p>I want to understand which one I should use, docker-machine or Vagrant?</p>

<p>Thank you for helping me understand pros/cons with both and concept of it :)</p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","60954502","Docker container doesn't connect to host","<docker><flask><ip><dockerfile><port>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I am not able to connect to the docker container from my host machine (Windows 10 Home).</p>

<pre><code>#This is how my Dockerfile looks like

FROM ubuntu:latest
RUN apt-get update -y
RUN apt-get install -y python-pip python-dev build-essential
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
ENTRYPOINT [""python""]
CMD [""app.py""]
</code></pre>

<p>After running <code>docker build -t flask-sample:latest</code> and then <code>docker run -d -p 5000:5000 flask-sample</code>, it shows <code>Running on http://0.0.0.0:5000/</code>, but nothing on my browser shows up.
Also, when I connect to the docker ip address on the port 5000 <code>(i.e. 192.168.99.100:5000)</code> everything works fine.</p>

<p>What am I missing here?</p>
"
"46778868","ng serve not working in Docker container","<angular><docker><docker-compose>","60929479","Unable to reach angular running in Docker container","<node.js><angular><docker><yaml><docker-container>","<p>I have this <a href=""https://github.com/Axiol/Docker-Angular-CLI"" rel=""noreferrer"">Docker Compose configuration</a> where I simply create a NodeJS container and install Angular CLI inside.</p>

<p>After a <code>docker-compose up -d</code>, I'm able to SSH inside the container with <code>docker-compose run node bash</code>. <code>ng new</code> works perfectly but <code>ng serve</code> does not seem to work. It's launched correctly, no error in the console. But, if I visit <code>localhost</code> (ad I mapped port 4200 to 80), nothing loads.</p>

<p>Am I missing something?</p>
","<p>I am brand new to Docker so pleas bear with me.</p>

<p><strong>Dockerfile:</strong></p>

<pre><code>FROM node:alpine
WORKDIR '/app'
COPY ./package.json .
EXPOSE 4200
RUN npm i
COPY . .
CMD [""npm"",""start""]
</code></pre>

<p><strong>Commands:</strong></p>

<pre><code>docker build -t angu .
docker run  -p 4300:4200 angu
</code></pre>

<p><a href=""https://i.stack.imgur.com/ruS9w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ruS9w.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Rf1nE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rf1nE.png"" alt=""enter image description here""></a></p>

<p><strong>I am not sure if I need to include <code>EXPOSE 4200</code> in Dockerfile. But it is not working either ways.</strong></p>
"
"16047306","How is Docker different from a virtual machine?","<docker><containers><virtual-machine><virtualization>","46822288","Should I use Docker to create Linux OS within a Linux OS?","<linux><docker><virtualbox>","<p>I keep rereading <a href=""https://docs.docker.com/"" rel=""noreferrer"">the Docker documentation</a> to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?</p>

<p>Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?</p>
","<p>I recently started to learn Docker, and know it creates and runs Ubuntu within a container with just a simple command.</p>

<pre><code>docker run -i -t ubuntu:14.04 /bin/bash
</code></pre>

<p>I also know that docker-machine uses VirtualBox to create Linux OS in a very handy way. </p>

<p>So what's the difference between them? </p>
"
"20212894","How do I get Flask to run on port 80?","<python><networking><flask><port>","47108504","Flask default port number","<docker><flask>","<p>I have a Flask server running through port 5000, and it's fine. I can access it at <a href=""http://example.com:5000"" rel=""noreferrer"">http://example.com:5000</a></p>

<p>But is it possible to simply access it at <a href=""http://example.com"" rel=""noreferrer"">http://example.com</a>? I'm assuming that means I have to change the port from 5000 to 80. But when I try that on Flask, I get this error message when I run it.</p>

<pre><code>Traceback (most recent call last):
  File ""xxxxxx.py"", line 31, in &lt;module&gt;
app.run(host=""0.0.0.0"", port=int(""80""), debug=True)
   File ""/usr/local/lib/python2.6/dist-packages/flask/app.py"", line 772, in run
run_simple(host, port, self, **options)
  File ""/usr/local/lib/python2.6/dist-packages/werkzeug/serving.py"", line 706, in run_simple
    test_socket.bind((hostname, port))
  File ""&lt;string&gt;"", line 1, in bind
socket.error: [Errno 98] Address already in use
</code></pre>

<p>Running <code>lsof -i :80</code> returns </p>

<pre><code>COMMAND   PID     USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
apache2   467     root    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2  4413 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14346 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14570 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14571 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
apache2 14573 www-data    3u  IPv4 92108840      0t0  TCP *:www (LISTEN)
</code></pre>

<p>Do I need to kill these processes first? Is that safe? Or is there another way to keep Flask running on port 5000 but have the main website domain redirect somehow?</p>
","<p>Am running Flask application inside a docker container, the output of the application can be seen at <a href=""http://localhost:5000"" rel=""nofollow noreferrer"">http://localhost:5000</a>.</p>

<p>How do i make configuration for this so that the url will be <a href=""http://localhost/myapp"" rel=""nofollow noreferrer"">http://localhost/myapp</a> (OR) in production something like <a href=""https://www.ex.com/myapp"" rel=""nofollow noreferrer"">https://www.ex.com/myapp</a> ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","46978590","Docker with PHP and MySql - $servername Error","<php><mysql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a project developed with PHP and MySql.</p>

<p>When I Use the connection to DB I set the connection as a follow:</p>

<pre><code> // Define $username and $password
$username=$_POST['username'];
$password=$_POST['password'];


/* -------------------------------- CONNESSIONE -------------------------------------- */
$servername = ""localhosto"";
$username_db = ""xxxxxx"";
$password_db = ""yyyyyy"";
$db = ""zzzzzz"";

try {
    $conn = new PDO(""mysql:host=$servername;dbname=$db"", $username_db, $password_db);
    // set the PDO error mode to exception
    $conn-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
    //echo ""Connected successfully&lt;br/&gt;&lt;br/&gt;"";
}
catch(PDOException $e)
{
    echo ""Connection failed: "" . $e-&gt;getMessage().""&lt;br/&gt;&lt;br/&gt;"";
}
</code></pre>

<p>the project run correctly and connect to MySql without problems.</p>

<p>Next step I was created a Docker Image with a follow docker file:</p>

<pre><code>FROM php:7.0-apache


RUN apt-get update \
  &amp;&amp; apt-get install -y --no-install-recommends libpq-dev \
  &amp;&amp; docker-php-ext-install mysqli pdo_pgsql pdo_mysql


COPY / /var/www/html
EXPOSE 80
</code></pre>

<p>I have create the image and container:</p>

<pre><code>docker build -t mysoftware C:\Users\mysoftware
docker run -d -p 5000:80 mysoftware
</code></pre>

<p>but the Php application don't connect to MySql.</p>

<p>I need to change the connection with the name of server:</p>

<pre><code>$servername = ""SERVER_NAME"";
</code></pre>

<p>Why I have an error if the app run correcly without container ??</p>
"
"37089162","Docker-compose volume mount before run","<docker><docker-compose><dockerfile>","47078038","Docker compose relative folder from host volume","<docker><docker-compose>","<p>I have a Dockerfile I'm pointing at from a docker-compose.yml.</p>

<p>I'd like the volume mount in the docker-compose.yml to happen before the <code>RUN</code> in the Dockerfile. </p>

<p>Dockerfile:</p>

<pre><code>FROM node

WORKDIR /usr/src/app

RUN npm install --global gulp-cli \
 &amp;&amp; npm install

ENTRYPOINT gulp watch
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: '2'

services:
  build_tools:
    build: docker/gulp
    volumes_from:
      - build_data:rw

  build_data:
    image: debian:jessie
    volumes:
      - .:/usr/src/app
</code></pre>

<p>It makes complete sense for it to do the Dockerfile first, then mount from docker-compose, however is there a way to get around it.</p>

<p>I want to keep the Dockerfile generic, while passing more specific bits in from compose. Perhaps that's not the best practice?</p>
","<p>I am trying to mount a folder from the host as a volume to the container. I am wondering at what step is this mounting process done - I would expect it to be during the build phase, but it seems that my folder is not mounted.</p>

<p>The folder structure:</p>

<pre><code> -&gt; app
  |-&gt; requirements.txt
 - docker
  |-&gt; web
    |-&gt; Dockerfile
 -&gt; docker-compose.yml
</code></pre>

<p>docker-compose.yml contains the following:</p>

<pre><code>version: '2'

services:
  web:
    build: ./docker/web
    volumes:
      - './app:/myapp'
</code></pre>

<p>The docker file of the container:</p>

<pre><code>FROM ubuntu:latest
WORKDIR /myapp
RUN ls
</code></pre>

<p>I am mounting the <strong>app</strong> directory from the host into <strong>/myapp</strong> inside the container, the build process sets the working directory and runs <strong>ls</strong> to see the content and I am expecting my reuqiremetns.txt file to be there.</p>

<p>What am I doing wrong?</p>

<p>docker-compose v1.16.1, docker v1.16.1. I am using docker for windows.</p>
"
"40746453","How to connect to docker host from container on Windows 10 (Docker for Windows)","<networking><docker><windows-10><virtual-machine><hyper-v>","46849857","Docker windows --network=host analog","<docker><docker-networking><docker-for-windows>","<p>At which IP address can a docker container connect to its host on Docker for Windows (on Windows 10)? How do you find this IP address?</p>

<p>Example: you have a service running at port 1234 on your Windows 10 machine. A program inside your container must access this service. What IP address should the program use to connect to the host?</p>
","<p>I want to run mongodb inside docker container (without installing it on host machine) and work with it locally.</p>

<p>For linux I do:</p>

<p><code>docker run --network=host mongo</code></p>

<p>and all works as I want. I am able to connect to mongodb which runs on <code>localhost:27017</code>. </p>

<p>But when I do the same on Windows 10 machine, I can't connect to <code>localhost:27017</code> from host machine. When I run command as above, in console I see that mongodb is running and waiting for connections. But I can't connect from windows host. </p>

<p>So, how to make it work?</p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","47060850","Docker - I'm attempting to run the hello-world example from the Docker site, but localhost:4000 renders nothing","<docker>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I'm referring to the example located here: <a href=""https://docs.docker.com/get-started/part2/#build-the-app"" rel=""nofollow noreferrer"">https://docs.docker.com/get-started/part2/#build-the-app</a></p>

<p>My attempt to run the app works as expected:</p>

<p><code>
docker_test docker run -p 4000:80 friendlyhello
* Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
</code></p>

<p>But, when I navigate to localhost:4000, the page doesn't load. Instead, I see a message in Chrome that reads ""This site cannot be reached.""</p>

<p>Do you have any idea what I might be doing wrong?</p>
"
"43655291","Dynamically set JAVA_HOME of docker container","<docker><dockerfile><java-home>","46795907","Setting java home in docker","<java><tomcat><docker>","<p>My docker container requires JAVA_HOME to be set. I have added it to the Dockerfile as below </p>

<pre><code>ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/jre
</code></pre>

<p>However, this is a hardcoding of the value. Is there a way I can get this value dynamically from the image itself</p>
","<p>I have a base ubuntu docker image and am installing java 8 on it. I need to set java home in the docker file as tomcat needs it. How do i know the location of jre installed to set java home.</p>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","47011890","Docker embed commands usage on Windows","<windows><docker>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I'm running <code>Docker v 17.09.0-ce</code> on <code>Windows 10</code>.
When I try to run embed commands like this:
<code>docker restart $(docker ps -a)</code> it throws me the error:
<code>unknown shorthand flag: 'a' in -a)</code>. Without the <code>-a</code> flag the error looks like this: </p>

<pre><code>Error response from daemon: No such container: $(docker
Error response from daemon: No such container: ps)
</code></pre>

<p>These kind of embed commands suggestions are widespread across the web. How do I make it working?</p>
"
"7475223","mysql_config not found when installing mysqldb python interface","<python><linux><ssh><mysql-python>","52703161","mysql_config not found with MySQL in Docker","<python><mysql><python-3.x><docker><pip>","<p>I am trying to get a Python script to run on the linux server I'm connected to via ssh. The script uses mysqldb. I have all the other components I need, but when I try to install mySQLdb via setuptools like so:, </p>

<pre><code>python setup.py install
</code></pre>

<p>I get the following error report related to the <code>mysql_config</code> command.</p>

<pre><code>sh: mysql_config: command not found
Traceback (most recent call last):
  File ""setup.py"", line 15, in &lt;module&gt;
    metadata, options = get_config()
  File ""/usr/lib/python2.5/MySQL-python-1.2.3/setup_posix.py"", line 43, in get_config
    libs = mysql_config(""libs_r"")
  File ""/usr/lib/python2.5/MySQL-python-1.2.3/setup_posix.py"", line 24, in mysql_config
    raise EnvironmentError(""%s not found"" % (mysql_config.path,))
EnvironmentError: mysql_config not found
</code></pre>

<p>Has anyone else encountered this error and if so how did you resolve it/what can I do to successfully install mysqldb? </p>
","<p>I have MySQL 5.7 installed on Docker running perfectly and python 3.7 installed locally.</p>

<p>I tried to install the flask-mysqldb using the command </p>

<p><code>pip install flask-mysqldb</code> </p>

<p>and I received an error </p>

<p><code>OSError: mysql_config not found</code></p>

<p>I never had to install a MySQL client connector in my machine and never had any problem to connect any system. </p>

<p>Is this related to my Docker config? 
How can I solve this issue?</p>
"
"23513045","How to check if a process is running inside docker container?","<shell><docker><containers>","52580008","How does Java application know it is running within a Docker container","<java><docker><dockerfile>","<p>[Updated1] I have a shell which will change TCP kernel parameters in some functions, but now I need to make this shell run in Docker container, that means, the shell need to know it is running inside a container and stop configuring the kernel. </p>

<p>Now I'm not sure how to achieve that, here is the contents of <code>/proc/self/cgroup</code> inside the container: </p>

<pre><code>9:hugetlb:/
8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>Any flags above can I use to figure out if this process is running inside a container?</p>

<p>[Updated2]: I have also noticed <a href=""https://stackoverflow.com/questions/20010199/determining-if-a-process-runs-inside-lxc-docker"">Determining if a process runs inside lxc/Docker</a>, but it seems not working in this case, the content in <code>/proc/1/cgroup</code> of my container is:</p>

<pre><code>8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>No /lxc/containerid</p>
","<p>I have written a DockerFile for my application primarily  to allow it to be run within a NAS machine (via Docker). The web interface allows the user to traverse the filesystem tree  looking for Music files, but when using Docker the filesystem tree is irrelevant except for the /Music volume which is the mount point for the users actual Music folder on the NAS. </p>

<p>So I only want to display the /Music folder instead of the whole filesystem tree and to do that the application needs to be aware it is actually running within a Docker rather than an actual native Linux OS.</p>

<p>What is the correct way for the application to know it is in docker, the application is written in Java.</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","52816723","Docker :: How to transfer a docker image from one mac system to another without docker hub","<docker><docker-image>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>Am working in a restricted environment where access to docker hub is prohibited. Is there a way to transfer a docker image from one mac system to another without using docker hub ?</p>
"
"24309526","How to change the docker image installation directory?","<docker>","52526367","How to change docker image storage directory?","<image><docker><storage>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>I have a centos running on t2.medium instance in aws. </p>

<p>Then i created a EBS and Mount EBS. I have already docker installed before mounting and now I wanted to pull the docker image and use that volume. Since docker was installed on<code>/</code> and the volume i mounted is on <code>/data</code>. How can I make sure that the image I pull is used by the mounted volume.</p>

<pre><code>dev/xvda1      8.0G  1.6G  6.5G  20% /
devtmpfs        1.9G     0  1.9G   0% /dev
tmpfs           1.9G     0  1.9G   0% /dev/shm
tmpfs           1.9G   17M  1.9G   1% /run
tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup
tmpfs           379M     0  379M   0% /run/user/1000
/dev/xvdf        50G   53M   47G   1% /data
</code></pre>

<p>Any help is appreciated.</p>

<p>Thanks</p>
"
"25503412","How do I know when my docker mysql container is up and mysql is ready for taking queries?","<mysql><bash><shell><docker>","52514506","Docker compose mysql container runs after application that depends on it","<java><mysql><docker><docker-compose>","<p>I am deploying a few different docker containers, mysql being the first one. I want to run scripts as soon as database is up and proceed to building other containers. The script has been failing because it was trying to run when the entrypoint script, which sets up mysql (from <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">this official mysql container</a>), was still running.</p>

<pre><code>sudo docker run --name mysql -e MYSQL_ROOT_PASSWORD=MY_ROOT_PASS -p 3306:3306 -d mysql
[..] wait for mysql to be ready [..]
mysql -h 127.0.0.1 -P 3306 -u root --password=MY_ROOT_PASS &lt; MY_SQL_SCRIPT.sql
</code></pre>

<p>Is there a way to wait for a signal of an entrypoiny mysql setup script finishing inside the docker container? Bash sleep seems like a suboptimal solution.</p>

<p>EDIT: Went for a bash script like this. Not the most elegant and kinda brute force but works like a charm. Maybe someone will find that useful.</p>

<pre><code>OUTPUT=""Can't connect""
while [[ $OUTPUT == *""Can't connect""* ]]
do
    OUTPUT=$(mysql -h $APP_IP -P :$APP_PORT -u yyy --password=xxx &lt;       ./my_script.sql 2&gt;&amp;1)
done
</code></pre>
","<p>I have a simple java application that depends on MySQL.</p>

<p>This is how my <code>docker-compose.yaml</code> looks like:</p>

<pre><code>version: ""3.3""
services:
  docker-mysql:
    image: mysql:latest
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=test
      - MYSQL_PASSWORD=root
    volumes:
      - /var/lib/mysql
    ports:
      - 3306:3306
  my-app:
    build: .
    depends_on:
      - docker-mysql
    ports:
      - 8080:8080
</code></pre>

<p>And this is my <code>Dockerfile</code> to create a containerized app, i.e. <code>my-app</code>:</p>

<pre><code>FROM openjdk:8
EXPOSE 8080
ADD /target/Service1-0.0.1.jar Service1.jar
ENTRYPOINT [""java"",""-jar"",""Service1.jar""]
</code></pre>

<p>When I try <code>docker-compose up</code>, the application launches Tomcat successfully, but can't connect to MySQL container, since it launches after the app:</p>

<p><a href=""https://i.stack.imgur.com/63JJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/63JJX.png"" alt=""enter image description here""></a>
<sup>Docker Compose Log</sup></p>
"
"27812548","How to set an environment variable in a running docker container","<docker>","52830315","Is it possible to make changes in existing environment file once we deploy docker container?","<docker>","<p>If I have a docker container that I started a while back, what is the best way to set an environment variable in that running container? I set an environment variable initially when I ran the run command. </p>

<pre><code>$ docker run --name my-wordpress -e VIRTUAL_HOST=domain.com --link my-mysql:mysql -d spencercooley/wordpress
</code></pre>

<p>but now that it has been running for a while I want to add another <code>VIRTUAL_HOST</code> to the environment variable. I do not want to delete the container and then just re-run it with the environment variable that I want because then I would have to migrate the old volumes to the new container, it has theme files and uploads in it that I don't want to lose. </p>

<p>I would just like to change the value of <code>VIRTUAL_HOST</code> environment variable. </p>
","<p>I have environment file which i used for docker deployment,is it possible to update same environment file after docker run?</p>
"
"29973619","How to make VS Code to treat other file extensions as certain language?","<visual-studio-code><vscode-settings>","52589372","How to mark certain files as docker is Vscode?","<docker><visual-studio-code>","<p>Or is there a way to switch the current file's language to use syntax highlight feature?</p>

<p>For example, <code>*.jsx</code> actually uses JavaScript but VS Code doesn't recognize it.</p>
","<p>The aim is to get docker syntax support in vscode. In order to achieve this, <a href=""https://code.visualstudio.com/docs/azure/docker"" rel=""noreferrer"">the docker plugin was installed</a>, but it turns out that only Dockerfile and docker-compose are seen as docker files, but the the repository contains .docker files. Would it be possible to mark .docker files as dockerfiles in vcsode?</p>

<p><strong>Example</strong></p>

<p>In this example the someimage.docker should be marked as docker so that a docker icon would appear like the icon that is put in front of the Dockerfile. When the file is opened, there should be docker syntax support.</p>

<p><a href=""https://i.stack.imgur.com/UQHRJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UQHRJ.png"" alt=""enter image description here""></a></p>

<p><strong>Attempt to solve the issue</strong></p>

<p>The first attempt to solve the issue was <a href=""https://code.visualstudio.com/docs/getstarted/settings"" rel=""noreferrer"">consulting the settings</a>, but it does not seem to be possible to associated certain extensions with the docker format.</p>
"
"30050923","Sed error : bad option in substitution expression","<linux><bash><shell><sed>","52376340","Sed how to find and replace a value using a bash variable","<node.js><bash><docker><sed>","<p>I have a configuration file (<code>gpsd.default</code>) containing data with the following format:</p>

<pre><code># If you must specify a non-NMEA driver, uncomment and modify the next line     
GPSD_SOCKET=""/var/run/gpsd.sock""                                                
GPSD_OPTIONS=""""                                                                
GPS_DEVICES="""" 
</code></pre>

<p>I am making a change on the file with sed:</p>

<pre><code>sed -i 's/^GPS_DEVICES="""".*/GPS_DEVICES=""dev/ttyUSB1""/' /etc/default/gpsd.default
or 
sed -i '4s/^.*/GPS_DEVICES=""dev/ttyUSB1""/' /etc/default/gpsd.default
</code></pre>

<p>The above <code>sed</code> command returns error:</p>

<blockquote>
  <p>sed: bad option in substitution expression</p>
</blockquote>

<p>Because the new line contains ""<code>/</code>"" in its expression.</p>

<p>How to update my <code>sed</code> command to make it work?</p>
","<p>I'm attempting to substitute a value in <code>package.json</code> by using <code>sed</code> to find a static value and replace it with a dynamic bash variable . However, Circle CI is reporting this is wrong but it looks good to me. </p>

<p>Circle CI</p>

<pre><code>GETTING CURRENT DEPLOYMENT INFORMATION...
sed: bad option in substitution expression
sed: bad option in substitution expression
</code></pre>

<p>Dockerfile</p>

<pre><code>COPY --from=build /home/node/app/ .

## Get current GIT BRANCH and COMMIT HASH and provide to image    
RUN mkdir /root/.gitinfo
COPY ./.git/HEAD /root/.gitinfo/.
COPY ./.git/config /root/.gitinfo/.
RUN echo ""GETTING CURRENT DEPLOYMENT INFORMATION...""; \
    cat /root/.gitinfo/config | grep url | head -1 | xargs &gt; /GIT_VERSION_URL.txt; \
    cat /root/.gitinfo/HEAD | head -1 | xargs &gt; /GIT_VERSION_REF.txt; \
    FILE_LINE=$(cat /GIT_VERSION_URL.txt | head -1 | xargs | tr -d '\n'); \
    sed -i ""s/DOCKER_WILL_PROVIDE_VALUE_A1/$FILE_LINE/g"" /home/node/app/package.json; \
    FILE_LINE=$(cat /GIT_VERSION_REF.txt | head -1 | xargs | tr -d '\n'); \
    sed -i ""s/DOCKER_WILL_PROVIDE_VALUE_A2/$FILE_LINE/g"" /home/node/app/package.json; \
    cat /GIT*; \
    rm -rf /root/.gitinfo;
## END
</code></pre>

<p>package.json</p>

<pre><code>...
  ""git"": {
    ""GIT_URL"": ""DOCKER_WILL_PROVIDE_VALUE_A1"",
    ""GIT_REF"": ""DOCKER_WILL_PROVIDE_VALUE_A2""
  },
...
</code></pre>

<p>Why won't sed do the replacement?</p>

<p>EDIT: I do not believe this is a duplicate. What makes this situation unique is that I'm trying to run this inside Docker <em>and</em> doing a dynamic variable replacement in the replace regex. I am NOT using ""/"" in the regex so this <a href=""https://stackoverflow.com/questions/30050923/sed-error-bad-option-in-substitution-expression"">Sed error : bad option in substitution expression</a> is not similar to my problem. Thanks!</p>

<p>EDIT 2: I actually am using ""/"" in the GIT URL. As Charles pointed out below, this is the likely culprit.</p>
"
"31324981","How to access host port from docker container","<docker><docker-container>","52350205","Guzzle HTTP from Docker to NodeJS in Host","<php><node.js><laravel><docker>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<p>folks.</p>

<p>I have a servive running on my host machine. It is a NodeJS app with Express. It works fine at ""localhost:3000"".</p>

<p>Then, in a separate project, I have a Laravel App running fine inside Docker, and I access it at ""<a href=""http://localhost"" rel=""nofollow noreferrer"">http://localhost</a>"".</p>

<p>Now, my Laravel app needs to call the NodeJS app. I saw in Docker documentation I should use ""host.docker.internal"", since it will resolve to my host machine.</p>

<p>The <code>this-&gt;http</code> is a <code>Guzzle\Client</code> instance. </p>

<p>In my PHP code I have this:</p>

<pre><code>$response = $this-&gt;http-&gt;request('POST', env($store-&gt;remote), [
    'form_params' =&gt; [
        'login' =&gt; $customer-&gt;login,
        'password' =&gt; $customer-&gt;password,
    ]);
</code></pre>

<p>If I call the NodeJS app from Postman it works fine. But calling from that PHP I got this error:</p>

<pre><code>""message"": ""Client error: `POST http://host.docker.internal:3000` resulted in a `404 Not Found` response:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\""en\""&gt;\n&lt;head&gt;\n&lt;meta charset=\""utf-8\""&gt;\n&lt;title&gt;Error&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;pre&gt;Cannot POST /&lt;/p (truncated...)\n"",
    ""exception"": ""GuzzleHttp\\Exception\\ClientException"",
    ""file"": ""/var/www/html/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php"",
    ""line"": 113,
</code></pre>

<p>Does anyone have any clue how I can call my node app from PHP in Docker?</p>

<p><strong>EDIT</strong></p>

<p>I was thinking if I should not open the port 80 and bind it to port 3000 in my PHP instance (since the request is running in php docker image). I put in my Docker file these ports attribute:</p>

<pre><code>php:
    build: ./docker
    volumes:
      - .:/var/www/html
      - ./.env:/var/www/html/.env
      - ./docker/config/php.ini:/usr/local/etc/php/php.ini
      - ./docker/config/php-fpm.conf:/usr/local/etc/php/php-fpm.conf
      - ./docker/config/xdebug.ini:/usr/local/etc/php/conf.d/docker-php-ext-xdebug.ini
    links:
      - mysql
    ports:
      - ""3000:80""
</code></pre>

<p>So, port 80 in my PHP instance would bind to my OSX port 3000. But Docker complains port 3000 is in use: </p>

<blockquote>
  <p>Cannot start service php: b'driver failed programming external connectivity on endpoint project_php_1 (241090....): Error starting userland proxy: Bind for 0.0.0.0:3000 failed: port is already allocated'</p>
</blockquote>

<p>Yes! In fact it is allocated. It is allocated by my NodeJS app, that is where I want to go. It looks like I do not know very well how ports and DNS works inside Docker for Mac.</p>

<p>Any help is very appreciated.</p>

<p><strong>SOLVED</strong></p>

<p>Hey, guys. I figured it out. I turned off Docker container, point a regular Apache to my Laravel project and I got what was happening: CORS.</p>

<p>I already had <code>cors</code> in my Express app, but after configure it better, it worked!</p>

<p>Here it is, in case anyone stumbled here and needs it:</p>

<p>1) Add <a href=""https://github.com/expressjs/cors"" rel=""nofollow noreferrer"">cors</a> to your Express (if you haven't yet)</p>

<p>2) Configure cors to your domains. For now, I will keep it open, but, for production APPS, please, take care and control wisely who can query your app:</p>

<pre><code>// Express app:
app.use(
cors({
    ""origin"": ""*"",
    ""methods"": ""GET,HEAD,PUT,PATCH,POST,DELETE"",
    ""preflightContinue"": false,
    ""optionsSuccessStatus"": 204
  })
);
app.options('*', cors());
</code></pre>

<p>3) Use the <code>host.docker.internal</code> address (in my case, <code>host.docker.internal:3000</code> , since my app is running on that port) from PHP to get to your Express App in OSX host machine. In my case, it will be a different domain/IP when it gets to production.</p>

<p>4) Just use <code>Guzzle\Client</code> to make your http call:</p>

<pre><code>$response = $this-&gt;http-&gt;request('POST', env($store-&gt;remote) . '/store-api/customers/login', [
                'json' =&gt; [
                    ""login"" =&gt; $customer-&gt;login,
                    ""password"" =&gt; encrypt($customer-&gt;password),
                ]
            ]);
</code></pre>

<p>A important point to note: Express waits for <code>json</code> (in my app, at least), so do NOT use ""form_data"", use ""json"" option to POST requests:</p>

<p>At least, it was NOT a duplication of the other answers, as marked by @Phil, because those answers points to the same solution I have already mentioned, use the 'host.docker.internal' address.</p>
"
"41723411","Don't see Django in Docker container","<django><docker>","52460387","Can't connect to docker container with Web Browser","<docker>","<p>I want to run Django in a simple Docker container.</p>

<p>First I built my container with Docker-file. There wasn't anything special in it (only FROM, RUN and COPY commands)</p>

<p>Then I ran my container with command </p>

<pre><code>docker run -tid -p 8000:8000 --name &lt;container_name&gt; &lt;image&gt;
</code></pre>

<p>Entered my container:</p>

<pre><code>docker exec -it &lt;container_name&gt; bash
</code></pre>

<p>Ran Django server:</p>

<pre><code>python manage.py runserver
</code></pre>

<p>Got:</p>

<pre><code>Starting development server at http://127.0.0.1:8000/
</code></pre>

<p>But when I go to 127.0.0.1:8000 I see nothing:</p>

<pre><code>The 127.0.0.1 page isn’t working
</code></pre>

<p>There are no Nginx or other working servers.</p>

<p>What am I doing wrong?</p>

<p><strong>Update 1 (Dockerfile)</strong></p>

<pre><code>FROM ubuntu:16.04
MAINTAINER Max Malyshev &lt;user&gt;
COPY . /root
WORKDIR /root
RUN apt-get update
RUN apt-get install python-pip -y
RUN apt-get install postgresql -y
RUN apt-get install rabbitmq-server -y
RUN apt-get install libpq-dev python-dev -y
RUN apt-get install npm -y
RUN apt-get install mongodb -y
RUN pip install -r requirements.txt
</code></pre>
","<p>I ma new to docker and try to start a django app with my own image.<br>
It seems like the djano web server runs but I am unable to see connect with my host maschine.</p>

<p>My Dockerfile:</p>

<pre><code>FROM python:3
ENV PYTHONUNBUFFERED 1
RUN mkdir /code
WORKDIR /code
ADD requirements.txt /code/
RUN pip install -r requirements.txt
ADD . /code/
RUN python3 /code/manage.py makemigrations
RUN python3 /code/manage.py migrate

EXPOSE 8000
CMD python3 /code/manage.py runserver
</code></pre>

<p>To build the image:<br>
<code>docker build -t $USER/myapp .</code></p>

<p>To run the image I already tried:<br>
<code>docker run --rm -it -p 8000:8000 $USER/myapp</code><br>
Result of <code>docker ps</code>:  </p>

<pre><code>CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS                    NAMES  
f2f40d835fad        user/myapp   ""/bin/sh -c 'pytho...""   8 seconds ago       Up 6 seconds        0.0.0.0:8000-&gt;8000/tcp   amazing_leakey
</code></pre>

<p><code>docker run -rm -it -p 127.0.0.1:8000:8000 $USER/myapp</code>  </p>

<p>Result of <code>docker ps</code>:  </p>

<pre><code>CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS                      NAMES
6c9ab64419a9        christian/musicweb   ""/bin/sh -c 'pytho...""   11 seconds ago      Up 10 seconds       127.0.0.1:8000-&gt;8000/tcp   practical_borg
</code></pre>

<p><code>docker run --rm -it $USER/myapp</code>  </p>

<p>Result of <code>docker ps</code>:  </p>

<pre><code>CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS               NAMES
f8803e5e7439        christian/musicweb   ""/bin/sh -c 'pytho...""   6 seconds ago       Up 5 seconds        8000/tcp            upbeat_goldwasser
</code></pre>

<p>I always get this output, so django seems to run fine:  </p>

<blockquote>
  <p>Performing system checks...<br>
  System check identified no issues (0 silenced).<br>
  September 22, 2018 - 19:02:58<br>
  Django version 2.1.1, using settings 'myapp.settings'<br>
  Starting development server at <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a><br>
  Quit the server with CONTROL-C.  </p>
</blockquote>

<p>What did I miss, thanks for your help.</p>
"
"41984399","denied: requested access to the resource is denied : docker","<docker><dockerfile>","52454188","How to overcome access to resource denied for docker push in official tutorial","<docker><push>","<p>I am following <a href=""https://docs.docker.com/engine/getstarted/step_four/"" rel=""noreferrer"">this link</a> to create my first docker Image and it went successfully and now I am trying to push this Image into my docker repository from this <a href=""https://docs.docker.com/engine/getstarted/step_six/"" rel=""noreferrer"">link</a>. But whenever I am trying to push this Image into repository , I  got this type of error. </p>

<pre><code>denied: requested access to the resource is denied
</code></pre>

<p><a href=""https://i.stack.imgur.com/GCu19.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GCu19.png"" alt=""enter image description here""></a></p>

<p>Could anyone give me some hint towards this problem ? Any help would appreciated.</p>

<p>Note: I have successfully login into docker </p>
","<p>I googled and saw this but I don't know what it is saying about image tags, etc:  <a href=""https://stackoverflow.com/questions/41984399/denied-requested-access-to-the-resource-is-denied-docker"">link</a></p>

<p>I am trying to complete the tutorial here: <a href=""https://docker-curriculum.com/"" rel=""nofollow noreferrer"">tutorial official Docker</a></p>

<p>Here is my output for <code>docker images</code></p>

<pre><code>nobu@nobu-ThinkPad-T420:~/docker/docker-curriculum/flask-app$ docker images
REPOSITORY                TAG                 IMAGE ID            CREATED             SIZE
prakhar1989/catnip        latest              517dbced092c        About an hour ago   700MB
nginx                     latest              06144b287844        2 weeks ago         109MB
busybox                   latest              e1ddd7948a1c        7 weeks ago         1.16MB
prakhar1989/catnip        &lt;none&gt;              c984660fe008        7 weeks ago         700MB
hello-world               latest              2cb0d9787c4d        2 months ago        1.85kB
python                    3-onbuild           292ed8dee366        2 months ago        691MB
prakhar1989/static-site   latest              f01030e1dcf3        2 years ago         134MB
</code></pre>

<p>What I don't understand is which column above shows the image name?  I see a column for repo and then for ID but neither seems to fit the bill and the tutorial skips over this key piece of info.  </p>

<p>I was able to run the app on <code>localhost:8888</code>.  </p>

<p>I don't know why I am getting:</p>

<pre><code>nobu@nobu-ThinkPad-T420:~/docker/docker-curriculum/flask-app$ docker push prakhar1989/catnip
The push refers to repository [docker.io/prakhar1989/catnip]
f438d02b6568: Preparing 
0a0917397206: Preparing 
b24e1ed3eb49: Preparing 
29778035860f: Preparing 
fc88d2431f4d: Preparing 
1dc1b82fa010: Waiting 
09e3fd9cf357: Waiting 
138d1921c15b: Waiting 
d714f65bc280: Waiting 
fd6060e25706: Waiting 
d7ed640784f1: Waiting 
1618a71a1198: Waiting 
denied: requested access to the resource is denied
</code></pre>

<p>I have an account on Docker Hub and set up a public repository.
Can someone give me hints on what command I need to enter to push this? </p>
"
"42736107","Where is NuGet.Config file located in Visual Studio project?","<c#><visual-studio><.net-core><visual-studio-2017><visual-studio-2019>","52770960","How to use local nuget package sources for Dockerfile dotnet restore","<docker><.net-core><dockerfile>","<p>I am wondering where is NuGet.Config file located in Visual Studio <strong>2017</strong> project? I tried to create my own NuGet.Config file in the root of the project, but I didn't find any new repositories (NuGet sources). Does some one have any idea?</p>

<p>Here is the file I am trying to achieve for my <strong>.Net Core</strong> project:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;configuration&gt;
  &lt;packageSources&gt;
    &lt;add key=""AspNetCore"" value=""https://dotnet.myget.org/F/aspnetcore-ci-dev/api/v3/index.json"" /&gt;
    &lt;add key=""AspNetCoreTools"" value=""https://dotnet.myget.org/F/aspnetcore-tools/api/v3/index.json"" /&gt;
    &lt;add key=""NuGet"" value=""https://api.nuget.org/v3/index.json"" /&gt;
  &lt;/packageSources&gt;
&lt;/configuration&gt;
</code></pre>
","<p>I'm trying to make use of local nuget package for my dotnet restore, I tried to follow this tutorial: <a href=""https://blog.bigfont.ca/dotnet-restore-without-an-internet-connection/"" rel=""nofollow noreferrer"">dotnet restore w/out internet</a> </p>

<p><strong>My problem:</strong></p>

<p>It doesn't see the path even though it exist on that path.<img src=""https://i.stack.imgur.com/uAIih.jpg"" alt=""click for image"">. </p>

<p>The server I'm using is on a Corporate Network that is why I can't use dotnet restore, so I'm also experiencing the problem with nuget.org similar to this <a href=""https://stackoverflow.com/questions/46435320/dotnet-restore-works-locally-but-fails-when-building-docker-container"">link</a>.</p>

<p><strong>Environment:</strong></p>

<p>For the sample project, I used:</p>

<ul>
<li>the basic .Net Core web app from Visual Studio 2017</li>
<li>Docker Enterprise Edition(no UI), Windows container</li>
<li>Windows Server 2016 as OS.</li>
</ul>

<h2><strong>UPDATE 10/15/2018</strong></h2>

<p>While the answer of @omajid has been very helpful, I believe docker volume mount is only possible when using docker run and can't be used in Dockerfile(which will be used for Build Pipeline). Got this link which is similar to what I want to achieve. <a href=""https://stackoverflow.com/questions/23439126/how-to-mount-a-host-directory-in-a-docker-container"">How to mount a host directory in a Docker container</a></p>
"
"47270150","COPY . . command in Dockerfile for ASP.NET","<asp.net><docker><dockerfile>","52593703","How does the default netcore docker-compose template work?","<.net><visual-studio><docker><.net-core>","<p>The Visual Studio tooling for Docker creates a Dockerfile for ASP.NET projects containing a <code>COPY . .</code> command as below:</p>

<pre><code>WORKDIR /src
COPY *.sln ./
...
COPY . .
</code></pre>

<p>From what I've read, the <code>&lt;src&gt;</code> parameter is relative to the context, so isn't affected by the <code>WORKDIR /src</code> command. The <code>&lt;dest&gt;</code> however is relative to the <code>WORKDIR</code> so will be pointing at <code>/src</code>. </p>

<p>Is this command just bringing over the remaining files from the root for packaging (docker-compose.yml, .dockerignore, etc.)? If so, then why is this done ahead of the <code>RUN dotnet build...</code> command?</p>

<p>Full Dockerfile below:</p>

<pre><code>FROM microsoft/aspnetcore:2.0 AS base
WORKDIR /app
EXPOSE 80

FROM microsoft/aspnetcore-build:2.0 AS build
WORKDIR /src
COPY *.sln ./
COPY MyProject/MyProject.csproj MyProject/
RUN dotnet restore
COPY . . # The line mentioned above
WORKDIR /src/MyProject
RUN dotnet build -c Release -o /app

FROM build AS publish
RUN dotnet publish -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""MyProject.dll""]
</code></pre>
","<p>I'm trying to understand how docker works. Today I tried do dockerise my netcore application, so I hit the <code>dockerize</code> button and it created the following Dockerfile for me:</p>

<pre><code>FROM microsoft/dotnet:2.0-runtime AS base
WORKDIR /app

FROM microsoft/dotnet:2.0-sdk AS build
WORKDIR /src
COPY MyProj.sln ./
COPY MyProj.Console/MyProj.Console.csproj MyProj.Console/
COPY MyProj.Core/MyProj.Core.csproj MyProj.Core/
COPY MyProj.Solidity/MyProj.Solidity.csproj MyProj.Solidity/
RUN dotnet restore -nowarn:msb3202,nu1503
COPY . .
WORKDIR /src/MyProj.Console
RUN dotnet build -c Release -o /app

FROM build AS publish
RUN dotnet publish -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""MyProj.Console.dll""]
</code></pre>

<p>I actually don't understand this part:</p>

<pre><code>FROM microsoft/dotnet:2.0-sdk AS build
WORKDIR /src
COPY MyProj.sln ./
COPY MyProj.Console/MyProj.Console.csproj MyProj.Console/
COPY MyProj.Core/MyProj.Core.csproj MyProj.Core/
COPY MyProj.Solidity/MyProj.Solidity.csproj MyProj.Solidity/
RUN dotnet restore -nowarn:msb3202,nu1503
</code></pre>

<p>The main point here is: how does it even work? It just copies several <code>csproj</code> files to output directory, but I don't see any <code>cs</code> files. Where is the entire program logic? How is it supposed to work? I'm really trying hard to learn how it works, but I just can't figure this out.</p>
"
"47387155","Could not use lookup file module for a file under /etc/","<ansible><jinja2><ansible-2.x>","52612659","ansible test fails because lookup('file', '/path/to/file') returns old content, in gitlab-ci","<python><docker><ansible><gitlab-ci>","<p>I am deploying a CentOS machine and one among the tasks was to read a file that is rendered the Consul service which places it under <code>/etc/sysconfig</code>. I am trying to later read it in a variable using the <code>lookup</code> module but it is throwing an error below:</p>

<blockquote>
  <p>fatal: [ansible_vm1]: FAILED! => {""failed"": true, ""msg"": ""could not locate file in lookup: /etc/sysconfig/idb_EndPoint""}</p>
</blockquote>

<p>But I am running the lookup task way below the point where the <code>idb_EndPoint</code> file is generated and also I looked it up manually logging in to verify the file was available.</p>

<pre><code> - name: importing the file contents to variable
   set_fact:
     idb_endpoint: ""{{ lookup('file', '/etc/sysconfig/idb_EndPoint') }}""
   become: true
</code></pre>

<p>I also tried previlege escalations with another user <code>become_user: deployuser</code> along with <code>become: true</code> but didn't work still. Using the Ansible version 2.2.1.0.</p>
","<p>I wrote some ansible tests, using the assert module. The real task changes a file, the test reads it's content and checks if it contains some string.</p>

<p>Everything works fine on an normal VM (ubuntu EC2 instance). However, it fails in gitlab-ci in a docker container. Please bear with me if I sound confused. I am confused.</p>

<p>The main task and the debugging task look like this:</p>

<pre><code>- name: Disable core dumps
  become: true
  pam_limits: 
    comment: "" disable core dumps""
    domain: '*'
    limit_item: core
    limit_type: hard
    value: 0

- name: debug file content
  become: true
  vars:
     contents: ""{{ lookup('file', '/etc/security/limits.conf') }}""
  debug:
    var: contents
</code></pre>

<p>Checking the debug output, I can see that my line <code>*  hard core 0</code> is not in the contents variable. Consequently, a check like this fails:</p>

<pre><code> name: Assert that the line ""* hard core 0"" is in limits.conf
  become: true
  vars:
     contents: ""{{ lookup('file', '/etc/security/limits.conf') }}""
  assert:
    that:
       contents is search('[^#][*]\s+hard\s+core\s+0.*')
</code></pre>

<p>However, this check succeeds:</p>

<pre><code>- name: Get line with core configuration in limits.conf
  shell: grep -o -E '^\*\s+hard\s+core\s+0.*$' /etc/security/limits.conf
  register: core_line
</code></pre>

<p>The issue really seems to be that the file lookup doesn't see the file that the others see, the others being humans logging onto the machine, or the grep command.</p>

<p>Again, on a VM the file content is correct and the test succeeds, as expected. However, gitlab-ci spins up some docker container (which seems to be standard) and some virtual machine (which is special to this environment - in this case you won't be able to help me and I apologize for bothering you). Somewhere on the way things get weird, and I get confused</p>

<p>This is <em>not</em> an issue with the pam_limits module. It works just fine. The same happens when I use the lineinfile module. </p>

<p>The ansible version is ansible 2.6.4, python 2.7.6, GitLab Community Edition 10.8.7 </p>
"
"48368411","what is docker run -it flag?","<docker><dockerfile>","52770770","what do flags -i and -t mean in terms of docker run?","<docker>","<p>I was doing some complex stuff with docker, but as turn out I don't know what <code>-it</code> flag means.
Recently I've come across on some example of <code>docker run</code> command which has confused me a little.</p>
<pre><code>docker run -itd ubuntu:xenial /bin/bash 
</code></pre>
<p>My question is what is sense to write <code>-it</code> flag here, if container during instantiation run <code>bin/bash</code></p>
<p>In documentation we have an example</p>
<pre><code>docker run --name test -it debian
</code></pre>
<p>with explanation</p>
<blockquote>
<p>The -it instructs Docker to allocate a pseudo-TTY connected to the
container’s stdin; creating an interactive bash shell in the
container.</p>
</blockquote>
<p>and explanation for -t flag from help page</p>
<blockquote>
<p>-t, --tty                     Allocate a pseudo-TTY</p>
</blockquote>
<p>if I delete -it flag during</p>
<pre><code>docker run -d ubuntu:xenial /bin/bash
</code></pre>
<p>my newly created container doesn't live so much</p>
<p>in <code>docker ps -a</code></p>
<p>it is designated as exited</p>
<p>Sorry, if my question quite stupid, I can't find explanation on the Internet (I have significant misunderstanding of that point).</p>
","<p>I am very new to Docker and also to Unix/Linux world. I have been using docker to build my images and spin up some containers and do understand the concept of containerization fairly well. However, sometimes I do see some people spinning up containers using flags like :</p>

<p><code>docker run -i -t imagename</code></p>

<p>I tried to understand the value of it and came across docker documentation here : <a href=""https://docs.docker.com/v1.13/engine/reference/run/"" rel=""nofollow noreferrer"">https://docs.docker.com/v1.13/engine/reference/run/</a></p>

<p>and it has some very arcane explainations like</p>

<p><code>-i:  Keep STDIN open even if not attached</code></p>

<p><code>-t: Allocate a pseudo-tty</code></p>

<p>what does it even mean?</p>
"
"52664744","Does Docker build --no-cache actually download and refresh the base image?","<docker>","52387389","Does ""docker build --no-cache"" pull new version of base images?","<docker>","<p>Does docker build --no-cache refresh <strong>updated</strong> remote base images or not? Documentation does not seem to specify.</p>
","<p>Does docker <code>build --no-cache &lt;args&gt;</code> pull new version of base image (e.g. <code>--pull</code>) or it's only disabling caching of Dockerfile commands?</p>

<p>Background:</p>

<p>I have multistage build where one of the source images is local only. The <code>--pull</code> option cannot be used in this case. Question is if <code>--no-cache</code> will force the pulling of remote base images.</p>
"
"7120426","How to invoke bash, run commands inside the new shell, and then give control back to user?","<shell><bash>","41610878","How To run from host machine bash script inside docker container and remain in bash in container","<linux><bash><docker>","<p>This must either be really simple or really complex, but I couldn't find anything about it... I am trying to open a new bash instance, then run a few commands inside it, and give the control back to the user <em>inside that same instance</em>.</p>

<p>I tried:</p>

<pre><code>$ bash -lic ""some_command""
</code></pre>

<p>but this executes <code>some_command</code> inside the new instance, then closes it. I want it to stay open.</p>

<p>One more detail which might affect answers: if I can get this to work I will use it in my <code>.bashrc</code> as alias(es), so bonus points for an <code>alias</code> implementation!</p>
","<p>I'm trying to automate something that should be a fairly simple task, but I can't find the right flag / way to do it. </p>

<p>I'm automating a bunch of docker containers, and I want to know which shell I'm using in some human readable form.</p>

<p>So, I start/create a container:</p>

<pre><code>docker run -v /home/igor/Downloads:/home/Downloads/ -h debian7 -p 8081:8080 -itd --name igorDebian72 debian:7
</code></pre>

<p>And then trying to get into its bash which has nicely changed prompt and colours:</p>

<pre><code>sudo docker exec -ti igorDebian72 /bin/bash -c ""`export PS1=""\e[1;32m[\u@ \\H \\W \\@]\$ \e[m ""`""
</code></pre>

<p>But, this seems to ""run"" the command and exit back to host, so it's not useful. </p>

<p>If I run these two things separately:</p>

<pre><code>sudo docker exec -ti igorDebian72 /bin/bash 
</code></pre>

<p>This get's me into the containers bash, and then:</p>

<pre><code>export PS1=""\e[1;32m[\u@ \\H \\W \\@]\$ \e[m ""
</code></pre>

<p>Gives me exactly what I want. Bash prompt with visible container name and a different colour than host machine. </p>

<p>How to achieve that in a single line?</p>

<p>P.S. In a meantime and using comments from here I've tried this:</p>

<pre><code>sudo docker exec -ti igorDebian72 bash --rcfile  &lt;(echo 'export PS1=""\e[1;32m[\u@ \\H \\W \\@]\$ \e[m ""')

sudo docker exec -ti igorDebian72 bash --rcfile  &lt;(export PS1=""\e[1;32m[\u@ \\H \\W \\@]\$ \e[m "")
</code></pre>

<p>It remained in bash, but seems like the script was not executed. </p>

<p>I tried saving it in a .sh file, then doing this:</p>

<pre><code>sudo docker exec -ti igorDebian72 bash --rcfile &lt;(echo trueColors.sh)
</code></pre>

<p>Again, remains in the container, but does not run the script. </p>
"
"15179446","Why does my bash code fail when I run it with sh?","<bash><sh><substitution>","41724266","Bad substitution","<bash><docker><debian>","<p>I have a line of code that works fine in my terminal:</p>

<pre><code>for i in *.mp4; do echo ffmpeg -i ""$i"" ""${i/.mp4/.mp3}""; done
</code></pre>

<p>Then I put the exact same line of code in a script <code>myscript.sh</code>:</p>

<pre><code>#!/bin/sh
for i in *.mp4; do echo ffmpeg -i ""$i"" ""${i/.mp4/.mp3}""; done
</code></pre>

<p>However, now I get an error when running it:</p>

<pre><code>$ sh myscript.sh
myscript.sh: 2: myscript.sh: Bad substitution
</code></pre>

<p>Based on other questions I tried changing the shebang to <code>#!/bin/bash</code>, but I get the exact same error. Why can't I run this script?</p>
","<p>I have a bash script containing this line:</p>

<pre><code>dir=""$( cd ""$( dirname ""${BASH_SOURCE[0]}"" )"" &amp;&amp; pwd )""
</code></pre>

<p>This runs fine on OSX.</p>

<p>However, when I move it move the script on a Docker container running Debian, it fails on the above line with this message:</p>

<pre><code>Bad substitution
</code></pre>

<p>Any idea why this happens?</p>
"
"30209776","Docker container will automatically stop after ""docker run -d""","<docker>","41715801","DockerFile : how to get bash command line after start?","<bash><docker><dockerfile>","<p>According to tutorial I read so far, use ""<code>docker run -d</code>"" will start a container from image, and the container will run in background. This is how it looks like, we can see we already have container id.</p>

<pre><code>root@docker:/home/root# docker run -d centos
605e3928cdddb844526bab691af51d0c9262e0a1fc3d41de3f59be1a58e1bd1d
</code></pre>

<p>But if I ran ""<strong><code>docker ps</code></strong>"", nothing was returned.</p>

<p>So I tried ""<strong><code>docker ps -a</code></strong>"", I can see container already exited:</p>

<pre><code>root@docker:/home/root# docker ps -a
CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                         PORTS               NAMES
605e3928cddd        centos:latest         ""/bin/bash""         31 minutes ago      Exited (0) 31 minutes ago                          kickass_swartz
</code></pre>

<p>Anything I did wrong? How can I troubleshoot this issue?</p>
","<blockquote>
  <p><em>This question is not duplicated, because I want to obtain an interactive shell without running with -it flags.</em></p>
</blockquote>

<p>I'm moving first steps into Docker to create images only for internal use.</p>

<p>I start from this <code>envirornment_full.df</code>:</p>

<pre><code>FROM ubuntu:16.04
ENTRYPOINT [""/bin/bash""]
</code></pre>

<p>I then build</p>

<pre><code>docker rmi environment:full
docker build -t environment:full  -f environment.df .
</code></pre>

<p>Then run</p>

<pre><code>docker run environment:full
</code></pre>

<p>Running <code>docker images -a</code>m I see my image</p>

<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED         SIZE
environment         full                aa91bbd39167        4 seconds ago   129 MB
</code></pre>

<p>So I run it</p>

<pre><code>docker run environment:full 
</code></pre>

<p>I see nothing happening .... </p>

<pre><code>$ docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED         STATUS                      PORTS               NAMES
5847c0a18f30        environment:full    ""/bin/bash""         21 seconds ago  Exited (0) 20 seconds ago                       admiring_mirzakhani
</code></pre>

<p>Also </p>

<pre><code>$ docker run environment:full -ti
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
root@aa768a585f33:/# exit
</code></pre>

<p>I'd like to have the ubuntu prompt, like if I was in a SSH connection. And this <em>without user must enter -i or -tty flags</em>. </p>

<p>How can I realize this?</p>
"
"31696439","How to build a docker container for a Java application","<java><maven><gradle><docker><dockerfile>","41491698","Problems with running .jar file after mvn package inside Dockerfile","<java><maven><docker><dockerfile><executable-jar>","<p><em>What I want to do is build a docker image for my Java application but the following considerations should be true for most compiled languages.</em></p>
<h1>problem</h1>
<p>On my build server I want to produce a docker image for my application as the deliverable. For this I have to compile the application using some build tool (typically Gradle, Maven or Ant) and then add the created JAR file to the docker image. As I want the docker image to just execute the JAR file I will of course start from a base image with Java already installed.</p>
<h1>There are three ways of doing this:</h1>
<h2>let the build tool control the process</h2>
<p>In this case my build tool controls the whole process. So it prepares the JAR file and after the JAR is created it calls Docker to create the image. This works as the JAR is created beforehand and Docker can be  oblivious of the build process needed to create the JAR.</p>
<p>But my Dockerfile is no longer standalone. It depends on steps to happen outside of Docker for it work. In my Dockerfile I will have a <code>COPY</code> or <code>ADD</code> statement that is supposed to copy the JAR file to the image. This statement will fail when the jar is not created beforehand. So just executing the Dockerfile might not work. This becomes a problem if you want to integrate with services that just build using the present Dockerfile like the auto-build feature on DockerHub.</p>
<h2>let Docker control the build</h2>
<p>In this case all necessary steps to create the image are added to the Dockerfile so the image can be created by just executing the Docker build.</p>
<p>The main problem with this approach is that there is no way to add to a Dockerfile commands that should be executed outside the docker image being created. This means I have to add my source code and my build tools to the docker image and build my JAR file inside the image. This will result in my image being bigger than it has to be due to all the files added that will be unnecessary at runtime. This will also add extra layers to my image.</p>
<h3>Edit:</h3>
<p>As @adrian-mouat pointed out if I would add the sources, build the application and deleted the sources in one RUN statement I could avoid adding unnecessary files and layers to the Docker image. This would mean creating some insane chained command.</p>
<h2>two separate builds</h2>
<p>In this case we split our build in two: first we create the JAR file using our build tool and upload it to a repository (Maven or Ivy repository). We then trigger a separate Docker build that just adds the JAR file from the repository.</p>
<h1>conclusion</h1>
<p>In my opinion the better way would be <em>letting the build tool control the process</em>. This is will result in a clean docker image and as the image is what we want to deliver this is of importance. To avoid having a potentially not working Dockerfile lying around this should be created as part of the build. So no one would accidentally use it to start a broken build.</p>
<p>But this will not allow me to integrate with DockerHub.</p>
<h1>question</h1>
<p>Is there another way I am missing?</p>
<h1>update June 2020</h1>
<p>In the years since I first created this question a lot of stuff has changed. At this point I would advocate using <a href=""https://github.com/GoogleContainerTools/jib"" rel=""nofollow noreferrer"">Googel's JIB Tool</a>. It integrates with the most common Java Build Tools (Maven and Gradle) and allows you to create container directly from your build. This is much more concise than any of the old approaches I considered all these years ago.</p>
<h1>update February 2021</h1>
<p>I found this blog post and video from James Ward that reflects better what is currently state of the art.
<a href=""https://cloud.google.com/blog/topics/developers-practitioners/comparing-containerization-methods-buildpacks-jib-and-dockerfile"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/topics/developers-practitioners/comparing-containerization-methods-buildpacks-jib-and-dockerfile</a></p>
","<p>I have a problem that my main class cannot be loaded when running mvn package from docker (at the same time it works perfectly when doing mvn package from Eclipse), Related problem with it is that I cannot figure out where actually Docker saves .jar file. I know where Eclipse stores it, but where docker stores it I don't understand. I wrote simple app in Eclipse and defined my pom.xml so I can build fat .jar (with all dependant compiled .jars inside it). In that I call Maven by myself (manually in Eclipse doing mvn package, which is what I want to avoid - I would like to Docker do the Maven stuff instead of me). And everything works fine this way when I do it manually. Now, my Dockerfile was like this at that point:</p>

<pre><code>FROM openjdk:8-jdk-alpine
EXPOSE 4567  
ADD target/ReadDocFile.jar /opt/demo/ReadDocFile.jar
CMD [""java"",""-jar"",""/opt/demo/ReadDocFile.jar""]
</code></pre>

<p>As you can see, I'm copying from my target folder (where I manually stored my fat .jar using Eclipse) to some path inside docker container and call it. <strong>And everything works well if calling .jar file from local project like that.</strong></p>

<p>Now, I don't want to call mvn package by myself but I want to Docker do it for me. So I wrote:</p>

<pre><code>ADD src /code/src
RUN [""mvn"", ""package""]

EXPOSE 4567  
ADD target/ReadDocFile.jar /opt/demo/ReadDocFile.jar
CMD [""java"",""-jar"",""/opt/demo/ReadDocFile.jar""]
</code></pre>

<p><strong>But after I call that same pom.xml that is working with mvn package inside Eclipse (and this time via Dockerfile as you can see with mvn package in RUN part in the upper code), compilation fails:</strong> </p>

<blockquote>
  <p>Error: Could not find or load main class
  com.dario.apachepoi.PrintSomething</p>
</blockquote>

<p>Why is that? It's the same pom.xml. I can't understand why on the same pom.xml compilation won't work? Seems to me it's looking on local folder (target) but shouldn't it look somewhere inside docker layer for .jar file (where?) ?</p>

<p>My full Dockerfile looks like this:</p>

<pre><code>FROM maven:3.3-jdk-8-alpine

WORKDIR /code

# Prepare by downloading dependencies
ADD pom.xml /code/pom.xml
RUN [""mvn"", ""dependency:resolve""]  
RUN [""mvn"", ""verify""]

# Adding source, compile and package into a fat jar
ADD src /code/src
RUN [""mvn"", ""package""]

EXPOSE 4567  
#ADD target/ReadDocFile.jar /opt/demo/ReadDocFile.jar
CMD [""java"",""-jar"",""target/ReadDocFile.jar""]
</code></pre>

<p>Also, that path target/ReadDocFile.jar bothers me a lot - it's looking on my local store. Shouldn't be looking inside docker container? It that is the case, how can I figure out what is the path where maven running in docker stores my jar?</p>

<p>My pom.xml file (compiled well manually with mvn package inside Eclipse, but not compiled well when running mvn package inside docker):</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.dario.apachepoi&lt;/groupId&gt;
  &lt;artifactId&gt;apachepoi&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
   &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-examples --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi-examples&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-excelant --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi-excelant&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml-schemas --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi-ooxml-schemas&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-scratchpad --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
        &lt;artifactId&gt;poi-scratchpad&lt;/artifactId&gt;
        &lt;version&gt;3.15&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.xmlbeans/xmlbeans --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.xmlbeans&lt;/groupId&gt;
        &lt;artifactId&gt;xmlbeans&lt;/artifactId&gt;
        &lt;version&gt;2.6.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/com.github.virtuald/curvesapi --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.github.virtuald&lt;/groupId&gt;
        &lt;artifactId&gt;curvesapi&lt;/artifactId&gt;
        &lt;version&gt;1.04&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.commons/commons-lang3 --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
        &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;
        &lt;version&gt;3.0&lt;/version&gt;
    &lt;/dependency&gt;

  &lt;/dependencies&gt;


    &lt;build&gt;
        &lt;finalName&gt;ReadDocFile&lt;/finalName&gt;
        &lt;plugins&gt;

            &lt;!-- download source code in Eclipse, best practice --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.10&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;downloadSources&gt;true&lt;/downloadSources&gt;
                    &lt;downloadJavadocs&gt;false&lt;/downloadJavadocs&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;!-- Set a compiler level --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;${jdk.version}&lt;/source&gt;
                    &lt;target&gt;${jdk.version}&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

        &lt;!-- Maven Shade Plugin --&gt;
        &lt;plugin&gt;
          &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
          &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
          &lt;version&gt;2.4.3&lt;/version&gt;
          &lt;executions&gt;
             &lt;!-- Run shade goal on package phase --&gt;
            &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;shade&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;transformers&gt;
                &lt;!-- add Main-Class to manifest file --&gt;
                    &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer""&gt;
                    &lt;mainClass&gt;com.dario.apachepoi.PrintSomething&lt;/mainClass&gt;
                &lt;/transformer&gt;
              &lt;/transformers&gt;
            &lt;/configuration&gt;
              &lt;/execution&gt;
          &lt;/executions&gt;
        &lt;/plugin&gt;

        &lt;/plugins&gt;

    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p><strong>Please help!</strong></p>

<p><strong><em>Edit:</em></strong> I think something else must be written instead of my target folder? But what, what docker path to .jar generated inside docker?</p>
"
"12136948","Why does shell ignore quoting characters in arguments passed to it through variables?","<bash><variables><syntax><quoting><expansion>","55308746","Why is this docker shell command not working","<bash><docker>","<p>These work as advertised:</p>
<pre><code>grep -ir 'hello world' .
grep -ir hello\ world .
</code></pre>
<p>These don't:</p>
<pre><code>argumentString1=&quot;-ir 'hello world'&quot;
argumentString2=&quot;-ir hello\\ world&quot;
grep $argumentString1 .
grep $argumentString2 .
</code></pre>
<p>Despite <code>'hello world'</code> being enclosed by quotes in the second example, grep interprets <code>'hello</code> (and <code>hello\</code>) as one argument and <code>world'</code> (and <code>world</code>) as another, which means that, in this case, <code>'hello</code> will be the search pattern and <code>world'</code> will be the search path.</p>
<p>Again, this only happens when the arguments are expanded from the <code>argumentString</code> variables. grep properly interprets <code>'hello world'</code> (and <code>hello\ world</code>) as a single argument in the first example.</p>
<p>Can anyone explain why this is? Is there a proper way to expand a string variable that will preserve the syntax of each character such that it is correctly interpreted by shell commands?</p>
","<p>Below commands in (tried in zsh and bash)</p>



<pre><code>» docker build --label $(echo -n 'key=""name value""') .
""docker build"" requires exactly 1 argument.
See 'docker build --help'.

Usage:  docker build [OPTIONS] PATH | URL | -

Build an image from a Dockerfile
</code></pre>

<p>but if you try to check the actual command </p>

<pre><code>» echo  docker build --label $(echo -n 'key=""name value""') .
docker build --label key=""name value"" .
</code></pre>

<p>and then run it, it works. What am I missing</p>

<pre><code>» docker build --label key=""name value"" .
Sending build context to Docker daemon  4.096kB
Step 1/2 : from busybox
 ---&gt; 3a093384ac30
Step 2/2 : LABEL key=name value
 ---&gt; Using cache
 ---&gt; 8e0f5a40215e
Successfully built 8e0f5a40215e
</code></pre>
"
"28490874","docker run <IMAGE> <MULTIPLE COMMANDS>","<docker><docker-image>","55015086","docker run throw error `No such file or directory` when use 2 commands","<linux><docker>","<p>I'm trying to run MULTIPLE commands like this.</p>

<pre><code>docker run image cd /path/to/somewhere &amp;&amp; python a.py
</code></pre>

<p>But this gives me ""No such file or directory"" error because it is interpreted as...</p>

<pre><code>""docker run image cd /path/to/somewhere"" &amp;&amp; ""python a.py""
</code></pre>

<p>It seems that some ESCAPE characters like """" or () are needed.</p>

<p>So I also tried </p>

<pre><code>docker run image ""cd /path/to/somewhere &amp;&amp; python a.py""
docker run image (cd /path/to/somewhere &amp;&amp; python a.py)
</code></pre>

<p>but these didn't work.</p>

<p>I have searched for <a href=""https://docs.docker.com/reference/run/"">Docker Run Reference</a> but have not find any hints about ESCAPE characters.</p>
","<p>I found that if I use two commands, the second command cann't read the file in volume, (while the first can read it.) like that:</p>

<pre><code>[root@iZu51 test]# echo 'hello world' &gt;&gt; /data/test/a.txt
[root@iZu51 test]# docker run --rm -v /data/test:/data debian:stretch-slim cat /data/a.txt
hello world
[root@iZu51 test]# docker run --rm -v /data/test:/data debian:stretch-slim cat /data/a.txt &amp;&amp; cat /data/a.txt
hello world
cat: /data/a.txt: No such file or directory
[root@iZu51 test]# docker run --rm -v /data/test:/data debian:stretch-slim cat /data/a.txt; cat /data/a.txt
hello world
cat: /data/a.txt: No such file or directory

</code></pre>

<p>How to fixed it? or it's a bug</p>
"
"34714891","Assign LAN IP address to Docker container different from host's IP address","<docker><ip><port><containers><public>","55172020","How connect to docker container from another computer in local network","<docker>","<p>Am not well versed with Unix networking, adding virtual interfaces etc, trying to learn it now. We are trying to dockerize our application.<br>
My requirement is : To assign an ip to a docker container which is accessible from an external application/browser. </p>

<p>The container ip should be pingable from a different computer in the same network basically.I don't want to use port forwarding. </p>

<ol>
<li><p>I want to access a docker container just like we access a VM using an ip 
address.[ Without the port mapping, -p flag. If i run any server like Apache or Tomcat 
inside the container, it should be accessible using the container ip and 
port. For example: <a href=""http://container_ip:8443]"" rel=""nofollow"">http://container_ip:8443]</a><br>
Is this possible in docker?</p></li>
<li><p>Running ifconfig on my Unix box(RHEL 7.1) shows docker0, ens,lo and veth  interfaces. There is no eth0. Kind of confused on this.</p></li>
</ol>
","<p>I have running docker container on one computer (in this container I start tomcat and mysql server) and want connect to this container from another computer in local network. How I can this do?</p>
"
"37461868","Difference between RUN and CMD in a Dockerfile","<docker><dockerfile>","55298322","How to create a custom Docker image by applying a custom patch to a file without having status 'Exited (0)'","<docker><docker-compose><dockerfile><customization><docker-image>","<p>I'm confused about when should I use <code>CMD</code> vs <code>RUN</code>. For example, to execute bash/shell commands (i.e. <code>ls -la</code>) I would always use <code>CMD</code> or is there a situation where I would use <code>RUN</code>? Trying to understand the best practices about these two similar <code>Dockerfile</code> directives.</p>
","<p>I am using a PhpIPAM Docker image and I use docker-compose to run my containers (<a href=""https://hub.docker.com/r/pierrecdn/phpipam"" rel=""nofollow noreferrer"">https://hub.docker.com/r/pierrecdn/phpipam</a>). I'm in a Windows environment.</p>

<p>Here is my docker-compose.yml</p>

<pre><code>version: ""3.3""

services:
    mysql:
        image: 'mysql:5.6'
        environment:
            - MYSQL_ROOT_PASSWORD=pass
        volumes:
            - 'D:/Docker/phpIPAM/mysql-db:/var/lib/mysql'
    ipam:
        depends_on:
            - mysql
        image: pierrecdn/phpipam
        environment:
            - MYSQL_ENV_MYSQL_USER=root
            - MYSQL_ENV_MYSQL_ROOT_PASSWORD=pass
            - MYSQL_ENV_MYSQL_HOST=mysql
        ports:
            - ""6080:80""
            - ""6025:25""
            - ""6022:22""
</code></pre>

<p>When I launch the command:</p>

<pre><code>PS D:\Docker\phpIPAM&gt; docker-compose up
</code></pre>

<p>All is working fine, my 2 containers are correctly running and I can work on the app. </p>

<pre><code>1974e242966a        pierrecdn/phpipam   ""docker-php-entrypoi…""   40 seconds ago      Up 38 seconds       0.0.0.0:6022-&gt;22/tcp, 0.0.0.0:6025-&gt;25/tcp, 0.0.0.0:6080-&gt;80/tcp   phpipam_ipam_1
6a638b4d1422        mysql:5.6           ""docker-entrypoint.s…""   42 seconds ago      Up 39 seconds       3306/tcp                                                           phpipam_mysql_1
</code></pre>

<hr>

<p><strong>N</strong>ow I would like to customize PhpIPAM and apply a patch to a specific file. Actually I just replace the file in the image by the patched one (note: the patch is ok, I tested it in a running container).</p>

<p>So in a new folder I-:</p>

<ol>
<li>copied my patched file of PhpIPAM</li>
<li>copied and adapt my docker-compose.yml file to build the new image</li>
<li>created a docker file named 'phpipam2440' with the operation to replace a file by my new patched file</li>
</ol>

<p>Here is my new docker-compose.yml</p>

<pre><code>version: ""3.3""

services:
    mysql:
        image: 'mysql:5.6'
        environment:
            - MYSQL_ROOT_PASSWORD=pass
        volumes:
            - 'D:/Docker/phpIPAM/mysql-db:/var/lib/mysql'
    ipam2440:
        depends_on:
            - mysql
        build:
            context: .
            dockerfile: ./phpipam2440
        environment:
            - MYSQL_ENV_MYSQL_USER=root
            - MYSQL_ENV_MYSQL_ROOT_PASSWORD=pass
            - MYSQL_ENV_MYSQL_HOST=mysql
        ports:
            - ""6080:80""
            - ""6025:25""
            - ""6022:22""
        tty: true
</code></pre>

<p>and here is my docker file 'phpipam2440'</p>

<pre><code>FROM pierrecdn/phpipam 

ADD class.Common.php /var/www/html/functions/classes/

CMD [""echo"",""Patch Fix #2440 applied""]
</code></pre>

<p>Now when I run </p>

<pre><code>PS D:\Docker\phpIPAM\patch-2440&gt; docker-compose up
</code></pre>

<p>Here is the result</p>

<pre><code>2e22a2de7d28        patch-2440_ipam2440   ""docker-php-entrypoi…""   55 seconds ago      Exited (0) 53 seconds ago                       patch-2440_ipam2440_1
c65d2cd5eadf        mysql:5.6             ""docker-entrypoint.s…""   56 seconds ago      Up 55 seconds               3306/tcp            patch-2440_mysql_1
</code></pre>

<p>The container from my custom image is 'Exited' with code 0.</p>

<p>That mean all went well and the container has reach ""the end of what it had to do"".</p>

<p>However I used the option ""tty: true"" in my docker-compose and this should prevent my container to Exit.</p>

<p>What am I doing wrong ?</p>

<p>Thx</p>

<p>PS: For info here is the complete output of the docker-compose command:</p>

<pre><code>PS D:\Docker\phpIPAM\patch-2440&gt; docker-compose up
Creating network ""patch-2440_default"" with the default driver
Building ipam2440
Step 1/3 : FROM pierrecdn/phpipam
 ---&gt; 5385f0cfaf12
Step 2/3 : ADD class.Common.php /var/www/html/functions/classes/
 ---&gt; b5e288da30ef
Step 3/3 : CMD [""echo"",""Patch Fix #2440 applied""]
 ---&gt; Running in 6738048b3ae1
Removing intermediate container 6738048b3ae1
 ---&gt; 989fdb6c8c8a

Successfully built 989fdb6c8c8a
Successfully tagged patch-2440_ipam2440:latest
WARNING: Image for service ipam2440 was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
Creating patch-2440_mysql_1 ... done
Creating patch-2440_ipam2440_1 ... done
Attaching to patch-2440_mysql_1, patch-2440_ipam2440_1
mysql_1     | 2019-03-22 10:47:13 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
mysql_1     | 2019-03-22 10:47:13 0 [Note] mysqld (mysqld 5.6.43) starting as process 1 ...
mysql_1     | 2019-03-22 10:47:13 1 [Note] Plugin 'FEDERATED' is disabled.
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Using atomics to ref count buffer pool pages
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: The InnoDB memory heap is disabled
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Memory barrier is not used
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Compressed tables use zlib 1.2.11
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Using Linux native AIO
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Using CPU crc32 instructions
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Initializing buffer pool, size = 128.0M
mysql_1     | 2019-03-22 10:47:13 1 [Note] InnoDB: Completed initialization of buffer pool
ipam2440_1  | Patch Fix #2440 applied
mysql_1     | 2019-03-22 10:47:14 1 [Note] InnoDB: Highest supported file format is Barracuda.
mysql_1     | 2019-03-22 10:47:14 1 [Note] InnoDB: 128 rollback segment(s) are active.
mysql_1     | 2019-03-22 10:47:14 1 [Note] InnoDB: Waiting for purge to start
mysql_1     | 2019-03-22 10:47:14 1 [Note] InnoDB: 5.6.43 started; log sequence number 3398964
mysql_1     | 2019-03-22 10:47:14 1 [Note] Server hostname (bind-address): '*'; port: 3306
mysql_1     | 2019-03-22 10:47:14 1 [Note] IPv6 is available.
mysql_1     | 2019-03-22 10:47:14 1 [Note]   - '::' resolves to '::';
mysql_1     | 2019-03-22 10:47:14 1 [Note] Server socket created on IP: '::'.
mysql_1     | 2019-03-22 10:47:14 1 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
mysql_1     | 2019-03-22 10:47:14 1 [Warning] 'proxies_priv' entry '@ root@e1de09823bf2' ignored in --skip-name-resolve mode.
mysql_1     | 2019-03-22 10:47:14 1 [Note] Event Scheduler: Loaded 0 events
mysql_1     | 2019-03-22 10:47:14 1 [Note] mysqld: ready for connections.
mysql_1     | Version: '5.6.43'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
patch-2440_ipam2440_1 exited with code 0
</code></pre>
"
"43170089","Docker won't start on Windows: Not Enough memory to start docker","<windows><docker><virtual-machine><hyper-v>","55011050","I am unable to load docker on my laptop as it says there is not enough memory?","<docker>","<p>I am trying to get started with Docker on Windows. My machine has 4GB of RAM and a 1.9GHz - 2.5GHz Intel i5 processor, running Windows 10 Pro x64. I know these aren't powerful specs, but I would have thought I should be able to run Docker?</p>

<p>However, having downloaded Docker, I get the error message:</p>

<blockquote>
  <p>Not Enough memory to start docker</p>
</blockquote>

<p>I have seen various forum posts and github issues about this and followed all the advice I can see, such as modifying the settings in Docker, I tried these:</p>

<p><a href=""https://i.stack.imgur.com/8kXrR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8kXrR.png"" alt=""enter image description here""></a></p>

<p>They also mentioned changing the settings of the Hyper-V VM however, this seems to be deleted and recreated with the Docker specified settings on every attempted launch. I tried 2048MB, 1792MB, 1536MB, 1280MB and 1024MB of RAM, all of which failed. </p>

<p>What else can I do? Surely I can run docker in some form on my machine? NB: I have closed all non-essential background apps. There doesn't seem to be many other suggestions for what seems to be a fairly common issue, where the given solutions don't work?</p>
","<p>i have about 800gb free i am running docker on windows 10 it is up to date and the correct version of it is installed for my operating system. I am unable to open it at all. below are my laptop specs and a link to the image that is displayed when i attempt to open docker for the first time.</p>

<p>My laptop specs are:</p>

<p>i3-7020u (2.3GHz 3MBL3 Cache)
itel HD graphics</p>

<p>20GB Memory 16GB intel optane memory + 4GB DDR4 memory
1000GB HDD</p>

<p><a href=""https://i.stack.imgur.com/j24Xp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j24Xp.png"" alt=""enter image description here""></a> </p>
"
"47177850","Could not resolve host in docker","<git><docker><jenkins><dns>","55375540","Unable to go get dependency packages when building docker file","<docker><go>","<p>I'm trying to do an jenkins on docker, on my machine (Ubuntu).</p>
<p>I have to access to the git repo, in my company.
But in jenkins, I get this error :</p>
<blockquote>
<p>Could not resolve host: gogs.mycompany.com</p>
</blockquote>
<p>I think this is an DNS error, so I tried to launch my docker like that (with --dns and --dns-search)</p>
<blockquote>
<p>sudo docker run -p 8080:8080 -p 50000:50000 -v
/home/xero/jenkins:/var/jenkins_home
--name=myproject-jenkins2 --dns=127.0.1.1 --dns-search=mycompany.lan jenkins</p>
</blockquote>
<p>Here my /etc/resolv.conf :</p>
<blockquote>
<p>nameserver 127.0.1.1</p>
<p>search mycompany.lan</p>
</blockquote>
<p>What I'm doing wrong ?</p>
","<p>I'm following the <a href=""https://hackernoon.com/how-to-develop-go-grpc-microservices-and-deploy-in-kubernates-5eace0425bf8"" rel=""nofollow noreferrer"">tutorial</a> learning about the implementation and deployment of micro-service in Go. When I try to rebuild the docker image based on the Dockfile and go file, I always get error messages listed as below. The Dockerfile and go file are attached, may I ask what could be the possible reasons causing this error? I have already tried the solution offered by <a href=""https://stackoverflow.com/questions/47177850/could-not-resolve-host-in-docker"">Could not reserve host in docker</a>, but there is no <code>--dns</code> flag in <code>docker build</code>.</p>

<p>Dockerfile:</p>

<pre><code>FROM golang:latest

COPY . /go/src/add
WORKDIR /go/src/add
RUN go get -d -v ./...
RUN go install -v ./...
ENTRYPOINT go run main.go
EXPOSE 3000
</code></pre>

<p>Go file:</p>

<pre><code>package main

import (
    ""fmt""
    ""github.com/shuza/kubernetes-go-grpc/pd""
    ""golang.org/x/net/context""
    ""google.golang.org/grpc""
    ""google.golang.org/grpc/reflection""
    ""log""
    ""net""
)

type server struct{}

func main() {
    lis, err := net.Listen(""tcp"", "":3000"")
    if err != nil {
        log.Fatalf(""Failed to listen:  %v"", err)
    }

    s := grpc.NewServer()
    pb.RegisterAddServiceServer(s, &amp;server{})
    reflection.Register(s)
    if err := s.Serve(lis); err != nil {
        log.Fatalf(""Failed to serve: %v"", err)
    }
}

func (s *server) Compute(cxt context.Context, r *pb.AddRequest) (*pb.AddResponse, error) {
    result := &amp;pb.AddResponse{}
    result.Result = r.A + r.B

    logMessage := fmt.Sprintf(""A: %d   B: %d     sum: %d"", r.A, r.B, result.Result)
    log.Println(logMessage)

    return result, nil
}
</code></pre>

<p>Error Message:</p>

<pre><code># cd .; git clone https://github.com/gorilla/mux /go/src/github.com/gorilla/mux
Cloning into '/go/src/github.com/gorilla/mux'...
fatal: unable to access 'https://github.com/gorilla/mux/': Could not resolve host: github.com
package github.com/gorilla/mux: exit status 128
# cd .; git clone https://github.com/shuza/kubernetes-go-grpc /go/src/github.com/shuza/kubernetes-go-grpc
Cloning into '/go/src/github.com/shuza/kubernetes-go-grpc'...
fatal: unable to access 'https://github.com/shuza/kubernetes-go-grpc/': Could not resolve host: github.com
package github.com/shuza/kubernetes-go-grpc/pd: exit status 128
package golang.org/x/net/context: unrecognized import path ""golang.org/x/net/context"" (https fetch: Get https://golang.org/x/net/context?go-get=1: dial tcp: i/o timeout)
package google.golang.org/grpc: unrecognized import path ""google.golang.org/grpc"" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp: i/o timeout)
</code></pre>
"
"53073411","Selenium: WebDriverException:Chrome failed to start: crashed as google-chrome is no longer running so ChromeDriver is assuming that Chrome has crashed","<python><selenium><google-chrome><selenium-webdriver><selenium-chromedriver>","55050693","Problem in the E2E tests of Vue.js, Nigthwatch, Selenium","<selenium><docker><vue.js><nightwatch.js><e2e-testing>","<p>Recently I switched computers and since then I can't launch chrome with selenium. I've also tried Firefox but the browser instance just doesn't launch. </p>

<pre><code>from selenium import webdriver

d = webdriver.Chrome('/home/PycharmProjects/chromedriver')

d.get('https://www.google.nl/')
</code></pre>

<p>i get the following error:</p>

<pre><code>selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: crashed
  (unknown error: DevToolsActivePort file doesn't exist)
  (The process started from chrome location /opt/google/chrome/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)
  (Driver info: chromedriver=2.43.600233, platform=Linux 4.15.0-38-generic x86_64)
</code></pre>

<p>i have the latest chrome version and chromedriver installed</p>

<p>EDIT:
After trying @b0sss solution i am getting the following error.</p>

<pre><code>selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: crashed
  (chrome not reachable)
  (The process started from chrome location /opt/google/chrome/google-chrome is no longer running, so chromedriver is assuming that Chrome has crashed.)
  (Driver info: chromedriver=2.43.600233 (523efee95e3d68b8719b3a1c83051aa63aa6b10d),platform=Linux 4.15.0-38-generic x86_64)
</code></pre>
","<p>I'm trying to run e2e tests from a Vue.js application in a docker container.</p>

<p>The nightwatch configuration file looks like this:</p>

<p><em>nightwatch.json</em></p>

<pre><code>{
  ""test_settings"": {
    ""default"": {
      ""desiredCapabilities"": {
        ""browserName"": ""chrome"",
        ""javascriptEnabled"": true,
        ""acceptSslCerts"": true,
        ""chromeOptions"" : {
          ""args"" : [""disable-web-security"", ""ignore-certificate-errors"", ""headless""]
        }
      }
    }
  }
}
</code></pre>

<p>I am using this image in the container: <a href=""https://hub.docker.com/r/pijzl/vuejs-docker-unit-e2e"" rel=""nofollow noreferrer"">pijzl/vuejs-docker-unit-e2e</a></p>

<p>When running at the physical machine prompt the tests pass, however when running in docker I encounter the following error:</p>

<blockquote>
  <p>Error retrieving a new session from the selenium server</p>
  
  <p>Connection refused! Is selenium server started? { value:    { message:
  'unknown error: Chrome failed to start: exited abnormally\n  (unknown
  error: DevToolsActivePort file doesn\'t exist)\n  (The process started
  from chrome location /usr/bin/chromium is no longer running, so
  ChromeDriver is assuming that Chrome has crashed.)\n  (Driver info:
  chromedriver=2.46.628388
  (4a34a70827ac54148e092aafb70504c4ea7ae926),platform=Linux
  4.9.125-linuxkit x86_64) (WARNING: The server did not provide any stacktrace information)\nCommand duration or timeout: 597
  milliseconds\nBuild info: version: \'3.141.59\', revision:
  \'e82be7d358\', time: \'2018-11-14T08:25:53\'\nSystem info: host:
  \'df8c8d010486\', ip: \'172.17.0.2\', os.name: \'Linux\', os.arch:
  \'amd64\', os.version: \'4.9.125-linuxkit\', java.version:
  \'1.8.0_171\'\nDriver info: driver.version: unknown',
       error: 'unknown error' },   status: 13 }</p>
</blockquote>

<p>Anyone have any ideas how to solve the problem?</p>
"
"55361762","apt-get update fails with 404 in a previously working build","<docker><travis-ci><debian-jessie>","55356500","No debian image can update/install packages","<docker><ubuntu><debian><nodes><debian-jessie>","<p>I am running a Travis build and it fails when building the mysql:5.7.27 docker image. The Dockerfile runs <code>apt-get update</code> and then I get an error <code>W: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages  404  Not Found</code>.</p>

<p>Using curl I can see it is redirecting, but the redirect-to URL results in a 404. Has anyone seen this sort of behaviour and have a remedy? Is it basically unfixable until debian makes changes?</p>

<pre><code>➜  ms git:(develop) curl --head http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
HTTP/1.1 302 Found
Date: Tue, 26 Mar 2019 16:03:04 GMT
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Location: http://cdn-fastly.deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
Content-Type: text/html; charset=iso-8859-1

➜  ms git:(develop) curl --head http://cdn-fastly.deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
HTTP/1.1 404 Not Found
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Content-Type: text/html; charset=iso-8859-1
Via: 1.1 varnish
Content-Length: 316
Accept-Ranges: bytes
Date: Tue, 26 Mar 2019 16:03:17 GMT
Via: 1.1 varnish
Age: 45
Connection: keep-alive
X-Served-By: cache-ams21028-AMS, cache-cdg20741-CDG
X-Cache: HIT, HIT
X-Cache-Hits: 6, 2
X-Timer: S1553616198.734091,VS0,VE0
</code></pre>
","<p>Unable to fetch updates from debain repository.
All our VM update/install are also facing the same error. Even if I do curl or browse from another network, I'm getting the same error for debian url.</p>

<p>Getting the below error:</p>

<blockquote>
<pre><code>RUN apt-get update -y &amp;&amp;     apt-get install -y     vim zip
 ---&gt; Running in 339cd82feb32
Get:1 http://security.debian.org jessie/updates InRelease [44.9 kB]
Ign http://deb.debian.org jessie InRelease
Ign http://deb.debian.org jessie-updates InRelease
Get:2 http://deb.debian.org jessie Release.gpg [2420 B]
Ign http://deb.debian.org jessie-updates Release.gpg
Get:3 http://deb.debian.org jessie Release [148 kB]
Ign http://deb.debian.org jessie-updates Release
Err http://deb.debian.org jessie-updates/main amd64 Packages

Err http://deb.debian.org jessie-updates/main amd64 Packages

Err http://deb.debian.org jessie-updates/main amd64 Packages

Err http://deb.debian.org jessie-updates/main amd64 Packages

Err http://deb.debian.org jessie-updates/main amd64 Packages
  404  Not Found
Get:4 http://deb.debian.org jessie/main amd64 Packages [9098 kB]
Get:5 http://security.debian.org jessie/updates/main amd64 Packages [822 kB]
Fetched 10.1 MB in 18s (543 kB/s)
W: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages  404  Not Found

E: Some index files failed to download. They have been ignored, or old ones used instead.
The command '/bin/sh -c apt-get update -y &amp;&amp;     apt-get install -y     vim zip' returned a non-zero code: 100
</code></pre>
</blockquote>
"
"2839321","Connect Java to a MySQL database","<java><mysql><jdbc>","44780571","How to connect with MySQL DB running as container in docker?","<java><mysql><docker><jdbc>","<p>How do you connect to a MySQL database in Java?</p>

<p>When I try, I get </p>

<pre><code>java.sql.SQLException: No suitable driver found for jdbc:mysql://database/table
    at java.sql.DriverManager.getConnection(DriverManager.java:689)
    at java.sql.DriverManager.getConnection(DriverManager.java:247)
</code></pre>

<p>Or </p>

<pre><code>java.lang.ClassNotFoundException: com.mysql.jdbc.Driver
</code></pre>

<p>Or</p>

<pre><code>java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver
</code></pre>
","<p>Can some give me any idea about how to connect with my mysql db which is running as a container in Docker on a virtual machine?</p>

<p>I have no idea about it, please help me.</p>

<p>What I am trying to do is :- I am writing a java program on my local machine and now I want to establish a jdbc connection with mysql. My MySQL DB is running as a docker container on a Virtual machine. </p>

<p>Does someone has any idea.</p>

<p>Connection con = DriverManager.getConnection(""jdbc:mysql://10.0.2.15/<strong>""what should I put here""</strong>,""root"",""myrootpassword"");</p>

<p>my ip address for the container is 172.17.0.2 and my guest ip is 10.0.2.15. My sql is running on port 3306.</p>

<p>Thanks in Advance</p>
"
"26153686","How do I run a command on an already existing Docker container?","<docker>","44796700","How do I run nginx -t when nginx is running inside a docker container?","<bash><nginx><docker>","<p>I created a container with <code>-d</code> so it's not interactive.</p>

<pre><code>docker run -d shykes/pybuilder bin/bash
</code></pre>

<p>I see that the container has exited:</p>

<pre><code>CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS                      PORTS               NAMES
d6c45e8cc5f0        shykes/pybuilder:latest   ""bin/bash""          41 minutes ago      Exited (0) 2 seconds ago                        clever_bardeen
</code></pre>

<p>Now I would like to run occasional commands on the machine and exit. Just to get the response.</p>

<p>I tried to start the machine. I tried attaching. I thought I could call <code>run</code> with a container, but that does not seem to be allowed. Using <code>start</code> just seems to run and then exist quickly.</p>

<p>I'd like to get back into interactive mode after exiting.</p>

<p>I tried:</p>

<pre><code>docker attach d6c45e8cc5f0
</code></pre>

<p>But I get:</p>

<pre><code>2014/10/01 22:33:34 You cannot attach to a stopped container, start it first
</code></pre>

<p>But if I start it, it exits anyway. Catch 22. I can't win.</p>
","<p>Is there a way to run commands like</p>

<pre><code>service nginx start
nginx -t
</code></pre>

<p>when nginx is running in a docker container without having to start the container's bash shell?</p>
"
"30494050","How do I pass environment variables to Docker containers?","<docker><environment-variables><dockerfile>","45029096","How can i set an ENV variable in a Dockerfile the content from a .txt file?","<node.js><docker><environment-variables><dockerfile>","<p>I'm new to Docker, and it's unclear how to access an external database from a container. Is the best way to hard-code in the connection string?</p>

<pre><code># Dockerfile
ENV DATABASE_URL amazon:rds/connection?string
</code></pre>
","<p>I'm trying to Dockerize a Node.js app and, while creating the Dockerfile, i came across the problem that the credetials for connecting to the database are stored in a .txt file. How can i set the content from that .txt file as an ENV variable in my Dockerfile?</p>
"
"37526509","How to install pdo driver in php docker image?","<php><mysql><pdo><docker>","44603941","How to enable pdo_mysql in the php docker image","<php><mysql><docker>","<p>I am using as a base the <a href=""https://hub.docker.com/_/php/"" rel=""noreferrer"">php docker container</a> with the tag:</p>



<pre class=""lang-none prettyprint-override""><code>php:5.6-apache
</code></pre>

<p>I linked it with a basic <code>mysql:5.6</code> image which I can reach at the host <code>mysql</code>. I created a DB, and filled a table with basic values.</p>

<p>Yet trying to access my app, I get:</p>

<pre class=""lang-none prettyprint-override""><code>Fatal error: Uncaught exception 'PDOException' with message
could not find driver' in /var/www/html/index.php:30 
Stack trace: #0 [internal function]: 
PDO-&gt;__construct('mysql:host=mysq...', 'root', 'root', Array) 
#1 [internal function]: Phalcon\Db\Adapter\Pdo-&gt;connect(Array)
#2 /var/www/html/index.php(30): Phalcon\Db\Adapter\Pdo-__construct(Array)
#3 [internal function]: {closure}()
#4 [internal function]: Phalcon\Di\Service-&gt;resolve(NULL, Object(Phalcon\Di\FactoryDefault))
#5 [internal function]: Phalcon\Di-&gt;get('db', NULL)
#6 [internal function]: Phalcon\Di-&gt;getShared('db')
#7 [internal function]: Phalcon\Mvc\Model\Manager-&gt;_getConnection(Object(Reviews), NULL)
#8 [internal function]: Phalcon\Mvc\Model\Manager-&gt;getReadConnection(Object(Reviews))
#9 [internal function]: Phalcon\Mvc\Model-&gt;getReadConnection()
#10 [internal function]: Phalcon\Mvc\Model\MetaData\Strategy\Introspection-&gt;getMetaData(Object(Reviews), Object(Phalcon\Di\FactoryDefault))
#11 [internal function]: Phalcon\Mvc\Model\MetaData-&gt;_initialize(Object(Rev in /var/www/html/index.php on line 30
</code></pre>

<p>Hence, I thought that the php container was lacking the <code>php-mysql</code> component I installed via:</p>

<pre class=""lang-none prettyprint-override""><code>apt-get install php5-mysql
</code></pre>

<p>I also added a mysql.ini at:</p>

<pre class=""lang-none prettyprint-override""><code>cat /usr/local/etc/php/conf.d/mysql.ini
; configuration for php MySQL module
; priority=20
extension=pdo_mysql.so
</code></pre>

<p>If I <code>echo phpinfo();die</code> it tells me that:</p>

<pre class=""lang-none prettyprint-override""><code>Additional .ini files parsed:
    /usr/local/etc/php/conf.d/mysql.ini,
    /usr/local/etc/php/conf.d/phalcon.ini
</code></pre>

<p>Yet still, the error persists.</p>

<p>Furthermore, when running:</p>

<pre class=""lang-none prettyprint-override""><code>php -i|grep PDO
</code></pre>

<p>I get:</p>

<pre class=""lang-none prettyprint-override""><code>PHP Warning:  PHP Startup: Unable to load dynamic library '/usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so' - /usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so: cannot open shared object file: No such file or directory in Unknown on line 0
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite
PDO Driver for SQLite 3.x =&gt; enabled
</code></pre>

<p>so it seems the mysql extension isn't even activated.</p>

<p>What am I doing wrong?</p>
","<p>I have a basic <code>Dockerfile</code> with the following in:</p>

<pre><code>FROM php:7.1-apache
RUN apt-get update &amp;&amp; docker-php-ext-install pdo_mysql
COPY . /var/www
EXPOSE 80
</code></pre>

<p>I have a docker-compose.yml file</p>

<pre><code>version: ""3""
services:
  app:
    build: .
    ports:
      - ""80:80""
    volumes:
      - .:/var/www
    depends_on:
      - mysql
  mysql:
    image: mysql:8
    ports:
      - ""3306:3306""
    environment:
      MYSQL_DATABASE: ""app""
      MYSQL_USER: ""app""
      MYSQL_PASSWORD: ""app""
      MYSQL_ROOT_PASSWORD: ""test""
</code></pre>

<p>I then ran <code>docker build -t app . &amp;&amp; docker-compose up</code> at the root of the project. Everything seems to build correctly, but when outputting <code>phpinfo</code> I don't see the mysql_pdo extension.</p>

<p><a href=""https://i.stack.imgur.com/PyaQI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PyaQI.png"" alt=""enter image description here""></a></p>

<p>Are there any steps I am missing?</p>
"
"39240560","Spring boot with docker unable to find valid certification path to requested target error","<spring><maven><docker><spring-boot>","44940149","SpringBoot Docker unable to find valid certification path to requested target","<maven><docker><spring-boot><docker-toolbox><docker-maven-plugin>","<p>I'm using spring boot and am trying to set it up with <strong><em>Docker</em></strong>. I've tried everything I could find on google and nothing seems to get me going. I'm running </p>

<pre><code> mvn clean package docker:build 
</code></pre>

<p>Running this will do the spring-boot tests, run DB migrations, build the JAR, and then when it comes to Building the <strong><em>Docker image</em></strong>, I get the following error:</p>

<pre><code>Failed to execute goal com.spotify:docker-maven-plugin:0.4.9:build (default-cli) 
on project app: Exception caught: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: javax.net.ssl.SSLHandshakeException: 
sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: 
unable to find valid certification path to requested target -&gt; [Help 1]
</code></pre>

<p>Here is the Dockerfile I'm using:</p>

<pre><code>FROM java:8-jdk
export DOCKER_TLS_VERIFY=""1""
export DOCKER_HOST=""tcp://192.168.99.100:2376""
export DOCKER_CERT_PATH=""/Users/james/.docker/machine/machines/default""
export DOCKER_MACHINE_NAME=""default""
EXPOSE 8080
VOLUME /tmp
ADD app-0.0.1-SNAPSHOT.jar app.jar
RUN sh -c 'touch /app.jar'
ENTRYPOINT [""java"",""-Djava.security.egd=file:/dev/./urandom"",""-jar"",""/app.jar""]
</code></pre>

<p>And here is my docker-maven-plugin configuration:</p>

<pre><code> ... pom stuff
&lt;docker.image.prefix&gt;jamesone1&lt;/docker.image.prefix&gt;
    ... other pom stufff
&lt;plugin&gt;
    &lt;groupId&gt;com.spotify&lt;/groupId&gt;
    &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;0.4.9&lt;/version&gt;
    &lt;configuration&gt;
            &lt;dockerHost&gt;https://192.168.99.100:2376&lt;/dockerHost&gt;
            &lt;imageName&gt;${docker.image.prefix}/${project.artifactId}&lt;/imageName&gt;
            &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt;

            &lt;resources&gt;
                &lt;resource&gt;
                    &lt;targetPath&gt;/&lt;/targetPath&gt;
                    &lt;directory&gt;${project.build.directory}&lt;/directory&gt;
                    &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;
                &lt;/resource&gt;
            &lt;/resources&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<p>I'm using the dock for mac &amp; am using a <strong><em>docker-machine</em></strong> with the following env: </p>

<pre><code>export DOCKER_TLS_VERIFY=""1""
export DOCKER_HOST=""tcp://192.168.99.100:2376""
export DOCKER_CERT_PATH=""/Users/james/.docker/machine/machines/default""
export DOCKER_MACHINE_NAME=""default""
</code></pre>

<p>What's going on?! Am I missing something?</p>
","<p>I am using DockerToolBox. I am trying to use Spring-Boot with Docker. I am following <a href=""https://spring.io/guides/gs/spring-boot-docker/"" rel=""nofollow noreferrer"">this tutorial</a>. pom.xml for my project is like this : </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;jk-spring-boot-docker&lt;/artifactId&gt;
    &lt;version&gt;0.1.0&lt;/version&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;1.5.2.RELEASE&lt;/version&gt;
    &lt;/parent&gt;

    &lt;properties&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;docker.image.prefix&gt;jayram&lt;/docker.image.prefix&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;


    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;com.spotify&lt;/groupId&gt;
                &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;0.4.13&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;imageName&gt;${docker.image.prefix}/${project.artifactId}&lt;/imageName&gt;
                    &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt;
                    &lt;!--dockerHost&gt;https://192.168.99.100:2376&lt;/dockerHost--&gt;
                    &lt;resources&gt;
                        &lt;resource&gt;
                            &lt;targetPath&gt;/&lt;/targetPath&gt;
                            &lt;directory&gt;${project.build.directory}&lt;/directory&gt;
                            &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;
                        &lt;/resource&gt;
                    &lt;/resources&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>When I run this command : <code>mvn package docker:build</code>, I get following error : </p>

<pre><code>Failed to execute goal com.spotify:docker-maven-plugin:0.4.13:build (default-cli) on project jk-spring-boot-docker: Exception caught: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: org.apache.http.conn.HttpHostConnectException: Connect to localhost:2375 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused: connect -&gt; [Help 1]
</code></pre>

<p>I tried adding <code>&lt;dockerHost&gt;https://192.168.99.100:2376&lt;/dockerHost&gt;</code> in pom.xml, but I get following error : </p>

<pre><code>Failed to execute goal com.spotify:docker-maven-plugin:0.4.13:build (default-cli) on project jk-spring-boot-docker: Exception caught: Timeout: GET https://192.168.99.100:2376/version: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: org.apache.http.conn.ConnectTimeoutException: Connect to 192.168.99.100:2376 [/192.168.99.100] failed: connect timed out
</code></pre>

<p><strong>UPDATE</strong> : </p>

<p>Docker was not running. I started docker, and then I run the command, it gave following error :</p>

<pre><code>Failed to execute goal com.spotify:docker-maven-plugin:0.4.13:build (default-cli) on project jk-spring-boot-docker: Exception caught: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
</code></pre>

<p>Please help me resolve this.</p>
"
"44251094","I want to share code content across several containers using docker-compose volume directive","<docker><docker-compose>","44678042","named docker volume not updating using docker-compose","<docker><docker-compose>","<p>I have a PHP application that I need to containerize. I am setting up the following:</p>

<ol>
<li>container for varnish </li>
<li>container for nginx </li>
<li>container for php-fpm</li>
<li>container for cron </li>
<li>container for tooling</li>
<li>container with PHP code baked into</li>
</ol>

<p>Container 2,3,4,5 all need to have access to the same PHP application codebase that is baked into container 6.</p>

<p>I would like to set this up to be able to revert to previous releases for the application by just changing the version tag of the codebase container.</p>

<p>My current composer file is something like -></p>

<pre><code>version ""3""
services:
  web:
   image:nginx
   links:
     - phpfpm
   volumes:
     - code:/var/www/html

 phpfpm:
  image:php-fpm
  links:
    - db
  volumes:
    - code:/var/www/html

 code:
   build:
   context: ./
   dockerfile: Dockerfile
   volumes:
     - code:/var/www/html

volumes:
  code:
    driver: local
</code></pre>

<p>At this point the <strong>code volume</strong> was created. Copy of the php code from container code was copied to the volume.</p>

<p>This is good as all new changes will be persisted to the volume although when I pull a new version of the codebase my volume will not get updated.</p>

<p>What I would like to achieve is that my nginx and cron and tooling continer all see the latest version of the codebase container's content and as well I want to be able to run several one of containers calls using that php code that is in container 6.</p>

<p>How do I need to do to go about that using v3 syntax?</p>

<p>Thanks</p>
","<p>I'm trying to have one service to build my client side and then share it to the server using a named volume. Every time I do a <code>docker-compose up --build</code> I want the client side to build and update the named volume <code>clientapp:</code>. How do I do that?</p>

<p>docker-compose.yml</p>

<pre><code>version: '2' 

volumes:
  clientapp:

services:
  database:
    image: mongo:3.4
    volumes:
      - /data/db
      - /var/lib/mongodb
      - /var/log/mongodb

  client:
    build: ./client
    volumes:
      - clientapp:/usr/src/app/client

  server:
    build: ./server
    ports:
      - ""3000:3000""
    environment:
      - DB_1_PORT_27017_TCP_ADDR=database
    volumes:
      - clientapp:/usr/src/app/client
    depends_on:
      - client
      - database
</code></pre>

<p>client Dockerfile</p>

<pre><code>FROM node:6

ENV NPM_CONFIG_LOGLEVEL warn

RUN mkdir -p /usr/src/app

WORKDIR /usr/src/app

COPY package.json /usr/src/app

RUN npm install

COPY . /usr/src/app

# builds my application into /client
CMD [""npm"", ""build""]
</code></pre>
"
"44339237","Missing Carriage Return in Docker for Mac Containers","<bash><macos><docker>","44690115","Docker Exec -it bash Terminal Output Improperly Formatted","<docker><iterm2>","<p>All of a sudden all of my <code>docker/docker-compose exec/run</code> commands are printing logs that are lacking a carriage return making command line impossible to read due to indentation (see photos below).</p>

<p>I re-installed docker to factory settings, but that didn't fix anything.</p>

<p>where else should I look to solve this sort of problem?</p>

<p><strong>Update</strong> 
This is an active issue in <a href=""https://github.com/docker/for-mac/issues/1681"" rel=""noreferrer"">docker-for-mac</a>.</p>

<p>I just updated to 17.06.0-rc1-ce-mac13 and that is when I started having the problems.</p>

<p>Also, can you leave a comment if you are voting to close?</p>

<p><a href=""https://i.stack.imgur.com/vkF2u.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vkF2u.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/fEw3A.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fEw3A.jpg"" alt=""enter image description here""></a></p>
","<p>The command promp after running <code>docker exec -it ... bash</code> is not formatted nicely. The problem happens on the OSX terminal or iTerm2. Any setting for the iTerm2 I need to change or is that related to the bash command? Thanks.</p>

<p>Picture:<a href=""https://i.stack.imgur.com/Qj8ot.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qj8ot.png"" alt=""enter image description here""></a></p>
"
"44339237","Missing Carriage Return in Docker for Mac Containers","<bash><macos><docker>","44466906","Docker Container Shell Tab Hell","<django><shell><docker>","<p>All of a sudden all of my <code>docker/docker-compose exec/run</code> commands are printing logs that are lacking a carriage return making command line impossible to read due to indentation (see photos below).</p>

<p>I re-installed docker to factory settings, but that didn't fix anything.</p>

<p>where else should I look to solve this sort of problem?</p>

<p><strong>Update</strong> 
This is an active issue in <a href=""https://github.com/docker/for-mac/issues/1681"" rel=""noreferrer"">docker-for-mac</a>.</p>

<p>I just updated to 17.06.0-rc1-ce-mac13 and that is when I started having the problems.</p>

<p>Also, can you leave a comment if you are voting to close?</p>

<p><a href=""https://i.stack.imgur.com/vkF2u.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vkF2u.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/fEw3A.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fEw3A.jpg"" alt=""enter image description here""></a></p>
","<p>Richard Hendricks has infected my Docker container and is adding tabs to every new line of output even after just hitting enter repeatedly , example below. Please help stop this madness!</p>

<pre><code>(devops)Daves-MacBook-Pro:database-manager dave$ docker exec -it devops-api sh
/var/devops #
              /var/devops #
                            /var/devops #
                                          /var/devops #
                                                        /var/devops #
                                                                      /var/devops #
                                                                                    /var/devops #
                                                                                                  /var/devops #
                                                                                                                /var/devops #
                                                                                                                              /var/devops # exit
                                                                                                                                                (devops)Daves-MacBook-Pro:database-manager dave$
</code></pre>

<p>EDIT:
The image in my Dockfile is <code>python:3.6-alpine</code> and building the image using <code>docker-compose build</code> and <code>docker-compose up -d</code></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61093838","Can't connect node container to mysql (locally hosted mysql)?","<mysql><docker><docker-compose><dockerfile><port>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p><strong>I have a node container that I'm trying to connect to locally hosted mysql db (outside container).</strong>  I've tried ""localhost"", ""mysql"" and now ip address of the instance container is running on 172.17.x.x:3306  <em>(found using docker inspect  | grep Gateway)</em> in config.js not working. ideas?</p>

<p>Mysql binds to 0.0.0.0</p>

<p>Error: </p>

<pre><code>Unable to connect to the database: { SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306
    at Promise.tap.then.catch.err (/usr/src/app/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:123:19)
    at tryCatcher (/usr/src/app/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/usr/src/app/node_modules/bluebird/js/release/promise.js:547:31)
    at Promise._settlePromise (/usr/src/app/node_modules/bluebird/js/release/promise.js:604:18)
    at Promise._settlePromise0 (/usr/src/app/node_modules/bluebird/js/release/promise.js:649:10)
    at Promise._settlePromises (/usr/src/app/node_modules/bluebird/js/release/promise.js:725:18)
    at _drainQueueStep (/usr/src/app/node_modules/bluebird/js/release/async.js:93:12)
    at _drainQueue (/usr/src/app/node_modules/bluebird/js/release/async.js:86:9)
    at Async._drainQueues (/usr/src/app/node_modules/bluebird/js/release/async.js:102:5)
    at Immediate.Async.drainQueues [as _onImmediate] (/usr/src/app/node_modules/bluebird/js/release/async.js:15:14)
    at runCallback (timers.js:705:18)
    at tryOnImmediate (timers.js:676:5)
    at processImmediate (timers.js:658:5)
  name: 'SequelizeConnectionRefusedError',
  parent:
   { Error: connect ECONNREFUSED 127.0.0.1:3306
       at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
     errno: 'ECONNREFUSED',
     code: 'ECONNREFUSED',
     syscall: 'connect',
     address: '127.0.0.1',
     port: 3306,
     fatal: true },
  original:
   { Error: connect ECONNREFUSED 127.0.0.1:3306
       at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
     errno: 'ECONNREFUSED',
     code: 'ECONNREFUSED',
     syscall: 'connect',
     address: '127.0.0.1',
     port: 3306,
     fatal: true } }
</code></pre>

<p>config.js</p>

<pre><code>console.log(""Running...."");
var sequelize = new Sequelize(""dbname"", ""admin"", ""pass"", {
  host: ""172.17.x.x:3306"",
  dialect: ""mysql"",
  port: 3306,
  pool: {
    max: 5,
    min: 0,
    idle: 10000,
  },
});
</code></pre>

<p><strong>docker file</strong></p>

<pre><code>FROM node:10

# Create app directory
WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
# where available (npm@5+)
COPY package*.json ./

RUN npm install
# If you are building your code for production
# RUN npm ci --only=production

# Bundle app source
COPY . .

EXPOSE 8000
CMD [ ""npm"", ""start"" ]
</code></pre>

<p><strong>is it my docker run command (need to port map to 8000?): docker run -i -t 49920e8c68b2</strong></p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","61244450","Docker compose wait for spring boot app is up","<spring><spring-boot><docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I using docker-compose in version 3.3 and I want to wait for a container with spring app is up and after that other containers with spring boot app should be started. I tried with health check but it's doesn't work. This is my docker-compose looks like:</p>

<pre><code>version: '3.3'
services:
  eureka:
    build: ./eureka
    ports:
      - 8761:8761
    networks:
      - spring-cloud-network
    environment:
      - SPRING_ZIPKIN_BASEURL=http://zipkin:9411
    depends_on:
      - zipkin
    healthcheck:
      test: [""CMD"", ""curl"", ""-f"", ""http://localhost:8761""]
      interval: 10s
      timeout: 10s
      retries: 5
  zipkin:
    build: ./zipkin
    ports:
      - 9411:9411
    networks:
      - spring-cloud-network
    healthcheck:
      test: [""CMD"", ""curl"", ""-f"", ""http://localhost:9411""]
      interval: 10s
      timeout: 10s
      retries: 5
</code></pre>

<p>Is it possible to achieve what I want?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","61172935","Django container is not entirely waiting for a new postgres container","<python><django><postgresql><docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>Ok, so, I'm trying to start a project using Django and docker. The first step is simple, I just want to launch a default django app in container and see the django starting screen in my browser. I also want to hook up a simplest postgres container to run along with it. Everything great.
I created a simple image for my app, nothing fancy:</p>

<pre class=""lang-sh prettyprint-override""><code>FROM python:3.7
RUN mkdir /app
COPY requirements.txt /app/
WORKDIR /app
RUN pip install -r requirements.txt
COPY . /code/
</code></pre>

<p>And a docker-compose.yml:</p>

<pre class=""lang-yaml prettyprint-override""><code>version: '3'

services:
  python:
    build:
      context: .
      dockerfile: docker/python/Dockerfile
    volumes:
      - .:/app
    command: python manage.py runserver 0.0.0.0:8000
    ports:
      - ""8000:8000""
    depends_on:
      - db
    restart: always
  db:
    image: postgres:9.5
    environment:
      POSTGRES_USER: cards
      POSTGRES_DB: cards
      POSTGRES_PASSWORD: cards
</code></pre>

<p>As you can see, I'm not even copying any source code into the docker image, I'm mounting a volume for django to read it from host.</p>

<p>The problem is when I start the containers, the django server is starting too soon for the postgres to be ready, and psycopg2 throws an OperationalError.
Here are the logs, when I start fresh containers:</p>

<pre><code>Creating network ""backend_default"" with the default driver
Creating backend_db_1 ... done
Creating python       ... done
Attaching to backend_db_1, python
db_1      | The files belonging to this database system will be owned by user ""postgres"".
db_1      | This user must also own the server process.
db_1      |
db_1      | The database cluster will be initialized with locale ""en_US.utf8"".
db_1      | The default database encoding has accordingly been set to ""UTF8"".
db_1      | The default text search configuration will be set to ""english"".
db_1      |
db_1      | Data page checksums are disabled.
db_1      |
db_1      | fixing permissions on existing directory /var/lib/postgresql/data ... ok
db_1      | creating subdirectories ... ok
db_1      | selecting default max_connections ... 100
db_1      | selecting default shared_buffers ... 128MB
db_1      | selecting default timezone ... Etc/UTC
db_1      | selecting dynamic shared memory implementation ... posix
db_1      | creating configuration files ... ok
db_1      | creating template1 database in /var/lib/postgresql/data/base/1 ... ok
db_1      | initializing pg_authid ... ok
db_1      | setting password ... ok
db_1      | initializing dependencies ... ok
db_1      | creating system views ... ok
db_1      | loading system objects' descriptions ... ok
db_1      | creating collations ... ok
db_1      | creating conversions ... ok
db_1      | creating dictionaries ... ok
db_1      | setting privileges on built-in objects ... ok
db_1      | creating information schema ... ok
db_1      | loading PL/pgSQL server-side language ... ok
db_1      | vacuuming database template1 ... ok
db_1      | copying template1 to template0 ... ok
db_1      | copying template1 to postgres ... ok
python    | Watching for file changes with StatReloader
python    | Performing system checks...
python    |
db_1      | syncing data to disk ...
db_1      | WARNING: enabling ""trust"" authentication for local connections
db_1      | You can change this by editing pg_hba.conf or using the option -A, or
db_1      | --auth-local and --auth-host, the next time you run initdb.
db_1      | ok
db_1      |
db_1      | Success. You can now start the database server using:
db_1      |
db_1      |     pg_ctl -D /var/lib/postgresql/data -l logfile start
db_1      |
db_1      | waiting for server to start....LOG:  database system was shut down at 2020-04-12 14:17:12 UTC
db_1      | LOG:  MultiXact member wraparound protections are now enabled
db_1      | LOG:  database system is ready to accept connections
db_1      | LOG:  autovacuum launcher started
python    | System check identified no issues (0 silenced).
python    | Exception in thread django-main-thread:
python    | Traceback (most recent call last):
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 217, in ensure_connection
python    |     self.connect()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 195, in connect
python    |     self.connection = self.get_new_connection(conn_params)
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/postgresql/base.py"", line 178, in get_new_connection
python    |     connection = Database.connect(**conn_params)
python    |   File ""/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py"", line 127, in connect
python    |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
python    | psycopg2.OperationalError: could not connect to server: Connection refused
python    |     Is the server running on host ""db"" (192.168.160.2) and accepting
python    |     TCP/IP connections on port 5432?
python    |
python    |
python    | The above exception was the direct cause of the following exception:
python    |
python    | Traceback (most recent call last):
python    |   File ""/usr/local/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
python    |     self.run()
python    |   File ""/usr/local/lib/python3.7/threading.py"", line 870, in run
python    |     self._target(*self._args, **self._kwargs)
python    |   File ""/usr/local/lib/python3.7/site-packages/django/utils/autoreload.py"", line 54, in wrapper
python    |     fn(*args, **kwargs)
python    |   File ""/usr/local/lib/python3.7/site-packages/django/core/management/commands/runserver.py"", line 120, in inner_run
python    |     self.check_migrations()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/core/management/base.py"", line 453, in check_migrations
python    |     executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/migrations/executor.py"", line 18, in __init__
python    |     self.loader = MigrationLoader(self.connection)
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/migrations/loader.py"", line 49, in __init__
python    |     self.build_graph()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/migrations/loader.py"", line 212, in build_graph
python    |     self.applied_migrations = recorder.applied_migrations()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/migrations/recorder.py"", line 73, in applied_migrations
python    |     if self.has_table():
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/migrations/recorder.py"", line 56, in has_table
python    |     return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 256, in cursor
python    |     return self._cursor()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 233, in _cursor
python    |     self.ensure_connection()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 217, in ensure_connection
python    |     self.connect()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/utils.py"", line 89, in __exit__
python    |     raise dj_exc_value.with_traceback(traceback) from exc_value
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 217, in ensure_connection
python    |     self.connect()
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/base/base.py"", line 195, in connect
python    |     self.connection = self.get_new_connection(conn_params)
python    |   File ""/usr/local/lib/python3.7/site-packages/django/db/backends/postgresql/base.py"", line 178, in get_new_connection
python    |     connection = Database.connect(**conn_params)
python    |   File ""/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py"", line 127, in connect
python    |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
python    | django.db.utils.OperationalError: could not connect to server: Connection refused
python    |     Is the server running on host ""db"" (192.168.160.2) and accepting
python    |     TCP/IP connections on port 5432?
python    |
python    |
db_1      |  done
db_1      | server started
db_1      | CREATE DATABASE
db_1      |
db_1      |
db_1      | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
db_1      |
db_1      | waiting for server to shut down...LOG:  received fast shutdown request
db_1      | LOG:  aborting any active transactions
db_1      | LOG:  autovacuum launcher shutting down
db_1      | .LOG:  shutting down
db_1      | LOG:  database system is shut down
db_1      |  done
db_1      | server stopped
db_1      |
db_1      | PostgreSQL init process complete; ready for start up.
db_1      |
db_1      | LOG:  database system was shut down at 2020-04-12 14:17:13 UTC
db_1      | LOG:  MultiXact member wraparound protections are now enabled
db_1      | LOG:  database system is ready to accept connections
db_1      | LOG:  autovacuum launcher started
</code></pre>

<p>As you can see, django container starts up in he middle of the postgres setup, and it fails to connect. If I just restart the django container (another thing, even if the runserver command failed, docker container does not think that was an error and continues working without restarting - thats I think understandable because it launches itself with StatReloader) it works of course, but I would be happy if I could have an application running from the first launch, not to restart it on purpose...</p>

<p>Is there anyway to prevent this from happening? Have I made any mistake here, or maybe should I take another approach on that?</p>

<p>Thanks in advance.</p>
"
"32052975","How could I bind docker container to specific external interface","<docker>","61058370","How to bind secured PHP source code running inside Docker to a specific IP or MAC address?","<php><docker><security><ioncube>","<p>I have two network interfaces, <code>eth0</code> and <code>eth1</code>,</p>

<p>How could I bind all docker container to <code>eth1</code>, and let all network traffic go out and in via the <code>eth1</code></p>

<p>Thanks~</p>

<h1>update</h1>

<p>I tried to bind to the <code>eth1</code> with 133.130.60.36.</p>

<p>But i still got no luck, i still get the eth0 IP as the public IP in the container. the network flow is not go out via eth1</p>

<pre><code>➜  ~  docker run -d --name Peach_1 -p 133.130.60.36::9998 -ti sample/ubuntu-vnc-selenium-firefox

➜  ~  docker ps
CONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                     NAMES
eb28f0d1c337        sample/ubuntu-vnc-selenium-firefox   ""/opt/bin/run_sele_s   4 minutes ago       Up 4 minutes        5901/tcp, 133.130.60.36:32768-&gt;9998/tcp   Peach_1

➜  ~  docker exec -ti Peach_1 zsh

➜  /  curl ipecho.net/plain ; echo
133.130.101.114
</code></pre>
","<p>There are tools like <a href=""https://www.ioncube.com/"" rel=""nofollow noreferrer"">ioncube</a> or <a href=""https://www.sourceguardian.com"" rel=""nofollow noreferrer"">php encoder</a>, which make it possible to restrict PHP source code usage to specific IP addresses or MAC addresses. In theory it works perfectly fine, when the source code is deployed on a server (or servers) with specific static address. But I do not understand, how to secure source code, when it is deployed, using tools like Docker.</p>

<p>The problem is that Docker dynamically assigns IP and MAC addresses to running containers, so it seems like such tools as ioncube or php encoder can not work in this environment. Am I right or is it still possible to bind dockerized source code to specific addresses?</p>
"
"39913757","Restrict Internet Access - Docker Container","<docker><docker-networking>","61237950","How to disable external network calls in docker compose?","<docker><networking><docker-compose><circleci>","<p>I have a situation to restrict internet access of the container in load balancer network. for example in that below picture</p>

<p><a href=""https://i.stack.imgur.com/tChKt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tChKt.png"" alt=""easy for your reference""></a></p>

<p>Only <strong>container4</strong> connects to the Internet; other <strong>three</strong> only communicate through <strong>container4</strong> with the outside world. For example if <strong>container1</strong> needs smtp support, it will forward smtp request to <strong>container4</strong> to get access.</p>

<p>No container other than <strong>container4</strong> should be allowed to access the Internet directly! This should be enforced on Docker level.</p>

<p>I believe it will be configurable on <strong>docker network creation</strong>, can any one explain how to achieve this?</p>
","<p>I am running a set of tests against a large legacy service that (for now) makes external network calls to other services over the internet.</p>

<p>The goal for this project is to mock these external services as other docker containers within the same <code>docker-compose</code> network. Some of these services are microservices run by our organization, others are third-party vendors.</p>

<p>We are logging external calls to identify which external endpoints are being called by this service, and mocking those calls accordingly. But because I'll be putting this in CI, and this service has many contributors in a large organization, I want to enforce that no external services are called by isolating the entire <code>docker-compose</code> network from the internet.</p>

<p>Short of turning off my wifi, How can I disable external calls from these services, while still allowing services in the same docker network to communicate with each other?</p>

<p>The options given here: <a href=""https://docs.docker.com/network/#network-drivers"" rel=""nofollow noreferrer"">https://docs.docker.com/network/#network-drivers</a> do not seem to be what I want.</p>

<p>We use CircleCI, so if there is a solution within that configuration thats fine. I just haven't found anything related to that in those docs either, and would prefer a <code>docker-compose</code> solution so I can test it locally.</p>
"
"45928842","multiple volumes to single target directory?","<docker><unionfs>","61154399","Is many to one (HOST<->Container) volume mapping possible?","<docker><docker-compose><docker-volume>","<p>Is there a way to mount multiple volumes from a host to form a single target mount point? A bit like this:</p>

<pre><code>docker run --name ubuntu_bash \
    --rm --interactive --tty \
    --volume=/media/Large/videos:/videos \
    --volume=/media/Small/videos:/videos \
    ubuntu find /videos
</code></pre>

<p>I'm guessing the answer is no but with ""overlay"" having so many meanings in the context of Docker it's a bit difficult to search for this on the web.</p>

<p>If not, is there a Docker Store image that might help? Unfortunately a lot of Docker images don't give sufficient instructions on how to use them.</p>
","<p>For example something link this (snippet below),</p>

<pre><code>ports:
      - ${MONGO_PORT}:${MONGO_PORT}
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    volumes:
      - qm-dbdata:/data/db/raw
      - /data/db/raw:/data/db/raw
</code></pre>
"
"46907584","Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error","<python><docker><dockerfile><docker-image>","61195945","ERROR: Service 'web' failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder561233561/bin/start.sh: no such file or directory","<linux><bash><docker><nginx><docker-compose>","<p>I want to create a docker image. This is my work directory:
Dockerfile.in   test.json   test.py</p>

<p>And this is my Dockerfile:</p>

<pre><code>COPY ./test.json /home/test.json
COPY ./test.py /home/test.py

RUN python test.py
</code></pre>

<p>When i launch this command: 
<code>docker build -f Dockerfile.in -t 637268723/test:1.0 .</code></p>

<p>It gives me this error: </p>

<pre><code>`Step 1/5 : COPY ./test.json /home/test.json
 ---&gt; Using cache
 ---&gt; 6774cd225d60
 Step 2/5 : COPY ./test.py /home/test.py
 COPY failed: stat /var/lib/docker/tmp/docker-builder428014112/test.py: 
 no such file or directory`
</code></pre>

<p>Can anyone help me?</p>
","<p>I'm using centos7 in virtual box</p>

<p>I was starting to create a service</p>

<p>After running  COPY ./bin/start.sh /start.sh, its throwing the below error</p>

<pre><code>ERROR: Service 'web' failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder561233561/bin/start.sh: no such file or directory
</code></pre>

<p>Image shows the folder structure</p>

<p>DockerFile</p>

<pre><code>FROM remote-host

COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo

RUN                                                                          \
  yum -y install nginx-1.12.2 openssl --enablerepo=nginx                  &amp;&amp; \
  yum -y install https://centos7.iuscommunity.org/ius-release.rpm         &amp;&amp; \
  yum -y install                                                             \
    php72u-fpm                                                               \
    php72u-cli                                                               \
    php72u-mysqlnd                                                           \
    php72u-soap                                                              \
    php72u-xml                                                               \
    php72u-zip                                                               \
    php72u-json                                                              \
    php72u-mcrypt                                                            \
    php72u-mbstring                                                          \
    php72u-zip                                                               \
    php72u-gd                                                                \
     --enablerepo=ius &amp;&amp; yum clean all

EXPOSE 80 443

VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm

COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf

COPY ./bin/start.sh /start.sh

RUN chmod +x /start.sh

CMD /start.sh
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: '3'
services:
  jenkins:
    container_name: jenkins
    image: jenkins-ansible
    build:
      context: jenkins-ansible
    ports:
      - ""8080:8080""
    volumes:
      - $PWD/jenkins_home:/var/jenkins_home
    networks:
      - net
  remote_host:
    container_name: remote-host
    image: remote-host
    build:
      context: centos7
    volumes:
      - $PWD/script.sh:/tmp/newScript.sh
    networks:
      - net
  db_host:
    container_name: db
    image: mysql:5.7
    environment:
      - ""MYSQL_ROOT_PASSWORD=1234""
    volumes:
      - $PWD/db_data:/var/lib/mysql
    networks:
      - net
  web:
    container_name: web
    image: ansible-web
    build:
      context: jenkins-ansible/web
    ports:
      - ""80:80""
    networks:
      - net

networks:
  net:
</code></pre>

<p><a href=""https://i.stack.imgur.com/lcDzY.png"" rel=""nofollow noreferrer"">enter image description here</a>
The image shows the project structure
Please help me out to remove this error</p>
"
"46907584","Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error","<python><docker><dockerfile><docker-image>","61023611","Docker COPY newly created file in gitlab pipeline fails","<docker><gitlab><gitlab-ci><docker-in-docker>","<p>I want to create a docker image. This is my work directory:
Dockerfile.in   test.json   test.py</p>

<p>And this is my Dockerfile:</p>

<pre><code>COPY ./test.json /home/test.json
COPY ./test.py /home/test.py

RUN python test.py
</code></pre>

<p>When i launch this command: 
<code>docker build -f Dockerfile.in -t 637268723/test:1.0 .</code></p>

<p>It gives me this error: </p>

<pre><code>`Step 1/5 : COPY ./test.json /home/test.json
 ---&gt; Using cache
 ---&gt; 6774cd225d60
 Step 2/5 : COPY ./test.py /home/test.py
 COPY failed: stat /var/lib/docker/tmp/docker-builder428014112/test.py: 
 no such file or directory`
</code></pre>

<p>Can anyone help me?</p>
","<p>My Dockerfile includes a COPY command which copies a directory that was just created in the pipeline.</p>

<pre><code>COPY [""$CI_PROJECT_DIR/ui-dist/"", ""/content/ui-dist/""]
</code></pre>

<p>That COPY command fails with the following error:</p>

<pre><code>COPY failed: stat /var/lib/docker/tmp/docker-builder597514812/ui-dist: no such file or directory
</code></pre>

<p>However, I know the directory exists because right before I build the image, <code>ls $CI_PROJECT_DIR/ui-dist</code> prints <code>bundle.js</code></p>

<p>The runner is using the dind service. I'm not sure if that's relevant.</p>

<p><strong>EDIT:</strong></p>

<p>Okay, I tried removing the reference to the env variable. i.e. I changed the pipeline from</p>

<pre><code>- docker cp ui:/content/dist $CI_PROJECT_DIR/ui-dist
- docker build --tag ${IMAGE_ID} api
</code></pre>

<p>to</p>

<pre><code>- docker cp ui:/content/dist ui-dist
- docker build --tag ${IMAGE_ID} api
</code></pre>

<p>and I've updated the docker command to</p>

<pre><code>COPY [""ui-dist/"", ""/content/ui-dist/""]
</code></pre>

<p>and I'm still getting this error message:</p>

<pre><code>COPY failed: stat /var/lib/docker/tmp/docker-builder488922739/ui-dist: no such file or directory
</code></pre>
"
"47135390","Getting exit code out of docker-compose up","<docker><docker-compose><exit><exit-code><terminate>","61291536","How to stop docker-compose after process termination","<docker><docker-compose><exit><exit-code><terminate>","<p>I have an issue getting an exit code out of <code>docker-compose up</code>.<br />
I have the simplest container that runs a script that always exits with 1:</p>
<pre><code>#!/usr/bin/env sh
exit 1
</code></pre>
<p>My Dockerfile:</p>
<pre><code>FROM mhart/alpine-node:6
RUN mkdir /app
WORKDIR /app
</code></pre>
<p>And my docker-compose.yml:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:
  test_container:
    container_name: test_container
    build: .
    volumes:
      - ${PWD}/run.sh:/app/run.sh
    entrypoint: [&quot;/app/run.sh&quot;]
</code></pre>
<p>When I run it with:</p>
<pre class=""lang-sh prettyprint-override""><code>docker-compose -f docker-compose.yml up --force-recreate test_container
</code></pre>
<p>I can see in logs:</p>
<pre><code>Recreating test_container ... 
Recreating test_container ... done
Attaching to test_container
test_container exited with code 1
</code></pre>
<p>But when I <code>echo $?</code>, I get <code>0</code>.<br />
Docker version 17.09.0-ce, build afdb6d4. Running on OSX 10.12.6.<br />
Am I doing something wrong? Is that a known issue (I couldn't find anything out there)?</p>
","<p>By default <code>docker-compose up</code> keeps running even if some of the containers terminate and <code>restart</code> is set to <code>no</code>. </p>

<p>Is there a configuration to tell it to terminate (and stop all containers) when one of my containers finishes (the process in CMD terminates)?</p>

<p>Bonus points if it gets the exit code from the process of that container</p>
"
"48066994","Docker: ""no matching manifest for windows/amd64 in the manifest list entries""","<docker>","61002470","Docker Installation Error on Step 1/11 ./bin/acore-docker-build","<docker><azerothcore>","<p>I use Docker on Windows, and when I tried to pull a PHP image with this command</p>

<pre><code>$ docker pull php
</code></pre>

<p>I got this message:</p>

<pre><code>Using default tag: latest
latest: Pulling from library/php no matching manifest for windows/amd64 
        in the manifest list entries
</code></pre>

<p>How can I fix this problem?</p>
","<p>Having failed to install azerothcore normally, I have now tried to install it using docker.</p>

<p>But I am already running into an issue, probably because I am not very experienced with docker.</p>

<p>When I input <code>./bin/acore-docker-build</code>, it fails at step one, outputting this message:</p>

<pre><code>Sending build context to Docker daemon  862.4MB
Step 1/11 : FROM ubuntu:bionic
bionic: Pulling from library/ubuntu
no matching manifest for windows/amd64 10.0.18362 in the manifest list entries
Unable to find image 'acbuild:latest' locally
C:\Program Files\Docker\Docker\resources\bin\docker.exe: Error response from daemon: pull access denied for acbuild, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.
See 'C:\Program Files\Docker\Docker\resources\bin\docker.exe run --help'.
</code></pre>

<p>I am not sure what I am supposed to do about this. I am logged into a basic docker account. I went to the docker.exe referenced in the above message and changed permissions for the account running docker to have full permissions. No change. I ran git as administrator. No change.</p>

<p>What should I be doing differently?</p>

<p>Thanks for reading. Willing to clarify anything.</p>

<p>Also, I am using windows 10.</p>
"
"51290808","batch: launch pipenv shell, then run command in the virtual environment","<batch-file><pipenv>","60957364","activate a virtual environment and run a command within from a bash script","<bash><docker><pipenv>","<p>Here is a batch script:</p>

<pre><code>Z:
cd Z:\different_directory
pipenv shell
cd ..\another_directory


:End
cmd  /k
</code></pre>

<p>What happens here is that the pipenv shell gets launched, but the virtual environment does not cd. Instead, once I exit the pipenv, it then runs the cd command.</p>

<p>Is it possible to run a command from inside the pipenv using this batch script?</p>
","<p>I am using a bash script as an entry point to a docker container. My goal is to be able to activate the virtual environment within the docker container and then run a command within from the same bash script with no manual input.
The relevant bits of the script is as follows</p>

<pre><code>#!/bin/bash
pipenv shell
ln -s /usr/local/lib/python3.7/dist-packages /usr/local/lib/python3.7/site-packages
command to be run within venv
python3
</code></pre>

<p>the issue is that when pipenv shell is run the venv is activated and entered but the rest of the command does not run within it. i can manually run commands within the venv and the rest of the script waits for me to exit. How do I specify I want commands run within?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61211684","kafka docker works well on localhost but not on a remote server","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have an issue with kafka docker image. I am using also kafkamanager and I am running those images from .yml file. The thing is that on localhost everything works fine but on remote server it doesn't.</p>

<p>I am using this .yml file:</p>

<pre><code>version: ""2""

services:
  kafkaserver:
    image: ""spotify/kafka:latest""
    container_name: kafka
    hostname: kafkaserver
    networks:
     - kafkanet
    ports:
      - 2181:2181
      - 9092:9092
    environment:
      ADVERTISED_HOST: kafkaserver
      ADVERTISED_PORT: 9092
  kafka_manager:
    image: ""mzagar/kafka-manager-docker:1.3.3.4""
    container_name: kafkamanager
    networks:
      - kafkanet
    ports:
      - 9000:9000
    links:
      - kafkaserver
    environment:
      ZK_HOSTS: ""kafkaserver:2181""

networks:
  kafkanet:
    driver: bridge
</code></pre>

<p>I am developing in python using kafka-python. WHen i start my program, program stops at the point when producer should start sending messages. But he doesn't. </p>

<p>After a minute program will give me this error message: </p>

<blockquote>
  <p>kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs. </p>
</blockquote>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61223033","How do i connect my containerised project to kafka running on localhost?","<java><spring-boot><docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I've been trying to connect my containerised spring-boot project with a of kafka and zookeeper running on my localhost but i seem to be getting an error when  i run the docker images.</p>

<p><a href=""https://i.stack.imgur.com/p2sv9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p2sv9.png"" alt=""enter image description here""></a></p>

<p>does anyone know what could be causing this  error and if so , what the best way to go about fixing it?</p>

<p>i alreadyhave ports 9092 and 2021 exposed </p>

<p><strong>edit:</strong>
i was asked to post the text:</p>

<pre><code>2020-04-15 06:55:34,872 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG common.network.Selector.pollSelectionKeys - [Consumer clientId=consumer-1, groupId=message] Connection with /172.17.59.17 disconnected
java.net.NoRouteToHostException: No route to host
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)
        at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:216)
        at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:531)
        at org.apache.kafka.common.network.Selector.poll(Selector.java:483)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:539)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:212)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:249)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:326)
        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1251)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1201)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doPoll(KafkaMessageListenerContainer.java:993)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:949)
        at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:901)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:748)
2020-04-15 06:55:34,874 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG kafka.clients.NetworkClient.handleDisconnections - [Consumer clientId=consumer-1, groupId=message] Node -1 disconnected.
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61304223","Can't correctly send to kafka-docker from network device","<docker><networking><apache-kafka><bitnami>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to setup a Kafka / Docker Setup on my host maschine and connect to it from a network device. I'm using bitnami/kafka. Here is my docker-compose file:</p>

<pre><code>version: '2'

services:
  zookeeper:
    image: 'bitnami/zookeeper:3'
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper_data:/bitnami'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: 'bitnami/kafka:2'
    ports:
      - '9092:9092'
      - '29092:29092'
    volumes:
      - 'kafka_data:/bitnami'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
</code></pre>

<p>When I run my python script on my host maschine ( not within docker ) and mercury being the name of my host it works just fine:</p>

<pre><code>from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers=['mercury:29092'])
print(""Connected"")
producer.send('topic', b'It works!')
print('Theoretically send')
producer.close()
print('Closed')
</code></pre>

<p>When I try to run the same script from another network device it doesn't work. I don't even get an error. The script also runs just fine, so there is no delay when trying to connect or sending. That only happens when I f.e. use a wrong port or a topic that doesn't exist. Especially the last part let's me believe that the script can connect but doesn't send the message correctly. I'm not sure why that's the case. Is my port setup correct or do I need some extra kafka environment settings?</p>

<p>Thanks in advance for the help</p>
"
"53460002","How to use dotnet restore properly in Dockerfile","<asp.net-core><dockerfile>","61232378","Why we need multiple copy instruction in DockerFile for Asp.net core","<docker><asp.net-core>","<p>When adding docker support to a ASP.NET Core project VS (15.9.2) will add a default Dockerfile that does restore, build and publish. But instead of just copying all files into the Docker build container it first copies just the proj-files, does the  restore and then copies the rest before it builds. I was wondering why this is done like this? In what way is that different from just copying all files directly and then doing the restore?</p>

<p>The problem with this approach is that all proj-files in the solution will need to be copied separately and if the project is really big with projects being added and removed from time to time it's a bit hard to keep the Dockerfile in synch with this. I just like to know why this is done like this and if it would be just as ok to copy everything instead?</p>

<pre><code>FROM microsoft/dotnet:2.1-sdk AS build
WORKDIR /src
COPY [""Temp2/Temp2.csproj"", ""Temp2/""]
COPY [""Temp3/Temp3.csproj"", ""Temp3/""]
RUN dotnet restore ""Temp2/Temp2.csproj""
COPY . .
WORKDIR ""/src/Temp2""
RUN dotnet build ""Temp2.csproj"" -c Release -o /app
</code></pre>

<p>or</p>

<pre><code>FROM microsoft/dotnet:2.1-sdk AS build
WORKDIR /src
COPY . .
RUN dotnet restore ""Temp2/Temp2.csproj""
WORKDIR ""/src/Temp2""
RUN dotnet build ""Temp2.csproj"" -c Release -o /app
</code></pre>
","<p>Currently I am learning docker. I was going through the official documentation to create Docker images for ASP.NET Core that shows a DockerFile with following content </p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:2.2 AS build-env
WORKDIR /app

# Copy csproj and restore as distinct layers
COPY *.csproj ./
RUN dotnet restore

# Copy everything else and build
COPY . ./
RUN dotnet publish -c Release -o out

# Build runtime image
FROM mcr.microsoft.com/dotnet/core/aspnet:2.2
WORKDIR /app
COPY --from=build-env /app/out .
ENTRYPOINT [""dotnet"", ""aspnetapp.dll""]
</code></pre>

<p>I successfully created the image and container eventually. </p>

<p>Here, in the DockerFile we have two COPY instruction. </p>

<p>first one, copy only the *.csproj files from that directory and paste it to /app directory inside docker image.</p>

<p>second one, copy all the other files files and paste it to /app directory inside docker image.</p>

<p>As far as understand, this tow <code>COPY</code> instruction in doing the same thing. so, instead of two different copy instruction, we can write one and it works fine. Like this -</p>

<pre><code>COPY . ./
RUN dotnet restore
RUN dotnet publish -c Release -o out
</code></pre>

<p>so, why we are using two different instruction to copy files to docker image? Does it perform well with two copy instruction?</p>

<p>In the first <code>COPY</code> instruction, there is a comment <code>#Copy csproj and restore as distinct layers</code>. What it actually means or does? </p>
"
"58219201","Postgres: granting access to a role/user for future tables created by a different role/user","<postgresql><spring-boot><migration><flyway>","60935487","Postgresql: cannot set default privileges on tables","<postgresql><docker>","<p>I'm building a spring boot application. Flyway database migrations are executed at the application startup. </p>

<p>I decided to use two different roles: <strong>role__app</strong> (<strong>read/write</strong> rights on tables, sequences in <strong>app</strong> schema) and <strong>role__migration</strong> (<strong>advanced</strong> rights in <strong>app</strong>/<strong>migration</strong> schemas). </p>

<p>Flyway migrations are executed under <strong>role__migration</strong> so it becomes the owner of the created objects. I thought that the following statements would help:</p>

<pre><code>ALTER DEFAULT PRIVILEGES IN SCHEMA app GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO role__app;
ALTER DEFAULT PRIVILEGES IN SCHEMA app GRANT USAGE ON SEQUENCES TO role__app;
</code></pre>

<p>But when the new tables are added to the <strong>app</strong> schema the <strong>user__app</strong> (belongs to the <strong>role__app</strong>) doesn't have access to the tables.</p>

<p>Is it possible to maintain such a flow (with app, migrattion users/roles) by Postgres or by any other means?</p>

<hr>

<p>As a side note I should mention that I run the following statements on the target database:</p>

<pre><code>REVOKE CREATE ON SCHEMA public FROM PUBLIC;
REVOKE ALL ON DATABASE myDb FROM PUBLIC;
</code></pre>

<hr>

<p>Update 1</p>

<p>I added the <code>FOR ROLE</code> clause, yet I'm still getting the permission denied message for a created table (<strong>app.property</strong>) in app schema for user <strong>user__app</strong>. The owner of the table is <strong>user__mig</strong>.</p>

<p>Update 2</p>

<p>After logging in as postgres user in dbeaver we can see that <strong>user__mig</strong> has all necessary permissions ticked whereas the <strong>user__app</strong> has no permissions on the <strong>app.property</strong> table at all:</p>

<p><img src=""https://i.stack.imgur.com/DZWVp.png"" alt=""enter image description here""></p>

<hr>

<p>Here is a gist to reproduce the problem: <a href=""https://gist.github.com/happygrizzly/849a6a791f028ba5b191f73180ae35d1"" rel=""nofollow noreferrer"">https://gist.github.com/happygrizzly/849a6a791f028ba5b191f73180ae35d1</a></p>
","<p>On Postgres 9.6 on Docker I run this in my entrypoint:</p>

<pre><code>    psql -v ON_ERROR_STOP=1 --username ""$POSTGRES_USER"" &lt;&lt;-EOSQL
        BEGIN;
            CREATE ROLE $rouser LOGIN PASSWORD '$POSTGRES_READONLY_PASSWORD';
            GRANT CONNECT ON DATABASE $database TO $rouser;
            GRANT USAGE ON SCHEMA public TO $rouser;
            GRANT SELECT ON ALL TABLES IN SCHEMA public TO $rouser;

            ALTER DEFAULT PRIVILEGES GRANT SELECT ON TABLES TO $rouser;

            ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO $rouser;
            alter default privileges for role $POSTGRES_USER grant select on tables to $rouser;
            alter default privileges for role $POSTGRES_USER in schema public grant select on tables to $rouser;

            alter default privileges grant select on sequences to $rouser;
            alter default privileges in schema public grant select on sequences to $rouser;
            alter default privileges for role $POSTGRES_USER grant select on sequences to $rouser;
            alter default privileges for role $POSTGRES_USER in schema public grant select on sequences to $rouser;
        COMMIT;
EOSQL
</code></pre>

<p>Basically I just want $rouser to have select privileges on all tables created at any time.</p>

<p>This code above works only if the tables are already created, but not for future tables, basically the defaults are not working.</p>

<p>Am I doing something wrong?</p>
"
"16047306","How is Docker different from a virtual machine?","<docker><containers><virtual-machine><virtualization>","55660648","What is the purpose of a CentOS Docker image?","<docker>","<p>I keep rereading <a href=""https://docs.docker.com/"" rel=""noreferrer"">the Docker documentation</a> to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?</p>

<p>Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?</p>
","<p>From everything I've read and heard about Docker, the whole point of it is to encapsulate an application so that it is runnable on any machine. It achieves this by wrapping up the environment needed (tools/dependencies) into the docker image. A container is an instance of an image. There isn't an OS running in a docker container.</p>

<p>Now, if that's all true, then why does this exist: <a href=""https://hub.docker.com/_/centos"" rel=""nofollow noreferrer"">CentOS official docker image</a>? I thought docker images were just for applications, not entire operating systems.</p>

<p>Whenever I instantiate that image, I get a container that, when attached to it, appears to be a functioning CentOS shell.</p>

<p>You can do it yourself to see what I mean:</p>

<ul>
<li>Dockerfile contents: <code>FROM centos:centos7</code></li>
<li>Build the image: <code>docker build centos-img .</code></li>
<li>Run the container: <code>docker run -ti centos-img</code></li>
<li>To detach: <code>ctrl+p, ctrl+q</code></li>
<li>To reattach: <code>docker attach &lt;container id&gt;</code></li>
</ul>

<p>This really looks and feels like a VM.</p>

<p>If I run <code>cat /etc/os-release</code> it even says I am running CentOS.</p>

<p>What exactly is this? Could I use a CentOS docker image as if it were a virtual machine? What are the limitations?</p>

<p>(What's confusing me is <code>docker containers != VMs</code>, however, from by exploration, I created a docker image that looks and feels like a VM. Why would I ever want to run a VM again, if I can run a lightweight docker image that acts exactly like a VM?)</p>

<hr>

<p>Is this centOS docker image just a starting place, that I am supposed to build off of (ie, put my application in), so that it just functions as a host for my application? The more I read and dig into Dockerfile examples, the more I believe this is assumption is true.</p>

<hr>

<p>What I'm really trying to do:</p>

<p>I have a bunch of small applications/services that function together as a system that make up a large application. I currently have this system running on a centOS machine. I want the ability to easily run multiple instances of this large application, each in their own environment / without stepping on each other's toes. Would it be possible to do this by using a <code>centOS docker image + all of the small applications/services needed</code>, resulting in a <code>large application image</code>? With this image, I can spin up multiple containers, each one running a separate instance of the large application? Is that a reasonable/achievable thing to do with Docker?</p>

<hr>

<p>Some of my understanding may be incorrect, or I may be suggesting to use Docker in a way that it is not meant to be used. If so, feel free to point it out. Thanks!</p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","55643237","What files are produced by Docker's build command and where are they stored?","<docker><windows-10><dockerfile>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I've installed <a href=""https://hub.docker.com/search/?type=edition&amp;offering=community&amp;operating_system=windows"" rel=""nofollow noreferrer"">Docker Engine Community edition for Windows</a> on a computer running Windows 10 Pro.</p>

<p>I ran <code>docker build --tag my-python-app .</code></p>

<p>Which files did the command produce and where are they located?</p>

<p>Also, I've read a <a href=""https://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine"">Where are Docker images stored on the host machine?</a>.</p>

<p>Lastly, the simple Dockerfile contains:</p>

<pre><code>FROM python:alpine3.7
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE 5000
CMD python ./index.py
</code></pre>
"
"26282344","What does double-dash do when following a command?","<linux><bash>","55598218","What does ""--"" mean in this Docker compose command?","<bash><docker><docker-compose>","<p>Question:</p>

<p>How a app should parse command line when there is a Command after a double dash?
(Not duplicated of <a href=""https://unix.stackexchange.com/questions/11376/what-does-double-dash-mean-also-known-as-bare-double-dash"">this</a> and <a href=""https://stackoverflow.com/questions/14217853/what-does-the-double-dash-option-do-on-git-reset"">this</a>)</p>

<p>I know what will double dash normally do:</p>

<blockquote>
  <p>A -- signals the end of options and disables  further  option
  processing.   Any arguments after the -- are treated as file‐
  names and arguments.  An argument of - is equivalent to --</p>
</blockquote>

<p>So it set the following things as arguments, for example:
<code>myapp -f &lt;args&gt; ...</code> Then <code>$ myapp -f -- -a -b</code> will treat <code>-a</code> and <code>-b</code> as arguments of <code>-f</code>, instead of Flags</p>

<p>But what will happen when an app required:</p>

<pre><code>myapp cmd &lt;arg&gt; -f &lt;args&gt;...
</code></pre>

<p>And the command line is <code>$ myapp -f -- test cmd sth</code>, Should it parsed as:</p>

<ul>
<li><code>myapp</code> received a <code>-f</code> Flag with arguments <code>test</code>, <code>cmd</code> and <code>sth</code></li>
<li>or <code>myapp</code> received a <code>cmd</code> Command followed by <code>sth</code>, a <code>-f</code> with argument <code>test</code> </li>
</ul>

<p>I'm writing a command line parser for python so I need to know how it should behave.</p>

<p>Thx a lot :)</p>
","<p><a href=""https://docs.docker.com/compose/startup-order/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/startup-order/</a></p>

<p>Could someone point me to some docs or general knowledge of what the ""--"" means in this docker command?</p>

<p><code>command: [""./wait-for-it.sh"", ""db:5432"", ""--"", ""python"", ""app.py""]</code></p>

<p>I've always been weak at bash and unix things and wanted to learn more.</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","55674858","How to create a path when copying in a dockerfile?","<php><docker><dockerfile><supervisord>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I'm try to copy some file in my php/dockerfile:</p>

<pre><code>COPY ./supervisord.conf /etc/supervisor                 #work
COPY ../../config/supervisor/* /etc/supervisor/conf.d   #doesn't work

WORKDIR /home/wwwroot/
</code></pre>

<p>What should the correct path look like? This is my structure:</p>

<pre><code>|config -&gt; supervisor -&gt; *
|
|docker -&gt; php -&gt; Dockerfile
|              -&gt; supervisord.conf
|
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","55615551","How to publish flask port from container to host","<python><docker><flask><alpine>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I want to run a dockerized flask server locally and access any routes that I define in it. Setup to reproduce my problem:</p>

<p><strong>app.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>from flask import Flask

app = Flask(__name__)

@app.route('/')
def index():
    return 'Hello World!'

</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code>from python:3.7-alpine

COPY app.py app.py

RUN pip install flask

CMD [ ""flask"", ""run"" ]
</code></pre>

<p>Building this container works fine. Following <a href=""https://docs.docker.com/engine/reference/commandline/run/"" rel=""nofollow noreferrer"">the documentation</a> and <a href=""https://docs.docker.com/network/links/"" rel=""nofollow noreferrer"">some in-depth examples</a>, running it with <code>docker run -p 5000:5000 flask</code> should do the trick. It starts the container and looks good:</p>

<pre><code>* Environment: production   
  WARNING: Do not use the development server in a production environment.  
  Use a production WSGI server instead.  
* Debug mode: off  
* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
</code></pre>

<p>And <code>docker ps</code> shows me that the port mapping also seems to work:</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES
0581bf3af0ea        flask               ""flask run""         25 seconds ago      Up 24 seconds       0.0.0.0:5000-&gt;5000/tcp   nervous_brown
</code></pre>

<p>But I only get a 404 when I try to reach the site in my browser.</p>

<p>Funnily enough, running the image with <code>docker run --net=host flask</code> does work, but I'd rather not use it.</p>

<p>I'm really bad at understanding how networks work, what am I doing wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","55481781","Why can't I connect to my Flask server after it's deployed in a Docker container?","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have created a Flask web-app that serves some static files, and it works just fine locally. I now want to create a Docker image for it and run a container where I can access it.</p>

<p>Running:
<code>python server.py</code> 
Works fine and I can access the application at <a href=""http://localhost:5000/"" rel=""nofollow noreferrer"">http://localhost:5000/</a></p>

<p>I created a Dockerfile like so:</p>

<pre><code>FROM python:3.7
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt
CMD python server.py
EXPOSE 8082
</code></pre>

<p>When I run:
<code>docker run -p 8082:5000 emotional</code>
it outputs that it's running and everything is working fine within the container.</p>

<pre><code> * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>

<p>but when I navigate to <a href=""http://localhost:8082"" rel=""nofollow noreferrer"">http://localhost:8082</a> I get ERR_EMPTY_RESPONSE, and when I try <a href=""http://localhost:5000"" rel=""nofollow noreferrer"">http://localhost:5000</a> (which I know isn't supposed to work) it gives ERR_CONNECTION_REFUSED</p>
"
"31324981","How to access host port from docker container","<docker><docker-container>","55605479","Access Web Application running on host on Docker Container","<docker><dockerfile>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<p>I have a Web Application running on my host at port 8082 (localhost:8082). I want to access this web application through my docker container.</p>

<p>A quick check by installing wget on my container and running <code>wget localhost:8082</code> , I got - </p>

<pre><code>Connecting to localhost (localhost)|127.0.0.1|:8082... failed: Connection refused.
Connecting to localhost (localhost)|::1|:8082... failed: Network is unreachable.
</code></pre>

<p>Is there some configuration I need to do so that my container can access the app running on my host?</p>

<p>PS - Localhost is Linux (CentOS)</p>
"
"31324981","How to access host port from docker container","<docker><docker-container>","55479787","How to configure spring-boot microservice docker container to access local mongodb database?","<java><mongodb><spring-boot><docker><microservices>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<p>I have a microservice developed with spring boot that connects with a mongodb.
When a run it local, everything is ok.
When I create a docker container to my microservice, but NOT to my mongodb, I receive an error of ""connection refused"".
I was connecting to my mongodb at ""localhost:27017"", and then ""127.0.0.1:27017"", but both doesn't work. I know that the problem is this address because docker uses a network adapter different from the server, but I can't find how to configure to my spring boot container connects with my mongodb local.
Is this approach wrong?
I don't wanna to dockerize my mongodb because in the future the application will not be together with my database.</p>

<p>I'm using a Linux Ubuntu.</p>
"
"38446391","Docker Compose JVM parameters","<java><docker><docker-compose>","55555837","What is the best way to dockerize Spring Boot application?","<java><spring-boot><docker>","<p>I wrote a java application that takes an environment variable that takes an argument to set a key for a JWT token salt key. Is there a way for me to pass the command variables in Docker Compose?</p>

<pre><code>java -Djava.security.egd=file:/dev/./urandom -jar /user-profile-api.jar --key=blah
</code></pre>

<p>And to run the docker image you just </p>

<pre><code>docker run -p 8080:8080 docker_image --key=blah
</code></pre>
","<p>I want to customize <em>JVM options</em> and <em>application command line arguments</em> when I run dockerized Spring Boot application as example below:</p>

<pre><code>java -Xms128M -Xmx256M -jar application.jar --spring.application.name=AcmeApplication
</code></pre>

<p>What is the best way to construct ENTRYPOINT?</p>

<pre><code>ENTRYPOINT [""bash"", ""-c"", ""java -jar /application.jar""]
</code></pre>
"
"39527571","Are shell scripts sensitive to encoding and line endings?","<bash><shell><sh>","55421871","Dockerfile can't find shell script on build","<linux><bash><shell><docker><go>","<p>I am making a NW.js app on Mac, and want to run the app in dev mode by double-clicking on an icon. First step, I'm trying to make my shell script work.</p>

<p>Using VSCode on Windows (I wanted to gain time), I have created a <code>run-nw</code> file at the root of my project, containing this:</p>

<pre><code>#!/bin/bash

cd ""src""
npm install

cd ..
./tools/nwjs-sdk-v0.17.3-osx-x64/nwjs.app/Contents/MacOS/nwjs ""src"" &amp;
</code></pre>

<p>but I get this output:</p>

<pre><code>$ sh ./run-nw

: command not found  
: No such file or directory  
: command not found  
: No such file or directory  

Usage: npm &lt;command&gt;

where &lt;command&gt; is one of:  (snip commands list)

(snip npm help)

npm@3.10.3 /usr/local/lib/node_modules/npm  
: command not found  
: No such file or directory  
: command not found
</code></pre>

<p>I really don't understand:</p>

<ul>
<li>it seems that it takes empty lines as commands. In my editor (VSCode) I have tried to replace <code>\r\n</code> with <code>\n</code> (in case the <code>\r</code> creates problems) but it changes nothing.</li>
<li>it seems that it doesn't find the folders (with or without the <code>dirname</code> instruction), or maybe it doesn't know about the <code>cd</code> command ?</li>
<li>it seems that it doesn't understand the <code>install</code> argument to <code>npm</code></li>
<li>the part that really weirds me out, is that it still runs the app (if I did a <code>npm install</code> manually)...</li>
</ul>

<p>Not able to make it work properly, and suspecting something weird with the file itself, I created a new one directly on the Mac, using vim this time. I entered the exact same instructions, and... now it works without any issue.<br>
A diff on the two files reveals exactly zero difference.</p>

<p>What can be the difference? What can make the first script not work? How can I find out?</p>

<h2>Update</h2>

<p>Following the accepted answer's recommandations, after the wrong line endings came back, I checked multiple things. It turns out that since I copied my <code>~/.gitconfig</code> from my Windows machine, I had <code>autocrlf=true</code>, so every time I modified the bash file under Windows, it re-set the line endings to <code>\r\n</code>.<br>
So, in addition to running dos2unix (which you will have to install using Homebrew on mac), if you're using Git, check your config.</p>
","<p>I'm trying to docker build an application that utilizes go. To install go, the dockerfile has the following command (this executes fine, by the way):</p>
<pre><code>RUN wget https://dl.google.com/go/go1.11.linux-amd64.tar.gz \
    &amp;&amp; tar -xf go1.11.linux-amd64.tar.gz \
    &amp;&amp; mv go /usr/local
</code></pre>
<p>The problem arises when script runs shell files within the 'install' subdirectory. Note, output of the following two steps:</p>
<pre><code>Step 9/13 : RUN . install/install-lmdb-linux.sh
 ---&gt; Running in 0c666807720a
: not found install/install-lmdb-linux.sh:
: not found install/install-lmdb-linux.sh:
-e

Installing LMDB

Cloning into 'lmdb'...
: not found install/install-lmdb-linux.sh:
make: Entering directory '/root/bystro/lmdb/libraries/liblmdb'
gcc -pthread -O2 -g -W -Wall -Wno-unused-parameter -Wbad-function-cast -Wuninitialized   -c mdb.c
gcc -pthread -O2 -g -W -Wall -Wno-unused-parameter -Wbad-function-cast -Wuninitialized   -c midl.c
ar rs liblmdb.a mdb.o midl.o
ar: creating liblmdb.a
</code></pre>
<p>This shell script seems to be failing, but somehow recovers (?) and build continues to:</p>
<pre><code>Step 11/13 : RUN . install/install-go-packages.sh
 ---&gt; Running in 7700bf77c2b1
: not found install/install-go-packages.sh:
-e

Installing go packages (bystro-vcf, stats, snp)

: not found install/install-go-packages.sh:
: not found install/install-go-packages.sh:
: not found install/install-go-packages.sh:
Made /root/go path
: not found install/install-go-packages.sh:
: not found install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
: not found: install/install-go-packages.sh:
The command '/bin/sh -c . install/install-go-packages.sh' returned a non-zero code: 127
</code></pre>
<p>This script is the point at which build fails with exit code 127 (command not found). The task of this script is basically just to 'go install' some dependencies for the app.</p>
<h2>My Debug:</h2>
<p>After messing around with variations of the two RUN functions, I finally decided to log some of the situation: I check to see if the two shell scripts were actually present at runtime, and to see if Go was present at runtime. Both were present.</p>
<pre><code>Step 9/15 : RUN ls install/
 ---&gt; Running in 0f0ad051b009
export-bystro-libs.sh
export-go-path-linux.sh
install-apt-deps.sh
install-go-linux.sh
install-go-mac.sh
install-go-packages.sh
install-liftover-linux.sh
install-lmdb-linux.sh
install-mac-deps.sh
install-perl-libs.sh
install-perlbrew-linux.sh
install-rpm-deps.sh
update-packages.sh
</code></pre>
<pre><code>Step 12/15 : RUN go version
 ---&gt; Running in b8b9d08ef9c3
go version go1.11 linux/amd64
</code></pre>
<p>Also, note, both scripts execute perfectly when I run them manually.</p>
<h2>Question:</h2>
<p>Why is my build failing? It seems like Docker has the scripts, the tools to execute them, and the proper instruction to do so, so I'm not sure how to approach this.</p>
"
"40318257","Why Go can lower GC pauses to sub 1ms and JVM has not?","<go><garbage-collection><jvm>","55383082","go vs java garbage collector for micro-services","<docker><go><kubernetes>","<p>So there's that: <a href=""https://groups.google.com/forum/?fromgroups#!topic/golang-dev/Ab1sFeoZg_8"">https://groups.google.com/forum/?fromgroups#!topic/golang-dev/Ab1sFeoZg_8</a>:</p>

<blockquote>
  <p>Today I submitted changes to the garbage collector that make typical worst-case stop-the-world times less than 100 microseconds. This should particularly improve pauses for applications with many active goroutines, which could previously inflate pause times significantly.</p>
</blockquote>

<p>High GC pauses are one if the things JVM users struggle with for a long time.</p>

<p>What are the (architectural?) constraints which prevent JVM from lowering GC pauses to Go levels, but are not affecting Go?</p>
","<p>I was told that Go's garbage collector does not cause interruptions in the program unlike Java's garbage collector which can cause a program to pause. This makes Go's much superior for latency sensitive micro-services.</p>

<p>I tried understanding this but not being an expert, I did not. I found some stuff like -
sub 10ms latency <a href=""https://groups.google.com/forum/?fromgroups#!topic/golang-dev/Ab1sFeoZg_8"" rel=""nofollow noreferrer"">https://groups.google.com/forum/?fromgroups#!topic/golang-dev/Ab1sFeoZg_8</a></p>

<p>In general, are there reasons where Go is a superior choice for designing micro-services running on docker/kubernetes compared to Java (I know that until Java 9,  Java only understood memory and cpu resources at a host level. which meant that it made assumptions about sizing thread pools and memory allocations and Docker wouldn't always provide these resources).  </p>
"
"46102119","Docker stack deploy in windows","<windows><docker><docker-compose><windows-server-2016><docker-stack>","55660067","I have a linux command that I need to work on Windows (i.e. Powershell)","<bash><powershell><docker><docker-compose>","<p>i have some questions in docker stack windows.</p>

<p>Can we enable docker stack in windows 2016.  Is it fully supported. </p>

<p>if yes, how can we enable. how we can automate creating swarm cluster using this file.
 i have searched google but can't find related to windows. please provide me If you have any links to study. </p>
","<p>I have inherited a linux command that deploys a docker stack. I need the equivalent Windows version of it. I can't seem to figure it out, as I'm new to docker.  </p>

<pre><code>docker stack deploy -c &lt;(docker-compose config) --with-registry-auth $(basename $PWD)
</code></pre>

<p>What's the equivalent Windows (Powershell) command for the above? The part I can't figure out is the ""&lt;(docker-compose config)"" part. </p>
"
"51918919","How to integrate 'npm install' into ASP.NET CORE 2.1 Docker build","<docker><npm><asp.net-core><dockerfile>","55661403","Deploy on docker a net core 2 application","<c#><reactjs><docker><docker-compose><asp.net-core-2.0>","<p>I have not found a way to build a ASP.NET Core 2.1 Docker image while doing a proper <code>npm install</code> during the build process.</p>

<p>My <code>Dockerfile</code> looks like this (one that has been generated from Visual Studio):</p>

<pre><code>FROM microsoft/dotnet:2.1-aspnetcore-runtime AS base
WORKDIR /app
EXPOSE 80

FROM microsoft/dotnet:2.1-sdk AS build
WORKDIR /src
COPY --from=frontend . .
COPY [""myProject.WebUi/myProject.WebUi.csproj"", ""myProject.WebUi/""]
COPY [""myProject.SearchIndex/myProject.SearchIndex.csproj"", ""myProject.SearchIndex/""]
COPY [""myProject.SearchIndex.Common/myProject.SearchIndex.Common.csproj"", ""myProject.SearchIndex.Common/""]

RUN dotnet restore ""myProject.WebUi/myProject.WebUi.csproj""
COPY . .
WORKDIR ""/src/myProject.WebUi""
RUN dotnet build ""myProject.WebUi.csproj"" -c Release -o /app

FROM build AS publish
RUN dotnet publish ""myProject.WebUi.csproj"" -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""myProject.WebUi.dll""]
</code></pre>

<p>In the previous images from Microsoft (e.g. aspnetcore-build:2.0) were third-party tools provided, such as npm, yarn, bower, pip, ...)</p>

<p>At the moment I do a local <code>npm install</code> in the project folder. But for automatic building like it is offered from Docker Hub or Azure Container Registry the note modules are missing.</p>
","<p>When I create a simple React and Net Core 2.2. project with command
<code>dotnet new react</code> and add files to build and deploy on Docker the application but docker fail to build.</p>

<p>My Docker Compose File contains:</p>

<pre><code>version: '3'
services:
  netreact:
    container_name: netreactapp
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - ""5000:5000""
    restart: always
</code></pre>

<p>And my Dockerfile contains:</p>

<pre><code>FROM microsoft/dotnet:sdk AS base
WORKDIR /app
FROM microsoft/dotnet:sdk AS build
WORKDIR /src
COPY *.sln ./
COPY MyReactApp/*.csproj ./MyReactApp/
RUN dotnet restore
WORKDIR /src/MyReactApp
RUN dotnet publish -c Release -o /app /property:PublishWithAspNetCoreTargetManifest=false
FROM build AS publish
RUN dotnet publish -c Release -o /app
FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""MyReactApp.dll""]
</code></pre>

<p>So when I run the command to build and up <code>docker-compose up -d</code> my docker they fail on Npm restore, with following error:</p>

<pre><code>Step 16/22 : RUN dotnet publish -c Release -o /app /property:PublishWithAspNetCoreTargetManifest=false
 ---&gt; Running in dc0532c03e90
Microsoft (R) Build Engine version 16.0.450+ga8dc7f1d34 for .NET Core
Copyright (C) Microsoft Corporation. All rights reserved.

  Restore completed in 276.74 ms for /src/MyReactApp/MyReactApp.csproj.
  StWebApp -&gt; /src/MyReactApp/bin/Release/netcoreapp2.2/MyReactApp.dll
  StWebApp -&gt; /src/MyReactApp/bin/Release/netcoreapp2.2/MyReactApp.Views.dll
  /bin/sh: 2: /tmp/tmp813c90672ba947489ab89ec17eb80722.exec.cmd: npm: not found
/src/MyReactApp/MyReactApp.csproj(48,5): error MSB3073: The command ""npm install"" exited with code 127.
ERROR: Service 'netreactapp' failed to build: The command '/bin/sh -c dotnet publish -c Release -o /app /property:PublishWithAspNetCoreTargetManifest=false' returned a non-zero code: 1
</code></pre>

<p>I think it's because the pipeline of csproj tries to build ClientApp folder using npm, There are any workaround or way to fix it?</p>
"
"51938049","Mongodb connection error inside docker container","<node.js><mongodb><docker>","55491063","How to utilize a Mongo seed container to import users to admin?","<mongodb><docker><dockerfile>","<p>I've been trying to get a basic nodeJS api to connect to a mongo container. Both services are defined in a docker-compose.yml file. I've read countless similar questions here and on docker's forum all stating that the issue is your mongo connection URI. This is not my issue as you'll see below.</p>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3.7'

services: 
  api:
    build: ./
    command: npm run start:dev
    working_dir: /usr/src/api-boiler/
    restart: always
    environment: 
      PORT: 3001
      MONGODB_URI: mongodb://mongodb:27017/TodoApp
      JWT_SECRET: asdkasd9a9sdn2r3513032
    links:
      - mongodb
    ports:
      - ""3001:3001""
    volumes:
      - ./:/usr/src/api-boiler/ 
    depends_on:
      - mongodb

  mongodb:
    image: mongo
    restart: always
    volumes:
      - /usr/local/var/mongodb:/data/db
    ports:
      - 27017:27017
</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM node:10.8.0

WORKDIR /usr/src/api-boiler

COPY ./ ./

RUN npm install

CMD [""/bin/bash""]
</code></pre>

<p><strong>db/mongoose.js</strong>
<em>Setting up mongodb connection</em></p>

<pre><code>const mongoose = require('mongoose');

mongoose.Promise = global.Promise;
mongoose.connect(
  process.env.MONGODB_URI,
  { useMongoClient: true }
);

module.exports.mongoose = mongoose;
</code></pre>

<p>But no matter what the api container cannot connect. I'm tried setting the mongo uri to 0.0.0.0:3001 but no joy. I checked the config settings used to launch mongo in the container using db.serverCmdLineOpts(). And, the command bind_ip_all has been passed so mongo should accept connections from any ip. The typical issue is people forgetting to replace localhost with their mongo container name. EG:
mongodb://localhost:27017/TodoApp >> mongodb://mongodb:27017/TodoApp</p>

<p>But, this has been done. So pretty stumped.</p>

<p><strong>Logs - for good measure</strong></p>

<pre><code>Successfully built 388868008521
Successfully tagged api-boiler_api:latest
Starting api-boiler_mongodb_1 ... done
Recreating api-boiler_api_1   ... done
Attaching to api-boiler_mongodb_1, api-boiler_api_1
mongodb_1  | 2018-08-20T20:09:27.072+0000 I CONTROL  [main]             Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --    sslDisabledProtocols 'none'
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit     host=72af162616c8
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten] db     version v4.0.1
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     git version: 54f1582fc6eb01de4d4c42f26fc133e623f065fb
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     OpenSSL version: OpenSSL 1.0.2g  1 Mar 2016
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     allocator: tcmalloc
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     modules: none
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     build environment:
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]         distmod: ubuntu1604
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]         distarch: x86_64
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]         target_arch: x86_64
mongodb_1  | 2018-08-20T20:09:27.085+0000 I CONTROL  [initandlisten]     options: { net: { bindIpAll: true } }
mongodb_1  | 2018-08-20T20:09:27.088+0000 W STORAGE  [initandlisten]     Detected unclean shutdown - /data/db/mongod.lock is not empty.
mongodb_1  | 2018-08-20T20:09:27.093+0000 I STORAGE  [initandlisten]     Detected data files in /data/db created by the 'wiredTiger' storage engine,     so setting the active storage engine to 'wiredTiger'.
mongodb_1  | 2018-08-20T20:09:27.096+0000 W STORAGE  [initandlisten]     Recovering data from the last clean checkpoint.
mongodb_1  | 2018-08-20T20:09:27.097+0000 I STORAGE  [initandlisten]     wiredtiger_open config: create,cache_size=487M,session_max=20000,eviction=    (threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=    (enabled=true,archive=true,path=journal,compressor=snappy),file_manager=    (close_idle_time=100000),statistics_log=(wait=0),verbose=    (recovery_progress),
api_1      |
api_1      | &gt; api-boiler@0.1.0 start:dev /usr/src/api-boiler
api_1      | &gt; cross-env NODE_ENV=development node server/server.js
api_1      |
api_1      | Started on port 3001
api_1      | (node:24) UnhandledPromiseRejectionWarning: MongoError:     failed to connect to server [mongodb:27017] on first connect [MongoError:     connect ECONNREFUSED 172.18.0.2:27017]
</code></pre>
","<p>Mongo seed container comes in handy when we need to pre-insert some collections before initializing a mongo container. For instance, like the solution in this question:<a href=""https://stackoverflow.com/questions/31210973/how-do-i-seed-a-mongo-database-using-docker-compose"">How do I seed a mongo database using docker-compose?</a></p>

<p>But what about importing users to admin database with the following Docker set up:</p>

<ol>
<li><p><code>docker-compose.yml</code>:</p>

<pre><code>version: '3.4'
services:
  mongodb:
    image: mongo:latest
    ports: 
    - ""27017:27017""
    environment:
    # provide your credentials here
    - MONGO_INITDB_ROOT_USERNAME=root
    - MONGO_INITDB_ROOT_PASSWORD=root
    container_name: mongodb 
  mongodb_seed:
    build: mongodb_seed
    links:
    - mongodb
</code></pre>

<p>with this setting, <code>mongodb_seed</code> can be used to insert ready data into <code>mongodb</code> right before initializing</p></li>
<li><p><code>Dockerfile</code> of <code>mongodb_seed</code>:</p>

<pre><code>FROM mongo:latest
WORKDIR /tmp
COPY users.json .
COPY insert_users.js .
COPY import.sh .
CMD [""/bin/bash"", ""-c"", ""source import.sh""]
</code></pre>

<p>with this setting, the <code>import.sh</code> can be used to execute the <code>insert_users.js</code>containing code like:</p>

<pre><code>use admin
db.createUser()
</code></pre>

<p>in this case the <code>import.sh</code> might look like:</p>

<pre><code>#!/bin/bash
mongo -u root -p root --authenticationDatabase admin insert_users.js
</code></pre></li>
</ol>

<p>But this approach got <em>connection refused</em> in the log:</p>

<pre><code>    mongodb_seed_1  | connecting to: mongodb://127.0.0.1:27017/? authSource=admin&amp;gssapiServiceName=mongodb
    mongodb_seed_1  | 2019-04-03T07:39:27.902+0000 E QUERY    [js] Error: couldn't connect to server 127.0.0.1:27017, connection attempt failed: SocketException: Error connecting to 127.0.0.1:27017 :: caused by :: Connection refused :
    mongodb_seed_1  | connect@src/mongo/shell/mongo.js:343:13
    mongodb_seed_1  | @(connect):2:6
    mongodb_seed_1  | exception: connect failed
</code></pre>

<p>How to resolve this issue?</p>
"
"52010459","Elasticsearch fails to start on AWS kubernetes cluster","<elasticsearch><kubernetes><amazon-eks>","55496878","How to deploy elasticsearch in kubernetes established by AWS EKS","<docker><elasticsearch><kubernetes><kubernetes-helm><amazon-eks>","<p>I am running my kubernetes cluster on AWS EKS which runs kubernetes 1.10. 
I am following this guide to deploy elasticsearch in my Cluster 
<a href=""https://github.com/pires/kubernetes-elasticsearch-cluster"" rel=""nofollow noreferrer"">elasticsearch Kubernetes</a></p>

<p>The first time I deployed it everything worked fine. Now, When I redeploy it gives me the following error.</p>

<pre><code>ERROR: [2] bootstrap checks failed
[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]
[2018-08-24T18:07:28,448][INFO ][o.e.n.Node               ] [es-master-6987757898-5pzz9] stopping ...
[2018-08-24T18:07:28,534][INFO ][o.e.n.Node               ] [es-master-6987757898-5pzz9] stopped
[2018-08-24T18:07:28,534][INFO ][o.e.n.Node               ] [es-master-6987757898-5pzz9] closing ...
[2018-08-24T18:07:28,555][INFO ][o.e.n.Node               ] [es-master-6987757898-5pzz9] closed
</code></pre>

<p>Here is my deployment file.</p>

<pre><code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: es-master
  labels:
    component: elasticsearch
    role: master
spec:
  replicas: 3
  template:
    metadata:
      labels:
        component: elasticsearch
        role: master
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.27.2
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true
      containers:
      - name: es-master
        image: quay.io/pires/docker-elasticsearch-kubernetes:6.3.2
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: CLUSTER_NAME
          value: myesdb
        - name: NUMBER_OF_MASTERS
          value: ""2""
        - name: NODE_MASTER
          value: ""true""
        - name: NODE_INGEST
          value: ""false""
        - name: NODE_DATA
          value: ""false""
        - name: HTTP_ENABLE
          value: ""false""
        - name: ES_JAVA_OPTS
          value: -Xms512m -Xmx512m
        - name: NETWORK_HOST
          value: ""0.0.0.0""
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        resources:
          requests:
            cpu: 0.25
          limits:
            cpu: 1
        ports:
        - containerPort: 9300
          name: transport
        livenessProbe:
          tcpSocket:
            port: transport
          initialDelaySeconds: 20
          periodSeconds: 10
        volumeMounts:
        - name: storage
          mountPath: /data
      volumes:
          - emptyDir:
              medium: """"
            name: ""storage""
</code></pre>

<p>I have seen a lot of posts talking about increasing the value but I am not sure how to do it. Any help would be appreciated. </p>
","<p>I run elasticsearch in kubernetes cluster provided by amazon eks.</p>

<p>For deploy, I use <a href=""https://github.com/elastic/helm-charts/tree/master/elasticsearch"" rel=""nofollow noreferrer"">the official helm chart</a>.</p>

<p>After deploying elasticsearch pods failed on bootstrap:</p>

<pre><code>ERROR: [1] bootstrap checks failed
[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]
</code></pre>

<p>I try to change limits.conf into a container and into an instance node. But error continues to occur.</p>

<p>How I can solve this issue and increase the number of max file descriptors for elasticsearch pod?</p>
"
"55361762","apt-get update fails with 404 in a previously working build","<docker><travis-ci><debian-jessie>","55387506","How to fix 'Failed to fetch Debian Jessie-updates' issue on Docker?","<php><docker><debian>","<p>I am running a Travis build and it fails when building the mysql:5.7.27 docker image. The Dockerfile runs <code>apt-get update</code> and then I get an error <code>W: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages  404  Not Found</code>.</p>

<p>Using curl I can see it is redirecting, but the redirect-to URL results in a 404. Has anyone seen this sort of behaviour and have a remedy? Is it basically unfixable until debian makes changes?</p>

<pre><code>➜  ms git:(develop) curl --head http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
HTTP/1.1 302 Found
Date: Tue, 26 Mar 2019 16:03:04 GMT
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Location: http://cdn-fastly.deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
Content-Type: text/html; charset=iso-8859-1

➜  ms git:(develop) curl --head http://cdn-fastly.deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages
HTTP/1.1 404 Not Found
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Content-Type: text/html; charset=iso-8859-1
Via: 1.1 varnish
Content-Length: 316
Accept-Ranges: bytes
Date: Tue, 26 Mar 2019 16:03:17 GMT
Via: 1.1 varnish
Age: 45
Connection: keep-alive
X-Served-By: cache-ams21028-AMS, cache-cdg20741-CDG
X-Cache: HIT, HIT
X-Cache-Hits: 6, 2
X-Timer: S1553616198.734091,VS0,VE0
</code></pre>
","<p>My docker was working fine. I reset the disk image of Docker and then when I run the 'docker-compose up -d' command.</p>

<p>I started to get ""Failed to fetch <a href=""http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages"" rel=""nofollow noreferrer"">http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages</a>  404  Not Found</p>

<p>E: Some index files failed to download. They have been ignored, or old ones used instead.</p>

<p>"" error.</p>

<p>My DockerFile is </p>

<pre><code>    FROM php:7.1.16-apache

   #install all the system dependencies and enable PHP modules
   RUN apt-get update &amp;&amp; apt-get install -y \
  libicu-dev \
  libpq-dev \
  libmcrypt-dev \
  git \
  zip \
  unzip \
  python \
  python-setuptools \
  libmemcached-dev \
&amp;&amp; pecl install memcached \
&amp;&amp; rm -r /var/lib/apt/lists/* \
&amp;&amp; docker-php-ext-configure pdo_mysql --with-pdo-mysql=mysqlnd \
&amp;&amp; docker-php-ext-install \
  intl \
  mbstring \
  mcrypt \
  pcntl \
  pdo_mysql \
  pdo_pgsql \
  pgsql \
  zip \
  opcache \
&amp;&amp; docker-php-ext-enable memcached \
&amp;&amp; pecl install -o -f redis \
&amp;&amp;  rm -rf /tmp/pear \
&amp;&amp;  docker-php-ext-enable redis


   RUN pecl install xdebug

   RUN easy_install supervisor

   #RUN echo_supervisord_conf &gt; /etc/supervisord.conf

   ADD supervisord.conf /etc/supervisord.conf

   ADD php.ini $PHP_INI_DIR/php.ini

   #RUN echo ""zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20151012/xdebug.so"" to php.ini

   VOLUME /var/www/html

   WORKDIR /var/www/html

   #install composer
   RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/bin/ --filename=composer

   #set our application folder as an environment variable
   ENV APP_HOME /var/www/html

   #change uid and gid of apache to docker user uid/gid
   RUN usermod -u 1000 www-data &amp;&amp; groupmod -g 1000 www-data

   #change the web_root to laravel /var/www/html/public folder
   RUN sed -i -e ""s/html/html\/public/g"" /etc/apache2/sites-enabled/000-default.conf

   # enable apache module rewrite
   RUN a2enmod rewrite

   #copy source files and run composer
   #COPY . $APP_HOME

   # install all PHP dependencies
   #RUN composer install --no-interaction

   #change ownership of our applications
   RUN chown -R www-data:www-data $APP_HOME

   ADD startup.sh /root/

   RUN chmod 755 /root/startup.sh

   CMD [""/root/startup.sh""]
</code></pre>

<p>How can I fix the issue?</p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","58487218","How can I copy a file (.txt format) from my computer to docker container","<powershell><docker><hdfs>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>I am trying to copy a .txt file from my computer into a docker container using the command below which I run in powershell: </p>

<pre><code>docker cp SelectedWords.txt  27eab29f7d03:/selectwords/ 
</code></pre>

<p>The command is run, but when I check docker, the file does not appear.</p>

<p>Any suggestions please?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","58339308","Connecting to host machine with non static ip from docker-container","<docker><networking><docker-compose><docker-swarm><docker-networking>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I want to connect to host from inside the container, given that host ip is non-static.
I don't want to use the host network. I am using docker-compose to manage the containers.     </p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","58280196","How can I include python module from an outer folder in the docker image?","<python><git><docker><docker-compose>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have 2 services in one git repository written in the python and deployed to the containers with docker-compose and docker. And I wanted to add some authorization with the third party OpenID Connect service. I have written a module to deal with authorization and I desire to use it in every service.
My repository looks like this:</p>

<pre><code>services:
    first_service:
        Dockerfile
        docker-compose.yml
        first_service_python_module
    second_service:
        Dockerfile
        docker-compose.yml
        second_service_python_module
    docker-compose.yml
    .git
</code></pre>

<p>Initially, I wanted my authorization module to be in the 'services' folder but this way I cannot build my Docker images because Docker can't deal with the outer folder without special argument.
But I cannot specify this argument because of docker-compose files.
So I ended up with the next repository structure:</p>

<pre><code>services:
    first_service:
        Dockerfile
        docker-compose.yml
        first_service_python_module
        authorization_module
    second_service:
        Dockerfile
        docker-compose.yml
        second_service_python_module
        authorization_module
    docker-compose.yml
    .git
</code></pre>

<p>Obviously there is a lot of code duplication and it's hard to maintain.</p>

<p>I want to avoid code duplication. But also not to change docker and docker-compose files a lot. How can I do it?</p>
"
"28721699","Root password inside a Docker container","<docker>","58392611","su password in tryton docker","<python-3.x><docker><tryton>","<p>I'm using a Docker image which was built using the USER command to use a non-root user called <code>dev</code>.
Inside a container, I'm ""dev"", but I want to edit the <code>/etc/hosts</code> file.</p>

<p>So I need to be root. I'm trying the su command, but I'm asked to enter the root password.</p>

<p>What's the default root user's password inside a Docker container?</p>
","<p>I'm trying to setup a tryton demo, and I want to define my own modules inside the docker containers I'm using (these => <a href=""https://hub.docker.com/r/tryton/tryton"" rel=""nofollow noreferrer"">https://hub.docker.com/r/tryton/tryton</a>).</p>

<p>I try to create a new directory in the /home directory, in order to define there my own tryton modules, but I do not have enough permission so I execute <code>su</code>. The problem is that I do not know the root password.</p>

<p>Does anyone know which is it? Or, where should I define my tryton modules inside the docker container?</p>
"
"38487598","Why can I see the docker container process when I do a ""ps aux"" on the host?","<docker>","58270303","Docker processes show via host ps -a?","<linux><docker>","<p>From the host:</p>

<pre><code>ps aux | grep java

me@my-host:~/elastic-search-group$ ps aux | grep java
smmsp    20473  106  6.3 4664740 257368 ?      Ssl  17:48   0:09 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.4.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start
</code></pre>

<p>Then exec into the container:</p>

<pre><code>docker exec -it 473 /bin/bash
</code></pre>

<p>And look at the processes:</p>

<pre><code>root@473c4548b06f:/usr/share/elasticsearch# ps aux | grep java                                                                                                               
elastic+     1 14.0  6.3 4671936 257372 ?      Ssl  17:48   0:10 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/sh
</code></pre>

<p>From the host:</p>

<pre><code>sudo kill -9 20473
</code></pre>

<p>ends up killing the docker container.</p>

<p>Now, I may be mistaken, but I thought there was complete process segregation? Is this supposed to bleed out to the host? </p>
","<p>I'm surprised to find processes spawned within a Docker container show up when running ps -a on the host.</p>

<p>My question is: is this expected behavior?</p>
"
"40801772","What is the difference between docker-compose ports vs expose","<docker><docker-compose>","58240357","docker expose wrong ports open","<docker><docker-compose><dockerfile><wildfly>","<p>What is the difference between <code>ports</code> and <code>expose</code> options in <code>docker-compose.yml</code></p>
","<p>I have a docker image with a wildfly server that binds to ports 8080, 8443, 8787 and 9990 on the 0.0.0.0 interface (for development and remote debugging). </p>

<p>The <code>Dockerfile</code> itself does not include any <code>EXPOSE</code> statements, since I want to expose ports selectively using the <code>docker-compose.yml</code>. </p>

<p>Dockerfile (shortened)</p>

<pre><code>FROM ubuntu:bionic
ARG DEBIAN_FRONTEND=noninteractive
RUN     apt-get update &amp;&amp; \
        apt-get install -y -q some,packages,here
        &amp;&amp; apt-get clean \
        &amp;&amp; apt-get autoclean \
        &amp;&amp; apt-get --purge -y autoremove \
        &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
ENV WILDFLY_VERSION 16.0.0.Final
ENV WFLY_DIR /wildfly-$WILDFLY_VERSION
ENV JBOSS_HOME $WFLY_DIR
ADD /wildfly-$WILDFLY_VERSION 
RUN groupadd -g 999 wildfly &amp;&amp; useradd -r -u 999 -g wildfly --home= --shell=/bin/bash wildfly
RUN chown -R wildfly:wildfly /wildfly-$WILDFLY_VERSION 
RUN chmod 755  $WFLY_DIR/bin/*.sh 
USER wildfly
</code></pre>

<p>The docker-compose.yml file contains essentially this:</p>

<pre><code>version: ""3.5""
services:
  appsvr:
    build: ../../images/myimage/
    command: [""/bin/bash"", ""/start.sh""]
    expose:
      - 8443
      - 8787
</code></pre>

<p>I start the container with <code>docker-compose up -d</code> and wildfly comes up as expected. Now I check which ports are exposed and get the ip of the container:</p>

<pre><code> bash% docker inspect --format="" {{ .NetworkSettings.Ports }} "" containername
 map[8443/tcp:[] 8787/tcp:[]]
 bash% docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'  containername
172.20.0.10
</code></pre>

<p>That seems correct. I can connect to 8443, but I <strong>cannot</strong> connect to 8787, futher analysis with nmap on the IP yields:</p>

<pre><code> bash% nmap -p8000-10000 172.20.0.10

Starting Nmap 6.40 ( http://nmap.org ) at 2019-10-04 16:33 UTC
Nmap scan report for ip-172-20-0-10.eu-central-1.compute.internal (172.20.0.10)
Host is up (0.00029s latency).
Not shown: 1998 closed ports
PORT     STATE SERVICE
8080/tcp open  http-proxy
8443/tcp open  https-alt
9990/tcp open  osm-appsrvr

Nmap done: 1 IP address (1 host up) scanned in 0.05 seconds

</code></pre>

<p>That is odd, since I do not expose 8080 or 9990 (I checked and it is indeed wildfly on 8080 responding to requests).  And 8787 - although exposed via docker-compose (and also open inside the container after shelling into it!) - is not. </p>

<p>My docker-engine and docker-compose versions</p>

<pre><code>bash% docker version
Client:
 Version:           18.06.1-ce
 API version:       1.38
 Go version:        go1.10.3
 Git commit:        e68fc7a215d7133c34aa18e3b72b4a21fd0c6136
 Built:             Fri Jun 28 23:16:08 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server:
 Engine:
  Version:          18.06.1-ce
  API version:      1.38 (minimum version 1.12)
  Go version:       go1.10.3
  Git commit:       e68fc7a/18.06.1-ce
  Built:            Fri Jun 28 23:17:39 2019
  OS/Arch:          linux/amd64
  Experimental:     false

bash% docker-compose version
docker-compose version 1.21.2, build a133471
docker-py version: 3.3.0
CPython version: 3.6.5
OpenSSL version: OpenSSL 1.0.1t  3 May 2016

</code></pre>

<p>Is that to be expected? I don't think it is. Or did I misunderstand something conceptually about how EXPOSE should work (I was assuming ports not exposed are not visible and vice versa?) </p>
"
"42558221","How to CORS-enable Apache web server (including preflight and custom headers)?","<apache><http><cors><ip><preflight>","58397531","How to fix: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header","<reactjs><.htaccess><docker><cors>","<p>General:</p>

<pre><code>Request URL:x/site.php
Request Method:OPTIONS
Status Code:302 Found
Remote Address:x.x.x.x:80
</code></pre>

<p>Response Headers:</p>

<pre><code>view source
Access-Control-Allow-Headers:Content-Type
Access-Control-Allow-Origin:*
Access-Control-Max-Age:300
Cache-Control:no-store, no-cache, must-revalidate, post-check=0, pre-check=0
Content-Length:0
Content-Type:text/html; charset=UTF-8
Date:Thu, 02 Mar 2017 14:27:21 GMT
Expires:Thu, 19 Nov 1981 08:52:00 GMT
Location:y
Pragma:no-cache
Server:Apache/2.4.25 (Ubuntu)
</code></pre>

<p>Request Headers:</p>

<pre><code>view source
Accept:*/*
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8
Access-Control-Request-Headers:authorization
Access-Control-Request-Method:POST
Cache-Control:no-cache
Connection:keep-alive
DNT:1
Host:x
Origin:http://127.0.0.1:3000
Pragma:no-cache
Referer:http://127.0.0.1:3000/
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36
</code></pre>

<p>Apache virtualhost config looks as so:</p>

<pre><code>    &lt;IfModule mod_headers.c&gt;
           Header set Access-Control-Allow-Origin ""http://127.0.0.1:3000""
           Header set Access-Control-Allow-Origin ""http://127.0.0.1""
           Header set Access-Control-Max-Age ""300""
           Header set Access-Control-Allow-Credentials ""true""
           Header set Access-Control-Allow-Headers ""Origin, X-Requested-With, Content-Type, Accept""
           Header set Access-Control-Allow-Methods ""POST, GET, PUT, DELETE, PATCH, OPTIONS""
    &lt;/IfModule&gt;
</code></pre>

<p>The preflight request is skipping the apache config and hitting my webapp directly, which does a redirect (hence the 302 and the location: y).</p>

<p>I don't know why the preflight request is not being handled by apache?</p>
","<p>I am trying to build a simple API with a php backend and a React JS frontend. I am using two separate docker containers for this (api.dev.de and react.dev.de, as it is a requirement. I am using a slightly adapted version of the nginx proxy. However, when I send the request per React fetch() to the server, I get the error: </p>

<blockquote>
  <p>Access to fetch at 'https: //api.dev.de/index.php?read=users' from 
    origin 'https: //react.dev.de' has been blocked by CORS policy: 
    Response to preflight request doesn't pass access control check: No 
    'Access-Control-Allow-Origin' header is present on the requested 
    resource. If an opaque response serves your needs, set the request's 
    mode to 'no-cors' to fetch the resource with CORS disabled. </p>
</blockquote>

<p>Below you can see my request that I am trying to make and the .htaccess file.</p>

<p>I already looked up some solutions that worked for others, including <a href=""https://stackoverflow.com/questions/13421463/htaccess-access-control-allow-origin#answer-20453033"">this</a>
one that I also wrote into my Dockerfile (and first enabling the extension for this image in my docker-compose.yml).</p>

<p>Furthermore I dug through 
<a href=""https://stackoverflow.com/questions/10636611/how-does-access-control-allow-origin-header-work#answer-10636765"">this</a>
answer and navigated through those links to other articles to build up some knowledge, but still no success...</p>

<p>Here are my code snippets.</p>

<p>Persons.js:</p>

<pre><code>...

fetch('https://api.dev.de/index.php?read=users', {
      method: 'POST',
      headers: {
        'Authorization': 'Basic ' + btoa(""&lt;secretName&gt;:&lt;secretPass&gt;""),
        'Accept': 'application/json',
        'Content-Type': 'application/json',
      }
    })

...
</code></pre>

<p>.htaccess:</p>

<pre><code>Authtype Basic
AuthName ""Protected section. Only for developers.""
AuthUserFile /var/www/html/App/.htpasswd
Require valid-user

Header set Access-Control-Allow-Origin ""*""
Header set Access-Control-Allow-Credentials ""true""
</code></pre>

<p>The config in the Dockerfile:</p>

<pre><code>FROM thecodingmachine/php:7.3-v2-apache

...

RUN a2enmod headers

...
</code></pre>

<p>When executing the function, I get those console logs:</p>

<blockquote>
  <p>Access to fetch at 'https:// api.dev.de/index.php?read=users' from 
      origin 'https:// react.dev.de' has been blocked by CORS policy: 
      Response to preflight request doesn't pass access control check: No 
      'Access-Control-Allow-Origin' header is present on the requested 
      resource. If an opaque response serves your needs, set the 
      request's mode to 'no-cors' to fetch the resource with CORS 
      disabled.</p>
  
  <p>VM2133:1 POST <a href=""https://api.dev.de/index.php?read=users"" rel=""nofollow noreferrer"">https://api.dev.de/index.php?read=users</a> 
      net::ERR_FAILED</p>
</blockquote>

<p>However, when I delete all authentication config in the .htaccess file as well deleting the <code>Authorization</code> and <code>Content-Type</code> section from the Persons.js file, I get a valid response. But I can't just exclude my authorization from the page.</p>

<p>When I build the React App and paste it in the same docker container as the API and then call it, everything is working fine. So I assume it is the config of the docker container (correct me if I am wrong).</p>

<h1>Update:</h1>

<p>Since yesterday I tried out different things and came up with one last problem.</p>

<p>My fetch() function now looks like the following:</p>

<pre><code>fetch('https://api.dev.de/index.php?read=users&amp;pass=&lt;password&gt;', {
      method: 'GET',
      headers: {
        'Content-Type': 'multipart/form-data',
        'Accept': 'application/json'
      },
    })
</code></pre>

<p>I also changed my .htaccess file:</p>

<pre><code># Enabled in the Dockerfile but still checking if it is enabled.
&lt;IfModule mod_headers.c&gt;
    Header set Access-Control-Allow-Origin ""*""
    Header set Access-Control-Content-Type ""*""
    Header set Access-Control-Accept ""*""
    # This should enable the authentication header
    Header set Access-Control-Allow-Credentials ""true""
&lt;/IfModule&gt;
</code></pre>

<p>Now the request works, but when I change the fetch() function to send an authorization header (<code>'Authorization': 'Basic: ' + btoa('&lt;username&gt;:&lt;password&gt;')</code>) and the .htaccess to this:</p>

<pre><code>Authtype Basic
AuthName ""Protected section. Only for developers.""
AuthUserFile /var/www/html/App/.htpasswd
Require valid-user

&lt;IfModule mod_headers.c&gt;
    Header set Access-Control-Allow-Origin ""*""
    Header set Access-Control-Content-Type ""*""
    Header set Access-Control-Accept ""*""
    # This should enable the authentication header
    Header set Access-Control-Allow-Credentials ""true""
&lt;/IfModule&gt;
</code></pre>

<p>I still get the error:</p>

<blockquote>
  <p>Access to fetch at '<a href=""https://api.dev.de/index.php?read=users&amp;pass=crud_restAPI_call"" rel=""nofollow noreferrer"">https://api.dev.de/index.php?read=users&amp;pass=crud_restAPI_call</a>' from origin '<a href=""https://react.dev.de"" rel=""nofollow noreferrer"">https://react.dev.de</a>' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.</p>
</blockquote>

<p><s> Is this because the order of my .htaccess or do I need to modify something else?</s></p>

<h1>Update 2:</h1>

<p>As of my research, I found this answer to a similar issue: ""The preflight request (OPTIONS), which is where i encounter the 401 unauthorized. I think this is because I've read that OPTIONS strips out some headers, including the Authentication header, so without that, it can't authenticate"".</p>

<p>Source: <a href=""https://github.com/axios/axios/issues/2076"" rel=""nofollow noreferrer"">https://github.com/axios/axios/issues/2076</a></p>

<p>Checking the developer.mozilla.org guide (<a href=""https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Sending_a_request_with_credentials_included"" rel=""nofollow noreferrer"">https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Sending_a_request_with_credentials_included</a>) I wanted to send the credentials always (to get the preflight request to succeed).</p>

<p>However, this does not work...</p>

<p>Has somebody an idea why it doesn't?</p>

<p>Updated fetch() function:</p>

<pre><code>fetch('https://api.dev.de/index.php?read=users&amp;pass=&lt;password&gt;', {
      method: 'POST',
      credentials: 'include',
      headers: {
        'Authorization': 'Basic: ' + btoa('&lt;secretName&gt;:&lt;secretPass&gt;'),
        'Content-Type': 'multipart/form-data',
        'Accept': 'application/json'
      },
    })
</code></pre>

<p>I still get the same error:</p>

<blockquote>
  <p>Access to fetch at '<a href=""https://api.dev.de/index.php?read=users&amp;pass=crud_restAPI_call"" rel=""nofollow noreferrer"">https://api.dev.de/index.php?read=users&amp;pass=crud_restAPI_call</a>' from origin '<a href=""https://react.dev.de"" rel=""nofollow noreferrer"">https://react.dev.de</a>' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.</p>
</blockquote>
"
"42683478","TypeError: the JSON object must be str, not 'bytes'","<python><json>","58271989","BrokenFilesystemWarning in Werkzeug","<python><docker><flask><encoding><utf-8>","<p>I have the following, very basic code that throws; <code>TypeError: the JSON object must be str, not 'bytes'</code></p>

<pre><code>import requests
import json

url = 'my url'
user = 'my user'
pwd = 'my password'

response = requests.get(url, auth=(user, pwd))

if(myResponse.ok):
    Data = json.loads(myResponse.content)
</code></pre>

<p>I try to set decode to the Data variable, as follows but it throws the same error;  <code>jData = json.loads(myResponse.content).decode('utf-8')</code></p>

<p>Any suggestions? </p>
","<p>When I run my program locally, everything goes successfully, but when I run it on the docker I get: </p>

<pre><code>resp = api_instance.import(rt_import=jsonpickle.decode(file.read()))

TypeError: the JSON object must be str, not 'bytes'
</code></pre>

<p>which most likely results from <code>BrokenFilesystemWarning</code>.:</p>

<pre><code>/usr/local/lib/python3.5/dist-packages/werkzeug/filesystem.py:63: 
BrokenFilesystemWarning: Detected a misconfigured UNIX filesystem: Will 
use UTF-8 as filesystem encoding instead of 'ascii' 
BrokenFilesystemWarning)
</code></pre>

<p>Am I able to fix it somehow or do I have to rummage on the server side?</p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","58543011","Django Docker app container don't open on browser after run","<python><django><docker><dockerfile>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I dockerize my django app, this is my Dockerfile:</p>
<pre><code>FROM python:3.6-alpine
EXPOSE 8000
RUN apk add --no-cache make linux-headers libffi-dev jpeg-dev zlib-dev
RUN apk update &amp;&amp; apk add postgresql-dev gcc python3-dev musl-dev
RUN mkdir /Code
WORKDIR /Code
COPY ./requirements.txt .

RUN pip install --upgrade pip
RUN pip install -r requirements.txt
ENV PYTHONUNBUFFERED 1

COPY . /Code/

ENTRYPOINT python /Code/core/manage.py runserver 0.0.0.0:8000
</code></pre>
<p>well, i build my image and I run it:</p>
<pre><code>docker run -p 8000:8000 --link postgres:postgres cath11/test_app
</code></pre>
<p>all seems to be done:</p>
<p><a href=""https://i.stack.imgur.com/2tD7q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2tD7q.png"" alt=""enter image description here"" /></a></p>
<p>but when I open my browser on <a href=""http://0.0.0.0:8000/"" rel=""nofollow noreferrer"">http://0.0.0.0:8000/</a> or <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a> my app does not open</p>
<p><a href=""https://i.stack.imgur.com/vW0GL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vW0GL.png"" alt=""enter image description here"" /></a></p>
<p>Why my django app seems running on my container, I expose port but I cannot see it in my hosted browser?</p>
<p>So many thanks in advance</p>
"
"44097266","add yaml extension to php on using official Alpine Docker image","<php><docker><dockerfile><alpine><phpize>","58560148","Docker php:fpm-alpine : How to instal yaml php extention?","<php><docker><docker-compose><yaml><pecl>","<p>I'm using this offical php Docker image: <a href=""https://github.com/docker-library/php/blob/76a1c5ca161f1ed6aafb2c2d26f83ec17360bc68/7.1/alpine/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/docker-library/php/blob/76a1c5ca161f1ed6aafb2c2d26f83ec17360bc68/7.1/alpine/Dockerfile</a></p>

<p>Now I need to add support for yaml extension, that is not bundled with php.
I see the base image I'm using uses phpize.</p>

<p>I'm trying with this approach:</p>

<pre><code>FROM php:7.1.5-alpine

# Install and enable yaml extension support to php
RUN apk add --update yaml yaml-dev
RUN pecl channel-update pecl.php.net  
RUN pecl install yaml-2.0.0 &amp;&amp; docker-php-ext-enable yaml
</code></pre>

<p>But I get this errors:</p>

<pre><code>running: phpize
Configuring for:
PHP Api Version:         20160303
Zend Module Api No:      20160303
Zend Extension Api No:   320160303
Cannot find autoconf. Please check your autoconf installation and the
$PHP_AUTOCONF environment variable. Then, rerun this script.

ERROR: `phpize' failed
ERROR: Service 'php_env' failed to build: The command '/bin/sh -c pecl  install yaml-2.0.0 &amp;&amp; docker-php-ext-enable yaml' returned a non-zero code: 1
</code></pre>

<p>What is the most idiomatic docker way to use that image and add that support?</p>

<p>Should I use it as base, or is someway possible to add parameters in order to make wanted extension configurable?</p>
","<p>I'm trying to install php yaml extension.</p>

<p>My Dockerfile :</p>

<pre><code>FROM php:fpm-alpine


# gd
RUN apk add --update --no-cache \
      freetype-dev \
      libjpeg-turbo-dev \
      libpng-dev \
    &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \
    &amp;&amp; docker-php-ext-install -j""$(getconf _NPROCESSORS_ONLN)"" gd


# imagick
RUN apk add --update --no-cache autoconf g++ imagemagick-dev libtool make pcre-dev \
    &amp;&amp; pecl install imagick \
    &amp;&amp; docker-php-ext-enable imagick \
    &amp;&amp; apk del autoconf g++ libtool make pcre-dev


# Yaml
RUN apk add --no-cache \
      yaml-dev
RUN pecl install yaml
# I've trying with yaml-2.0.4 too...
RUN docker-php-ext-enable yaml

# Copy php conf files
COPY php-fpm.conf /usr/local/etc/php-fpm.d/www-user.conf
COPY php.ini /usr/local/etc/php/conf.d/50-setting.ini
</code></pre>

<p>Error :</p>

<pre><code>ERROR: Service 'php' failed to build: The command '/bin/sh -c pecl install yaml' returned a non-zero code: 1
</code></pre>

<p>Why it's so complicated to install a basic php extension in 2019 ?
It's normal that <strong>no documentation</strong> explain that ?</p>

<p>EDIT: No it's not same as ""<a href=""https://stackoverflow.com/questions/44097266/add-yaml-extension-to-php-on-using-official-alpine-docker-image"">add yaml extension to php on using official Alpine Docker image</a>"". I known how to search and I've found this answer before post my question but this solution doesn't work for me (and also answer is outdated...).</p>

<p>EDIT 2 : I found. So like David Maze said I have to install dependencies with yaml (of course it's not possible to automaticly install them, 2019, great job everyone !). So now I install imagick in same time as yaml :</p>

<pre><code>FROM php:fpm-alpine


# gd
RUN apk add --update --no-cache \
      freetype-dev \
      libjpeg-turbo-dev \
      libpng-dev \
    &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \
    &amp;&amp; docker-php-ext-install -j""$(getconf _NPROCESSORS_ONLN)"" gd


# imagick &amp; yaml
RUN apk add --update --no-cache autoconf g++ imagemagick-dev libtool make pcre-dev yaml-dev \
    &amp;&amp; pecl install imagick \
    &amp;&amp; docker-php-ext-enable imagick \
    &amp;&amp; pecl install yaml \
    &amp;&amp; docker-php-ext-enable yaml \
    &amp;&amp; apk del autoconf g++ libtool make pcre-dev


# Copy php conf files
COPY php-fpm.conf /usr/local/etc/php-fpm.d/www-user.conf
COPY php.ini /usr/local/etc/php/conf.d/50-setting.ini
</code></pre>
"
"46057625","Externalising Spring Boot properties when deploying to Docker","<docker><spring-boot><dockerfile><spring-boot-configuration>","58315604","Application.properties of Docker containerized spring boot application","<spring-boot><docker>","<p>In my Spring Boot app I want to externalise the properties to run in a Docker container.  When first deployed, the properties that are currently in <code>my-server/src/main/resources/application.yml</code> are loaded and used by the application as expected.  All works fine.</p>

<p>However, my problem is that I need these properties to be updatable as needed, so I need access to the <code>application.yml</code> file once on the Docker container.  But at this point, it's not included in the <code>build/docker/</code> directory before running the <code>buildDocker</code> task, so won't be copied over or accessible after first deployment.</p>

<p>So, what I have tried is to copy the Yaml file into the <code>docker/</code> build directory, copy it to an accessible directory (<code>/opt/meanwhileinhell/myapp/conf</code>), and use the <code>spring.config.location</code> property to pass a location of the config to the Jar in my Dockerfile:</p>

<pre><code>ENTRYPOINT  [""java"",\
...
""-jar"", ""/app.jar"",\
""--spring.config.location=classpath:${configDirectory}""]
</code></pre>

<p>Looking at the Command running on the Docker container I can see that this is as expected:</p>

<pre><code>/app.jar --spring.config.location=classpath:/opt/meanwhileinhell/myapp/conf]
</code></pre>

<p>However, when I update a property in this file and restart the Docker container, it isn't picking up the changes.  File permissions are:</p>

<pre><code>-rw-r--r-- 1 root root  618 Sep  5 13:59 application.yml
</code></pre>

<p>The <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html"" rel=""noreferrer"">documentation</a> states: </p>

<blockquote>
  <p>When custom config locations are configured, they are used in addition
  to the default locations. Custom locations are searched before the
  default locations.</p>
</blockquote>

<p>I can't seem to figure out what I'm doing wrong or misinterpreting, but probably more importantly, is this the correct way to externalise the config for this type of Docker scenario?</p>
","<p>How can we provide <code>application.properties</code> to spring boot app in a docker? Would normally copying the <code>application.properties</code> to the container work?</p>
"
"47900844","Reached error page: about:neterror when trying to navigate to other tabs if there is a form submit under that tab","<selenium><firefox><selenium-webdriver><webdriver><geckodriver>","58340812","Run StaticLiveServerTestCase on local pc and selenium in docker","<django><selenium><docker><docker-compose>","<p>when I use Selenium to do automation testing, I hit an issue, here are all scenarios:</p>

<p>There are several tabs on top of the page, now that I want to click those tabs and fill up all forms under those tabs, but if I submit <code>formA</code> which under <code>tabA</code>, then I can not navigate to other tabs automatically. If I didn't submit the form data, the issue will not be happened. Here is the log:</p>

<pre><code>1513753361368 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361388 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""pagehide"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""unload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361427 Marionette DEBUG Received DOM event ""DOMContentLoaded"" for ""about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82""
org.openqa.selenium.WebDriverException: Reached error page: about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82
Build info: version: '3.8.1', revision: '6e95a6684b', time: '2017-12-01T18:33:54.468Z'
System info: host: 'PC-20161127KZEG', ip: '192.168.131.1', os.name: 'Windows 7', os.arch: 'amd64', os.version: '6.1', java.version: '1.8.0_111'
Driver info: org.openqa.selenium.firefox.FirefoxDriver
Capabilities {acceptInsecureCerts: true, browserName: firefox, browserVersion: 57.0.2, javascriptEnabled: true, moz:accessibilityChecks: false, moz:headless: false, moz:processID: 42248, moz:profile: C:\Users\Administrator\AppD..., moz:webdriverClick: false, pageLoadStrategy: normal, platform: XP, platformName: XP, platformVersion: 6.1, rotatable: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}}
</code></pre>
","<p>For development I want to run my tests locally and selenium using selenium hub.</p>

<pre><code>  hub:
    image: selenium/hub
    environment:
      GRID_BROWSER_TIMEOUT: '300'
    ports:
      - ""4444:4444""
  firefox_node_1:
    image: selenium/node-firefox-debug
    depends_on:
      - hub
    environment:
      HUB_HOST: hub
    ports:
      - ""5900:5900""
</code></pre>

<p>In my test I use:</p>

<pre><code>class SimpleTestCase(StaticLiveServerTestCase):

    def setUp(self):
        super(SimpleTestCase, self).setUp()

        self.browser = webdriver.Remote(command_executor=f'http://localhost:4444/wd/hub',
                                        desired_capabilities=DesiredCapabilities.FIREFOX)


    def tearDown(self):
        self.browser.quit()
        super(SimpleTestCase, self).tearDown()

    def test_mainpage(self):
        self.browser.get(self.live_server_url)
</code></pre>

<p>But in this case I get network error because <code>firefox</code> node do not see my <code>localhost</code>pc</p>

<p>I tried to set host to <code>0.0.0.0</code></p>

<pre><code>class SimpleTestCase(StaticLiveServerTestCase):
    host = '0.0.0.0'
</code></pre>

<p>And of course container do not see my pc, is there a way to do this without dockerize my Django app</p>
"
"48467450","Cannot delete docker image","<docker>","58250324","How to remove a docker image that cannot be found by the CLI?","<docker><docker-image>","<p>While setting up a new build machine (CentOS 7, Docker CE 17.12.0-ce), I did a simple test:</p>

<pre><code>docker run -it --rm ubuntu bash
</code></pre>

<p>Which worked fine, but now I can't remove the Ubuntu image.</p>

<pre><code>[build ~]$ docker image rm ubuntu
Error: No such image: ubuntu
[build ~]$ docker image rm ubuntu:latest
Error: No such image: ubuntu:latest
[build ~]$ docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
ubuntu              latest              0458a4468cbc        24 hours ago        112MB
[build ~]$ docker rmi 0458a4468cbc
Error: No such image: 0458a4468cbc
</code></pre>

<p>It's not being used by any containers (not that that's the error anyway):</p>

<pre><code>[build ~]$ docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
[build ~]$
</code></pre>

<p>I even tried the nuke-from-orbit approach:</p>

<pre><code>[build ~]$ docker rmi $(docker images -q)
Error: No such image: 0458a4468cbc
</code></pre>

<p>And restarting the docker daemon:</p>

<pre><code>[build ~]$ sudo systemctl restart docker
[build ~]$ docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
ubuntu              latest              0458a4468cbc        24 hours ago        112MB
[build ~]$ docker rmi 0458a4468cbc
Error: No such image: 0458a4468cbc
</code></pre>

<p>What gives? Not that it really matters that the ubuntu image is there, I just don't understand why I can't clean it up.</p>

<p><strong>Update</strong></p>

<p>I tried pulling the image again, and, well:</p>

<pre><code>docker pull ubuntu
Using default tag: latest
latest: Pulling from library/ubuntu
Digest: sha256:e27e9d7f7f28d67aa9e2d7540bdc2b33254b452ee8e60f388875e5b7d9b2b696
Status: Downloaded newer image for ubuntu:latest
[build ~]$ docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
ubuntu              latest              0458a4468cbc        25 hours ago        112MB
ubuntu              latest              0458a4468cbc        25 hours ago        112MB
[build ~]$ docker image rm ubuntu
Untagged: ubuntu:latest
[build ~]$ docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
ubuntu              latest              0458a4468cbc        25 hours ago        112MB
[build ~]$
</code></pre>

<p><strong>Update 2</strong></p>

<p>Tried the same basic steps with a brand new CentOS 7 1708 install and had no problems at all, so safe to say this is just a weird one-off corruption of something on this machine. </p>

<p>Question still is, how do I clean it up? Is there something on the filesystem I can just <code>rm -rf</code> and let docker start fresh?</p>
","<p>I am starting with <code>docker</code> and there is a docker image I cannot remove. What command should I use to purge the list of locally installed images?</p>

<p>What I tried:</p>

<pre class=""lang-sh prettyprint-override""><code>$ docker image ls

REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE
dpage/pgadmin4               latest              8efd3d498978        5 months ago        245MB
</code></pre>

<p>But then removing the image does not work:</p>

<pre class=""lang-sh prettyprint-override""><code>$ docker image rm dpage/pgadmin4

Error: No such image: dpage/pgadmin4
</code></pre>

<p>I also tried with the ID:</p>

<pre class=""lang-sh prettyprint-override""><code>$ docker image rm 8efd3d498978

Error: No such image: 8efd3d498978
</code></pre>

<p>And tried to prune but no luck:</p>

<pre class=""lang-sh prettyprint-override""><code>$ docker system prune -af

Total reclaimed space: 0B
</code></pre>
"
"49610908","Exporting a PostgreSQL query to a csv file using Python","<python><sql><postgresql><export-to-csv><psycopg2>","58486367","How to execute copy command from Psycopg2 on remote database connection?","<python><postgresql><docker><psycopg2><psql>","<p>I need to export some rows from a table in a PostgreSQL database to a .csv file using a Python script:</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import sys, psycopg2

...

    conn = psycopg2.connect(""dbname=dbname user=user password=password"")
    cur = conn.cursor()

    sql = ""\copy (SELECT * FROM table WHERE month=6) TO '/mnt/results/month/table.csv' WITH CSV DELIMITER ';';""
    cur.execute(sql)
    cur.close()

...
</code></pre>

<p>But when I run the script I get this:</p>

<pre><code>Syntax error at or near «\»
LINE 1: \copy (SELECT * FROM TABLE WHERE month=6) TO '...
</code></pre>

<p>Does anyone know what can be wrong or give me a tip about?</p>
","<p>I'm running 2 <strong>docker</strong> containers.</p>

<ol>
<li><strong>Flask</strong> app - which has file volume attached.</li>
<li><strong>Postgres</strong> database which has a database volume attached.</li>
</ol>

<p>I'm connecting to the Postgres container using <strong>Psycopg2</strong> from Flask app.
And I have a CSV file that I want to copy to the Postgres table. This <strong>CSV</strong> file is present in the volume attached to flask app. and the postgres container has no connection to this volume.</p>

<p>When I create a connection using <code>psycopg2</code> and create a <code>cursor</code> and execute the <code>copy</code> command on that table. It is trying to copy from the folder that I have in Postgres container.</p>

<p>I tried <code>\copy</code> instead of <code>copy</code> for copying the data from the client container. But I'm getting <strong>Invalid syntax error</strong>. I even tried <code>\\copy</code>. That didn't work either.</p>

<blockquote>
  <p>Unable to process files due to exception: syntax error at or near ""\""
  LINE 1: \copy account(username,password,email) from
  '/tmp/redshift11... </p>
</blockquote>
"
"52601404","AWS SES Error: x509: certificate signed by unknown authority","<amazon-web-services><go><amazon-ses>","58128807","How to use IAM of the host machines inside docker container","<amazon-web-services><docker><go>","<p>I've been banging my head against the table with this one for a while now. I'm. I've successfully sent emails locally using an AWS access key and secret that has full access. Once I deploy to my staging environment I get an error using the same access key and secret.</p>

<p><code>RequestError: send request failed\ncaused by: Post https://email.us-east-1.amazonaws.com/: x509: certificate signed by unknown authority</code></p>

<p>Please help!</p>
","<p>I'm setting up a go server in docker on a ec2-machine this server uses the aws-sdk-go module to make calls to AWS api. This binary built from the following code gives certificate issues when running inside container. </p>

<p>The same binary runs fine and gives the expected result when running on host machine.</p>

<pre class=""lang-golang prettyprint-override""><code>package main

import (
    ""fmt""

    ""github.com/aws/aws-sdk-go/aws""
    ""github.com/aws/aws-sdk-go/aws/awserr""
    ""github.com/aws/aws-sdk-go/aws/session""
    ""github.com/aws/aws-sdk-go/service/autoscaling""
)

func main() {
    svc := autoscaling.New(session.New(), aws.NewConfig().WithRegion(""us-east-1""))
    input := &amp;autoscaling.DescribeAutoScalingGroupsInput{
        AutoScalingGroupNames: []*string{
            aws.String(""spark-worker-asg""),
        },
    }

    result, err := svc.DescribeAutoScalingGroups(input)
    if err != nil {
        if aerr, ok := err.(awserr.Error); ok {
            switch aerr.Code() {
            case autoscaling.ErrCodeInvalidNextToken:
                fmt.Println(autoscaling.ErrCodeInvalidNextToken, aerr.Error())
            case autoscaling.ErrCodeResourceContentionFault:
                fmt.Println(autoscaling.ErrCodeResourceContentionFault, aerr.Error())
            default:
                fmt.Println(aerr.Error())
            }
        } else {
            // Print the error, cast err to awserr.Error to get the Code and
            // Message from an error.
            fmt.Println(err.Error())
        }
        return
    }

    fmt.Println(result)
}
</code></pre>

<p>Error logs:</p>

<pre><code>RequestError: send request failed
caused by: Post https://autoscaling.us-east-1.amazonaws.com/: x509: certificate signed by unknown authority
</code></pre>
"
"56521549","""Failed to load HostKeys"" warning while connecting to SFTP server with pysftp","<python><python-3.x><ssh><paramiko><pysftp>","58334119","SFTP key error in Python when run in docker container","<python><docker><sftp><pysftp>","<p>I wrote a Python script to connect to SFTP server using key authentication. It connects to server successfully but shows the following warning (see below). What does it mean and how to remove it. What changes has to made in code?</p>

<p>My code:</p>

<pre><code>import os
import pysftp
import socket
import paramiko
import time
import os.path
import shutil

IP = ""127.0.X.X""
myUsername = ""USERNAME""
port = 22

cnopts = pysftp.CnOpts()
cnopts.hostkeys = None

import os
privatekeyfile = os.path.expanduser(""C:\\Users\\Rohan\\.ssh\\cool.prv"")
mykey = paramiko.RSAKey.from_private_key_file(privatekeyfile)

try:
    with pysftp.Connection(host=IP, username=myUsername,private_key=mykey,cnopts=cnopts) as sftp:
        try:
            r=str(socket.gethostbyaddr(IP))
            print(""connection successful with ""+r)

        except socket.herror:
            print(""Unknown host"")
except:
    print(""connection failed"")
</code></pre>

<p>Warning:</p>

<pre class=""lang-none prettyprint-override""><code>UserWarning: Failed to load HostKeys from C:\Users\Rohan\.ssh\known_hosts.  You will need to explicitly load HostKeys (cnopts.hostkeys.load(filename)) or disableHostKey checking (cnopts.hostkeys = None).
  warnings.warn(wmsg, UserWarning)
</code></pre>
","<p>I made a python code that connects with a SFTP server and reads data. The python script worked well when I ran it on my machine.</p>

<p>For deployment purpose I needed a docker image. So I made one.</p>

<p>However when I run the docker container I get the following Error:</p>

<p><a href=""https://i.stack.imgur.com/PKMe2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKMe2.png"" alt=""enter image description here""></a></p>

<p>Dockerfile code:</p>

<pre><code>FROM python:3
ADD sftp.py /
ADD config.properties /

RUN pip install pandas pysftp requests configparser

CMD [""python"",""./sftp.py""]
</code></pre>

<p>Python code for connecting to sftp:</p>

<pre><code>cnopts = pysftp.CnOpts(knownhosts=None)
cnopts.hostkeys = None
with pysftp.Connection(host=hostname,username=username,password=password,cnopts=cnopts) as sftp:
    sftp.chdir(""Optimove_Output"")
</code></pre>

<p>I am unable to figure out why it is not working within a docker container. As this code works when run without a container.</p>
"
"2754361","Using WPF controls in a background or ASP.Net environment","<asp.net><wpf>","47270783","DLL Not found C:\Windows\Microsoft.Net\Framework64\v4.0.30319\WPF\wpfgfx_v0400.dll","<c#><wpf><docker>","<p>I have noticed that some WPF controls have some decent effects available to them (drop shadow, reflection etc), and was wondering if it was possible to use these WPF controls solely for their available effects?</p>

<p>For example, I have an image manipulation library that resizes and letterboxes disparate sized images but I would like to add drop shadow effects to the resulting images.  The WPF image control has this effect available, but how easy is it to use in an environment where there will never be a GUI (console app or ASP.Net library/handler for example).</p>

<p>Thoughts?</p>

<p>Cheers</p>

<p>Moo</p>
","<p>I am hosting a WPF app inside of a Docker container. Im doing this to render images server side and pass them back to the client. When I try and use the app it tells me </p>

<pre><code>**[ModuleLoadException] The C++ module failed to load during appdomain initialization.**

   at &lt;CrtImplementationDetails&gt;.ThrowModuleLoadException(String errorMessage, Exception innerException)
   at &lt;CrtImplementationDetails&gt;.LanguageSupport.Initialize(LanguageSupport* )
   at .cctor()


**[DllNotFoundException] C:\Windows\Microsoft.NET\Framework64\v4.0.30319\WPF\wpfgfx_v0400.dll**

at MS.Internal.NativeWPFDLLLoader.LoadNativeWPFDLL(UInt16* relDllPath, UInt16* baseDllPath)
   at MS.Internal.NativeWPFDLLLoader.LoadCommonDLLsAndDwrite()
   at CModuleInitialize.{ctor}(CModuleInitialize* , IntPtr cleaningUpFunc)
   at ?A0x9df993ac.CreateCModuleInitialize()
   at ?A0x9df993ac.??__E?A0x9df993ac@cmiStartupRunner@@YMXXZ()
   at _initterm_m((fnptr)* pfbegin, (fnptr)* pfend)
   at &lt;CrtImplementationDetails&gt;.LanguageSupport.InitializePerAppDomain(LanguageSupport* )
   at &lt;CrtImplementationDetails&gt;.LanguageSupport._Initialize(LanguageSupport* )
   at &lt;CrtImplementationDetails&gt;.LanguageSupport.Initialize(LanguageSupport* )

**[Win32Exception] The specified module could not be found**
</code></pre>

<p>The module its looking for is inside of the container 
<a href=""https://i.stack.imgur.com/gXYYS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gXYYS.png"" alt=""Here is the Dll in its expected folder""></a></p>

<p>Does anyone have any idea whats going on ?</p>

<p>Here is the docker file:</p>

<pre><code>FROM microsoft/aspnet


RUN mkdir C:\WcfService

COPY . /app

RUN /vcredist_x86.exe /install /quiet /norestart /log 
%TEMP%\vcredist_2013_x86.log



RUN /vcredist_x64.exe /install /quiet /norestart /log 
%TEMP%\vcredist_2013_x64.log

RUN powershell.exe -executionpolicy bypass c:\app\SetupScript.ps1

EXPOSE 80
</code></pre>

<p>Here is the setup script.ps1</p>

<pre><code>Import-Module ""WebAdministration""
Add-WindowsFeature NET-WCF-TCP-Activation45
Add-WindowsFeature NET-WCF-HTTP-Activation45
Add-WindowsFeature Web-WebSockets
Add-WindowsFeature Web-Server
Add-WindowsFeature -Name NET-Framework-Core
$sharepath = ""C:\Windows\Microsoft.NET\Framework64\v4.0.30319\Temporary ASP.NET Files""
$Acl = Get-ACL $sharepath
$AccessRule= New-Object System.Security.AccessControl.FileSystemAccessRule(""everyone"",""full"",""ContainerInherit,Objectinherit"",""none"",""Allow"")
$Acl.AddAccessRule($AccessRule)
Add-LocalGroupMember -Group 'Administrators' -Member 'IIS AppPool\DefaultAppPool'; 
net localgroup administrators ""IIS AppPool\DefaultAppPool"" /add
Set-Acl $SharePath $Acl 
Remove-WebSite -Name 'Default Web Site'
New-Website -Name 'CentralServices' -Port 80 -PhysicalPath 'c:\app'
Start-WebSite -Name ""CentralServices""
</code></pre>

<p>Docker version 17.09.</p>
"
"21659637","How to fix 'sudo: no tty present and no askpass program specified' error?","<linux><sudo><tty>","47511077","unable to sudo in docker image creation","<ubuntu><docker><ubuntu-16.04><sudo>","<p>I am trying to compile some sources using a makefile. In the makefile there is a bunch of commands that need to be ran as <code>sudo</code>. </p>

<p>When I compile the sources from a terminal all goes fine and the make is paused the first time a <code>sudo</code> command is ran waiting for password. Once I type in the password, make resumes and completes.</p>

<p>But I would like to be able to compile the sources in NetBeans. So, I started a project and showed netbeans where to find the sources, but when I compile the project it gives the error:</p>

<pre><code>sudo: no tty present and no askpass program specified
</code></pre>

<p>The first time it hits a <code>sudo</code> command.</p>

<p>I have looked up the issue on the internet and all the solutions I found point to one thing: disabling the password for this user. Since the user in question here is root. I do not want to do that.</p>

<p>Is there any other solution?</p>
","<p>I am trying to create a docker image from *Ubuntu 16.04 to run some python stuff. In the script I create a user with following command:</p>

<pre><code>userad --create-home --gid sudo --shell /bin/bash testUser;\
echo 'testUser:superSecret007' | chpasswd;
</code></pre>

<p>This works fine. Next, I try to use this newly minted user account:</p>

<pre><code>USER testUser
WORKDIR /home/testUser
</code></pre>

<p>Then when it this this line:</p>

<pre><code>RUN sudo apt-get install -y libxml2;\
sudo pip install -r /home/testUser/requirements.txt;
</code></pre>

<p>It fails with following error:</p>

<pre><code>Error : sudo: no tty present and no askpass program specified
</code></pre>

<p>Note that <code>pip</code> is installed before creating user and also following runs successfully:</p>

<pre><code>sudo pip install --upgrade pip;
</code></pre>
"
"22800624","Will a docker container auto sync time with its host machine?","<time><timezone><clock><docker><ntp>","47526493","Docker VM time is not the host OS time","<docker>","<p>Do I need a NTP server inside a docker container to periodically sync the time or will the container re-sync time with its host machine? The docker container time zone is correctly set.</p>
","<p>I've followed installation docs in <a href=""http://docs.drone.io/installation/"" rel=""nofollow noreferrer"">http://docs.drone.io/installation/</a></p>

<p>Below is my <code>docker-compose.yml</code> file</p>

<pre><code>version: '2'
services:
 drone-server:
  image: drone/drone:0.8

 ports:
  - 80:8000
  - 9000
 volumes:
  - /var/lib/drone:/var/lib/drone/
 restart: always
 environment:
  - DRONE_OPEN=true
  - DRONE_HOST= localhost
  - DRONE_GITLAB=true
  - DRONE_GITLAB_CLIENT=dfsdfsdf
  - DRONE_GITLAB_SECRET=dsfdsf
  - DRONE_GITLAB_URL=https://tecgit01.com
  - DRONE_SECRET=${DRONE_SECRET}

drone-agent:
 image: drone/agent:0.8

 restart: always
 depends_on:
   - drone-server
 volumes:
   - /var/run/docker.sock:/var/run/docker.sock
 environment:
  - DRONE_SERVER=drone-server:9000
  - DRONE_SECRET=${DRONE_SECRET}
</code></pre>

<p>I'm running this on OSX(10.13.1) with Docker version 17.09.0-ce, build afdb6d4.</p>

<p>Local time in drone_agent is very different from the host time. This is causing the AWS API calls to fail when building my app. It throws this error.<a href=""https://forums.aws.amazon.com/thread.jspa?threadID=103764#"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=103764#</a>. I tried logging the current time inside the app to verify the time difference.</p>

<p>Is there a config to sync host time with the docker agent?</p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","47882763","Add files from local file system to running Docker container","<docker><containers>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>Is it possible to add files from local file system to a running docker container? Or I need to stop the container and add the files using cp command and run the container again?</p>
"
"22944631","How to get the IP address of the docker host from inside a docker container","<docker><ip>","47950121","How to access host network from a container form docker stack?","<docker><docker-networking>","<p>As the title says. I need to be able to retrieve the IP address the docker hosts and the portmaps from the host to the container, and doing that inside of the container. </p>
","<p>How do you access for example the url <code>http://&lt;host&gt;:80</code> apache running on the docker host from a running container?</p>

<p>i.e. the docker container ip is 10.0.0.3 and the host is 192.168.0.1?</p>
"
"25324860","How to create a bidirectional link between containers?","<docker>","47329086","Docker how to link two containers","<java><mysql><tomcat><docker><dockerhub>","<p>I have to link two containers so they can see each other. Of course the following...</p>

<pre><code>docker run -i -t --name container1 --link container2:container2 ubuntu:trusty /bin/bash
docker run -i -t --name container2 --link container1:container1 ubuntu:trusty /bin/bash
</code></pre>

<p>...fails at line 1 because a container needs to be up and running in order to be a link target:</p>

<pre><code>2014/08/15 03:20:27 Error response from daemon: Could not find entity for container2
</code></pre>

<p>What is the simplest way to create a bidirectional link?</p>
","<p>I am a newbie to docker, I wanted to run a Java web application which takes  name and mail and stores it in a MySQL dB now my doubt is how can I use a MySQL image and where I need to mention dB config in my app and how to make these app and dB container to run on tomcat server .is it possible to upload this entire image in docker hub to run on another host  which pulls this image and can run this app successfully ..</p>

<p>I want to dockerize  my name and mail application so that I can convert into an image so that any host can pull that image and run the app without any dependencies</p>
"
"25920029","Setting up MySQL and importing dump within Dockerfile","<mysql><docker>","47622801","Run mysql script through docker?","<mysql><docker><dockerfile>","<p>I'm trying to setup a Dockerfile for my LAMP project, but i'm having a few problems when starting MySQL. I have the folowing lines on my Dockerfile:</p>

<pre><code>VOLUME [""/etc/mysql"", ""/var/lib/mysql""]
ADD dump.sql /tmp/dump.sql
RUN /usr/bin/mysqld_safe &amp; sleep 5s
RUN mysql -u root -e ""CREATE DATABASE mydb""
RUN mysql -u root mydb &lt; /tmp/dump.sql
</code></pre>

<p>But I keep getting this error: </p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)
</code></pre>

<p>Any ideas on how to setup database creation and dump import during a Dockerfile build?</p>
","<p>
Hi there,
I'm trying to create a docker container that will run a mysql script which generates a database and then table.  My Dockerfile looks like this:</p>

<pre><code>FROM mysql:latest


WORKDIR /


ADD . /


EXPOSE 3306

CMD mysql -u ""root"" -proot &lt; ""schema.sql""
</code></pre>

<p>I create the image through this:</p>

<pre><code>docker build -t database .
</code></pre>

<p>And then I run it through this:</p>

<pre><code>docker run -d -p 3306:3306 database
</code></pre>



<p>At this point the script should be run- however instead I just get this random line in the terminal:</p>

<pre><code> 0b2503b42482a4fa840351925845392e1abdf6022b23447187ff49ed4f0fa05b
</code></pre>

<p>Grateful for your help!</p>
"
"26734402","How to upgrade docker container after its image changed","<docker>","47326900","Check for newer Docker build for image","<docker>","<p>Let's say I have pulled the official <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql:5.6.21 image</a>. </p>

<p>I have deployed this image by creating several docker containers.</p>

<p>These containers have been running for some time until MySQL 5.6.22 is released. The official image of mysql:5.6 gets updated with the new release, but my containers still run 5.6.21.</p>

<p>How do I propagate the changes in the image (i.e. upgrade MySQL distro) to all my existing containers? What is the proper Docker way of doing this?</p>
","<p>I'm running Jenkins from Docker with tag <code>lts</code>. behind this tag was version <code>2.73.2</code>. Now there is a newer <code>lts</code> version available: <code>2.73.3</code>. Docker doesn't automatically check for it. I had to do <code>docker pull jenkins/jenkins:lts</code> to get the new version.</p>

<p>But how can check if there is a newer build for a tag?</p>

<p>EDIT:
I want to make clear: This is not a duplicate! I asked how to check for a newer Docker image available. I know how to upgrade (as I said above).</p>
"
"28212380","Why docker container exits immediately","<docker>","47408287","docker container with apache server gets killed","<docker><containers><alpine>","<p>I run a container in the background using</p>

<pre><code> docker run -d --name hadoop h_Service
</code></pre>

<p>it exits quickly. But if I run in the foreground, it works fine. I checked logs using</p>

<pre><code>docker logs hadoop
</code></pre>

<p>there was no error. Any ideas?</p>

<p><strong>DOCKERFILE</strong></p>

<pre><code> FROM java_ubuntu_new
 RUN wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
 RUN dpkg -i cdh4-repository_1.0_all.deb
 RUN curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | apt-key add -
 RUN  apt-get update
 RUN apt-get install -y hadoop-0.20-conf-pseudo
 RUN dpkg -L hadoop-0.20-conf-pseudo
 USER hdfs
 RUN hdfs namenode -format
 USER root
 RUN apt-get install -y sudo
 ADD . /usr/local/
 RUN chmod 777 /usr/local/start-all.sh
 CMD [""/usr/local/start-all.sh""]
</code></pre>

<p>start-all.sh</p>

<pre><code> #!/usr/bin/env bash
 /etc/init.d/hadoop-hdfs-namenode start
 /etc/init.d/hadoop-hdfs-datanode start
 /etc/init.d/hadoop-hdfs-secondarynamenode start
 /etc/init.d/hadoop-0.20-mapreduce-tasktracker start
 sudo -u hdfs hadoop fs -chmod 777 /
 /etc/init.d/hadoop-0.20-mapreduce-jobtracker start
 /bin/bash
</code></pre>
","<p>I am new to Containers and trying wrap my head around it using the tutorial:</p>

<p><a href=""http://containertutorials.com/alpine/alpine-apache-server-static-site.html"" rel=""nofollow noreferrer"">http://containertutorials.com/alpine/alpine-apache-server-static-site.html</a></p>

<p>Here I create the dockerfile and try to run the container but it fails everytime. Any pointers?</p>

<pre><code>root@ubuntu:/home/skorada# docker run -it -p 4000:80 --name my-apache2- alpine-1  my-apache2-alpine
[s6-init] making user provided files available at /var/run/s6/etc...exited 0.
[s6-init] ensuring user provided files have correct perms...exited 0.
[fix-attrs.d] applying ownership &amp; permissions fixes...
[fix-attrs.d] done.
[cont-init.d] executing container initialization scripts...
[cont-init.d] 30-resolver: executing... 
[cont-init.d] 30-resolver: exited 0.
[cont-init.d] 40-resolver: executing... 
[cont-init.d] 40-resolver: exited 0.
[cont-init.d] done.
[services.d] starting services
[services.d] done.
AH00558: httpd: Could not reliably determine the server's fully qualified  domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message
[cont-finish.d] executing container finish scripts...
[cont-finish.d] done.
[s6-finish] syncing disks.
[s6-finish] sending all processes the TERM signal.
[s6-finish] sending all processes the KILL signal and exiting.
</code></pre>

<p>This is my dockerfile</p>

<pre><code>FROM smebberson/alpine-apache
ADD ./public-html/myindex.html /var/www/localhost/htdocs
RUN echo 'ServerName localhost' &gt;&gt; /etc/apache2/apache2.conf
</code></pre>

<p>Why doesnt the docker container up and running ? Unable to figure it out.
Have looked through different links but nothing has helped out so far.</p>
"
"28490874","docker run <IMAGE> <MULTIPLE COMMANDS>","<docker><docker-image>","47499687","Run inline command with pipe in docker container","<bash><docker><pipe><command-line-interface>","<p>I'm trying to run MULTIPLE commands like this.</p>

<pre><code>docker run image cd /path/to/somewhere &amp;&amp; python a.py
</code></pre>

<p>But this gives me ""No such file or directory"" error because it is interpreted as...</p>

<pre><code>""docker run image cd /path/to/somewhere"" &amp;&amp; ""python a.py""
</code></pre>

<p>It seems that some ESCAPE characters like """" or () are needed.</p>

<p>So I also tried </p>

<pre><code>docker run image ""cd /path/to/somewhere &amp;&amp; python a.py""
docker run image (cd /path/to/somewhere &amp;&amp; python a.py)
</code></pre>

<p>but these didn't work.</p>

<p>I have searched for <a href=""https://docs.docker.com/reference/run/"">Docker Run Reference</a> but have not find any hints about ESCAPE characters.</p>
","<p>I want to run a set of commands in a docker container, e.g.:</p>

<pre><code>docker run -i &lt;image&gt; cmd1 | cmd2
</code></pre>

<p>Both <code>cmd1</code> and <code>cmd2</code> should be run inside the container.</p>

<p>Is it possible? If not, what are the alternatives?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","47277737","containerising python rest services","<python-2.7><docker><flask><dockerfile><flask-restful>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>i am trying to build a flask rest service that will output random numbers in json format , i am trying to containerize and deploy this in docker.</p>

<p>I have run into a few issues here, basically the python rest service works on my local but when i deploy this in docker it does not work at all </p>

<p>below is my <strong>main.py</strong> </p>

<pre><code>from flask import Flask, request
from flask_restful import Resource, Api
from sqlalchemy import create_engine
from json import dumps
from flask.ext.jsonpify import jsonify
import random

app = Flask(__name__)
api = Api(app)

class generate_appid:
    def get_now(self):
        number_array=[0]
        for x in range(10):
            number_array.append(random.randint(0,9))
        number_string=''.join(map(str, number_array))
        return number_string
class appid(Resource):
    def get(self):
        number_initialise = generate_appid()
        appid1=number_initialise.get_now()
        return {'appid': appid1 }
api.add_resource(appid, '/appid')
if __name__=='__main__':
    app.run(port=5000)
</code></pre>

<p><strong>requirements.txt</strong></p>

<pre><code>aniso8601==1.2.0
appdirs==1.4.0
click==6.7
Flask==0.12
Flask-Jsonpify==1.5.0
Flask-RESTful==0.3.5
Flask-SQLAlchemy==2.1
itsdangerous==0.24
Jinja2==2.9.5
MarkupSafe==0.23
packaging==16.8
pyparsing==2.1.10
python-dateutil==2.6.0
pytz==2016.10
six==1.10.0
SQLAlchemy==1.1.5
Werkzeug==0.11.15
</code></pre>

<p>and <strong>Dockerfile</strong> </p>

<pre><code>FROM python:2.7-jessie
WORKDIR /usr/src/app
COPY requirements.txt /usr/src/app
RUN pip install --no-cache-dir -r requirements.txt
COPY . /usr/src/app
CMD [ ""python"", ""./startserver.py"" ]
EXPOSE 5000
</code></pre>

<hr>

<p>Output -<br>
I get the same output if i run this locally or deploy the docker images 
I use the below command to tun this in local <code>python main.py</code>
and for docker <code>docker run -d -p 5000:5000 --name pythonapi pythonappid:v1</code></p>

<pre><code>ExtDeprecationWarning: Importing flask.ext.jsonpify is deprecated, use flask_jsonpify instead.
  from flask.ext.jsonpify import jsonify
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
</code></pre>

<p>i hope i have included all the information necessary to help. </p>
"
"39197844","Ansible: ""sudo: a password is required\r\n""","<ssh><ansible>","47635480","sudo: a password is required ansible-playbook","<docker><ansible><sudo><ansible-2.x>","<p>quick question</p>

<p>I have setup an Ubuntu server with a user named test. I copy the authorized_keys to it, I can ssh no problem.
If I do <b>$ ansible -m ping ubu1</b>, no problem  I get a response</p>

<pre><code>    &lt;i&gt;&lt;p&gt;ubu1 | SUCCESS =&gt; {
        &lt;br&gt;""changed"": false, 
        &lt;br&gt;""ping"": ""pong""
    &lt;br&gt;}&lt;/i&gt;
</code></pre>

<p>What I dont get is this, If I do</p>

<p><b>$ ansible-playbook -vvvv Playbooks/htopInstall.yml</b> </p>

<pre><code>fatal: [ubu1]: FAILED! =&gt; {""changed"": false, ""failed"": true, ""invocation"": {""module_name"": ""setup""}, ""module_stderr"": ""OpenSSH_7.2p2 Ubuntu-4ubuntu2.1, OpenSSL 1.0.2g-fips  1 Mar 2016\r\ndebug1: Reading configuration data /etc/ssh/ssh_config\r\ndebug1: /etc/ssh/ssh_config line 19: Applying options for *\r\ndebug1: auto-mux: Trying existing master\r\ndebug2: fd 3 setting O_NONBLOCK\r\ndebug2: mux_client_hello_exchange: master version 4\r\ndebug3: mux_client_forwards: request forwardings: 0 local, 0 remote\r\ndebug3: mux_client_request_session: entering\r\ndebug3: mux_client_request_alive: entering\r\ndebug3: mux_client_request_alive: done pid = 6109\r\ndebug3: mux_client_request_session: session request sent\r\ndebug1: mux_client_request_session: master session id: 2\r\ndebug3: mux_client_read_packet: read header failed: Broken pipe\r\ndebug2: Received exit status from master 1\r\nShared connection to 192.168.1.112 closed.\r\n"", ""module_stdout"": ""sudo: a password is required\r\n"", ""msg"": ""MODULE FAILURE"", ""parsed"": false}
</code></pre>

<p>If I do <b>$ ansible-playbook --ask-sudo-pass Playbooks/htopInstall.yml</b>, then it ask my user password and the play is a success.</p>

<p>If I rename the authorized_keys it tells me I ""Failed to connect to the host via ssh."" which is ok.
What I dont understand is why is it asking for a sudo password. I definetly missed something along the way.</p>

<p>my <b>ansible.cfg</b> file looks like this</p>

<pre><code>[defaults]
nocows = 1
inventory = ./Playbooks/hosts
remote_user = test
private_key_file = /home/test/.ssh/id_ubu
host_key_checking = false
</code></pre>

<p>my hosts file looks like this</p>

<pre><code>[servers]
ubu1 ansible_ssh_host=192.168.1.112 ansible_ssh_user=test
</code></pre>
","<p>I have an ansible playbook which installs docker. It looks like this:</p>

<pre><code>---
- hosts: local
  connection: local
  become: yes
  become_user: root
  tasks:
    - name: add docker's key
      apt_key:
        keyserver: hkp://p80.pool.sks-keyservers.net:80
        id: 58118E89F3A912897C070ADBF76221572C52609D

    - name: add deb repo
      file: path=/etc/apt/sources.list.d/docker.list state=touch

    - name: register apt sources
      lineinfile: dest=""/etc/apt/sources.list.d/docker.list"" line=""{{item}}""
      with_items:
      - ""deb https://apt.dockerproject.org/repo ubuntu-trusty main""

    - name: install docker-engine
      apt: name=docker-engine state=present update-cache=yes force=yes
</code></pre>

<p>The problem is that when I run this playbook on my localhost I get an error:</p>

<pre><code>fatal: [127.0.0.1]: FAILED! =&gt; {""changed"": false, ""module_stderr"": ""sudo: a password is required\n"", ""module_stdout"": """", ""msg"": ""MODULE FAILURE"", ""rc"": 1}
</code></pre>

<p>So what parts of this playbook may cause such an error and how can I change them?</p>
"
"41471832","Vagrant, Docker, Puppet, Chef","<docker><vagrant><chef-infra><puppet>","47698812","Docker And Chef union, difference","<docker><chef-infra>","<p>I don't understand even the basic difference between the services in the title. Do these services just provide software to help you configure/organize/manage your VM's, or do they also provide physical infrastructure for your VM's to run on? In other words, are they just convenient interfaces between developers and AWS, Rackspace, and Azure?</p>
","<p>What is the common part and the differences between <a href=""https://en.wikipedia.org/wiki/Docker_(software)"" rel=""nofollow noreferrer"">docker</a> and <a href=""https://en.wikipedia.org/wiki/Chef_(software)"" rel=""nofollow noreferrer"">chef</a>?</p>
<p>As I understand, docker can include chef, but actually I don't have an exact example how do the related to each other...</p>
"
"43779323","TypeError: db.collection is not a function","<node.js><mongodb><rest><express>","47676639","Docker NodeJS connecting to mongo gives error ""db.collection is not a function'","<node.js><mongodb><docker>","<p>I am trying to post data to database that I have created on mLab and I am getting this error but I don't know whats going wrong.I also have read previously asked question on this topic but I am not able to solve my error as I am new to this. So here I am posting the code which I am trying to implement and It is taken from this tutorial <a href=""https://medium.freecodecamp.com/building-a-simple-node-js-api-in-under-30-minutes-a07ea9e390d2"" rel=""noreferrer"">https://medium.freecodecamp.com/building-a-simple-node-js-api-in-under-30-minutes-a07ea9e390d2</a>. </p>

<p>server.js</p>

<pre><code>const express = require('express');
const MongoClient = require('mongodb').MongoClient;
const bodyParser = require('body-parser');

const db = require('./config/db');


const app = express();

const port = 8000;

app.use(bodyParser.urlencoded({extened:true}));


MongoClient.connect(db.url,(err,database) =&gt;{

    if (err) return console.log(err)
    require('./app/routes')(app,{});
    app.listen(port,() =&gt; {
        console.log(""We are live on""+port); 
    });

})
</code></pre>

<p>db.js </p>

<pre><code>module.exports = {
  url : ""mongodb://JayTanna:Jay12345@ds147510.mlab.com:47510/testing""
};
</code></pre>

<p>index.js </p>

<pre><code>const noteroutes = require('./note_routes');

module.exports = function(app,db)
{
    noteroutes(app,db);

};
</code></pre>

<p>note_routes.js</p>

<pre><code>module.exports = function(app, db) {
  app.post('/notes', (req, res) =&gt; {
    const note = { text: req.body.body, title: req.body.title };
    db.collection('notes').insert(note, (err, result) =&gt; {
      if (err) { 
        res.send({ 'error': 'An error has occurred' }); 
      } else {
        res.send(result.ops[0]);
      }
    });
  });
};
</code></pre>
","<p>I've got a MongoDb server running in a Docker container. Everything seems to work fine. </p>

<p>I have another container which is running library/node and I want to connect to the MongoDb database. </p>

<p>I've followed <a href=""http://mongodb.github.io/node-mongodb-native/2.2/quick-start/quick-start/"" rel=""nofollow noreferrer"">http://mongodb.github.io/node-mongodb-native/2.2/quick-start/quick-start/</a>
Inside that second container I've run </p>

<pre><code>npm init 
npm install mongodb --save
</code></pre>

<p>I'm then running the following js file:</p>

<pre><code>var MongoClient = require('mongodb').MongoClient;
var url = 'mongodb://10.0.2.15:27017/mydb';
MongoClient.connect(url, function(err, db) {
console.log(""Connected successfully to server"");
var collection = db.collection('documents');   
db.close();
});
</code></pre>

<p>I can connect to the mongo container but trying to get collections fails with an error:</p>

<pre><code> TypeError: db.collection is not a function
    at /app/app.js:5:21
    at args.push (/app/node_modules/mongodb/lib/utils.js:431:72)
    at /app/node_modules/mongodb/lib/mongo_client.js:254:5
    at connectCallback (/app/node_modules/mongodb/lib/mongo_client.js:933:5)
    at /app/node_modules/mongodb/lib/mongo_client.js:794:11
    at _combinedTickCallback (internal/process/next_tick.js:131:7)
    at process._tickCallback (internal/process/next_tick.js:180:9)
</code></pre>

<p>What's going wrong here? Without the collection line the code works fine, suggesting a good connection to the db.</p>
"
"47272072","Celery workers unable to connect to redis on docker instances","<python><django><docker><docker-compose><celery>","47764204","Mysqli Error 2002: no such file or directory","<php><mysql><ubuntu><docker><nginx>","<p>I have a dockerized setup running a Django app within which I use Celery tasks. Celery uses Redis as the broker.</p>

<p><strong>Versioning:</strong></p>

<ul>
<li>Docker version 17.09.0-ce, build afdb6d4</li>
<li>docker-compose version 1.15.0, build e12f3b9</li>
<li>Django==1.9.6</li>
<li>django-celery-beat==1.0.1</li>
<li>celery==4.1.0</li>
<li>celery[redis]</li>
<li>redis==2.10.5</li>
</ul>

<p><strong>Problem:</strong></p>

<p>My celery workers appear to be unable to connect to the redis container located at localhost:6379. I am able to telnet into the redis server on the specified port. I am able to verify redis-server is running on the container.</p>

<p>When I manually connect to the Celery docker instance and attempt to create a worker using the command <code>celery -A backend worker -l info</code> I get the notice:</p>

<p><code>[2017-11-13 18:07:50,937: ERROR/MainProcess] consumer: Cannot connect to redis://localhost:6379/0: Error 99 connecting to localhost:6379. Cannot assign requested address..
Trying again in 4.00 seconds...</code></p>

<p><strong>Notes:</strong></p>

<p>I am able to telnet in to the redis container on port 6379. On the redis container, redis-server is running. </p>

<p>Is there anything else that I'm missing? I've gone pretty far down the rabbit hole, but feel like I'm missing something really simple.</p>

<p><strong>DOCKER CONFIG FILES:</strong></p>

<p>docker-compose.common.yml <a href=""http://paste.ubuntu.com/25956140/"" rel=""noreferrer"">here</a><br>
docker-compose.dev.yml <a href=""http://paste.ubuntu.com/25956144/"" rel=""noreferrer"">here</a></p>
","<p>I am trying to run nginx, php, mysql together with docker.</p>

<p>When i run docker-compose, everything looks fine, but when i try to make a connection to mysql via php code in my index page i'm getting this error.</p>

<pre><code>Warning: mysqli::__construct(): (HY000/2002): No such file or directory in /code/index.php on line 8
Connection failed: No such file or directory
</code></pre>

<ul>
<li>I have tried changing localhost to 127.0.0.1 and both with :3306 after it.</li>
</ul>

<p><strong>When changing localhost --> 127.0.0.1, i'm getting another connection failed statement:</strong></p>

<pre><code>Warning: mysqli::__construct(): (HY000/2002): Connection refused in /code/index.php on line 8
Connection failed: Connection refused
</code></pre>

<p><strong>php code:</strong></p>

<pre><code>&lt;?php

$servername = ""localhost"";
$username = ""root"";
$password = ""toor"";

// Create connection
$conn = new mysqli($servername, $username, $password);

// Check connection
if ($conn-&gt;connect_error) {
    die(""Connection failed: "" . $conn-&gt;connect_error);
}
echo ""Connected successfully"";

echo phpinfo();

?&gt;
</code></pre>

<p>Also when i look at <code>phpinfo();</code>, there is no value given to <code>mysqli.default_socket</code>.</p>

<p>when going into the mysql image after starting docker-compose, i'm able to go into mysql shell and execute commands just fine.</p>

<p>Any help is appreciated!</p>
"
"47662220","db.collection is not a function when using MongoClient v3.0","<node.js><mongodb>","47699143","Node.js mongodb: db.collection is not a function","<javascript><node.js><mongodb><docker>","<p>I have been trying <a href=""https://www.w3schools.com/nodejs/nodejs_mongodb_find.asp"" rel=""noreferrer"">W3schools tutorial</a> on nodeJS with MongoDB. </p>

<p>When I try to implement this example in a nodeJS environment and invoke the function with an AJAX call, I got the error below:</p>

<pre><code>TypeError: db.collection is not a function
    at c:\Users\user\Desktop\Web Project\WebService.JS:79:14
    at args.push (c:\Users\user\node_modules\mongodb\lib\utils.js:431:72)
    at c:\Users\user\node_modules\mongodb\lib\mongo_client.js:254:5
    at connectCallback (c:\Users\user\node_modules\mongodb\lib\mongo_client.js:933:5)
    at c:\Users\user\node_modules\mongodb\lib\mongo_client.js:794:11
    at _combinedTickCallback (internal/process/next_tick.js:73:7)
    at process._tickCallback (internal/process/next_tick.js:104:9)
</code></pre>

<p>Please find below my implemented code:</p>

<pre><code>var MongoClient = require('mongodb').MongoClient;
var url = ""mongodb://localhost:27017/mytestingdb"";

MongoClient.connect(url, function(err, db) {
  if (err) throw err;
  db.collection(""customers"").findOne({}, function(err, result) {
    if (err) throw err;
    console.log(result.name);
    db.close();
  });
});
</code></pre>

<p>Note that the error occurs whenever the execution hits:</p>

<pre><code>db.collection(""customers"").findOne({}, function(err, result) {}
</code></pre>

<p>Also, note (in case it matters) that I have installed the latest MongoDB package for node JS (<em>npm install mongodb</em>), and the MongoDB version is MongoDB Enterprise 3.4.4, with MongoDB Node.js driver v3.0.0-rc0.</p>
","<p>With typescript i want use mongo db in this way:</p>

<pre><code>import { config } from '../config';
import { MongoClient } from 'mongodb';

MongoClient.connect(config.mdb_uri, (err, db) =&gt; {
    if ( err ){
       console.log(err);
       return;
    }

    db.collection('mycollection').updateOne( 
      { '_id': 'x' },
      { $set: { 'info': 'OK' } },
      { upsert: true }
    );
});
</code></pre>

<p>but this code cause an exception into a docker container:</p>

<blockquote>
  <p>TypeError: db.collection is not a function</p>
</blockquote>

<p>i have added a <code>console.log(db)</code> and the result is a <code>MongoClient</code> object:</p>

<pre><code>MongoClient {
  domain: null,
  _events: {},
  _eventsCount: 0,
  _maxListeners: undefined,
  s:
   { url: 'mongodb://database:27017/mydb',
     options:
      { socketOptions: {},
        read_preference_tags: null,
        readPreference: [Object],
        dbName: 'dashboard',
        servers: [Array],
        server_options: [Object],
        db_options: [Object],
        rs_options: [Object],
        mongos_options: [Object],
        socketTimeoutMS: 360000,
        connectTimeoutMS: 30000,
        promiseLibrary: [Function: Promise] },
     promiseLibrary: [Function: Promise],
     dbCache: {},
     sessions: [] },
  topology:
   Server {
     domain: null,
     _events:
      { serverOpening: [Function],
        serverDescriptionChanged: [Function],
        serverHeartbeatStarted: [Function],
        serverHeartbeatSucceeded: [Function],
        serverHeartbeatFailed: [Function],
        serverClosed: [Function],
        topologyOpening: [Function],
        topologyClosed: [Function],
        topologyDescriptionChanged: [Function],
        joined: [Function],
        left: [Function],
        ping: [Function],
        ha: [Function],
        authenticated: [Function],
        error: [Function],
        timeout: [Function],
        close: [Function],
        parseError: [Function],
        open: [Object],
        fullsetup: [Object],
        all: [Object],
        reconnect: [Function] },
     _eventsCount: 22,
     _maxListeners: undefined,
     clientInfo:
      { driver: [Object],
        os: [Object],
        platform: 'Node.js v8.4.0, LE' },
     s:
      { coreTopology: [Object],
        sCapabilities: null,
        clonedOptions: [Object],
        reconnect: true,
        emitError: true,
        poolSize: 5,
        storeOptions: [Object],
        store: [Object],
        host: 'database',
        port: 27017,
        options: [Object],
        sessionPool: [Object],
        promiseLibrary: [Function: Promise] } } }
</code></pre>

<p>from package.json i have installed</p>

<pre><code>""dependencies"": {
  ...
  ""mongodb"": ""^3.0.0-rc0"",
  ...
},
""devDependencies"": {
  ...
  ""@types/mongodb"": ""^2.2.16"",
  ...
}
</code></pre>

<p>from my Dockerfile i install only the production dependencies:</p>

<pre><code>RUN npm install --only=production
CMD [""npm"", ""start""]
</code></pre>

<p>npm start, run the compiled *.ts </p>

<pre><code>""scripts"": {
  ...
  ""start"": ""node build/main.js"",
  ...
 }
</code></pre>

<p>the build is produced with</p>

<pre><code> ""build"": ""tsc -p tsconfig.release.json""
</code></pre>

<p>tsconfig.release.json is</p>

<pre><code>{
  ""extends"": ""./tsconfig.json"",
  ""compilerOptions"": {
    ""declaration"": false,
    ""removeComments"": true,
    ""jsx"": ""react""
  },
  ""include"": [
    ""src/**/*""
  ]
}
</code></pre>

<p>and tsconfig.json is</p>

<pre><code>{
  ""compilerOptions"": {
    ""target"": ""es6"",
    ""module"": ""commonjs"",
    ""moduleResolution"": ""node"",
    ""allowSyntheticDefaultImports"": true,
    ""jsx"": ""preserve"",
    ""allowJs"": true,
    ""importHelpers"": true,
    ""alwaysStrict"": true,
    ""sourceMap"": true,
    ""forceConsistentCasingInFileNames"": true,
    ""noFallthroughCasesInSwitch"": true,
    ""noImplicitReturns"": true,
    ""noUnusedLocals"": true,
    ""noUnusedParameters"": false,
    ""noImplicitAny"": false,
    ""noImplicitThis"": false,
    ""strictNullChecks"": false,
    ""declaration"": true,
    ""outDir"": ""build"",
    ""rootDir"": ""src"",
    ""emitDecoratorMetadata"": true,
    ""experimentalDecorators"": true
  },
  ""exclude"": [
    ""node_modules""
  ],
  ""include"": [
    ""src/**/*"",
    ""__tests__/**/*""
  ]
}
</code></pre>

<p>what i'm doing wrong?</p>

<p><strong>SOLVED!!</strong></p>

<p>i have installed mongodb 2.2.0 and now works, see <a href=""https://stackoverflow.com/a/47662979/2936170"">https://stackoverflow.com/a/47662979/2936170</a></p>
"
"218384","What is a NullPointerException, and how do I fix it?","<java><nullpointerexception>","50101933","How to run docker exec using Java on a docker container","<java><bash><shell><docker>","<p>What are Null Pointer Exceptions (<code>java.lang.NullPointerException</code>) and what causes them?</p>

<p>What methods/tools can be used to determine the cause so that you stop the exception from causing the program to terminate prematurely?</p>
","<p>I have a docker container called <code>pps-control</code>. If I want to execute a script that lives inside the container which for this example takes 1 arg, I do from my linux terminal:</p>

<blockquote>
  <p>docker exec pps-control /bin/sh -c ""./build/doControlStuff.sh 123456""</p>
</blockquote>

<p>My question is: <strong>How do I achieve doing the same using java code?</strong>
I know that there is something like:</p>

<pre><code>package com.mypackage;
import static com.palantir.docker.compose.execution.DockerComposeExecArgument.arguments;
import static com.palantir.docker.compose.execution.DockerComposeExecOption.options;
import com.palantir.docker.compose.DockerComposeRule;

public class TestClass {
    public static DockerComposeRule docker;
    public static void testMethod() {
        docker.exec(options(), ""pps-control"", arguments(""bash"", ""-c"", ""./build/doControlStuff.sh 123456""));
    }
}
</code></pre>

<p>But haven't been able to figure out how to use it (with correct syntax), any help would be appreciated with <strong>actual code</strong> example, thanks!</p>

<pre><code>My Docker version is 18.03.0-ce, build 0520e24
</code></pre>
"
"20105175","Equals vs space (""-o=value"" vs ""-o value"") in *nix command line programs - best practices?","<shell><command-line>","50319060","Docker command line arguments with values, when to use space vs when to use equals sign","<bash><docker><command-line-interface>","<p>I see no pattern in command line programs that take </p>

<pre><code>-o=value
</code></pre>

<p>versus</p>

<pre><code>-o value
</code></pre>

<p>For double dash long options, I see a major preference to the equals notation but not for short options. Is there a reason or best practice?</p>
","<p>I'm using the Docker command line interface. I've found out (by trying it) that I can add arguments with values both like this:</p>

<pre><code>$ docker build -t foo/bar .
</code></pre>

<p>And like this:</p>

<pre><code>$ docker build -t=foo/bar .
</code></pre>

<p>Both have the same result.</p>

<p>However, I can't seem to find in the docs which version is preferred (or deprecated). I also haven't found explicit mentioning of both forms, just implicit examples of both forms.</p>

<p>For example:</p>

<p>In <a href=""https://docs.docker.com/engine/reference/commandline/cli/#option-types"" rel=""nofollow noreferrer"">the Docker documentation</a> I see</p>

<blockquote>
  <p>Boolean options take the form <code>-d=false</code>.</p>
</blockquote>

<p>A little later I see:</p>

<blockquote>
  <p><code>$ docker run -i -t --name test busybox sh</code></p>
</blockquote>

<p>And below that I see:</p>

<blockquote>
  <p>Options like <code>--name=""""</code> expect a string, and they can only be specified
  once. Options like <code>-c=0</code> expect an integer, and they can only be
  specified once.</p>
</blockquote>

<p>In <a href=""https://docs.docker.com/engine/reference/run/#name---name"" rel=""nofollow noreferrer"">another part of the docs</a> I see:</p>

<blockquote>
  <p><code>$ docker run --name my-redis -d redis</code></p>
</blockquote>

<p>So both forms are used and (I suspect) valid. </p>

<p><strong>Is this true? Does it make a difference which form I use?</strong></p>
"
"23692470","Why can't I use Docker CMD multiple times to run multiple services?","<docker>","49630960","DOCKERFILE: Running multiple CMD. (Starting NGINX and PHP)","<php><docker><nginx><dockerfile>","<p>I have built a base image from Dockerfile named centos+ssh. In centos+ssh's Dockerfile, I use CMD to run ssh service.</p>

<p>Then I want to build a image run other service named rabbitmq,the Dockerfile:</p>

<pre><code>FROM centos+ssh
EXPOSE 22
EXPOSE 4149
CMD /opt/mq/sbin/rabbitmq-server start
</code></pre>

<p>To start rabbitmq container，run：</p>

<pre><code>docker run -d -p 222:22 -p 4149:4149 rabbitmq
</code></pre>

<p>but ssh service doesn't work, it sense rabbitmq's Dockerfile CMD override centos's CMD.</p>

<ol>
<li>How does CMD work inside docker image?</li>
<li>If I want to run multiple service, how to? Using supervisor?</li>
</ol>
","<p>I have a dockerfile that sets up NGINX, PHP, adds a Wordpress Repository. I want at boot time, to start PHP and NGINX. However, I am failing to do so. I tried adding the two commands in the CMD array, and I also tried to put them in a shell file and starting the shell file. Nothing worked. Below is my Dockerfile</p>

<pre><code>FROM ubuntu:16.04

WORKDIR /opt/

#Install nginx
RUN apt-get update
RUN apt-get install -y nginx=1.10.* php7.0 php7.0-fpm php7.0-mysql

#Add the customized NGINX configuration
RUN rm -f /etc/nginx/nginx.conf
RUN rm -f /etc/nginx/sites-enabled/*

COPY nginx/nginx.conf /etc/nginx/
COPY nginx/site.conf /etc/nginx/sites-enabled

#Copy the certificates
RUN mkdir -p /etc/pki/nginx
COPY nginx/certs/* /etc/pki/nginx/
RUN rm -f /etc/pki/nginx/placeholder

#Copy the build to its destination on the server
RUN mkdir -p /mnt/wordpress-blog/
COPY . /mnt/wordpress-blog/

#COPY wp-config.php
COPY nginx/wp-config.php /mnt/wordpress-blog/

#The command to run the container
CMD [""/bin/bash"", ""-c"", ""service php7.0-fpm start"", ""service nginx start""]
</code></pre>

<p>I tried to put the commands in the CMD in a shell file, and run the shell file in the CMD command. It still didn't work. what am i missing?</p>
"
"31479273","Get docker run command for container","<docker>","50232438","how do i get the command line i used to run docker container?","<docker>","<p>I have a container that I created, but I can't remember the exact <code>docker run</code> command I used to kick it off.  Is there any way that can be retrieved?</p>

<p>This is not the same as <a href=""https://stackoverflow.com/questions/27380641/see-full-command-of-running-stopped-container-in-docker"">See full command of running/stopped container in Docker</a>  What I want to know is the full docker command that spawned the container, not the command within the container.</p>
","<p>I have some container running for a while. so the bash history is already filled in with other commands. So how can I now extract the initial command I used to start container with all its parameters?</p>
"
"33980776","How to use Ansible's with_item with a variable?","<ansible>","50525007","Ansible extract substring from stdout line causes empty string","<regex><docker><ansible><yaml>","<p>I'm trying to transform some fields of the items of a list in an Ansible Playbook. Here is the simplest reproduction path, skipping the transformation. The result should be identical to the <code>users</code> variable.</p>

<pre><code>---
# Run with:
# ansible-playbook -i ""localhost,"" loop3.yml

- hosts: localhost
  connection: local
  gather_facts: false
  vars:
    users:
      - name: paul
        uid: 1
      - name: pete
        uid: 2
  tasks:
    - set_fact:
      args:
        useritem:
          name: '{{ item.name }}'
          uid:  '{{ item.uid }}'
      with_items:
        - users
      register: sf_result

    - debug: var=sf_result

    - set_fact:
        userslist: ""{{ sf_result.results | map(attribute='ansible_facts.useritem') | list }}""

    - debug: var=userslist
</code></pre>

<p>I get this error:</p>

<pre><code>TASK [set_fact useritem={u'name': u'{{ item.name }}', u'uid': u'{{ item.uid }}'}] ***
fatal: [localhost]: FAILED! =&gt; {""failed"": true, ""msg"": ""ERROR! 'unicode object' has no attribute 'name'""}
</code></pre>

<p>There are <a href=""https://stackoverflow.com/a/29817564"">several</a> <a href=""https://github.com/ansible/ansible/pull/8019"" rel=""nofollow noreferrer"">examples</a> very close to what I needbut I could find no working example using <code>set_fact</code> along with <code>with_items</code> and items as a map.</p>

<p>I've tried Ansible  1.9.2, 1.9.4, and 2.0.0-0.6.rc1 with different error messages but no more success. Ansible 2 should allow skipping the second call to <code>set_fact</code> but the error happens before getting there.</p>
","<p><strong>What i want to achieve:</strong></p>

<p>Get the list of running docker services on a specific node in ansible.</p>

<p><strong>How I tried to achieve this:</strong></p>

<pre><code>---
- name: echo text (this will later be docker stack ps SERVICE instead of echo)
  shell: echo 'SomePrefix_value1.1\nSomePrefix_value2.1'
  register: result

- name: Print values
  debug: 
    msg: '{{ item | regex_search(regexp) }}'
  with_items: result.stdout_lines
  vars:
   regexp: 'SomePrefix_([^\.]+)'
</code></pre>

<p>In a first shot I just wanted to provide some example output and print out the desired values. But when I run those tasks I get just an empty String printed out.</p>

<p><code>TASK [test-list : Print values] *****************************************************************************
ok: [127.0.0.1] =&gt; (item=None) =&gt; {
    ""msg"": """"
}</code></p>

<p>I tried to set regexp to <code>'.*'</code> to verify that the problem is in the regexp end the task is successfully with this regexp and it prints the following:</p>

<p><code>TASK [test-list : Print values] *****************************************************************************
ok: [127.0.0.1] =&gt; (item=None) =&gt; {
    ""msg"": ""result.stdout_lines""
}</code></p>

<p>I thought it has something to do with escaping and therefore I printed out the value of regexp and got the following output:</p>

<p><code>TASK [test-list : Print regexp] *****************************************************************************
ok: [127.0.0.1] =&gt; (item=None) =&gt; {
    ""regexp"": ""SomePrefix_([^\\.]+)""
}</code></p>

<p>I was wondering about the second <code>\</code> in the regexp, but after reading a bit it turned out that this is just in the debug output.</p>

<p>I don't know what I'm doing wrong. On <a href=""https://regex101.com"" rel=""nofollow noreferrer"">https://regex101.com</a> my regex works fine. Can anybody give me a hint?</p>

<p><strong>Solved by the question that is marked as duplication</strong></p>

<p>I had to adapt with_items to be with_items: '{{ result.stdout_lines }}'</p>
"
"38519643","Failed to load native library 'libnative-platform.so' for Linux amd64","<php><android><shell><ubuntu><gradle>","50271985","Docker in Windows: Failed to load native library 'libnative-platform.so' for Linux amd64","<java><windows><docker><gradle><docker-compose>","<p>First off, if this belongs on Ask Ubuntu, my apologies...I'm not really sure what forum this falls under.</p>

<p>I'm attempting to fork and build an Android project from Github using PHP and a shell script. Basically, I'm printing out some output from a shell script I wrote into a website. When you visit the page, and press some buttons, the project is forked and built using Gradle. The PHP code runs the shell script, and then prints the output into the browser as the command runs.</p>

<p>However, I am getting this error as the output of my script:</p>

<pre><code>FAILURE: Build failed with an exception.

* What went wrong:
Failed to load native library 'libnative-platform.so' for Linux amd64.
</code></pre>

<p>The script is simply running <code>gradle build</code> for now but I intend to change that later...right now, I'm just working on building the root project and outputting the result.</p>

<p>Here's the output if I run the command with the <code>--stacktrace</code> flag:</p>

<pre><code>* Exception is:
net.rubygrapefruit.platform.NativeException: Failed to load native library 'libnative-platform.so' for Linux amd64.
    at net.rubygrapefruit.platform.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:49)
    at net.rubygrapefruit.platform.Native.init(Native.java:55)
    at org.gradle.internal.nativeintegration.services.NativeServices.initialize(NativeServices.java:74)
    at org.gradle.internal.nativeintegration.services.NativeServices.initialize(NativeServices.java:60)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:203)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)
    at org.gradle.launcher.Main.doAction(Main.java:33)
    at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:622)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)
    at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
Caused by: java.io.IOException: No such file or directory
    at java.io.UnixFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:959)
    at net.rubygrapefruit.platform.internal.NativeLibraryLocator.find(NativeLibraryLocator.java:39)
    at net.rubygrapefruit.platform.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:41)
    ... 16 more
</code></pre>

<p>And here is the output with the <code>--debug</code> flag:</p>

<pre><code>06:21:43.710 [ERROR] [org.gradle.BuildExceptionReporter] 
06:21:43.740 [ERROR] [org.gradle.BuildExceptionReporter] FAILURE: Build failed with an exception.
06:21:43.747 [ERROR] [org.gradle.BuildExceptionReporter] 
06:21:43.747 [ERROR] [org.gradle.BuildExceptionReporter] * What went wrong:
06:21:43.748 [ERROR] [org.gradle.BuildExceptionReporter] Failed to load native library 'libnative-platform.so' for Linux amd64.
06:21:43.749 [ERROR] [org.gradle.BuildExceptionReporter] 
06:21:43.750 [ERROR] [org.gradle.BuildExceptionReporter] * Exception is:
06:21:43.752 [ERROR] [org.gradle.BuildExceptionReporter] net.rubygrapefruit.platform.NativeException: Failed to load native library 'libnative-platform.so' for Linux amd64.
06:21:43.753 [ERROR] [org.gradle.BuildExceptionReporter]    at net.rubygrapefruit.platform.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:49)
06:21:43.753 [ERROR] [org.gradle.BuildExceptionReporter]    at net.rubygrapefruit.platform.Native.init(Native.java:55)
06:21:43.754 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.internal.nativeintegration.services.NativeServices.initialize(NativeServices.java:74)
06:21:43.754 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.internal.nativeintegration.services.NativeServices.initialize(NativeServices.java:60)
06:21:43.755 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:203)
06:21:43.756 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)
06:21:43.756 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)
06:21:43.756 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)
06:21:43.757 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.Main.doAction(Main.java:33)
06:21:43.758 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
06:21:43.759 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
06:21:43.759 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
06:21:43.760 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
06:21:43.760 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:622)
06:21:43.761 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)
06:21:43.761 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)
06:21:43.762 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
06:21:43.762 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: java.io.IOException: No such file or directory
06:21:43.763 [ERROR] [org.gradle.BuildExceptionReporter]    at java.io.UnixFileSystem.createFileExclusively(Native Method)
06:21:43.763 [ERROR] [org.gradle.BuildExceptionReporter]    at java.io.File.createNewFile(File.java:959)
06:21:43.764 [ERROR] [org.gradle.BuildExceptionReporter]    at net.rubygrapefruit.platform.internal.NativeLibraryLocator.find(NativeLibraryLocator.java:39)
06:21:43.764 [ERROR] [org.gradle.BuildExceptionReporter]    at net.rubygrapefruit.platform.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:41)
06:21:43.765 [ERROR] [org.gradle.BuildExceptionReporter]    ... 16 more
06:21:43.765 [ERROR] [org.gradle.BuildExceptionReporter]
</code></pre>

<p>I did some research on this matter and found <a href=""https://community.bitnami.com/t/gradle-doesnt-work-libnative-platform-so/40686"" rel=""noreferrer"">this forum post</a>, but it didn't give me any leads as to how to fix the issue.</p>

<p>Further research led me to a <a href=""https://github.com/NixOS/nixpkgs/issues/14596"" rel=""noreferrer"">Github issue</a>, which appears to be for NixOS, but I'm running Ubuntu 12.04 CLI. Also, I should mention I'm running Gradle 2.10 as well.</p>

<p>I also tried running <code>gradle -Dorg.gradle.native=false build</code> as well, but the outcome was the same.</p>

<p>Now, as far as what user is calling the script, I believe it is www-data as I'm running an Apache2 web server and PHP. </p>

<p>Here's a little snippet of the PHP code that runs the script, which I found on another <a href=""https://stackoverflow.com/a/20109859/2844992"">SO post</a>:</p>

<pre><code>&lt;?php
    $command = ""sh /home/andrew/scripts/build.sh 2&gt;&amp;1"";
    while (@ ob_end_flush()); // end all output buffers if any
    $proc = popen($command, 'r');
    echo '&lt;pre style=""border-radius: 5px; padding:4px; color:black; background:#ffffff"" align=""left""&gt;';
    while (!feof($proc)){
        echo fread($proc, 1024);
        @ flush();
    }
    echo '&lt;/pre&gt;';
?&gt;
</code></pre>

<p>I'm at a loss now, so I'm hoping someone can lead me to a new, reliable answer.</p>
","<p>I am trying to run project in(by means of) docker. It works properly on unix servers. I am using Windows on my local machine.</p>

<p>Project structure looks like this:<br>
<a href=""https://i.stack.imgur.com/1wPjy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1wPjy.jpg"" alt=""enter image description here""></a></p>

<p><code>docker-compose.yml</code> content:</p>

<pre><code>version: '3'
services:
  sftp:
    image: atmoz/sftp
    restart: on-failure
    command: missftp:missftp:::destWorking,destRejected,destSuccess,attachments
  mailer:
    image: mailhog/mailhog
    ports:
      - 8025:8025
      - 1025:1025
    restart: on-failure
  mongo:
    image: mongo
    restart: on-failure
  mongo-express:
    image: mongo-express
    restart: on-failure
    ports:
      - 8081:8081
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongo
      - ME_CONFIG_BASICAUTH_USERNAME=admin
      - ME_CONFIG_BASICAUTH_PASSWORD=1234
  data-service:
    build:
      context: .
      dockerfile: Dockerfile.data-service
    ports:
      - 8801:8801
      - 8802:8802
    restart: on-failure
    volumes:
         - /opt/app/mis/attachments:/attachments
    environment:
      - SPRING_DATA_MONGODB_HOST=mongo
      - SPRING_MAIL_HOST=mailer
      - SPRING_MAIL_USERNAME=apikey
      - SPRING_MAIL_PASSWORD=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
      - SPRING_MAIL_PORT=465
      ...
  upload-service:
    build:
      context: .
      dockerfile: Dockerfile.upload-service
    ports:
      - 8082:8082
      - 8083:8083
    restart: on-failure
    environment:
      - SPRING_DATA_MONGODB_HOST=mongo
      ...
</code></pre>

<p><code>Dockerfile.data-service</code> file content:</p>

<pre><code>FROM gradle:alpine
COPY / ./
RUN gradle build
ENTRYPOINT java -jar ./mis-data-service/build/libs/mis-data-service-0.1.jar
</code></pre>

<p>Then I execute following command:</p>

<pre><code>docker-compose -f docker-compose.yml up
</code></pre>

<p>result:</p>

<pre><code>Building data-service
Step 1/4 : FROM gradle:alpine
 ---&gt; f438b7d58d0a
Step 2/4 : COPY / ./
 ---&gt; Using cache
 ---&gt; b72d0e76b86c
Step 3/4 : RUN gradle build
 ---&gt; Running in 7ba780a524e5

FAILURE: Build failed with an exception.

* What went wrong:
Failed to load native library 'libnative-platform.so' for Linux amd64.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org
ERROR: Service 'data-service' failed to build: The command '/bin/sh -c gradle build' returned a non-zero code: 1
</code></pre>

<p>What do I wrong? How to fix this error?</p>

<h2>P.S.</h2>

<p>I use Gradle 4.6</p>
"
"38532483","Where is /var/lib/docker on Mac/OS X","<macos><docker><docker-for-mac>","49965861","How to navigate to docker volumes folders on the host machine","<docker><docker-volume>","<p>I´m looking for the folder <code>/var/lib/docker</code> on my Mac after installing docker for Mac.</p>

<p>With <code>docker info</code>  I get</p>

<pre><code>    Containers: 5
     ...
    Server Version: 1.12.0-rc4
    Storage Driver: aufs
     Root Dir: /var/lib/docker/aufs
     Backing Filesystem: extfs
     Dirs: 339
     Dirperm1 Supported: true
    ...
    Name: moby
    ID: LUOU:5UHI:JFNI:OQFT:BLKR:YJIC:HHE5:W4LP:YHVP:TT3V:4CB2:6TUS
    Docker Root Dir: /var/lib/docker
    Debug Mode (client): false
    ....
</code></pre>

<p>But I don´t have a directory <code>/var/lib/docker</code> on my host.</p>

<p>I have checked <code>/Users/myuser/Library/Containers/com.docker.docker/</code> but couldn´t find anything there. Any idea where it is located?</p>
","<p>I'm trying to find out where VOLUME ""<strong>/shared</strong>"" is mapped to (Mac) and how to navigate to it on the host machine</p>

<p>I run</p>

<blockquote>
  <p>docker run -it --volume /shared ubuntu:latest /bin/bash</p>
</blockquote>

<p>Then I inspect where it's mapped to:</p>

<blockquote>
  <p>docker inspect d2e17f3d3bb4</p>
</blockquote>

<p>And see the excerpt from the config:</p>

<pre><code>""Mounts"": [
    {
        ""Type"": ""volume"",
        ""Name"": ""fb7ace0b18153bdad1744f0ac18eadc3775929dd5f448ebb228e7130f86c1a19"",
        ""Source"": ""/var/lib/docker/volumes/fb7ace0b18153bdad1744f0ac18eadc3775929dd5f448ebb228e7130f86c1a19/_data"",
        ""Destination"": ""/shared"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
</code></pre>

<p>And there is not such folder on the host machine:</p>

<blockquote>
  <p>/var/lib/docker/volumes/fb7ace0b18153bdad1744f0ac18eadc3775929dd5f448ebb228e7130f86c1a19/_data</p>
</blockquote>

<p>Not even <strong>/var/lib/docker/volumes</strong></p>

<p>/var/lib/docker doesn't exist</p>

<p><strong>Edited:</strong></p>

<p>Thanks to @BMitch </p>

<p>The answer is</p>

<ol>
<li><strong>Docker VM is located at</strong> </li>
</ol>

<blockquote>
  <p>~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2</p>
</blockquote>

<ol start=""2"">
<li><strong>Navigate to the VM</strong></li>
</ol>

<blockquote>
  <p>screen
  ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty</p>
</blockquote>

<ol start=""3"">
<li>Within the tty one can navigate to <strong>/var/lib/docker</strong> and docker volumes</li>
</ol>
"
"38846079","Only local connections are allowed Chrome and Selenium webdriver","<selenium><selenium-webdriver><selenium-chromedriver>","50379607","How to Run C# Selenium program in a Docker Linux container","<c#><selenium><docker><docker-compose><dockerfile>","<p>I am using Chrome webdriver 2.23 &amp; Selenium 2.53.1. 
I have tried a lot, but could not get it fixed. Whenever I run my selenium script, it is giving me the following error </p>

<pre><code>Starting ChromeDriver 2.23.409699 (49b0fa931cda1caad0ae15b7d1b68004acd05129) on port 12162
Only local connections are allowed.
</code></pre>
","<p>Using <code>dotnet</code> tools, I can build a C# project and run it by using the DLL. </p>

<ul>
<li><code>dotnet clean</code></li>
<li><code>dotnet build -c ""Release"" .</code></li>
<li><code>dotnet ./SeleniumSample/bin/Release/netcoreapp2.0/SeleniumSample.dll</code></li>
</ul>

<p>However, when I try to replicate this in a docker container, I am faced with multiple issues:</p>

<ol>
<li>Loading ChromeDriver</li>
<li>Loading Chrome in headless mode</li>
</ol>

<p>So far, I have not found samples of the combination [""C#"" , ""Selenium"", ""Docker""] anywhere else in the internet. So, I would love it if you could help me.</p>

<p>Snipplet of loading the Chrome Driver:</p>

<pre><code>ChromeOptions options = new ChromeOptions();
options.AddArgument(""--headless"");
driver = new ChromeDriver(Directory.GetCurrentDirectory(), options);
</code></pre>

<p><strong><em>Note:</em></strong> <code>Directory.GetCurrentDirectory()</code> will point to the directory containing <code>chromedriver.exe</code> generated by the build using <code>dotnet</code> cli.</p>

<p>Contents of <strong>SeleniumSample.csproj</strong> is shown below:</p>

<pre><code>&lt;Project Sdk=""Microsoft.NET.Sdk""&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;TargetFramework&gt;netcoreapp2.0&lt;/TargetFramework&gt;
    &lt;Configurations&gt;Debug;Release;ALL&lt;/Configurations&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;PackageReference Include=""Selenium.Firefox.WebDriver"" Version=""0.20.0"" /&gt;
    &lt;PackageReference Include=""Selenium.WebDriver"" Version=""3.12.0"" /&gt;
    &lt;PackageReference Include=""Selenium.WebDriver.ChromeDriver"" Version=""2.38.0.1"" /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;
</code></pre>

<p><strong>Dockerfile</strong> contents:</p>

<pre><code>FROM microsoft/dotnet
ARG source
WORKDIR /app
COPY ${source} .
RUN dotnet clean
RUN dotnet build -c ""Release""
WORKDIR ./SeleniumSample/bin/Release/netcoreapp2.0/
ENTRYPOINT dotnet ./SeleniumSample.dll
</code></pre>

<p>Code to execute the container of the created image is below after <code>docker-compose up</code>:</p>

<pre><code>docker run -i -t -P -v /lib/x86_64-linux-gnu/libglib-2.0.so.0:/lib/x86_64-linux-gnu/libglib-2.0.so.0 \ 
-v /usr/lib/x86_64-linux-gnu/libX11.so.6:/usr/lib/x86_64-linux-gnu/libX11.so.6 \ 
-v /usr/lib/x86_64-linux-gnu/libnss3.so:/usr/lib/x86_64-linux-gnu/libnss3.so \ 
-v /usr/lib/x86_64-linux-gnu/libnssutil3.so:/usr/lib/x86_64-linux-gnu/libnssutil3.so \ 
-v /usr/lib/x86_64-linux-gnu/libnspr4.so:/usr/lib/x86_64-linux-gnu/libnspr4.so \ 
-v /usr/lib/x86_64-linux-gnu/libxcb.so.1:/usr/lib/x86_64-linux-gnu/libxcb.so.1 \ 
-v /usr/lib/x86_64-linux-gnu/libplc4.so:/usr/lib/x86_64-linux-gnu/libplc4.so \ 
-v /usr/lib/x86_64-linux-gnu/libplds4.so:/usr/lib/x86_64-linux-gnu/libplds4.so \ 
-v /usr/lib/x86_64-linux-gnu/libXau.so.6:/usr/lib/x86_64-linux-gnu/libXau.so.6 \ 
-v /usr/lib/x86_64-linux-gnu/libXdmcp.so.6:/usr/lib/x86_64-linux-gnu/libXdmcp.so.6 \ 
seleniumsample_selenium-sample
</code></pre>

<p>The following error occurs: </p>

<pre><code>Starting ChromeDriver 2.38.552522 (437e6fbedfa8762dec75e2c5b3ddb86763dc9dcb) on port 40599
Only local connections are allowed.

Unhandled Exception: OpenQA.Selenium.WebDriverException: unknown error: cannot find Chrome binary
  (Driver info: chromedriver=2.38.552522 (437e6fbedfa8762dec75e2c5b3ddb86763dc9dcb),platform=Linux 4.13.0-41-generic x86_64)
   at OpenQA.Selenium.Remote.RemoteWebDriver.UnpackAndThrowOnError(Response errorResponse)
   at OpenQA.Selenium.Remote.RemoteWebDriver.Execute(String driverCommandToExecute, Dictionary`2 parameters)
   at OpenQA.Selenium.Remote.RemoteWebDriver.StartSession(ICapabilities desiredCapabilities)
   at OpenQA.Selenium.Remote.RemoteWebDriver..ctor(ICommandExecutor commandExecutor, ICapabilities desiredCapabilities)
   at OpenQA.Selenium.Chrome.ChromeDriver..ctor(ChromeDriverService service, ChromeOptions options, TimeSpan commandTimeout)
   at SeleniumSample.Utility.GetDriver(String browserName, String argument) in /app/SeleniumSample/Utility.cs:line 54
   at SeleniumSample.GmailUnreadEmails.Launcher() in /app/SeleniumSample/GmailUnreadEmails.cs:line 80
   at SeleniumSample.Launcher.Main(String[] args) in /app/SeleniumSample/Launcher.cs:line 8
Aborted (core dumped)
</code></pre>

<p>My guess is the <code>Chrome</code> is not installed in the docker image and so I am getting this error. If that is true, how do I fix this problem?</p>

<p><strong>EDIT 1</strong>:
after adding the below lines to the Dockerfile  </p>

<pre><code>RUN \
  apt-get update &amp;&amp; \
  apt-get install -y wget

RUN \
  wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - &amp;&amp; \
  echo ""deb http://dl.google.com/linux/chrome/deb/ stable main"" &gt; /etc/apt/sources.list.d/google.list &amp;&amp; \
  apt-get update &amp;&amp; \
  apt-get install -y google-chrome-stable &amp;&amp; \
  rm -rf /var/lib/apt/lists/*
WORKDIR /data
EXPOSE 5901
</code></pre>

<p>Now I get the below error-</p>

<pre><code>selenium-sample_1  | Only local connections are allowed.
selenium-sample_1  | 
selenium-sample_1  | Unhandled Exception: OpenQA.Selenium.WebDriverException: The HTTP request to the remote WebDriver server for URL http://localhost:45521/session timed out after 60 seconds. ---&gt; System.Net.WebException: The operation has timed out.
</code></pre>

<p><em>After this edit, the question becomes similar to <a href=""https://stackoverflow.com/questions/22322596/selenium-error-the-http-request-to-the-remote-webdriver-timed-out-after-60-sec?noredirect=1&amp;lq=1"">this</a> question but it is still worth having on the site because no one else has tried such a scenario to the best of my knowledge.</em></p>
"
"41087325","Docker container out of sync with host","<node.js><amazon-web-services><docker>","50311943","Why is the UTC time in C++ 8 seconds ahead of the actual UTC time?","<c++><docker>","<p>I have a simple Node app which sends messages to AWS SQS. For local development I am providing AWS SDK with <code>region</code>, <code>queueUrl</code>, <code>accessKeyId</code>, <code>secretAccessKey</code>.</p>

<p>Everything works fine until I dockerise the app and run as a container. Then whenever SQS wants to do something I get the following error</p>

<p><code>{ SignatureDoesNotMatch: Signature expired: 20161211T132303Z is now earlier than 20161211T142227Z (20161211T143727Z - 15 min.)
</code></p>

<p>If I add <code>correctClockSkew: true</code> it corrects the problem.</p>

<p>What is docker doing to require the <code>correctClockSkew: true</code> but not when running Node in MacOS</p>

<p>Node app</p>

<pre><code>process.env.TZ = 'Europe/London';
const AWS = require('aws-sdk');

AWS.config.update({
  region: 'eu-west-1',
  correctClockSkew: true //this has to be set when running inside a docker container?
});

const sqs = new AWS.SQS({
  apiVersion: '2012-11-05',
});

sqs.sendMessage({
  QueueUrl: 'https://sqs.eu-west-1.amazonaws.com/522682236448/logback-paddle-prod-errors',
  MessageBody: 'HelloSQS',
}, (err, data) =&gt; {
  if (err) throw err;
});
</code></pre>

<p>Dockerfile</p>

<pre><code>FROM node
RUN mkdir -p /usr/lib/app
WORKDIR /usr/lib/app
COPY app/ /usr/lib/app/
RUN npm install
CMD [""node"", ""index.js""]
</code></pre>

<p><code>docker run -d user/image</code></p>

<p>Edit</p>

<p>Originally I created the question because I kept getting AWS incorrect time errors, now I am getting it with ElasticSearch too. Why is my container reliably out of sync with the host by about 15 mins.</p>
","<p>I have a simple function to print the current UTC time every second. Here is the code:</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;time.h&gt;
#include &lt;chrono&gt;

using namespace std;

void printTime(){
    time_t now = time(0);
    tm *gmtm = gmtime(&amp;now);
    char* dt = asctime(gmtm);
    cout &lt;&lt; dt &lt;&lt; endl;
}

int main(void) {
    while(true){
        printTime();
        this_thread::sleep_for(chrono::milliseconds(1000));
    }

    return(0);
}
</code></pre>

<p>My code prints a different time than the actual UTC time (compared to both my local machine and a website).</p>

<p>For example ...</p>

<p>My code prints <code>Sun May 13 00:55:30 2018</code>.</p>

<p>My local machine prints <code>Sun May 13 00:55:22 2018</code> (I print it from a terminal with <code>date -u</code>).</p>

<p>Some website prints <code>Sun May 13 00:55:22 2018</code> (same as my local machine).</p>

<p>What did I do wrong? Please help :(</p>

<p>[EDIT]
I use Docker.</p>
"
"41729237","can I mount subdir of volume in docker run command","<docker><docker-compose>","50121450","Can I specify a subdirectory for a defined docker volume in compose?","<docker><docker-compose>","<p>I'm trying to avoid slow <code>osxfs</code> when using docker. So I'm running docker-sync volume container.</p>

<p>I want to mount only subdir of this volume to another container.</p>

<p><a href=""https://i.stack.imgur.com/2gKN1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2gKN1.png"" alt=""enter image description here""></a></p>

<p>Current example errors out:</p>

<pre><code>docker: Error response from daemon: create docker-sync/www/marcopolo.front: ""docker-sync/www/marcopolo.front"" includes invalid characters for a local volume name, only ""[a-zA-Z0-9][a-zA-Z0-9_.-]"" are allowed.
See 'docker run --help'.
</code></pre>
","<p>Hello I have a nfs docker volume, and want to mount a file out of one of its subdirectories. Is it possible to do that? My compose file leads to an error currently. Is there a working syntax or do I need a separate volume for each file?</p>

<pre><code>version: ""3""
    services:
      test:
        image: nginx
        volumes:
          - volume1/subpath/file1:/file1:ro
    ...
    volumes:
     volume1:
      external: true
</code></pre>

<p>Leads to:</p>

<pre><code>-&gt;service test: undefined volume ""volume1/subpath/file1""
</code></pre>
"
"41927249","Tilde Expansion Doesn't Work in Docker COPY Command","<docker><dockerfile><tilde-expansion>","49902627","COPY does not add my file to Docker container","<docker>","<p>I have a Dockerfile with the following line:</p>

<pre><code>COPY *.zip ~user1
</code></pre>

<p>The user <code>user1</code> already exists and has a home directory (i.e. <code>/home/user1</code>). The goal, of course, is to copy the zip file into that user's home dir, however the result of the above is that the zip file is copied to a file named literally <code>/~user1</code> in the image.</p>

<p>The following does work as expected:</p>

<pre><code>COPY *.zip /home/user1
</code></pre>

<p>Is this a bug in Docker or is there a <strong>limitation related to tilde expansion</strong> that I'm not aware of?</p>

<p>Using Docker 1.13.0 on Mac.</p>
","<p>I'm trying to add a private key <code>docker-upload</code> to a docker container:</p>

<pre><code>FROM centos:7

RUN yum -y -q install openssh-clients &amp;&amp; \
  yum clean all &amp;&amp; \
  rm -rf /var/cache/yum

RUN \
  mkdir -p ~/.ssh &amp;&amp; \
  touch ~/.ssh/known_hosts &amp;&amp; \
  ssh-keyscan -p 2022 -t rsa hostname &gt; ~/.ssh/known_hosts

COPY docker-upload ~/.ssh/

RUN ls -all ~/.ssh
</code></pre>

<p>When building with </p>

<p><code>docker build --no-cache -t privateregistry.com/build-containers:upload .</code> </p>

<p>I get that the ~/.ssh directory is empty:</p>

<pre><code>Step 5/7 : COPY docker-nerds-upload ~/.ssh/
 ---&gt; 2bafa09dd03c
Step 6/7 : RUN ls -all ~/.ssh
 ---&gt; Running in 1c74fc1d0e14
total 8
drwxr-xr-x 2 root root 4096 Apr 18 14:19 .
dr-xr-x--- 3 root root 4096 Apr 18 14:19 ..
-rw-r--r-- 1 root root    0 Apr 18 14:19 known_hosts
</code></pre>

<p>Why doesn't <code>COPY docker-upload ~/.ssh/</code> copy the file <code>docker-upload</code> into the <code>~/.ssh</code> directory in the docker container?</p>
"
"42374451","Multiple adb connections over WiFi","<android><adb>","49552262","'adb devices' parallel executing both on host and docker containers","<android><ubuntu><docker><adb><ubuntu-16.04>","<p>We are two people and want to connect to a <code>android device</code> from each respective notebook. When one person is connected, the other person gets a <code>device is offline</code> error.</p>

<p>Is it possible to connect via <code>adb shell</code> to a single <code>android device</code> from multiple machines?</p>
","<p>I have host (Ubuntu 16.04) with two connected Android devices and docker containers for them. Every container can see his device:</p>

<pre><code>admin@ubuntu:~$ adb devices
List of devices attached
851e4ce3        device
</code></pre>

<p>Now I need to see devices both on host and inside of containers. So I run <code>adb start-server</code> on host, and it can see both devices. But from that moment containers cannot see their phones (<code>adb devices</code> returns empty list).
And if I run <code>adb kill-server</code> on host, they would see their devices again.</p>

<p>Is there any way too see devices via adb on host and inside the containers at the same moment? Please help me to configure it.</p>
"
"42805750","Dockerfile CMD shell versus exec form","<docker><dockerfile>","50063585","Is there any difference between CMD [""./start.sh""] and CMD ./start.sh in docker file?","<docker><docker-compose><dockerfile>","<p>What's the difference between the shell form and exec form of docker RUN and CMD statements.</p>

<p><strong>eg:</strong></p>

<pre><code>RUN [ ""npm"", ""start"" ]
</code></pre>

<p>vs:</p>

<pre><code>RUN npm start 
</code></pre>

<p><strong>eg2</strong>: </p>

<pre><code>CMD [""python"",""my_script.py"",""argument""]
</code></pre>

<p>vs:</p>

<pre><code>CMD python my_script.py argument 
</code></pre>
","<p>I want to create a docker file in which my API file which is named as start.sh should be running in the background when I run it in the container.Is there any way to do so? </p>
"
"43606968","Using LOAD CSV to import a local file to Neo4j in a Docker container","<docker><import><neo4j>","49802437","Loading a CSV file into Neo4J running in Docker","<csv><docker><neo4j><neo4j-driver>","<p>So I've been trying to import an external CSV file into my graphdb.
My neo4j is stored in a Docker container.
I placed the file in NEO_HOME/import, as implied.
I called the LOAD CSV command with ""file:///mycsv.csv"" as an argument, and got the followng in return</p>

<blockquote>
  <p>Couldn't load the external resource at: file:/var/lib/neo4j/import/mycsv.csv</p>
</blockquote>

<p>Since I'm running the Docker container on a Windows environment, I don't see where the /var directory should be. Even when browsing the container itself via the Docker Quickstart Terminal. I still cannot find /var/lib...</p>

<p>When trying to change the .conf file to a different import directory, it didn't help as well.</p>

<p>Did somebody have this before?</p>
","<p>I have Neo4J running in Docker (Windows 10). I want to import a large CSV file into it, using Node and <code>neo4j-driver</code>.</p>

<p>My query is (limited to 5 rows, for testing purposes):</p>

<pre><code>let csvPath = 'file:///C:/Users/Test/largefile.csv';

let query = `
  USING PERIODIC COMMIT
  LOAD CSV FROM '${csvPath}' AS line
  WITH line LIMIT 5
  MERGE (:Person {name: line[1]})-[:connected_to]-&gt;(:Person {name: line[2]})
`;
</code></pre>

<p>No matter what I try to put in the path, the driver keeps looking for <code>file:/var/lib/neo4j/import/C:/Users/Test/largefile.csv</code>. </p>

<p>I tried with/without drive letter; tried local file and using <code>./</code>; tried diffrent paths - the driver always looks for the file at <code>/var/lib/neo4j/import</code> even though I have no such folder on my machine.</p>

<p>Is it possible to import the file from my host machine, or will I need to copy it into the Docker container, into that folder structure?</p>
"
"48235040","Run X application in a Docker container reliably on a server connected via SSH without ""--net host""","<docker><ssh><x11><x11-forwarding>","49991678","How to share SSH X11 forward's display with a a container?","<docker>","<p>Without a Docker container, it is straightforward to run an X11 program on a remote server using the SSH X11 forwarding (<strong>ssh -X</strong>). I have tried to get the same thing working when the application runs inside a Docker container on a server. When SSH-ing into a server with the -X option, an X11 tunnel is set up and the environment variable ""$DISPLAY"" is automatically set to typically ""localhost:10.0"" or similar. If I simply try to run an X application in a Docker, I get this error:</p>

<pre><code>Error: GDK_BACKEND does not match available displays
</code></pre>

<p>My first idea was to actually pass the $DISPLAY into the container with the ""-e"" option like this:</p>

<pre><code>docker run -ti -e DISPLAY=$DISPLAY name_of_docker_image
</code></pre>

<p>This helps, but it does not solve the issue. The error message changes to:</p>

<pre><code>Unable to init server: Broadway display type not supported: localhost:10.0
Error: cannot open display: localhost:10.0
</code></pre>

<p>After searching the web, I figured out that I could do some <strong>xauth</strong> magic to fix the authentication.  I added the following:</p>

<pre><code>SOCK=/tmp/.X11-unix
XAUTH=/tmp/.docker.xauth
xauth nlist $DISPLAY | sed -e 's/^..../ffff/' | xauth -f $XAUTH nmerge -
chmod 777 $XAUTH
docker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH \ 
  -e XAUTHORITY=$XAUTH name_of_docker_image
</code></pre>

<p>However, this only works if also add ""<strong>--net host</strong>"" to the docker command:</p>

<pre><code>docker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH \ 
  -e XAUTHORITY=$XAUTH --net host name_of_docker_image
</code></pre>

<p>This is not desirable since it makes the whole host network visible for the container.</p>

<p>What is now missing in order to get it fully to run on a remote server in a docker without ""--net host""?</p>
","<p>I have:</p>

<ul>
<li>Remote server that runs Docker container and a SSH daemon</li>
<li>My own computer that can run Docker container and connect to the remote server through SSH with X11 forward</li>
</ul>

<p>From my computer, I would like to open a GUI application located in the Docker container located in the Remote server.</p>

<ul>
<li><p>If I run the Docker container in my own computer, I can open GUI app without any problem.</p>

<ul>
<li><code>docker run -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix --env=""QT_X11_NO_MITSHM=1"" ...</code></li>
</ul></li>
<li><p>If I run the container in the Remote server through SSH. I get: <code>myapp: cannot connect to X server localhost:10.0</code></p></li>
</ul>

<p>I guess there is another configuration to be able to share the virtual display created by the SSH connection to the container.</p>

<p>I am not running any SSH server in the Docker container and I don't want to.</p>

<h3>How can I share the SSH X11 forward display with a Docker container?</h3>

<ul>
<li>If possible with a transparent way, no matter I run the container in local or remotely through SSH.</li>
</ul>
"
"49338902","how to install anaconda / miniconda on Linux silently","<linux><anaconda><miniconda>","49714499","skip installing confirm('yes' or 'no') in Dockerfile","<bash><shell><docker><anaconda><dockerfile>","<p>How do I install the anaconda / miniconda without prompts on Linux command line?</p>

<p>Is there a way to pass <code>-y</code> kind of option to agree to the T&amp;Cs, suggested installation location etc. by default?</p>
","<p>Here's part of my dockerfile:</p>

<pre><code>FROM ubuntu:16.04

#ubuntu:lastest

RUN apt-get update

COPY Anaconda3-4.4.0-Linux-x86_64.sh /root/

WORKDIR /root/

RUN bash Anaconda3-4.4.0-Linux-x86_64.sh

RUN conda upgrade --all

...
</code></pre>

<p>during the installation process of Anaconda(bash Anaconda3-4.4.0-Linux-x86_64.sh), there is a license needs agreement, which needs me to answer 'yes',</p>

<p><a href=""https://i.stack.imgur.com/jAjed.png"" rel=""nofollow noreferrer"">install confirm</a></p>

<p>but when it's run in dockerfile, it shows like
<a href=""https://i.stack.imgur.com/kZANM.png"" rel=""nofollow noreferrer"">this</a>,
like keep pressing ENTER, how can I skip this confirm or input 'yes'</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","43169160","Docker, How can I locate the image files after pulling","<docker>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>After pulling image, for example <strong>ubuntu</strong>, how can I locate the image file on local disk.</p>

<pre><code>$ docker pull ubuntu
</code></pre>

<p>Will the image work if I copy the image file to another machine?</p>

<p>OS: macOS</p>
"
"26598738","How to create User/Database in script for Docker Postgres","<bash><postgresql><docker><dockerhub>","42252649","How do I create a database in the Postgres Docker container?","<postgresql><docker><docker-compose><dockerfile>","<p>I have been trying to set up a container for a development postgres instance by creating a custom user &amp; database. I am using the <a href=""https://registry.hub.docker.com/_/postgres/"" rel=""noreferrer"">official postgres docker image</a>. In the documentation it instructs you to insert a bash script inside of the <code>/docker-entrypoint-initdb.d/</code> folder to set up the database with any custom parameters.</p>

<h2>My bash script: make_db.sh</h2>

<pre><code>su postgres -c ""createuser -w -d -r -s docker""
su postgres -c ""createdb -O docker docker""
</code></pre>

<h2>Dockerfile</h2>

<pre><code>FROM library/postgres

RUN [""mkdir"", ""/docker-entrypoint-initdb.d""]
ADD make_db.sh /docker-entrypoint-initdb.d/
</code></pre>

<p>The error I get from the <code>docker logs -f db</code> (db is my container name) is:</p>

<blockquote>
  <p>createuser: could not connect to database postgres: could not connect to server: No such file or directory</p>
</blockquote>

<p>It seems that the commands inside of the <code>/docker-entrypoint-initdb.d/</code> folder are being executed before postgres is started. My question is, how do I set up a user/database programmatically using the official postgres container? Is there any way to do this with a script?</p>
","<p>I want to create a database after starting the Postgres Docker image.</p>

<p>The problem is if I put my database creation command in my docker-compose file or a child dockerfile it overrides the base image's command to actually start postgres - so my psql has no started server to connect to.</p>

<p>E.g.:</p>

<pre><code>version: '2'
services:
  db:
    command: bash -c ""su - postgres -c '/usr/lib/postgresql/9.6/bin/psql  -h 127.0.0.1 -p 6543 -U postgres -c \""create database dev;\""'""
    environment:
      - PGDATA=/pg
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
    image: postgres:9.6
    ports:
      - '6543:5432'
    tmpfs: /pg
</code></pre>

<p>And the <a href=""https://hub.docker.com/_/postgres/"" rel=""nofollow noreferrer"">image documentation</a> doesn't say the exact command they are running to start Postgres. I tried putting <code>bash -c ""postgres &amp;&amp; ...</code> at the beginning of my command but it fails telling me I can't start Postgres as root.</p>

<p>The only possible solution I can find is to abandon docker-compose and dockerfile in favour of a bash script solution, like <code>docker run postgres:9.6 psql -h postgres -U postgres -c 'create database'</code> but this is horrible.</p>

<p>What to do?</p>
"
"31885409","Why would a correct shell script give a wrapped/truncated/corrupted error message?","<bash><shell><sh><carriage-return>","42120317","sh on docker: syntax error: unexpected end of file (expecting ""then"")","<docker><sh><dockerfile>","<p>I have a shell script with a command that seems like it should work, but instead it fails with an odd wrapped/truncated/corrupted error message. Example:</p>

<pre><code>$ ls -l myfile
-rw-r----- 1 me me 0 Aug  7 12:36 myfile
$ cat myscript 
ls -l myfile
$ bash myscript
: No such file or directory
</code></pre>

<p>The file clearly exist, but even if I didn't, this is the kind of error message I would normally get:</p>

<pre><code>$ ls -l idontexist
ls: cannot access idontexist: No such file or directory
</code></pre>

<p>Notice how it includes the tool name <code>ls</code>, a message string and the filename while mine does not.</p>

<p>Here's what I get if I try to use <code>mysql</code> instead. The error message looks like it's been wrapped, and now starts with a quote:</p>

<pre><code>Command:  mysql -h myhost.example.com
Expected: ERROR 2005 (HY000): Unknown MySQL server host 'myhost.example.com' (0)
Actual:   ' (0) 2005 (HY000): Unknown MySQL server host 'myhost.example.com
</code></pre>

<p>And here's my trivial ssh command that should work, or at least give a normal error message, but which instead is wrapped to start with a colon and ends with strange clobbering:</p>

<pre><code>Command:  ssh myhost
Expected: ssh: Could not resolve hostname myhost: Name or service not known
Actual:   : Name or service not knownname myhost
</code></pre>

<p>Why does this happen, and how do I fix it?</p>
","<p>I am running the startup.sh script from Dockerfile and I get the error below when I run the docker container:</p>

<pre><code>docker run -p 5308:5308 activity-logger
: not found line 2:
: not found line 5:
startup.sh: line 13: syntax error: unexpected end of file (expecting ""then"")
</code></pre>

<p>If I try to run <code>sh startup.sh</code> from my command line it seems to work well.
Any ideas?</p>

<p><strong>startup.sh</strong></p>

<pre><code>#!/bin/sh

export env_file=`echo microservice-configuration/$LS_ENVIRONMENT.environment.properties`
export startup_command=""npm run start:dist""

if [ -f $env_file ]; then
  echo ""Using environment specific configuration file $env_file""
  env $(cat $env_file | xargs) $startup_command
else
  echo ""There is no environment specific configuration file $env_file""
  $startup_command
fi
</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM node:6.9.4-alpine

# Create app directory
RUN mkdir -p /opt/app
WORKDIR /opt/app
COPY . /opt/app

RUN npm install -g yarn
RUN yarn
RUN yarn build
# This is not working very well - the dependencies cannot be installed afterwards
# RUN yarn --production

ENV NODE_ENV production

EXPOSE 5308

CMD [""/bin/sh"", ""startup.sh""]
</code></pre>
"
"36553617","How do I mount --bind inside a Docker container?","<docker><bind><mount>","41941210","Docker: unable to execute the mount command inside an ubuntu 16.10 container","<android><linux><ubuntu><docker><mount>","<p>I have this container based on <code>debian:jessie</code> (but this is not very relevant as I had the same issue with <code>alpine:3.3</code>). I get to the point where I need to</p>

<pre><code>mount --bind /htdocs/www /home/user/example.com/www
</code></pre>

<p>and I get</p>

<pre><code>mount: permission denied
</code></pre>

<p>I can't find anything in any kernel log, and <code>-vvv</code> yields nothing interesting. I obviously can do this on the host (with any other pair of subtree/node). In my example above /htdocs/www is the mountpoint of a Docker volume, but it doesn't appear like it's of any importance, as I can't <code>mount --bind</code> any pair of subtree/node inside the container.</p>
","<p>My container images is derived from ubuntu 16.10</p>

<p>When container starts, it downloads an Android rom .zip image, then extract from it new system.new.dat and convert into an .img file (so it's now containing and ext4 partition)</p>

<pre><code>system.img: Linux rev 1.0 ext4 filesystem data, 
            UUID=57f8f4bc-abf4-655f-bf67-946fc0f9f25b 
            (extents) (large files)
</code></pre>

<p>Then try to mount it (system is an actual empty but existing directory)</p>

<pre><code>sudo mount -t ext4 -o loop ./system.img /system
</code></pre>

<p>But I got this</p>

<pre><code>mount: &lt;path_to_&gt;/system: mount failed: Unknown error -1
</code></pre>

<p>I'm not aware of the cause. What can I do to mount this file?</p>
"
"37861791","How are Docker image names parsed?","<docker><docker-registry>","42115777","Parsing Docker image tag into component parts","<docker><docker-registry>","<p>When doing a <code>docker push</code> or when pulling an image, how does Docker determine if there is a registry server in the image name or if it is a path/username on the default registry (e.g. Docker Hub)?</p>

<p>I'm seeing the following from the <a href=""https://github.com/docker/docker/blob/master/image/spec/v1.1.md"" rel=""noreferrer"">1.1 image specification</a>:</p>

<blockquote>
  <p><strong>Tag</strong></p>
  
  <p>A tag serves to map a descriptive, user-given name to any single image
  ID. Tag values are limited to the set of characters [a-zA-Z_0-9]. </p>
  
  <p><strong>Repository</strong></p>
  
  <p>A collection of tags grouped under a common prefix (the name component
  before :). For example, in an image tagged with the name my-app:3.1.4,
  my-app is the Repository component of the name. A repository name is
  made up of slash-separated name components, optionally prefixed by a
  DNS hostname. The hostname must follow comply with standard DNS rules,
  but may not contain _ characters. If a hostname is present, it may
  optionally be followed by a port number in the format :8080. Name
  components may contain lowercase characters, digits, and separators. A
  separator is defined as a period, one or two underscores, or one or
  more dashes. A name component may not start or end with a separator.</p>
</blockquote>

<p>For the DNS host name, does it need to be fully qualified with dots, or is ""my-local-server"" a valid registry hostname? For the name components, I'm seeing periods as valid, which implies ""team.user/appserver"" is a valid image name. If the registry server is running on port 80, and therefore no port number is needed on the hostname in the image name, it seems like there would be ambiguity between the hostname and the path on the registry server. I'm curious how Docker resolves that ambiguity.</p>
","<p>A canonical Docker image tag is in the form:</p>

<pre><code>[[registry-address]:port/]name:tag
</code></pre>

<p>The address and port can be omitted, in which case Docker goes to the default registry which is the Docker Hub. For example the following are all valid:</p>

<pre><code>ubuntu:latest
nixos/nix:1.10
localhost:5000/myfirstimage:latest
localhost:5000/nixos/nix:latest
</code></pre>

<p>I need some code that will parse this string reliably into its component parts. However it seems impossible to do this unambiguously because the ""name"" component can contain a slash. For example the following tag is ambiguous:</p>

<pre><code>localhost/myfirstimage:latest
</code></pre>

<p>This could be an image with name <code>localhost/myfirstimage</code> on the Docker Hub, or it could be an image with the name <code>myfirstimage</code> on the registry running at address <code>localhost</code>.</p>

<p>Does anybody know how Docker itself parses such input?</p>
"
"38930903","Docker run failed with Error response from daemon","<macos><docker><docker-for-mac><docker-desktop>","42272687","docker for Mac. docker run -d -p 80:80 --name webserver nginx","<nginx><docker><webserver>","<p>I simply run the following command:</p>

<pre><code>docker run -d -p 80:80 --name webserver nginx
</code></pre>

<p>and after pulling all images returns this error:</p>

<blockquote>
  <p>docker: Error response from daemon: driver failed programming external
  connectivity on endpoint webserver
  (ac5719bc0e95ead1a4ec6b6ae437c4c0b8a9600ee69ecf72e73f8d2d12020f97): 
  Error starting userland proxy: Bind for 0.0.0.0:80:  unexpected error
  (Failure EADDRINUSE).</p>
</blockquote>

<p>Here is my docker Version info:</p>

<pre><code>Client:
Version:      1.12.0
API version:  1.24
Go version:   go1.6.3
Git commit:   8eab29e
Built:        Thu Jul 28 21:15:28 2016
OS/Arch:      darwin/amd64

Server:
Version:      1.12.0
API version:  1.24
Go version:   go1.6.3
Git commit:   8eab29e
Built:        Thu Jul 28 21:15:28 2016
OS/Arch:      linux/amd64
</code></pre>

<p>How to fix this?</p>
","<p>I am learnin ""Docker for Mac""</p>

<pre><code>$ docker run -d -p 80:80 --name webserver nginx
</code></pre>

<p>docker: Error response from daemon: driver failed programming external connectivity on endpoint webserver (a34c7702ea21c78966efa14e3b573ca150d8a7f01feebd99c02ce5fe1aaa42bb): Error starting userland proxy: Bind for 0.0.0.0:80: unexpected error (Failure EADDRINUSE).</p>

<p>anyone can help me?</p>
"
"41676835","Unable to find user root: no matching entries in passwd file in Docker","<docker><jenkins><docker-compose><bitbucket>","42466131","Cannot exec into docker container due to application","<bash><postgresql><docker>","<p>I have containers for multiple Atlassian products; <code>JIRA</code>, <code>Bitbucket</code> and <code>Confluence</code>. When I'm trying to access the running containers I'm usually using:</p>

<pre><code>docker exec -it -u root ${DOCKER_CONTAINER} bash
</code></pre>

<p>With this command I'm able to access as usual, but after running a script to extract and compress log files, I can't access that one container anymore. </p>

<h2>Excerpt from the 'clean up script'</h2>

<p>This is the first point of failure, and the script is running once each week (scheduled by Jenkins). </p>

<pre><code>docker cp ${CLEAN_UP_SCRIPT} ${DOCKER_CONTAINER}:/tmp/${CLEAN_UP_SCRIPT}
if [ $? -eq 0 ]; then
  docker exec -it -u root ${DOCKER_CONTAINER} bash -c ""cd ${LOG_DIR} &amp;&amp; /tmp/compressOldLogs.sh ${ARCHIVE_FILE}""
fi
</code></pre>

<p>When the script executes these two lines towards the Bitbucket container the result is:</p>

<pre><code>unable to find user root: no matching entries in passwd file
</code></pre>

<p>It's failing on the 'docker cp'-command, but only towards the Bitbucket container. After the script has ran, the container is unaccessible with both the 'bitbucket' (defined in Dockerfile) and 'root' users. </p>

<p>I was able to copy <code>/etc/passwd</code> out of the container, and it contains all of the users as expected. When trying to access by uid, I get the following error:</p>

<pre><code>rpc error: code = 2 desc = oci runtime error: exec failed: process_linux.go:75: starting setns process caused ""fork/exec /proc/self/exe: no such file or directory""
</code></pre>

<h2>Dockerfile for Bitbucket image:</h2>

<pre><code>FROM                        java:openjdk-8-jre

ENV BITBUCKET_HOME          /var/atlassian/application-data/bitbucket
ENV BITBUCKET_INSTALL_DIR   /opt/atlassian/bitbucket
ENV BITBUCKET_VERSION       4.12.0
ENV DOWNLOAD_URL            https://downloads.atlassian.com/software/stash/downloads/atlassian-bitbucket-${BITBUCKET_VERSION}.tar.gz

ARG user=bitbucket
ARG group=bitbucket
ARG uid=1000
ARG gid=1000

RUN mkdir -p $(dirname $BITBUCKET_HOME) \
    &amp;&amp; groupadd -g ${gid} ${group} \
    &amp;&amp; useradd -d ""$BITBUCKET_HOME"" -u ${uid} -g ${gid} -m -s /bin/bash ${user}

RUN mkdir -p                                ${BITBUCKET_HOME} \
    &amp;&amp; mkdir -p                             ${BITBUCKET_HOME}/shared \
    &amp;&amp; chmod -R 700                         ${BITBUCKET_HOME} \
    &amp;&amp; chown -R ${user}:${group}            ${BITBUCKET_HOME} \
    &amp;&amp; mkdir -p                             ${BITBUCKET_INSTALL_DIR}/conf/Catalina \
    &amp;&amp; curl -L --silent                     ${DOWNLOAD_URL} | tar -xz --strip=1 -C ""$BITBUCKET_INSTALL_DIR"" \
    &amp;&amp; chmod -R 700                         ${BITBUCKET_INSTALL_DIR}/ \
    &amp;&amp; chown -R ${user}:${group}            ${BITBUCKET_INSTALL_DIR}/

${BITBUCKET_INSTALL_DIR}/bin/setenv.sh

USER        ${user}:${group}

EXPOSE      7990
EXPOSE      7999

WORKDIR     $BITBUCKET_INSTALL_DIR
CMD         [""bin/start-bitbucket.sh"", ""-fg""]
</code></pre>

<h2>Additional info:</h2>

<ul>
<li>Docker version 1.12.0, build 8eab29e</li>
<li>docker-compose version 1.8.0, build f3628c7</li>
<li>All containers are running at all times, even Bitbucket works as usual after the issue occurres</li>
<li>The issue disappears after a restart of the container</li>
</ul>
","<p>Yesterday I restarted my docker container for my database. My applications can still connect as normal however:</p>

<p>The following line no-longer works</p>

<pre><code>docker exec -i -t database_1 bash
</code></pre>

<p>Instead it gives me this error:</p>

<pre><code>unable to find user postgres: no matching entries in passwd file
</code></pre>

<p>I don't know why it's not just giving me bash</p>
"
"42293193","RSelenium on docker: where are files downloaded?","<r><selenium><docker><containers><rselenium>","42607389","download file with Rselenium & docker toolbox","<r><docker><download><rselenium>","<p>I am using Selenium using a docker image:</p>

<pre><code>require(RSelenium)

  if (length(system(""docker ps -l"", intern = TRUE))&lt;2)
    try({system(""docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.0"")})
</code></pre>

<p>It works, I can connect to any url and navigate. However when I click a button to download a file, it sometimes saves it (partially, saved as <code>xxxxxxx.csv.part</code>) to <code>/tmp/mozilla_mozillaUser0</code>, and sometimes to ... nowhere, or maybe another location I cannot find... </p>

<p>Is there a reason for that?</p>

<p>Also I tried to open the driver using </p>

<pre><code>makeFirefoxProfile(list(browser.download.dir = ""D:/temp""))
</code></pre>

<p>but it returns a weird error</p>

<pre><code>Error in file(tmpfile, ""rb"") : cannot open the connection
In addition: Warning messages:
1: running command '""zip"" -r9Xjq ""C:\Users\rocks\AppData\Local\Temp\RtmpoPhjUb\file31076202d4f.zip"" ""C:\Users\rocks\AppData\Local\Temp\RtmpoPhjUb/firefoxprofile/prefs.js"" ' had status 127 
2: In file(tmpfile, ""rb"") :
  cannot open file 'C:\Users\rocks\AppData\Local\Temp\RtmpoPhjUb\file31076202d4f.zip': No such file or directory
</code></pre>

<p>where I can understand why this desn't work given that all links are in windows but my selenium runs in a docker container or Ubuntu.</p>

<p>My Setup: R running on Windows, and I have a docker image of Ubuntu that contains the selenium server.</p>

<p><strong>EDIT:</strong> <em>turns out my issue was because firefox was not installed on the host machine</em> (which is not needed in theory).
I fixed the issue by using (instead of invoking <code>makeFirefoxProfile</code>):</p>

<pre><code>fprof = structure(list(firefox_profile = ""UEsDBBQAAAAIANJiVEobimJN8QAAABkCAAAIAAAAcHJlZnMuanOFkT9PwzAQxXckvkOUCSQnlliZkDqygcSITHyJTW3fyXdNIj49btqBoLTd7t57v/OfOzDkT8rQP9RfGafStRanFNDY1vpcq6rWDiNohnDM6t3Z5frx+f7uGt5jsJBfPUuZ8nQrHU0yQxHY4fThIL2JyeLTUNjeBIYLvINAkF+IuE0wlor3LZsR3nHneX+8fjwE8VSG6bn58aQMUfCdEY9J/+tPiabDWA5hBrs2LxmRJ8xrqeNRCcyyFD6Wl2lKQ3UuvwkGVf3Nk+1VtQBOYjihFIxPqlrlYO4grNExlc/jZsOZt8XIGzJ2AtKwZDBx2ewvUEsBAj8AFAAAAAgA0mJUShuKYk3xAAAAGQIAAAgAJAAAAAAAAAAgAAAAAAAAAHByZWZzLmpzCgAgAAAAAAABABgAzaOo9TCL0gHdkgMtLYvSAd2SAy0ti9IBUEsFBgAAAAABAAEAWgAAABcBAAAAAA==""), .Names = ""firefox_profile"")
remDr &lt;- remoteDriver(extraCapabilities = ePrefs, port = 4445)
</code></pre>
","<p>I m trying to download files by Rselenium but it looks impossible.I don't arrive to download even with an easy example:</p>

<h3>1) i have installed docker toolbox (<a href=""https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-docker.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-docker.html</a>)</h3>

<h3>2) i ran the firefox standalone image : 3.1.0 and now i m testing the older 2.52.0</h3>

<h3>3) i have installed the rselenium package on My R X64 3.3.2 and i read all the questions &amp; answers on stackoverflow</h3>

<h3>4) i have tried the following code, by the way, when i analyse the firefox options about:config , i don't find the ""browser.download.dir"" options:</h3>

<pre><code>require(RSelenium)
fprof &lt;- makeFirefoxProfile(list(browser.download.dir = ""C:/temp""
                             ,  browser.download.folderList = 2L
                             , browser.download.manager.showWhenStarting     = FALSE
                             , browser.helperApps.neverAsk.saveToDisk =  ""application/zip""))
remDr &lt;- remoteDriver(browserName = ""firefox"",remoteServerAddr = ""192.168.99.100"",port = 4445L,extraCapabilities = fprof)
remDr$open(silent = TRUE)
remDr$navigate(""https://www.chicagofed.org/applications/bhc/bhc-home"")
# click year 2012
webElem &lt;- remDr$findElement(""name"", ""SelectedYear"")
webElems &lt;- webElem$findChildElements(""css selector"", ""option"")
webElems[[which(sapply(webElems, function(x){x$getElementText()}) == ""2012"" )]]$clickElement()

# click required quarter

webElem &lt;- remDr$findElement(""name"", ""SelectedQuarter"")
Sys.sleep(1)
webElems &lt;- webElem$findChildElements(""css selector"", ""option"")
webElems[[which(sapply(webElems, function(x){x$getElementText()}) == ""4th Quarter"" )]]$clickElement()

# click button

webElem &lt;- remDr$findElement(""id"", ""downloadDataFile"")
webElem$clickElement()
</code></pre>

<h3>6) i have no error but i have no file</h3>

<h3>7) At the end , i would like to download the excel file on this page by Rselenium:</h3>

<p>[link]<a href=""https://app2.msci.com/products/indexes/performance/country_chart.html?asOf=Feb%2028,%202010&amp;size=30&amp;scope=C&amp;style=C&amp;currency=15&amp;priceLevel=0&amp;indexId=83#"" rel=""nofollow noreferrer"">https://app2.msci.com/products/indexes/performance/country_chart.html?asOf=Feb%2028,%202010&amp;size=30&amp;scope=C&amp;style=C&amp;currency=15&amp;priceLevel=0&amp;indexId=83#</a></p>
"
"5274294","How can you run a command in bash over and over until success?","<bash><command><while-loop>","61734089","Docker bash script wait for container log to contain ""done""","<bash><docker>","<p>I have a script and want to ask the user for some information, but the script cannot continue until the user fills in this information. The following is my attempt at putting a command into a loop to achieve this but it doesn't work for some reason:</p>
<pre class=""lang-sh prettyprint-override""><code>echo &quot;Please change password&quot;
while passwd
do
    echo &quot;Try again&quot;
done
</code></pre>
<p>I have tried many variations of the while loop:</p>
<pre class=""lang-sh prettyprint-override""><code>while `passwd`
while [[ &quot;`passwd`&quot; -gt 0 ]]
while [ `passwd` -ne 0 ]]
# ... And much more
</code></pre>
<p>But I can't seem to get it to work.</p>
","<p>I'm looking to write a bash script that will not progress until the log of a container states ""[services.d] done.""</p>

<p>Example:</p>

<pre><code>#!/bin/bash

while [ docker logs container | grep ""[services.d] done."" ] = ""false"";
do
    sleep 1
    echo ""working...""
done
</code></pre>
"
"6865538","Solving a ""communications link failure"" with JDBC and MySQL","<java><mysql><jdbc>","61489956","Can't connect Java/Tomcat app to Docker/MySQL","<java><mysql><docker><tomcat><jdbc>","<p>I'm trying to connect to the local MySQL server but I keep getting an error.</p>

<p>Here is the code.</p>

<pre><code>public class Connect {

    public static void main(String[] args) {
        Connection conn = null;

        try {
            String userName = ""myUsername"";
            String password = ""myPassword"";

            String url = ""jdbc:mysql://localhost:3306/myDatabaseName"";
            Class.forName(""com.mysql.jdbc.Driver"").newInstance();
            conn = DriverManager.getConnection(url, userName, password);
            System.out.println(""Database connection established"");
        } catch (Exception e) {
            System.err.println(""Cannot connect to database server"");
            System.err.println(e.getMessage());
            e.printStackTrace();
        } finally {
            if (conn != null) {
                try {
                    conn.close();
                    System.out.println(""Database Connection Terminated"");
                } catch (Exception e) {}
            }
        }
    }
}
</code></pre>

<p>and the errors :</p>

<pre><code>Cannot connect to database server
Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
        at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1116)
        at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:344)
        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2333)
        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2370)
        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2154)
        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:792)
        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:381)
        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:305)
        at java.sql.DriverManager.getConnection(DriverManager.java:582)
        at java.sql.DriverManager.getConnection(DriverManager.java:185)
        at Connect.main(Connect.java:16)
    Caused by: java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at java.net.Socket.connect(Socket.java:478)
        at java.net.Socket.&lt;init&gt;(Socket.java:375)
        at java.net.Socket.&lt;init&gt;(Socket.java:218)
        at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:257)
        at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:294)
        ... 15 more
</code></pre>

<p>I've set the classpath, made sure my.cnf had the skip network option commented out. </p>

<p>java version is 1.2.0_26 (64 bit)
mysql 5.5.14
mysql connector 5.1.17</p>

<p>I made sure that the user had access to my database.</p>
","<p>Can't seem to get my Tomcat to connect to my Docker container.</p>

<p>Logs are spitting out the following at the root of the stack trace:</p>

<pre><code>Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at com.mysql.jdbc.Util.getInstance(Util.java:408)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:919)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:898)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:887)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:861)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2095)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2020)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:768)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:385)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:323)
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:117)
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:123)
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:196)
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467)
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541)
    ... 176 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at sun.reflect.GeneratedConstructorAccessor66.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990)
    at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:335)
    at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2187)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2036)
    ... 192 more
Caused by: java.net.UnknownHostException: mysql
    at java.net.InetAddress.getAllByName0(InetAddress.java:1280)
    at java.net.InetAddress.getAllByName(InetAddress.java:1192)
    at java.net.InetAddress.getAllByName(InetAddress.java:1126)
    at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:188)
    at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:299)
    ... 194 more
</code></pre>

<p>Setup:</p>

<ul>
<li>Windows 10</li>
<li>JDK/JRE 1.8</li>
<li>Tomcat 7</li>
<li>Docker 19</li>
<li>MySQL 5.7</li>
</ul>

<p>Configuration:</p>

<pre><code>database.jdbcUrl=jdbc:mysql://internal-mysql:3307/app_main?characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;sendFractionalSeconds=false&amp;useSSL=false&amp;rewriteBatchedStatements=true
</code></pre>

<p>docker-compose.yml:</p>

<pre><code>  mysql:
    restart: always
    image: ""percona:5.7""
    ports:
      - ""3307:3307""
    environment:
      MYSQL_USER: root
      MYSQL_PASSWORD: password
    volumes:
      - ./mysql/conf:/etc/mysql:ro
      - ./mysql/initdb.d:/docker-entrypoint-initdb.d:rw
    healthcheck:
      test: [""CMD-SHELL"", ""mysqlshow --host=127.0.0.1 --port 3307 --user=root--password=password app_main""]
      interval: 15s
      timeout: 1s
      retries: 5
      start_period: 30s
    networks:
      default:
        aliases:
          - internal-mysql
</code></pre>

<p>Miscellaneous:</p>

<ul>
<li>Tried setting the my.cnf setting <code>bind-address = 0.0.0.0</code> and <code>skip-networking</code> is not set.</li>
<li>Windows Firewalls are off.</li>
</ul>

<p><strong>Edit</strong>: I am able to connect to the Docker/MySQL container perfectly fine through a standalone program like HeidiSQL or Workbench via <code>internal-mysql:3307</code>.  So it seems the connection issues lie solely on the Java and/or Tomcat side and not the container.</p>
"
"21553353","What is the difference between CMD and ENTRYPOINT in a Dockerfile?","<docker>","61503220","Dockerfile: when to use CMD or ENTRYPOINT","<docker><dockerfile><containers>","<p>In Dockerfiles there are two commands that look similar to me: <code>CMD</code> and <code>ENTRYPOINT</code>. But I guess that there is a (subtle?) difference between them - otherwise it would not make any sense to have two commands for the very same thing.</p>

<p>The documentation states for <code>CMD</code></p>

<blockquote>
  <p>The main purpose of a CMD is to provide defaults for an executing container.</p>
</blockquote>

<p>and for <code>ENTRYPOINT</code>:</p>

<blockquote>
  <p>An ENTRYPOINT helps you to configure a container that you can run as an executable.</p>
</blockquote>

<p>So, what's the difference between those two commands?</p>
","<p>I am new to Docker and trying to write some <code>Dockerfile</code>.</p>

<p>file1 is:</p>

<pre><code>FROM ruby-alpine

RUN apk build-base ruby-nokogirl

CMD ['rspec']
</code></pre>

<p>and file 2 is:</p>

<pre><code>FROM ruby-alpine
RUN apk build-base ruby-nokogirl

ENTRYPOINT ['rspec']
</code></pre>

<p>However, both works fine.
But I got a confusion on when should I use <code>CMD</code> command and when should I use <code>ENTRYPOINT</code>?</p>

<p>Any suggestion?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61533423","Communication between Docker Application and localhost application","<ruby-on-rails><ruby><docker><tcp>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have an application running inside Docker(M1). And an another application running on localhost(M2-Not Dockerized). Both are Ruby on Rails applications running on ports 3000 and 3001. I am calling M2 from inside of M1 by using </p>

<p><code>response = http_client.post(""127.0.0.1:3001"", query, {}, options)</code></p>

<p>But I keep on getting</p>

<p><code>Error: Failed to open TCP connection to 127.0.0.1:3001 (Connection refused - connect(2) for ""127.0.0.1"" port 3001), Backtrace: [""/usr/local/lib/ruby/2.5.0/net/http.rb:939:in `rescue in block in connect'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:936:in `block in connect'"", ""/usr/local/lib/ruby/2.5.0/timeout.rb:93:in `block in timeout'"", ""/usr/local/lib/ruby/2.5.0/timeout.rb:103:in `timeout'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:935:in `connect'""</code></p>

<p>If i use - </p>

<p><code>response = http_client.post(""localhost:3001"", query, {}, options)</code></p>

<p>then i get </p>

<p><code>Error: Failed to open TCP connection to localhost:3001 (Cannot assign requested address - connect(2) for ""localhost"" port 3001), Backtrace: [""/usr/local/lib/ruby/2.5.0/net/http.rb:939:in `rescue in block in connect'"", ""/usr/local/lib/ruby/2.5.0/net/http.rb:936:in `block in connect'""</code></p>

<p>So it changed from Connection refused to Cannot assign requested address.</p>

<p>I am able to access both the applications by calling localhost:3000 and localhost:3001 from the browser individually. I am assuming it is happening because of a Dockerized application calling a Non-Dockerized one. Can someone suggest how to establish the connection between the two, I have tried many things. I am running Docker-2.1.0.5 on macOS Mojave 10.14.6 and Ruby 2.5.0. Let me know if any other information is required.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61561009","Docker How to allow access to local ports in container?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have two containers in Docker. And I can connect to their ports from local machine. But I can't connect from one Docker to another by local network, Docker container doesn't see ports of other containers.</p>

<p>All work if I use <code>network_mode: host</code>. But it create other problems.</p>

<p>How I can allow access to ports of one Docker container for other containers? WITHOUT USE <code>host.docker.internal</code>. Can I use 127.0.0.1? I have modules, which need only 127.0.0.1.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61391498","Connection between node.js docker container and local redis server(127.0.0.1)","<docker><redis>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>i'm running a node.js application in a docker container. Here is my docker-compose file - </p>

<pre><code>version: '3.7'
services:
  notification-app:
    container_name: ${CONTAINER_NAME}
    build:
      context: ./
      dockerfile: ./Dockerfile
      args:
        - APP_ENV=${APP_ENV}
    ports:
      - ${EXPORTED_PORT}:${APP_PORT}
    environment:
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
</code></pre>

<p>And here is my .env file - </p>

<pre><code>CONTAINER_NAME=sheba_socket

APP_ENV=development
APP_PORT=3000
EXPORTED_PORT=3000

REDIS_HOST=localhost
REDIS_PORT=6379
</code></pre>

<p>I'm running a REDIS SERVER locally (No container, directly on machine).
When i built and run this container, i found this error.</p>

<pre><code>[ioredis] Unhandled error event: Error: getaddrinfo ENOTFOUND redis
    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:64:26)
</code></pre>

<p>I think, the problem is here <strong>REDIS_HOST=localhost</strong>.
I am not sure how to define <strong>redis host</strong>.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61661204","How to send http request from docker container to the local machine?","<java><docker><playframework><netty>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have the web application which is running on the docker container and that one must send requests to my local machine (localhost) and when I'm trying to do that I get the following exception:</p>

<pre><code>java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9033
</code></pre>

<p>I'm trying to give you some more details. I looked at this <a href=""https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach"">post</a> but it does not work for me. <br/>
First of all, I want to explain my project's structure:
1. I have a project that consist of two parts: frontend (angular 9) and backend (play 2.8.x framework)
2. I have another one project (web-service)</p>

<p>My first projects (backend, frontend) are placing in the docker container and the second (web-service) is placing at the localhost. I have the following flow between these projects:<br/>
1. The user sends a request (from the web browser) to the backend it may be login request for instance.<br/>
2. The backend process this request (retrieve data from the database) and sends these data to the web-service which is placing on the localhost.
<br/>
and I have a problem with step 2 where I trying to send data to the web-service. I'm getting the exception which was writing above. <br/>
I'm trying to set <code>--network=host</code> but in this case, I get <code>404 status code</code> in the browser.
How can I solve this issue?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61769017","connect php docker container to existing apache and mysql (both are not in a container)","<php><apache><docker><subdomain>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a server that does not use any docker containers at the moment. For testing I would like to introduce a php container that is working with the already present apache installation and the mysql database. I know that apache and mysql could live inside of docker containers as well, but I would like to only have a php container. The application (Laravel based) should be accessible via a subdomain.</p>

<p>The Dockerfile looks like this:</p>

<pre><code>FROM php:7.2-fpm-alpine
RUN docker-php-ext-install pdo pdo_mysql
</code></pre>

<p>Is something like this possible? I searched the internet, but everywhere I looked docker containers are usually used for mysql as well.</p>
"
"25444099","Why docker has ability to run different linux distribution?","<linux><docker>","61742110","How does host kernel supports various docker container OS flavours ? How kernel sharing happens?","<linux><docker><kernel><containers><lxc-docker>","<p>We can use docker to pull different images. And these images are different linux distribution.
But no matter which linux distro docker is running on, docker can run these different linux distribution just like in a virtual machine.</p>

<p>I know docker uses <a href=""http://en.wikipedia.org/wiki/Aufs"" rel=""noreferrer"">aufs</a> to control different read-write access level. So it can reuse some file on the host machine. But how can docker run <code>apt-get</code> in a container when my host runs <code>arch linux</code>? Does the image contain the <code>apt-get</code> binary? But different linux distribution have different libs and software version. Even the configuration file are different.How can docker ""run"" ubuntu in a arch linux?</p>
","<p>On a docker host, we can run multiple containers with different OS flavours<br>
For instance Ubuntu host, Fedora container, Centos container,... How Ubuntu kernel supports to Fedora OS and CentOS ? </p>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","61401777","Two docker images into a single docker image","<mongodb><docker><rabbitmq>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I have a requirement to create a single docker image by merging both ""RabbitMQ-management"" and ""MongoDB"" images. Is it possible to do it? </p>

<p>I've tried with below Dockerfile and can able to access RabbitMQ, but mongodb is not working. Could you please help me. </p>

<pre><code># cat Dockerfile 
FROM rabbitmq:3.7.17

RUN rabbitmq-plugins enable --offline rabbitmq_management

# extract ""rabbitmqadmin"" from inside the ""rabbitmq_management-X.Y.Z.ez"" plugin zipfile
# see https://github.com/docker-library/rabbitmq/issues/207
RUN apt-get update &amp;&amp; apt-get install -y gnupg2
RUN gpg --keyserver ha.pool.sks-keyservers.net --recv-keys 0C49F3730359A14518585931BC711F9BA15703C6 &amp;&amp; \
gpg --export $GPG_KEYS &gt; /etc/apt/trusted.gpg.d/mongodb.gpg
ARG MONGO_PACKAGE=mongodb-org
ARG MONGO_REPO=repo.mongodb.org
ENV MONGO_PACKAGE=${MONGO_PACKAGE} MONGO_REPO=${MONGO_REPO}
ENV MONGO_MAJOR 3.4
ENV MONGO_VERSION 3.4.18
RUN echo ""deb http://$MONGO_REPO/apt/debian jessie/${MONGO_PACKAGE%-unstable}/$MONGO_MAJOR main"" | tee ""/etc/apt/sources.list.d/${MONGO_PACKAGE%-unstable}.list""
RUN echo ""/etc/apt/sources.list.d/${MONGO_PACKAGE%-unstable}.list""
RUN apt-get update
RUN apt-get install -y ${MONGO_PACKAGE}=$MONGO_VERSION
RUN set -eux; \
        erl -noinput -eval ' \
                { ok, AdminBin } = zip:foldl(fun(FileInArchive, GetInfo, GetBin, Acc) -&gt; \
                        case Acc of \
                                """" -&gt; \
                                        case lists:suffix(""/rabbitmqadmin"", FileInArchive) of \
                                                true -&gt; GetBin(); \
                                                false -&gt; Acc \
                                        end; \
                                _ -&gt; Acc \
                        end \
                end, """", init:get_plain_arguments()), \
                io:format(""~s"", [ AdminBin ]), \
                init:stop(). \
        ' -- /plugins/rabbitmq_management-*.ez &gt; /usr/local/bin/rabbitmqadmin; \
        [ -s /usr/local/bin/rabbitmqadmin ]; \
        chmod +x /usr/local/bin/rabbitmqadmin; \
        apt-get update; apt-get install -y --no-install-recommends python ca-certificates; rm -rf /var/lib/apt/lists/*; \
        rabbitmqadmin --version

EXPOSE 15671 15672 27017
</code></pre>

<p>I'm using below command to run RabbitMQ (it is working)</p>

<pre><code># docker run -it -p 15672:15672 -p 5672:5672  --hostname my-rabbitmq image
</code></pre>

<p>Command to start Mongodb container (it is not working)</p>

<pre><code>docker run -d -v /tmp/mongodb:/data/db -p 27017:27017 image mongod
</code></pre>
"
"27563460","Let two Containers getting linked to eachother","<docker>","61650502","How to establish comminication between microservices in different docker containers?","<python><linux><docker><flask><microservices>","<p>I have a couple of docker Containers and one special case that two of them have to talk to each other, so they must know each other at best via link.
I can link one Container to the other but the problem is, that i can't tell them that the second one can talk back to the first.</p>

<p>I tried to create and run the first container and stopped it, then i created started the second container and stopped it as well. Next up i started the first container again with the link to the second and started the second one linked to the first.
After this my machine went crazy the docker process took all CPU and Memory and neither of the containers was accessible. When you killed the process a new one poped with the same.
Even when i deinstalled docker restarted the machine and installed docker again it get back to to the crazy state without even starting one of the containers.</p>

<p>Got anyone a solution how to link two containers to each other or let them talk to each other in both directions? </p>
","<p>I have two microservices made using Flask, running on localhost:3000 and localhost:5000, they work fine and communicate well on local machine but after containerizing them I'm getting below error.</p>

<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=3000): Max retries exceeded with url: /update (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2a08eaa850&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
</code></pre>

<p>The steps I followed for containerization are as below.    </p>

<pre><code>$ sudo docker build -t history:0.1  
$ sudo docker build -t calculation:0.1  
$ sudo docker run -d -p 0.0.0.0:3000:3000 --name history history:0.1  
$ sudo docker run -d -p 0.0.0.0:5000:5000 --name calculation calculation:0.1  
$ curl -X GET 0.0.0.0:3000/hist  
{  
""hist"": []  
}  
$ curl -X GET 0.0.0.0:5000/sum/5/6  

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2309, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2295, in wsgi_app
    response = self.handle_exception(e)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1741, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.8/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.8/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/app/calculate.py"", line 15, in add
    update(a,b,ans,'+')
  File ""/app/calculate.py"", line 10, in update
    requests.get('http://localhost:3000/update',json={""a"":a,""b"":b,""ans"":ans,""operator"":operator})
  File ""/usr/local/lib/python3.8/site-packages/requests/api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/sessions.py"", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/sessions.py"", line 637, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=3000): Max retries exceeded with url: /update (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2a08904040&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))

--&gt;

</code></pre>

<p>Here calculation service is calling the history service to update the data in history.</p>

<p>I am new to docker. How can I solve it?</p>
"
"28302178","How can I add a volume to an existing Docker container?","<docker>","61620738","docker: make folder of running container persistent","<docker><docker-compose>","<p>I have a Docker container that I've created simply by installing Docker on Ubuntu and doing:</p>
<pre><code>sudo docker run -i -t ubuntu /bin/bash
</code></pre>
<p>I immediately started installing Java and some other tools, spent some time with it, and stopped the container by</p>
<pre><code>exit
</code></pre>
<p>Then I wanted to add a volume and realised that this is not as straightforward as I thought it would be. If I use <code>sudo docker -v /somedir run ...</code> then I end up with a fresh new container, so I'd have to install Java and do what I've already done before just to arrive at a container with a mounted volume.</p>
<p>All the documentation about mounting a folder from the host seems to imply that mounting a volume is something that can be done when creating a container. So the only option I have to avoid reconfiguring a new container from scratch is to commit the existing container to a repository and use that as the basis of a new one whilst mounting the volume.</p>
<p>Is this indeed the only way to add a volume to an existing container?</p>
","<p>In <code>docker-compose.yml</code> we are able to define persistent volumes:</p>

<pre><code>volumes:
  -foo:bar
</code></pre>

<p>For this to work, the container has to be rebuild.</p>

<p>Now I need to ""save"" the data on a non-persistent folder in a running container. Is there a way to do this. </p>

<p>Can I add a persistent volume to an existing and mounted container?</p>
"
"29261811","Use docker-compose env variable in Dockerbuild file","<docker><docker-compose>","61764279","Accessing docker compose arm variable in Docker file","<node.js><docker><docker-compose><swarm>","<p>Having the following docker-compose file:</p>

<pre><code>db:
    build: .
    environment:
        - MYSQL_ROOT_PASSWORD=password
        - ENV=test
    env_file: .env
</code></pre>

<p>Is there any way to use the env variables declared in docker-compose.yml (either as environment or declared in the env_file) as part of Dockerfile without declaring them in the Dockerfile? Something like this:</p>

<pre><code>FROM java:7
ADD ${ENV}/data.xml /data/
CMD [""run.sh""]
</code></pre>
","<p>My Docker compose file is as follows</p>

<pre class=""lang-yaml prettyprint-override""><code>
  environment:
    external:
      name: dev
services:
   app:
      deploy:
         replicas: 1
      environment:
         NODE_ENV: development

</code></pre>

<p>Compose Swarm is as follows:</p>

<pre class=""lang-yaml prettyprint-override""><code>
version: ""3.3""

services:
  app:
    image: ""${IMAGE_NAME}""
    networks:
      - environment
    environment:
      NODE_ENV: ${ENVIRONMENT}
      PORT: 8080
    deploy:
      resources:
        limits:
          cpus: ""0.20""
          memory: 1500M
        reservations:
          cpus: ""0.20""
          memory: 100M
</code></pre>

<p>My docker file is as follows:</p>

<pre><code>FROM node:10.16.3-alpine
USER node
RUN npm install
RUN pwd
RUN ls  -lah

RUN npm run buildconfigs ${NODE_ENV}

EXPOSE 8080

CMD [""node"", ""app.js""]
`
</code></pre>

<p>I would like my RUN npm run buildconfigs ${NODE_ENV} in the docker file to be replaced to </p>

<p>RUN npm run buildconfigs development. </p>

<p>I am not sure why ${NODE_ENV} is not getting picked up from compose file. Would appreciate a direction on  how to capture compose file variable in the docker file. Thank you</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","61395555","I have a web app running inside a docker container, how do I access it on localhost?","<python><docker><flask><docker-compose><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a python flask web app running inside a docker container, how do I access it on my host?</p>

<p>When I run the up, it is ready and listening on port 5000 inside the container, but I want this exposed so I can access it
xxxsssddd  | 2020-04-23 19:28:48,416  * Running on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</p>

<p>below is my docker-compose.yml</p>

<pre><code>version: '2.2'

services:
    mywebapp:
        build:
            context: .
            dockerfile: mywebabb.yml
        ports:
            - 5000:5000
        networks:
            default:

        volumes:
            - .:/opt/mywebapp
</code></pre>
"
"31153394","Check Resources Used by each Docker Container","<ubuntu><docker><ubuntu-14.04>","61729268","Docker-compose: show containers usage of resources","<docker><docker-compose>","<p>How do you check the amount of resources (CPU, memory etc) being used by each Docker container that is running on the (Ubuntu) server?</p>
","<p>How to see resource usage by docker containers?</p>

<p>I am using docker-compose version 2.1 and 2.4.
I would like to see running containers and their resources usage: RAM, CPU.
<code>docker-compose ps</code> returns containers, images, but not resources.</p>

<pre><code>$ docker-compose ps
             Name                           Command                  State                                  Ports
--------------------------------------------------------------------------------------------------------------------------------------------
service-1                        /app.sh                         Up (healthy)     8001/tcp
service-2                        /app.sh                         Up (healthy)     0.0.0.0:80-&gt;4000/tcp
kafka                            /etc/confluent/docker/run       Up               0.0.0.0:29092-&gt;29092/tcp, 0.0.0.0:7071-&gt;7071/tcp, 9092/tcp
mongo                            /init.sh                        Up (unhealthy)   0.0.0.0:27017-&gt;27017/tcp
zookeeper                        /etc/confluent/docker/run       Up               2181/tcp, 2888/tcp, 0.0.0.0:32181-&gt;32181/tcp, 3888/tcp

</code></pre>
"
"32750748","How to edit files in stopped/not starting docker container","<bash><docker>","61724371","Change file contents within a stopped Apache Docker container","<apache><docker><httpserver>","<p>Trying to fix errors and debug problems with my application that is split over several containers, I frequently edit files in containers:</p>

<ul>
<li><p>either I am totally lazy and install nano and edit directly in container or</p></li>
<li><p>I docker cp the file out of the container, edit it, copy it back and restart the container</p></li>
</ul>

<p>Those are intermediate steps before coming to new content for container build, which takes a lot longer than doing the above (which of course is only intermediate/fiddling around).</p>

<p>Now I frequently break the starting program of the container, which in the breaking cases is either a node script or a python webserver script, both typically fail from syntax errors.</p>

<p>Is there any way to save those containers? Since they do not start, I cannot docker exec into them, and thus they are lost to me. I then go the rm/rmi/build/run route after fixing the offending file in the build input.</p>

<p><strong>How can I either edit files in a stopped container, or cp them in or start a shell in a stopped container - anything that allows me to fix this container?</strong></p>

<p>(It seems a bit like working on a remote computer and breaking the networking configuration - connection is lost ""forever"" this way and one has to use a fallback, if that exists.)</p>

<p><a href=""https://stackoverflow.com/questions/24553790/how-to-edit-docker-container-files-from-the-host"">How to edit Docker container files from the host?</a> looks relevant but is outdated.</p>
","<p>I was running an Apache HTTP server from a Docker container. I made a few changes to the httpd.conf file (using docker exec -it and then editing the file) and saved it. When I restarted the Docker container from host OS, the container is failed owing to syntax errors on the httpd.conf file.</p>

<p>Now, I cannot enter the container through docker exec -it (since it is in stopped state) and correct my errors! How do I edit the file again to correct the errors? Or is there a way to revert the changes I made?</p>
"
"34911622","Dockerfile - set ENV to result of command","<dockerfile><env>","61266261","Run shell command while setting ENV in dockerfile","<docker><environment-variables><dockerfile><export>","<p>Is it possible to set a docker ENV variable to the result of a command?
Like:</p>

<pre><code>ENV MY_VAR whoami
</code></pre>

<p>i want MY_VAR to get the value ""root"" or whatever whoami returns</p>
","<p>Inside my dockerfile:</p>

<pre><code>
ENV MY_ENCODED_VALUE=""bXkgbmFtZSBpcyByYWtpYgo=""

ENV MY_DECODED_VALUE=$(echo $MY_ENCODED_VALUE | base64 -d)

</code></pre>

<p>in the second line, i want to decode the encoded value and put the decoded value into my environment variable.</p>

<hr>

<p>But i am getting the following error</p>

<p><code>Error response from daemon: failed to parse dockerfile: Syntax error - can't find = in ""$MY_ENCODED_VALUE"". Must be of the form: name=value</code></p>

<hr>

<p>What does it even mean? What's supposed to be the right syntax here?</p>
"
"35293574","Can I run Android Studio (Android SDK emulator) in a Microsoft hyper-v virtual machine?","<android><android-studio><android-emulator><hyper-v>","61395720","Not possible to run Android Studio and Docker on Windows because of Hyper V","<android><windows><docker><hyper-v>","<p>Can I run Android Studio and Android SDK emulator in a Microsoft hyper-v virtual machine? Please read carefully. </p>

<p>I already use Hyper-V a lot for other purposes. Now I need to develop a app for Android. </p>

<p>I've installed a new virtual machine (windows 10) and installed Android Studio. I cannot run the android emulator because it's lacking the 'intel HAXM software'. </p>

<p>I read a lot about that it's not possible to install Hyper-V NEXT to Android Studio, but non of the post actually say anything about installing in a Hyper-V machine. </p>
","<p>So, I am having an issue with running both, Android Studio and Docker, because Docker needs Hyper V and Android Studio won't launch a virtual device when Hyper V is active.
Has anyone else experienced this?</p>

<p><a href=""https://i.stack.imgur.com/0pnTb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0pnTb.png"" alt=""This is my Android Studio error, when Hyper V is activated""></a></p>

<p>EDIT:
Virtualization is activated.</p>

<p><a href=""https://i.stack.imgur.com/sUvWh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sUvWh.png"" alt=""enter image description here""></a></p>
"
"37584961","How to encrypt docker images or source code in docker images?","<encryption><docker>","61426490","encrypt access to docker image","<docker><encryption><containers>","<p>Say I have a docker image, and I deployed it on some server. But I don't want other user to access this image. Is there a good way to encrypt the docker image ? </p>
","<p>I'm relatively new to docker and created an image of my dash app and pushed it to my private repository. I would like to secure my app/ python script as one can easy pull the image from my private repository and run it.</p>

<p>Is there some easy way to do so (besides creating a private registry and so forth)? </p>

<ul>
<li>I thought about mounting a text file that contains some predefined credentials. However, as my app is really simple and runs locally there is no other way than hardcoding and comparing it with the submitted credentials from the txt file? Or is this complete non-sense? Is one able to inspect the code of my python script via docker inspect/ or creating an interactive shell and investigating all files? </li>
<li>Another thought: Is it possible to require some certificate from the user via certbot? How would this look like?</li>
</ul>

<p>Are there other ways to encrypt access to a docker image?</p>
"
"39223249","Multiple RUN vs. single chained RUN in Dockerfile, which is better?","<docker><dockerfile>","61438916","DockerFile one-line vs multi-line instruction","<docker><dockerfile>","<p><code>Dockerfile.1</code> executes multiple <code>RUN</code>:</p>
<pre><code>FROM busybox
RUN echo This is the A &gt; a
RUN echo This is the B &gt; b
RUN echo This is the C &gt; c
</code></pre>
<p><code>Dockerfile.2</code> joins them:</p>
<pre><code>FROM busybox
RUN echo This is the A &gt; a &amp;&amp;\
    echo This is the B &gt; b &amp;&amp;\
    echo This is the C &gt; c
</code></pre>
<p>Each <code>RUN</code> creates a layer, so I always assumed that fewer layers is better and thus <code>Dockerfile.2</code> is better.</p>
<p>This is obviously true when a <code>RUN</code> removes something added by a previous <code>RUN</code> (i.e. <code>yum install nano &amp;&amp; yum clean all</code>), but in cases where every <code>RUN</code> adds something, there are a few points we need to consider:</p>
<ol start=""0"">
<li><p>Layers are supposed to just add a diff above the previous one, so if the later layer does not remove something added in a previous one, there should not be much disk space saving advantage between both methods.</p>
</li>
<li><p>Layers are pulled in parallel from Docker Hub, so <code>Dockerfile.1</code>, although probably slightly bigger, would theoretically get downloaded faster.</p>
</li>
<li><p>If adding a 4th sentence (i.e. <code>echo This is the D &gt; d</code>) and locally rebuilding, <code>Dockerfile.1</code> would build faster thanks to cache, but <code>Dockerfile.2</code> would have to run all 4 commands again.</p>
</li>
</ol>
<p>So, the question: <strong>Which is a better way to do a Dockerfile?</strong></p>
","<p>To my knowledge of the way docker build works is that for each line of instruction, it creates a separate image/layer. However, it is very efficient in managing to reuse the layers or avoid rebuilding those layers if nothing has changed.</p>

<p>So does it matter if I put below instruction either on same line or multi-line? For convenience, I would prefer the single line option unless it is not an efficient option.</p>

<p><strong>Multi-Line Instruction</strong></p>

<pre><code>RUN apt-get -y update
RUN apt-get -y install ...
</code></pre>

<p><strong>Single-Line Instruction</strong></p>

<pre><code>RUN apt-get -y update &amp;&amp; apt-get -y install
</code></pre>
"
"41267305","Docker for Mac VM IP","<macos><docker><docker-machine>","61683926","default docker-machine ip","<macos><docker>","<p>I just migrated to using Docker for Mac, from previously using Docker Toolbox with virtualbox for OSX.</p>

<p>I used to get the machine IP address with <code>$(docker-machine ip default)</code>.</p>

<p>Is there a reliable way to get the Hyperkit IP address?</p>

<p>Thanks!</p>
","<p>normally, the default docker-machine ip is 192.168. 99.100. I work on MAC and want to check it , but when I type </p>

<pre><code>$docker-machine ip 
</code></pre>

<p>I have the following error message : </p>

<pre><code>Error: No machine name(s) specified and no ""default"" machine exists.
</code></pre>

<p>What can I do ?</p>
"
"42345235","How to specify Memory & CPU limit in docker compose version 3","<docker><docker-compose>","61426882","How to specify max cpu and ram in docker compose 3.7","<docker><docker-compose>","<p>I am unable to specify CPU &amp; memory for services specified in version 3 .</p>

<p>With version 2 it works fine with ""mem_limit"" &amp; ""cpu_shares"" parameters under the services . But it fails while using version 3 , putting them under deploy section doesn't seem worthy unless i am using swarm mode .</p>

<p>Can somebody help ?</p>

<pre><code>version: ""3""
services:
  node:
    build:
     context: .
      dockerfile: ./docker-build/Dockerfile.node
    restart: always
    environment:
      - VIRTUAL_HOST=localhost
    volumes:
      - logs:/app/out/
    expose:
      - 8083
    command: [""npm"",""start""]
    cap_drop:
      - NET_ADMIN
      - SYS_ADMIN
</code></pre>
","<p>How to specify max cpu and ram in docker compose 3.7.?</p>

<p>My compose file is:</p>

<pre><code>version: ""3.7""
services:
  mongodb_container:
    image: mongo:latest
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: psw
      MONGO_INITDB_DATABASE: admin
    ports:
      - 27017:27017
    command: mongod --auth
    volumes:
      - ./data-docker/mongo:/data
</code></pre>
"
"42660690","Is it possible to shut down the host machine by executing a command on one of its docker container?","<docker>","61632208","Is it possible to restart a computer from within a docker container on Linux?","<docker>","<p>I have a host machine which has one docker container. The container is active and running a particular service. On meeting a particular condition, I want to remove the container and shut down the machine also. Is it possible to do so?
I am planning to modify the code which runs the service to handle the shutting down of the machine? Any suggestions are welcome!</p>
","<p>Is it possible to run the command reboot on the host from container ? I tried to mount a volume with the command but without any success. </p>

<p>The idea would be to have a container with some logic inside that I can deploy anywhere on Debian based machines and allow me to reboot a server.  </p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","61541234","Docker Desktop for Windows- Where exactly the volume is present","<docker>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I created a volume using Docker Desktop. Where does the volume is located. 
It is not intuitive as it was was the docker toolbox to locate the volume.</p>

<pre><code>PS C:\Users\admin&gt; docker volume inspect test                   
[                                                               
    {                                                           
        ""CreatedAt"": ""2020-05-01T10:59:22Z"",                    
        ""Driver"": ""local"",                                      
        ""Labels"": {},                                           
        ""Mountpoint"": ""/var/lib/docker/volumes/test/_data"",     
        ""Name"": ""test"",                                         
        ""Options"": {},                                          
        ""Scope"": ""local""                                        
    } 
</code></pre>

<p>Where does exactly "" /var/lib/docker/volumes/test/"" is present ?     I wanted to navigate to the path of it and run some commands for that path.</p>

<p>I referred in many articles, either they are not straightforward,                                                 </p>
"
"48376928","On Windows Setup, how can I get docker image from local machine","<docker><kubernetes><minikube>","61454180","minikube's docker engine in powershell","<docker><hyper-v><minikube>","<p>I could understand, different ways to access docker image from local machine to Minikube VM.</p>

<p><a href=""https://stackoverflow.com/questions/46065342/kubernetes-minikube-cant-get-docker-image-from-local-registry"">(Kubernetes + Minikube) can&#39;t get docker image from local registry</a></p>

<p>All these examples are for Mac/Linux user.</p>

<p>I'm looking for an equivalent suggestion for Windows user. </p>

<p>What's windows equivalent to -> eval $(minikube docker-env) </p>
","<p>I've installed minikube on hyper-v windows. everything looks fine but there is a problem in switching to minikube docker engine. I always use <code>eval $(minikube docker-env)</code> in docker toolbox terminal , is there any equivalent for this in PowerShell?</p>
"
"53429062","Docker: Springboot container can not connect to PostgreSql Container Connection error","<postgresql><spring-boot><docker><docker-compose>","61329370","Docker postgres on local machine: connection to localhost:5432 refused. Check that the hostname and port are correct","<java><postgresql><spring-boot><docker><dockerfile>","<p>I am building my first Springboot 2.0 application. I am trying to put my Springboot application into one docker container and my PostgresDB into another container.  </p>

<p><strong>My Dockerfile</strong></p>

<pre><code>    FROM frolvlad/alpine-oraclejdk8:slim
    VOLUME /tmp
    ADD springboot-api-demo-0.1*.jar app.jar
    RUN sh -c 'touch /app.jar'
    EXPOSE 9443
    ENTRYPOINT [ ""sh"", ""-c"", ""java $JAVA_OPTS -Djava.security.egd=file:/dev/urandom -jar /app.jar"" ]
</code></pre>

<p><strong>My docker-compose.yml file</strong></p>

<pre><code>version: ""2.1""

services:
  springboot-api-demo:
    image: ""fw/springboot-api-demo""
    mem_limit: 1024m
    ports:
      - ""8080:8080""
    environment:
      - SPRING_PROFILES_ACTIVE=local
      - AWS_REGION=local
      - ENVIRONMENT=local
      - AUTH_ENABLED=false
  postgres:
    container_name: pgdb
    image: postgres:9.6-alpine
    environment:
    - 'POSTGRES_ROOT_PASSWORD=postgres'
    - 'POSTGRES_USER=postgres'
    - 'POSTGRES_PASSWORD=postgres'
    ports:
    - ""54321:5432""
</code></pre>

<p>I am using Springboot JPA Data 2.0 with below config data in my <strong>application.properties</strong></p>

<pre><code>spring.datasource.url= jdbc:postgresql://localhost:54321/java_learning
spring.datasource.username=postgres
spring.datasource.password=postgres
</code></pre>

<p>I can test that Both of the Images are up. Also from docker log and docker events, I see that postgres  Container is running fine, even I can access it and also created a DB too.
But springboot container started but i died because it could not connect to postgress and throwing error below. </p>

<blockquote>
  <p>Unable to obtain connection from database: The connection attempt
  failed</p>
</blockquote>

<p>Note that my host machine already has Postgres on port 5432 thats why I did a port mapping ofr 54321:5432 on my postgres container. Here is Proof :) -</p>

<pre><code>➜  springboot-api-demo git:(master) ✗ lsof -i:54321              
COMMAND     PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 44345 shailendra.singh   18u  IPv4 0xf62897fbdd69e31d      0t0  TCP *:54321 (LISTEN)
com.docke 44345 shailendra.singh   21u  IPv6 0xf62897fbdd119975      0t0  TCP localhost:54321 (LISTEN)

➜  springboot-api-demo git:(master) ✗ lsof -i:5432 
COMMAND  PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 715 shailendra.singh    5u  IPv6 0xf62897fbb43e03b5      0t0  TCP localhost:postgresql (LISTEN)
postgres 715 shailendra.singh    6u  IPv4 0xf62897fbbaeea9bd      0t0  TCP localhost:postgresql (LISTEN)
</code></pre>

<p>I am not sure what is the problem. But my Springboot application is not able to connect my postgres container which is running fine with proper creadentials. </p>
","<p>I have a basic spring-boot application that does CRUD operations on a a postgres instance that is running on docker on my local machine. I am using Intellij to run my spring-boot application. When I run from the IDE, the application works fine with no errors. </p>

<p>But when I build a docker image out of the .jar file of my spring-boot app, I get the below error when I try to do a docker run.</p>

<p>Googled for a day before I posted the question here, help in any direction would help me!</p>

<p><strong>Docker file contents</strong></p>

<pre><code>FROM openjdk:11-jre-slim

COPY  target/demo-*.jar /demo.jar

CMD [""java"", ""-jar"", ""/demo.jar""]
</code></pre>

<p><strong>Error stack trace</strong></p>

<pre><code>Error Code : 0
Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

    at org.flywaydb.core.internal.jdbc.JdbcUtils.openConnection(JdbcUtils.java:60) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.internal.jdbc.JdbcConnectionFactory.&lt;init&gt;(JdbcConnectionFactory.java:80) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.Flyway.execute(Flyway.java:438) ~[flyway-core-6.0.8.jar!/:na]
    at org.flywaydb.core.Flyway.migrate(Flyway.java:149) ~[flyway-core-6.0.8.jar!/:na]
    at org.springframework.boot.autoconfigure.flyway.FlywayMigrationInitializer.afterPropertiesSet(FlywayMigrationInitializer.java:65) ~[spring-boot-autoconfigure-2.2.6.RELEASE.jar!/:2.2.6.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1855) ~[spring-beans-5.2.5.RELEASE.jar!/:5.2.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1792) ~[spring-beans-5.2.5.RELEASE.jar!/:5.2.5.RELEASE]
    ... 54 common frames omitted
Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:285) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:211) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.Driver.makeConnection(Driver.java:459) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.2.11.jar!/:42.2.11]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:354) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:554) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.2.jar!/:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.2.jar!/:na]
    at org.flywaydb.core.internal.jdbc.JdbcUtils.openConnection(JdbcUtils.java:56) ~[flyway-core-6.0.8.jar!/:na]
    ... 60 common frames omitted
Caused by: java.net.ConnectException: Connection refused (Connection refused)
    at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.doConnect(Unknown Source) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source) ~[na:na]
    at java.base/java.net.AbstractPlainSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.SocksSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.Socket.connect(Unknown Source) ~[na:na]
    at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:81) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:93) ~[postgresql-42.2.11.jar!/:42.2.11]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:197) ~[postgresql-42.2.11.jar!/:42.2.11]
    ... 72 common frames omitted


</code></pre>
"
"54062327","Running Docker inside Docker container: Cannot connect to the Docker daemon","<docker><ubuntu-16.04>","61321848","Docker within Docker Container","<docker>","<p>I created a Dockerfile to run Docker inside Docker:</p>

<pre><code>    FROM ubuntu:16.04
RUN apt-get update &amp;&amp; \
    apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common &amp;&amp; \
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp;\
    apt-key fingerprint 0EBFCD88

RUN add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"" &amp;&amp; \
   apt-get update &amp;&amp; \
   apt-get install -y docker-ce &amp;&amp; \
   systemctl enable docker
</code></pre>

<p>After i launched my container and run docker ps i got:
""Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?""</p>

<p>i executed the command dockerd inside my container resulted: </p>

<p>Error starting daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables -t nat -N DOCKER: iptables v1.6.0: can't initialize iptables table `nat': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
 (exit status 3)</p>

<p>Please advise</p>
","<p>Im entering a docker container via:
<code>docker run -it ubuntu bash</code></p>

<p>When im within the container, i am installing Docker:
<code>curl -sSL https://get.docker.com/ | sh</code></p>

<p>Yet when I fire <code>dockerd</code> im getting:</p>

<pre><code>INFO[2020-04-20T11:49:01.220025800Z] stopping event stream following graceful shutdown  error=""context canceled"" module=libcontainerd namespace=plugins.moby
failed to start daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables -t nat -N DOCKER: iptables v1.6.1: can't initialize iptables table `nat': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
 (exit status 3)
</code></pre>

<p>Any ideas how to fix this?</p>
"
"54976184","Docker base image includes a volume. How can I stop mounting it in my derived image","<docker>","61595224","Undo Dockerfile VOLUME directive from a base image","<docker><docker-volume>","<p>I have a docker base image that uses a volume and the base image is out of my control</p>

<p>For example: <code>VOLUME [""/my/path""]</code></p>

<p>I'm trying to build a derived image that doesn't define a volume. I want the data in /my/path to be transient and never persisted.</p>

<p>Is there a way I can disable a volume that came from a parent container? </p>
","<p>I have an image derived from the Postgres official image, whose <a href=""https://github.com/docker-library/postgres/blob/master/11/alpine/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile</a> includes the following:</p>

<pre><code>VOLUME /var/lib/postgresql/data
</code></pre>

<p>I'd like to create my own image based on this official image, but I don't want it to reference any volume.  I'd like the Postgres data to be inside my image.</p>

<p>Any ideas please?</p>
"
"60713384","Why git --version statement does not get recognize?","<git><docker>","61306915","Multistage build deletes directories of previous stages?","<node.js><docker>","<p>I have the following Dockerfile with content: </p>

<pre><code>FROM ubuntu:bionic AS os
RUN apt-get update
RUN apt-get install -y git
RUN git --version

FROM node:13.10.1-buster-slim

FROM python:3.7.7-slim-stretch as test
RUN pip install --user pipenv
RUN git --version
RUN git clone git@gitlab.com:silentdata/cdtc-identity-service.git

WORKDIR cdtc-identity-service

RUN pipenv install

CMD python service_test.py
</code></pre>

<p>Building the image, I've got the following output: </p>

<pre><code>Sending build context to Docker daemon  43.59MB
Step 1/12 : FROM ubuntu:bionic AS os
 ---&gt; 72300a873c2c
Step 2/12 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 42013f860b31
Step 3/12 : RUN apt-get install -y git
 ---&gt; Using cache
 ---&gt; 8f27d95fcb6e
Step 4/12 : RUN git --version
 ---&gt; Using cache
 ---&gt; ae49a9465233
Step 5/12 : FROM node:13.10.1-buster-slim
 ---&gt; 500c5a190476
Step 6/12 : FROM python:3.7.7-slim-stretch as test
 ---&gt; c9ec5ac0f580
Step 7/12 : RUN pip install --user pipenv
 ---&gt; Using cache
 ---&gt; 3a9358e72deb
Step 8/12 : RUN git --version
 ---&gt; Running in 545659570a84
/bin/sh: 1: git: not found
The command '/bin/sh -c git --version' returned a non-zero code: 127
</code></pre>

<p>Why the <code>git</code> command could not be found at the second time?</p>
","<p>I've got this docker file, that's supposed to install usd_from_gltf (the first stage) and then run the nodejs server with access to the usd_from_gltf command. But when I run this, it seems the container directories get all deleted and overwritten by the other stage. How do I prevent this?</p>

<pre><code>FROM leon/usd:latest

WORKDIR /usr/src/ufg

# Configuration
ARG UFG_RELEASE=""3bf441e0eb5b6cfbe487bbf1e2b42b7447c43d02""
ARG UFG_SRC=""/usr/src/ufg""
ARG UFG_INSTALL=""/usr/local/ufg""
ENV USD_DIR=""/usr/local/usd""
ENV LD_LIBRARY_PATH=""${USD_DIR}/lib:${UFG_SRC}/lib""
ENV PATH=""${PATH}:${UFG_INSTALL}/bin""
ENV PYTHONPATH=""${PYTHONPATH}:${UFG_INSTALL}/python""

# Build + install usd_from_gltf
RUN git init &amp;&amp; \
    git remote add origin https://github.com/google/usd_from_gltf.git &amp;&amp; \
    git fetch --depth 1 origin ""${UFG_RELEASE}"" &amp;&amp; \
    git checkout FETCH_HEAD &amp;&amp; \
    python ""${UFG_SRC}/tools/ufginstall/ufginstall.py"" -v ""${UFG_INSTALL}"" ""${USD_DIR}"" &amp;&amp; \
    cp -r ""${UFG_SRC}/tools/ufgbatch"" ""${UFG_INSTALL}/python"" &amp;&amp; \
    rm -rf ""${UFG_SRC}"" ""${UFG_INSTALL}/build"" ""${UFG_INSTALL}/src""


FROM node:12

WORKDIR /usr/src/app

COPY package.json ./

RUN yarn install

COPY . .
CMD [""yarn"", ""start""]
</code></pre>
"
"61491853","Docker: Can't access binary copied to certain images","<docker>","61512853","Problem with building go proyect in docker (Build in Alpine and execute in Oracle Linux)","<docker><go><alpine><instantclient><oraclelinux>","<p>I am trying to access a binary <code>COPY</code>ed from the <code>migrate</code> container.   When I <code>COPY</code> to <code>python:3.7-alpine</code> it works, but when I <code>COPY</code> to <code>debian:buster-slim</code> it can't be found.</p>

<p>Minimum steps to reproduce:</p>

<p>1.Create Dockerfile.test</p>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM python:3.7-alpine
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""2"">
<li>Build and run. This works.</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>Usage: migrate OPTIONS COMMAND [arg...]
       migrate [ -version | -help ]
</code></pre>

<ol start=""3"">
<li>Stop and remove container</li>
</ol>

<pre><code>docker stop migrate_test;docker rm migrate_test;
</code></pre>

<ol start=""4"">
<li>Change image in Dockerfile.test</li>
</ol>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM debian:buster-slim
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""5"">
<li>Build and run. This doesn't work</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>/bin/sh: 1: /migrate: not found
</code></pre>
","<p>I have a problem building go proyect, I build with golang:alpine and copy the result to oraclelinux:7-slim the problem is when I execute the docker image the result is a error </p>

<pre><code>""standard_init_linux.go:211: exec user process caused ""no such file or directory"""".
</code></pre>

<p>I tried build with other base image by example ""FROM golang:1.14 as builder"" and  execute in oraclelinux:7-slim and the result is satisfactory.</p>

<p>I need execute in oraclelinux:7-slim because a need use Oracle Instant client to connect with database.</p>

<p>I would like build with Alpine because the golang:1.14 is very heavy.</p>

<p>Attached the dockerfile</p>

<pre><code># Start from golang base image
FROM golang:alpine as builder

RUN apk update &amp;&amp; apk add --no-cache git &amp;&amp; \
    apk add build-base

# Set the current working directory inside the container
WORKDIR /app

# Copy go mod and sum files
COPY go.mod go.sum ./

# Download all dependencies. Dependencies will be cached if the go.mod and the go.sum files are not changed 
RUN go mod download 

# Copy the source from the current directory to the working Directory inside the container 
COPY . .

# Build the Go app
RUN CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -o main .

# Start a new stage from scratch
FROM oraclelinux:7-slim

ARG release=19
ARG update=6

# Install instant client
RUN yum -y update &amp;&amp; \
    yum -y install oracle-release-el7 &amp;&amp; yum-config-manager --enable ol7_oracle_instantclient &amp;&amp; \
    yum -y install oracle-instantclient${release}.${update}-basiclite &amp;&amp; \    
    rm -rf /var/cache/yum

ENV LANG=C.UTF-8

# Copy the Pre-built binary file from the previous stage
WORKDIR /root/

COPY --from=builder /app/main .
COPY --from=builder /app/.env .

# Expose port 8080 to the outside world
EXPOSE 8080

#Command to run the executable
ENTRYPOINT [""./main""]
</code></pre>
"
"61491853","Docker: Can't access binary copied to certain images","<docker>","61514445","How to use multi-stage build to eliminate /go/pkg files","<linux><docker><alpine><docker-build>","<p>I am trying to access a binary <code>COPY</code>ed from the <code>migrate</code> container.   When I <code>COPY</code> to <code>python:3.7-alpine</code> it works, but when I <code>COPY</code> to <code>debian:buster-slim</code> it can't be found.</p>

<p>Minimum steps to reproduce:</p>

<p>1.Create Dockerfile.test</p>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM python:3.7-alpine
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""2"">
<li>Build and run. This works.</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>Usage: migrate OPTIONS COMMAND [arg...]
       migrate [ -version | -help ]
</code></pre>

<ol start=""3"">
<li>Stop and remove container</li>
</ol>

<pre><code>docker stop migrate_test;docker rm migrate_test;
</code></pre>

<ol start=""4"">
<li>Change image in Dockerfile.test</li>
</ol>

<pre><code>FROM migrate/migrate:v4.6.2 AS migrate
FROM debian:buster-slim
COPY --from=migrate /migrate /
CMD ""/migrate""
</code></pre>

<ol start=""5"">
<li>Build and run. This doesn't work</li>
</ol>

<pre><code>docker build . -t migrate_test -f Dockerfile.test
docker run --name migrate_test migrate_test:latest
</code></pre>

<pre><code>/bin/sh: 1: /migrate: not found
</code></pre>
","<p>I have this dockerfile:</p>

<pre><code>FROM golang:1.13 as cm_base

ADD ./netrc /root/.netrc

ENV GO111MODULE='on'
ENV GOPROXY='direct'
ENV GOSUMDB='off'

RUN mkdir -p /tmp/cm-go-api
WORKDIR /tmp/cm-go-api

ENV github_token='&lt;some-access-token&gt;'
ADD ""https://raw.githubusercontent.com/ChannelMeter/cm-go-api/master/go.mod?token=$github_token""  
RUN go mod download 

ENV PROJECT_DIR '/go/src/github.com/channelmeter/cm-go-api'
WORKDIR ""$PROJECT_DIR""
RUN git init
RUN git remote add origin 'https://github.com/ChannelMeter/cm-go-api.git'
ARG commit_id
RUN git fetch --depth 1 origin ""$commit_id""
RUN git checkout ""$commit_id""

RUN go install -v

FROM alpine:3.11.5

COPY --from=cm_base /go/bin /go/bin

EXPOSE 1992 

ENTRYPOINT [""/go/bin/cm-go-api""]
</code></pre>

<p>the above breaks tho - the binary executable doesn't seem to work, but if I remove the multi-stage part, it works. For example, I remove these 3 lines:</p>

<pre><code>    # FROM alpine:3.11.5
    # COPY --from=cm_base /go/bin /go/bin
</code></pre>

<p>does anyone know why the binary file produced by the original go build command would not work with alpine?</p>

<p>If I use the original base image, it will work:</p>

<pre><code>    FROM golang:1.13
    COPY --from=cm_base /go/bin /go/bin
</code></pre>

<p>The <strong>error</strong> when using Alpine as the 2nd stage is:</p>

<blockquote>
  <p>standard_init_linux.go:211: exec user process caused ""no such file or
  directory""</p>
</blockquote>
"
"61713741","docker compose up working fine but browser showing site can't be reached","<docker><docker-compose><dockerfile><docker-machine>","61711737","docker compose up showing that server is running but browser showing site can not be reached","<docker><docker-compose>","<p><strong>docker-compose up</strong> showing this:</p>

<pre><code>Recreating tutorial_product-service_1 ...
Recreating tutorial_product-service_1 ... done
Attaching to tutorial_product-service_1
product-service_1  |  * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
product-service_1  |  * Restarting with stat
product-service_1  |  * Debugger is active!
product-service_1  |  * Debugger PIN: 192-693-276

</code></pre>

<p>and my docker-compose.yml</p>

<pre><code>version: '3'

services:
 product-service:
  container_name: tutorial_product-service_1
  build: ./product
  volumes:
   - ./product:/usr/src/app
  ports:
   - 5002:80

</code></pre>

<p>I have installed docker on AWS EC2 Ubuntu instance and its IP is : <a href=""http://18.225.37.175"" rel=""nofollow noreferrer"">http://18.225.37.175</a></p>

<p>But when I run <a href=""http://18.225.37.175:5002/"" rel=""nofollow noreferrer"">http://18.225.37.175:5002/</a>
it shows me site can not be reached.</p>

<p>Any help?</p>
","<p>after running <strong>docker-compose up</strong> command</p>

<pre><code>Starting tutorial_product-service_1 ...
Starting tutorial_product-service_1 ... done
Attaching to tutorial_product-service_1
product-service_1  |  * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
product-service_1  |  * Restarting with stat
product-service_1  |  * Debugger is active!
product-service_1  |  * Debugger PIN: 176-459-861

</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: '3'

services:
 product-service:
  container_name: tutorial_product-service_1
  build: ./product
  volumes:
   - ./product:/usr/src/app
  networks:
   testing_net:
    ipv4_address: 193.167.10.1
  ports:
   - 5001:80

networks:
 testing_net:
  ipam:
   config:
    - subnet: 193.167.10.0/16
</code></pre>

<p>This is the URL</p>

<pre><code>http://193.167.10.1:5001/
</code></pre>

<p>but it is showing site can not be reached.
Anyone can help me?</p>
"
"7308586","using batch echo with special characters","<batch-file><cmd><escaping><echo>","62428755","How can I write shell output if shell script is generated by batch","<docker><batch-file><escaping>","<p>This maybe really easy but there were no answers for it over the net. I want to echo a XML line via batch into a file but it misunderstands the XML closing tag for redirection "">"". The line is as follows:</p>

<pre><code>echo &lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt; &gt; myfile.xml
</code></pre>

<p>is there any way to give a hint to batch parser not to interpret a special string? I used double-quotes but it writes them to the file as well! The file should look like this after echo:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt;
</code></pre>
","<p>I have a Docker container <code>myContainer</code> and two simple batch scripts:</p>

<ul>
<li><code>generateShell.bat</code></li>
</ul>

<pre><code>@echo off 
echo #!/bin/bash &gt; helloworld.sh
echo ...&gt;&gt; helloworld.sh
</code></pre>

<ul>
<li><code>runShell.bat</code></li>
</ul>

<pre><code>@echo off 
call generateShell.bat
docker exec -i myContainer bash &lt; helloworld.sh
</code></pre>

<p>When I run <code>runShell.bat</code>, <code>helloworld.sh</code> runs in Docker container. I want to generate something in helloworld.sh so that writes output to .txt file in container. I tried </p>

<pre><code>echo echo Hello World &gt; output.txt&gt;&gt; helloworld.sh
</code></pre>

<p>but in that case I get only </p>

<pre><code>Hello World
</code></pre>

<p>in Windows command prompt and no generated .txt files neither in container nor on host.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62539446","Docker TCP client","<docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have TCP server outside and docker container , docker container is an TCP client.how to handle this can anyone help on this. How to connect to server and communicate. Can any one help me on this.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62590973","Connection to localhost refused while running dockerised app","<java><docker><apache-pulsar>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a simple Java application that runs in the command line and connects to a local Apache Pulsar on port 6650 (<code>pulsar://localhost:6650</code>). I've jar'ed the application and dockerised it. The Apache Pulsar server is on the same machine as the dockerised app. However, when I run the docker container, it says <code>Connection refused: localhost/127.0.0.1:6650</code>.</p>
<p>I'm new to Docker so I believe it may be something I'm doing wrong with Docker. I've tried the following commands to run my container:</p>
<pre><code>docker run -it -p 6650:6650 pulsar_logging_consumer
docker run -it --network=&quot;host&quot; pulsar_logging_consumer
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM openjdk:latest
COPY target/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar /usr/src/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar
EXPOSE 6650
CMD java -jar /usr/src/pulsar_logging_consumer-1.0-SNAPSHOT-jar-with-dependencies.jar
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","62599588","unable to connect to flask docker image","<python><docker><flask><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I'm a beginner in docker and I have a folder called <code>MovieFlix</code> that contains a <code>flask web service</code> app with some <code>html templates</code> that I <code>connect to a mongodb image from docker</code> . I want to run my flask app as a docker image . For that I have succesfully created a <code>DockerFile</code> which I list below :</p>
<pre><code>FROM ubuntu:16.04
MAINTAINER bill &lt;bill@gmailcom&gt;
RUN apt-get update
RUN apt-get install -y python3 python3-pip 
RUN pip3 install flask pymongo flask_bcrypt 
EXPOSE 80
WORKDIR &quot;/MovieFlix&quot;
COPY webservice.py . //copy the flask app i want to run 
COPY templates . //copy my html templates
CMD [&quot;python3&quot;, &quot;webservice.py&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;80&quot;] 
</code></pre>
<p>My image is built succesfully however when I run <code>docker -p 5000:80 flask(my image name )</code>
and copy the url <code>http://127.0.0.1:5000/</code> in my search engine I get that <code>the page does not work and 127.0.0.1 did not send data</code>  I can run my <code>webservice.py</code> normally on vscode and the mongodb image is running so I do not think the problem is there and I run my flask app with</p>
<pre><code>if __name__ == '__main__':
    app.run(debug=True, host='127.0.0.1', port=5000)
</code></pre>
<p>I would appreciate your help with guiding me to solve this issue .  Thank you in advance .</p>
"
"37586811","Pass commands as input to another command (su, ssh, sh, etc)","<bash><shell><unix><ssh><sh>","43340101","no such file or directory using script file","<linux><bash><docker>","<p>I have a script where I need to start a command, then pass some additional commands <em>as commands</em> to that command.  I tried</p>

<pre><code>su
echo I should be root now:
who am I
exit
echo done.
</code></pre>

<p>... but it doesn't work: The <code>su</code> succeeds, but then the command prompt is just staring at me.  If I type <code>exit</code> at the prompt, the <code>echo</code> and <code>who am i</code> etc start executing!  And the <code>echo done.</code> doesn't get executed at all.</p>

<p>Similarly, I need for this to work over <code>ssh</code>:</p>

<pre><code>ssh remotehost
# this should run under my account on remotehost
su
## this should run as root on remotehost
whoami
exit
## back
exit
# back
</code></pre>

<p>How do I solve this?</p>

<blockquote>
  <p>I am looking for answers which solve this in a general fashion, and which are not specific to <code>su</code> or <code>ssh</code> in particular.  The intent is for this question to become a <a href=""https://meta.stackoverflow.com/questions/291992/what-is-a-canonical-question-answer-and-what-is-their-purpose"">canonical</a> for this particular pattern.</p>
</blockquote>
","<p>I am receiving 'no such file or directory' when I start mysql even though the file exists in the correct directory.</p>

<p>thescript:</p>

<pre><code>#!/bin/bash

/usr/bin/docker exec -i mysql /bin/bash &lt;&lt;EOF
mysql -uroot -ppasswd 
CREATE DATABASE theDB CHARACTER SET utf8 COLLATE utf8_general_ci;
GRANT ALL PRIVILEGES on theDB.* to user@'%' IDENTIFIED BY 'passwd';
FLUSH PRIVILEGES;
EOF
</code></pre>

<p>docker.yml:</p>

<pre><code>version: ""2""
services:
  mysql:
    image: mysql:latest
    restart: always
    container_name: mysql
    environment:
      -  MYSQL_ROOT_PASSWORD=passwd
    volumes:
      -  /temp/mysql_cnf:/etc/mysql/mysql.conf.d
    ports:
      -  3306:3306
    command: /bin/sh -c ""chmod +x /home/user/mysql-container/thescript.sh &amp;&amp; /bin/sh /home/user/mysql-container/thescript.sh""
</code></pre>

<p>~   </p>
"
"44110876","Kubernetes service external ip pending","<nginx><kubernetes><load-balancing>","62448415","Kubernetes Service External -Ip for docker desktop","<docker><kubernetes>","<p>I am trying to deploy nginx on kubernetes, kubernetes version is v1.5.2,
I have deployed nginx with 3 replica, YAML file is below,</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment-example
spec:
  replicas: 3
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.10
        ports:
        - containerPort: 80
</code></pre>
<p>and now I want to expose its port 80 on port 30062 of node, for that I created a service below,</p>
<pre><code>kind: Service
apiVersion: v1
metadata:
  name: nginx-ils-service
spec:
  ports:
    - name: http
      port: 80
      nodePort: 30062
  selector:
    app: nginx
  type: LoadBalancer
</code></pre>
<p>this service is working good as it should be, but it is showing as pending not only on kubernetes dashboard also on terminal.
<a href=""https://i.stack.imgur.com/ix5v1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ix5v1.png"" alt=""Terminal output"" /></a><a href=""https://i.stack.imgur.com/TUMOB.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TUMOB.png"" alt=""Dash board status"" /></a></p>
","<p>I am facing issue related to kubernetes service external Ip. It is showing as 'pending'. I created deployment of type Load Balancer. I am using docker-desktop. </p>

<p>Please find below my YAML for creating deployment-</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: load-balancer
  name: nginxdeployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: load-balancer
  template:
    metadata:
      labels:
        app: load-balancer
    spec:
      containers:
      - image: nginx
        name: nginxcontainer
        ports:
        - containerPort: 8080
</code></pre>

<p>Here is the command which I have used to expose my deployment- 
 kubectl expose deployment nginxdeployment --type=LoadBalancer --name=nginxservice.</p>

<p>Below is the screenshot of exposed service in CLI-
<a href=""https://i.stack.imgur.com/YZbxg.png"" rel=""nofollow noreferrer"">Kubernets CLI screenshot</a></p>

<p>As highlighted in <strong>yellow</strong> in above image, it is showing <strong>pending</strong> under external IP for load balancer Service. </p>

<p>Please help me how come I can resolve this issue. It should IP instead of <strong>pending</strong>. </p>

<p>Thank you in Advance!!!</p>
"
"46879196","MySQLi not found dockerized php","<php><docker><mysqli><mysqlnd>","62513604","Docker- Call to undefined function mysqli_connect()","<php><docker><mysqli><docker-compose>","<p>I'm trying to dockerize my website. I've got Nginx and PHP up and running and it's working find except I can't connect to a db. When the page is loaded I get the error:</p>

<pre><code>Fatal error: Uncaught Error: Class 'MySQLi' not found in /private/conn.php:8 Stack trace: #0 /public_html/index.php(2): require() #1 {main} thrown in /private/conn.php on line 8
</code></pre>

<p>My connection line on line 8 is:</p>

<pre><code>$db = new mysqli(""$host"", ""$user"", ""$pass"", ""$db"", ""$port"");
</code></pre>

<p>Now this used to work fine on my old set up but since moving to a new one and installing php7.1.5 I can't get it running. Now, I haven't used the mysqlnd so this may be a misunderstanding on my part but the mysqlnd includes the mysqli driver.</p>

<p>Output of phpinfo:
<a href=""https://i.stack.imgur.com/SwS4S.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SwS4S.jpg"" alt=""config""></a>
<a href=""https://i.stack.imgur.com/RmAl0.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RmAl0.jpg"" alt=""mysqlnd""></a></p>

<p>How can I get this running? Or should I be using a different driver now?</p>
","<p>I am getting below error when trying to access from browser &quot;http://192.168.x.x:9091/testcon.php&quot;</p>
<pre><code>Fatal error: Uncaught Error: Call to undefined function mysqli_connect() in /var/www/html/testcon.php
</code></pre>
<p>Contents of testcon.php is like below,</p>
<pre><code>&lt;?php
$servername = &quot;db&quot;;
$username = &quot;root&quot;;
$password = &quot;pass1234&quot;;
$dbname='server';
$conn = mysqli_connect($servername, $username, $password, $dbname);

// Check connection
if ($conn-&gt;connect_error) {
 die(&quot;Connection failed: &quot; . $conn-&gt;connect_error);
}
echo &quot;Connected successfully&quot;;
?&gt;
</code></pre>
<p>But the same is working from inside container. If i run like &quot;php testcon.php&quot; it will work</p>
<pre><code>root@e951040c5265:/var/www/html# php testcon.php
Connected successfullyroot@e951040c5265:/var/www/html#
</code></pre>
<p>I have also edited /usr/local/etc/php/php.ini-development and uncommented line &quot;extension=mysqli&quot;, Still facing same issue</p>
<p>docker images used are below,</p>
<p>mysql:8.0.1</p>
<p>php:7.4.7-apache</p>
<p>phpmyadmin/phpmyadmin</p>
<p>TIA</p>
"
"48034906","Node Docker Runs, but can't see the application","<node.js><docker><hapijs>","62499052","docker-compose up is not mapping port","<node.js><postgresql><docker><docker-compose>","<p>It appears that my Hapi app is running in a Docker container, but I can't hit it in the browser. I thought that <code>docker run -d -p 8080:3000</code> would have done it, but I guess not. I'm running boot to docker and neither <code>http://localhost:8080/hello</code> nor <code>http://192.168.99.100:8080/hello</code> is working.</p>

<p>I've tried tons of variations on this as well.</p>

<p>This is what I see when I run <code>docker inspect &lt;container id&gt;</code>:</p>

<pre><code>Server running at: http://localhost:8080
</code></pre>

<p>Here's my Hapi.js server:</p>

<pre><code>'use strict';

const Hapi = require('hapi');

// Create a server with a host and port
const server = Hapi.server({
    host: 'localhost',
    port: 3000
});

// Add the route
server.route({
    method: 'GET',
    path:'/hello',
    handler: function (request, h) {
        return 'hello world';
    }
});

async function start() {

    try {
        await server.start();
    }
    catch (err) {
        console.log(err);
        process.exit(1);
    }

    console.log(`App running at: ${server.info.uri}/hello`);
}

start();
</code></pre>

<p>Here's my Dockerfile:</p>

<pre><code>FROM node:8.9.3

MAINTAINER My Name &lt;email@email.com&gt;

ENV NODE_ENV=production
ENV PORT=3000
ENV user node

WORKDIR /var/www
COPY package.json yarn.lock ./

RUN cd /var/www &amp;&amp; yarn

COPY . .

EXPOSE $PORT

ENTRYPOINT [""yarn"", ""start""]
</code></pre>

<p>Here's my package.json:</p>

<pre><code>{
    ""name"": ""my-app"",
    ""version"": ""1.0.0"",
    ""repository"": ""https://github.com/myname/myrepo.git"",
    ""author"": ""My Name"",
    ""license"": ""MIT"",
    ""private"": true,
    ""dependencies"": {
        ""hapi"": ""17.2.0""
    },
    ""scripts"": {
        ""start"": ""node ./src/server""
    }
}
</code></pre>
","<p>I know this has been asked multiple times already, but I am not able to find any solution to this. I am creating an application with Node.js and is trying to use <code>postgres</code> as a database with it. I am using <code>docker-compose</code> to start both the web server as well as the postgres server container. Here's how my <code>docker-compose.yml</code> file looks like -</p>
<pre class=""lang-yaml prettyprint-override""><code>
version: '3'
services:
  web:
    build: .
    ports:
      - 3000:3000
  postgres:
    image: 'postgres:12'
    environment:
      - POSTGRES_USER=my_admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=my_db
    ports:
      - 5432:5432
    volumes:
      - my_db:/var/lib/postgresql/data
volumes:
  my_db:
</code></pre>
<p>Here's how my dockerfile looks like -</p>
<pre><code>FROM node:12.16.2

ENV NODE_ENV production
# Add Tini
ENV TINI_VERSION v0.18.0
RUN wget https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini
RUN chmod +x /tini
ENTRYPOINT [&quot;/tini&quot;, &quot;-e&quot;, &quot;143&quot;, &quot;--&quot;]

# Setup code
WORKDIR /app

COPY package.json yarn.lock /app/
RUN yarn install --frozen-lockfile

COPY . /app

# Run on port 3000
EXPOSE 3000
CMD [&quot;yarn&quot;, &quot;start&quot;]
USER node
</code></pre>
<p>When I run <code>docker-compose up</code>, both the containers start and I can see the output. But when I try to run <code>curl -i http://localhost:3000</code> it shows an error, but works if I ssh into the container.</p>
<p>Am I missing something here?</p>
<p><strong>Update:</strong></p>
<p>Running <code>docker ps</code> shows this -</p>
<pre class=""lang-sh prettyprint-override""><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
0b2a43079005        postgres:12         &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 7 minutes        0.0.0.0:5432-&gt;5432/tcp   todo_postgres_1
0b0f1afd4fa1        todo_web            &quot;/tini -e 143 -- yar…&quot;   7 minutes ago       Up 7 minutes        0.0.0.0:3000-&gt;3000/tcp   todo_web_1
</code></pre>
<p><strong>Update 2:</strong></p>
<p>Here's how my. <code>app.ts</code> file looks like (I am using <code>hapijs</code>) -</p>
<p>import Hapi from '@hapi/hapi';</p>
<pre class=""lang-js prettyprint-override""><code>const init = async () =&gt; {
  const server = Hapi.server({
    port: 3000,
    host: 'localhost',
  });

  server.route({
    method: 'GET',
    path: '/',
    handler: (request, reply) =&gt; {
      return reply.response('Hello');
    },
  });

  await server.start();
  console.log('Server running on %s', server.info.uri);
};

process.on('unhandledRejection', (err) =&gt; {
  console.log(err);
  process.exit(1);
});

init();
</code></pre>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","62603737","How to fix Docker : ""Got permission denied while trying to connect to the Docker daemon socket ""","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>After installing the docker, if I try login to docker hub registry. I am getting following error:</p>
<pre><code>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/auth: dial unix /var/run/docker.sock: connect: permission denied
</code></pre>
<p>Just I would like to know, why I got this issue and how to resolve issue?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62422757","org.apache.spark.SparkException: java.nio.channels.ClosedChannelException","<python><docker><apache-spark><pyspark><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am trying to integrate Kafka with Apache spark which is both in separate docker containers. I am producing random data from the Kafka container using Kafka Produce and trying to consume it with Spark.</p>

<p>The consumer code within spark looks like this:</p>

<pre><code>import sys
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from pyspark.sql.context import SQLContext


if __name__ == '__main__':

    n_secs = 1
    topic = ""mytopic""

    conf = SparkConf().setAppName(""KafkaStreamProcessor"").setMaster(""local[*]"")
    sc = SparkContext(conf=conf)
    sc.setLogLevel(""WARN"")
    ssc = StreamingContext(sc, n_secs)

    kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {
                            'bootstrap.servers':'kafka:29092', 
                            'fetch.message.max.bytes':'15728640',
                            'auto.offset.reset':'largest'})
                            # Group ID is completely arbitrary

    lines = kafkaStream.map(lambda x: x[1])
    counts = lines.flatMap(lambda line: line.split("" "")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    time.sleep(600) # Run stream for 10 minutes just in case no detection of producer
    # ssc.awaitTermination()
    ssc.stop(stopSparkContext=True,stopGraceFully=True)
</code></pre>

<p>But when I run this code from the spark container. I get this error:</p>

<pre><code>2020-06-16 10:21:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7db27f95{/metrics/json,null,AVAILABLE,@Spark}
Traceback (most recent call last):
  File ""/app/business_rules/jobs/BusinessRule.py"", line 34, in &lt;module&gt;
    'auto.offset.reset':'largest'})
  File ""/spark/python/lib/pyspark.zip/pyspark/streaming/kafka.py"", line 146, in createDirectStream
  File ""/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o27.createDirectStreamWithoutMessageHandler.
: org.apache.spark.SparkException: java.nio.channels.ClosedChannelException
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at scala.util.Either.fold(Either.scala:98)
    at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:386)
    at org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:223)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStream(KafkaUtils.scala:721)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStreamWithoutMessageHandler(KafkaUtils.scala:689)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p>I am not sure what am I doing wrong, Any help would be appreciated.</p>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","62558603","error during connect: The system cannot find the file specified..,the docker client must be run elevated to connect","<docker><windows-10>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>I have installed docker on my windows 10 but when I am trying to execute any docker commands (except - v) it gives me below error. Any help ?</p>
<p><em><strong>error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/images/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></em></p>
"
"55343991","Error running docker container: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown","<docker><flask><docker-compose><dockerfile><docker-image>","62584751","unable to run flask app as a docker image","<docker><flask><dockerfile>","<p>I am trying to dockerize a simple Python-Flask application but I am getting an error while running my container.</p>

<p>docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown.</p>

<p>Workdir on localhost:</p>

<pre><code>/home/ubuntu/flask_web
- app.py
- Dockerfile
- requirements.txt
</code></pre>

<p>app.py</p>

<pre><code>#flask_web/app.py

from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hey, we have Flask in a Docker container'


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0')
</code></pre>

<p>Dockerfile</p>

<pre><code>FROM ubuntu:16.04

MAINTAINER xyz ""xyz@gmail.com""

RUN apt-get update \
    &amp;&amp; apt-get install -y software-properties-common vim \
    &amp;&amp; add-apt-repository ppa:jonathonf/python-3.6 \
    &amp;&amp; apt-get update -y \
    &amp;&amp; apt-get install -y build-essential python3.6 python3.6-dev python3-pip python3.6-venv \
    &amp;&amp; pip3 install --upgrade pip

# We copy just the requirements.txt first to leverage Docker cache
COPY ./requirements.txt /app/requirements.txt

WORKDIR /app

RUN pip install -r requirements.txt

COPY . /app

ENTRYPOINT [ ""python"" ]

CMD [ ""app.py"" ]
</code></pre>

<p>Commands:</p>

<pre><code>docker build -t flask-test:latest .
docker run -p 5000:5000 flask-test
</code></pre>

<p>Expected : Flask web should run on port 5000</p>

<p>Actual Result: </p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""python\"": executable file not found in $PATH"": unknown.
</code></pre>
","<p>I'm a complete beginner in docker in windows 10 and I have a flask project with some html templates inside a directory . I create a dockerfile and insert it inside my directory and then I build a docker image to run . However Instead of running my docker image I get the error <code>C:\Program Files\Docker\Docker\resources\bin\docker.exe: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;python\&quot;: executable file not found in $PATH&quot;: unknown.</code> which I do not understand or know how to solve .</p>
<p>My dockerfile :</p>
<pre><code>FROM ubuntu:16.04
MAINTAINER bill&lt;bill@gmailcom&gt;
RUN apt-get update
RUN apt-get install -y python3 python3-pip 
RUN pip3 install flask pymongo
EXPOSE 5000
WORKDIR &quot;/MovieFlix2020_E17136_SKENTOS_VASILIS&quot;
CMD [&quot;python&quot;, &quot;webservice.py&quot;, &quot;--host&quot;, &quot;127.0.0.1&quot;, &quot;--port&quot;, &quot;5000&quot;]
</code></pre>
<p>I build the image inside my <code>&quot;/MovieFlix2020_E17136_SKENTOS_VASILIS&quot;</code> directory with <code>docker build -t flask .</code>
The image is succesfully built but I get the warning :
<a href=""https://i.stack.imgur.com/3QxUH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3QxUH.png"" alt=""enter image description here"" /></a></p>
<p>And in the end with <code>run flask</code> which has to run my newly created image I get the error I wrote above .</p>
<p>I would appreciate your help with guiding me to solve this issue . Thank you in advance</p>
"
"55522726","Docker failing to start rails","<ruby-on-rails><docker><redis><resque>","62603593","Docker: failed to open TCP connection to localhost:12345","<mysql><ruby><api><docker><docker-compose>","<p>I'm trying to start my rails application using rails. It depends on redis, which seems to be working but when rails server starts it fails with <code>Cannot assign requested address - connect(2) for [::1]:6379 (Errno::EADDRNOTAVAIL)</code>.</p>

<h3>Dockerfile</h3>

<pre><code>FROM ruby:2.6.1

RUN apt-get update -yqq &amp;&amp; \
  apt-get install -yqq --no-install-recommends \
  nodejs \
  nano

COPY Gemfile* /usr/src/app/
WORKDIR /usr/src/app
RUN bundle install
RUN gem install foreman
RUN gem install rake -v 12.3.2

COPY . /usr/src/app/

CMD [ ""bin/rails"", ""s"", ""-b"", ""0.0.0.0"" ]
</code></pre>

<h3>.docker-compose.yml</h3>

<pre><code>version: '3'

services:
  postgres:
    image: 'postgres:10.3-alpine'
    volumes:
      - 'postgres:/var/lib/postgresql/data'
    env_file:
      - '.env'

  redis:
    image: 'redis'
    # volumes:
    #   - 'redis:/data'

  rails:
    depends_on:
      - 'postgres'
      - 'redis'
    build: .
    ports:
      - '3000:3000'
    volumes:
      - '.:/usr/src/app'
    env_file:
      - '.env'

volumes:
  redis:
  postgres:
</code></pre>

<h3>log</h3>

<pre><code>rails_1     | =&gt; Booting Puma
rails_1     | =&gt; Rails 5.2.2.1 application starting in development
rails_1     | =&gt; Run `rails server -h` for more startup options
rails_1     | Exiting
rails_1     | /usr/local/lib/ruby/2.6.0/socket.rb:1213:in `__connect_nonblock': Cannot assign requested address - connect(2) for [::1]:6379 (Errno::EADDRNOTAVAIL)
rails_1     |   from /usr/local/lib/ruby/2.6.0/socket.rb:1213:in `connect_nonblock'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:180:in `connect_addrinfo'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:220:in `block in connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `each'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `each_with_index'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:218:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/connection/ruby.rb:296:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:342:in `establish_connection'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:104:in `block in connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:299:in `with_reconnect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:103:in `connect'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:372:in `ensure_connected'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:224:in `block in process'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:312:in `logging'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:223:in `process'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis/client.rb:123:in `call'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:1347:in `block in sadd'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:50:in `block in synchronize'
rails_1     |   from /usr/local/lib/ruby/2.6.0/monitor.rb:230:in `mon_synchronize'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:50:in `synchronize'
rails_1     |   from /usr/local/bundle/gems/redis-4.1.0/lib/redis.rb:1346:in `sadd'
rails_1     |   from /usr/local/bundle/gems/redis-namespace-1.6.0/lib/redis/namespace.rb:442:in `call_with_namespace'
rails_1     |   from /usr/local/bundle/gems/redis-namespace-1.6.0/lib/redis/namespace.rb:328:in `method_missing'
rails_1     |   from /usr/local/bundle/gems/resque-2.0.0/lib/resque/data_store.rb:65:in `method_missing'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:94:in `set_schedule'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:51:in `block in schedule='
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:50:in `each'
rails_1     |   from /usr/local/bundle/bundler/gems/resque-scheduler-bbf4930c2802/lib/resque/scheduler/scheduling_extensions.rb:50:in `schedule='
rails_1     |   from /usr/src/app/config/initializers/resque.rb:8:in `&lt;top (required)&gt;'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `load'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `block in load'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:257:in `load_dependency'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/dependencies.rb:285:in `load'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:657:in `block in load_config_initializer'
rails_1     |   from /usr/local/bundle/gems/activesupport-5.2.2.1/lib/active_support/notifications.rb:170:in `instrument'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:656:in `load_config_initializer'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:614:in `block (2 levels) in &lt;class:Engine&gt;'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:613:in `each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/engine.rb:613:in `block in &lt;class:Engine&gt;'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:32:in `instance_exec'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:32:in `run'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:61:in `block in run_initializers'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:228:in `block in tsort_each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:350:in `block (2 levels) in each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:422:in `block (2 levels) in each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:431:in `each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:421:in `block in each_strongly_connected_component_from'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:50:in `each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:50:in `tsort_each_child'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:415:in `call'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:415:in `each_strongly_connected_component_from'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:349:in `block in each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `call'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:347:in `each_strongly_connected_component'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:226:in `tsort_each'
rails_1     |   from /usr/local/lib/ruby/2.6.0/tsort.rb:205:in `tsort_each'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/initializable.rb:60:in `run_initializers'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/application.rb:361:in `initialize!'
rails_1     |   from /usr/src/app/config/environment.rb:5:in `&lt;top (required)&gt;'
rails_1     |   from config.ru:3:in `require_relative'
rails_1     |   from config.ru:3:in `block in &lt;main&gt;'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:55:in `instance_eval'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:55:in `initialize'
rails_1     |   from config.ru:in `new'
rails_1     |   from config.ru:in `&lt;main&gt;'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:49:in `eval'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:49:in `new_from_string'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/builder.rb:40:in `parse_file'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:319:in `build_app_and_options_from_config'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:219:in `app'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:27:in `app'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:354:in `wrapped_app'
rails_1     |   from /usr/local/bundle/gems/rack-2.0.7/lib/rack/server.rb:283:in `start'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:53:in `start'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:147:in `block in perform'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:142:in `tap'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands/server/server_command.rb:142:in `perform'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor/command.rb:27:in `run'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor/invocation.rb:126:in `invoke_command'
rails_1     |   from /usr/local/bundle/gems/thor-0.20.3/lib/thor.rb:387:in `dispatch'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/command/base.rb:65:in `perform'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/command.rb:46:in `invoke'
rails_1     |   from /usr/local/bundle/gems/railties-5.2.2.1/lib/rails/commands.rb:18:in `&lt;top (required)&gt;'
rails_1     |   from bin/rails:4:in `require'
rails_1     |   from bin/rails:4:in `&lt;main&gt;'
</code></pre>
","<p>my problem is that I currently cannot login to my web ap that's running on docker. My docker-compose consists of my rails app, a mysql db, a restful api and a reverse proxy (someone recently just helped me out with this!). Everything is running fine currently (or at least I think so) and I can even test my api endpoint by going to localhost/api (and I see my doc for the api).</p>
<p>Problem is, when I login, it goes throug the restful api but I get an error as follow :</p>
<pre><code>Failed to open TCP connection to localhost:12345 (Cannot assign requested address - connect(2) for &quot;localhost&quot; port 12345)
</code></pre>
<p>I've tried to expose the port in the docker-compose, but it didn't work. Here are the important files :</p>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: '3.5'
services:
  app:
    image: 'test/testapp:first-test'
    depends_on:
      - db
    environment:
      DB_USER: root
      DB_NAME: test_dev
      DB_PASSWORD: root
      DB_HOST: db
      DB_PORT: 3306
      RAILS_ENV: development
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - .:/test_app
    networks:
      test-network:
        aliases:
          - app

  api:
    depends_on:
      - db
    image: 'test/testapi:first-test'
    ports:
      - &quot;12345:12345&quot;
    expose:
      - &quot;12345&quot;
    volumes:
      - .:/test_api
    networks:
      test-network:
        aliases:
          - api  

  reverse-proxy:
    depends_on:
      - app
      - api
    image: nginx:alpine
    volumes: 
      - $PWD/default.conf:/etc/nginx/conf.d/default.conf
    networks:
      test-network:
        aliases:
          - reverse-proxy
    ports:
      - 80:80
      - 443:443

  db:
    image: mysql:latest
    restart: always
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: test_dev
      MYSQL_USERNAME: root
      MYSQL_PASSWORD: root
    ports:
      - '3306:3306'
    volumes:
      - .:/test_db
    networks:
      test-network:
        aliases:
          - mysql-db
        
networks:
  test-network:

</code></pre>
<p>nginx conf</p>
<pre><code># This is a default site configuration which will simply return 404, preventing
# chance access to any other virtualhost.

server {
    listen 80 default_server;
    listen [::]:80 default_server;

    # Frontend
    location / {
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        proxy_pass http://hubsite:3000; # same name as network alias
    }

    # Backend
    location /api {
        proxy_set_header X-Real-IP  $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header Host $host;
        proxy_pass http://hubapi:12345/;  # &lt;--- note this has an extra /
    }

    # location = /404.html {
    #     internal;
    # }
}
</code></pre>
<p>Now this is very &quot;alpha&quot;, I'm pretty sure my volumes are all wrong and I should &quot;link&quot; my services altogether.</p>
<p>If anyone has information on this, I'd very much appreciate it! Thank you!</p>
"
"58481850","no matches for kind ""Deployment"" in version ""extensions/v1beta1""","<kubernetes>","62387374","having problem with giving deployment using kubernetes","<docker><kubernetes><docker-compose><devops><kompose>","<p>While deploying mojaloop, Kubernetes responds with the following errors:</p>
<blockquote>
<p>Error: validation failed: [unable to recognize &quot;&quot;: no matches for kind
&quot;Deployment&quot; in version &quot;apps/v1beta2&quot;, unable to recognize &quot;&quot;: no
matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;, unable
to recognize &quot;&quot;: no matches for kind &quot;StatefulSet&quot; in version
&quot;apps/v1beta2&quot;, unable to recognize &quot;&quot;: no matches for kind
&quot;StatefulSet&quot; in version &quot;apps/v1beta1&quot;]</p>
</blockquote>
<p>My Kubernetes version is 1.16.<br />
How can I fix the problem with the API version?<br />
From investigating, I have found that Kubernetes doesn't support apps/v1beta2, apps/v1beta1.<br />
How can I make Kubernetes use a not deprecated version or some other supported version?</p>
<p>I am new to Kubernetes and anyone who can support me I am happy</p>
","<p>i had this docker compose file which is working absolutely fine.But the i use ""kompose convert -f docker-compose.yam -o deploy.yaml"" in order to get yaml file for kubernetes deployment.</p>

<p>but when i go for ""kubectl apply -f deploy.yaml""</p>

<p>i am getting this error 
""service/cms created
service/mysqldb created
persistentvolumeclaim/my-datavolume configured
unable to recognize no matches for kind ""Deployment"" in version ""extensions/v1beta1""
unable to recognize no matches for kind ""Deployment"" in version ""extensions/v1beta1""</p>

<p>i am using minikube. 
Please help me out.</p>

<p>docker-compose file content</p>

<pre><code>version: ""2""
services:
  cms:
    image: 1511981217/cms_mysql:0.0.2
    ports:
      - ""8080:8080""
    networks:
      - cms-network
    depends_on:
      - mysqldb

  mysqldb:
    image: mysql:8
    ports:
      - ""3306:3306""
    networks:
      - cms-network
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=cmsdb  
    volumes:
      - my-datavolume:/var/lib/mysql

networks:
  cms-network: 

volumes:
  my-datavolume:
</code></pre>

<p>deploy.yaml file content</p>

<pre><code>apiVersion: v1
items:
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
      kompose.version: 1.19.0 (f63a961c)
    creationTimestamp: null
    labels:
      io.kompose.service: cms
    name: cms
  spec:
    ports:
    - name: ""8080""
      port: 8080
      targetPort: 8080
    selector:
      io.kompose.service: cms
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
      kompose.version: 1.19.0 (f63a961c)
    creationTimestamp: null
    labels:
      io.kompose.service: mysqldb
    name: mysqldb
  spec:
    ports:
    - name: ""3306""
      port: 3306
      targetPort: 3306
    selector:
      io.kompose.service: mysqldb
  status:
    loadBalancer: {}
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
      kompose.version: 1.19.0 (f63a961c)
    creationTimestamp: null
    labels:
      io.kompose.service: cms
    name: cms
  spec:
    replicas: 1
    strategy: {}
    template:
      metadata:
        annotations:
          kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
          kompose.version: 1.19.0 (f63a961c)
        creationTimestamp: null
        labels:
          io.kompose.service: cms
      spec:
        containers:
        - image: 1511981217/cms_mysql:0.0.2
          name: cms
          ports:
          - containerPort: 8080
          resources: {}
        restartPolicy: Always
  status: {}
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
      kompose.version: 1.19.0 (f63a961c)
    creationTimestamp: null
    labels:
      io.kompose.service: mysqldb
    name: mysqldb
  spec:
    replicas: 1
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kompose.cmd: kompose convert -f docker-compose.yml -o kubemanifests_2.yaml
          kompose.version: 1.19.0 (f63a961c)
        creationTimestamp: null
        labels:
          io.kompose.service: mysqldb
      spec:
        containers:
        - env:
          - name: MYSQL_DATABASE
            value: cmsdb
          - name: MYSQL_ROOT_PASSWORD
            value: root
          image: mysql:8
          name: mysqldb
          ports:
          - containerPort: 3306
          resources: {}
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: my-datavolume
        restartPolicy: Always
        volumes:
        - name: my-datavolume
          persistentVolumeClaim:
            claimName: my-datavolume
  status: {}
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    creationTimestamp: null
    labels:
      io.kompose.service: my-datavolume
    name: my-datavolume
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 100Mi
  status: {}
kind: List
metadata: {}
```


</code></pre>
"
"59148975","Running a ECR image locally with helm and Kubernetes","<amazon-web-services><docker><kubernetes><kubernetes-helm><kubectl>","62420919","How to pull a private container from AWS ECR to a local cluster","<amazon-web-services><docker><kubernetes>","<p>I'm new to Kubernetes and as a tutorial for myself I've been working on deploying a basic project to Kubernetes with helm (v3).
I have an image in AWS's ECR as well as a local helm chart for this project.
However, I am struggling to run my image with Kubernetes.</p>

<p>My image is set up correctly. If I try something like <code>docker run my_image_in_ecr</code> locally it behaves as expected (after configuring my IAM access credentials locally).
My helm chart is properly linted and in my image map, it specifies:</p>

<pre><code>image:
  repository: my_image_in_ecr
  tag: latest
  pullPolicy: IfNotPresent
</code></pre>

<p>When I try to use helm to deploy though, I'm running into issues. 
My understanding is to run my program with helm, I should:</p>

<ol>
<li><p>Run helm install on my chart</p></li>
<li><p>Run the image inside my new kubernetes pod</p></li>
</ol>

<p>But when I look at my kubernetes pods, it looks like they never get up and running.</p>

<pre><code>hello-test1-hello-world-54465c788c-dxrc7           0/1     ImagePullBackOff    0          49m
hello-test2-hello-world-8499ddfb76-6xn5q           0/1     ImagePullBackOff    0          2m45s
hello-test3-hello-world-84489658c4-ggs89           0/1     ErrImagePull        0          15s
</code></pre>

<p>The logs for these pods look like this:</p>

<pre><code>Error from server (BadRequest): container ""hello-world"" in pod ""hello-test3-hello-world-84489658c4-ggs89"" is waiting to start: trying and failing to pull image
</code></pre>

<p>Since I don't know how to set up imagePullSecrets properly with Kubernetes I was expecting this to fail. But I was expecting a different error message such as bad auth credentials. </p>

<ol>
<li>How can I resolve the error in image pulling? Is this issue not even related to the fact that my image is in ecr?</li>
<li>How can I properly set up credentials (such as imagePullSecrets) to authorize pulling the image from ecr? I have followed some guides such as <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/"" rel=""nofollow noreferrer"">this one</a> and <a href=""https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry"" rel=""nofollow noreferrer"">this one</a> but am confused on how to tranlate this information into a proper authorization configuration for ecr.</li>
</ol>
","<p>I am currently having trouble trying to pull my remote docker image hosted via AWS ECR. I am getting this error when running a deployment </p>

<p>Step 1)</p>

<p>run </p>

<pre><code>aws ecr get-login-password --region cn-north-1 | docker login --username AWS --password-stdin xxxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn
</code></pre>

<p>Step 2)</p>

<p>run <code>kubectl create -f backend.yaml</code></p>

<p>from here the following happens:</p>

<pre><code>➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     Pending            0          2s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS              RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ContainerCreating   0          4s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ErrImagePull       0          6s

➜  backend git:(kubernetes-fresh) ✗ kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
backend-89d75f7df-qwqdq   0/1     ImagePullBackOff   0          7s
</code></pre>

<p>So then I run <code>kubectl describe pod backend</code> and it will output:</p>

<pre><code>Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  117s                default-scheduler  Successfully assigned default/backend-89d75f7df-qwqdq to minikube
  Normal   Pulling    32s (x4 over 114s)  kubelet, minikube  Pulling image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest""
  Warning  Failed     31s (x4 over 114s)  kubelet, minikube  Failed to pull image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest"": rpc error: code = Unknown desc = Error response from daemon: Get https://xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/v2/baopals/manifests/latest: no basic auth credentials
  Warning  Failed     31s (x4 over 114s)  kubelet, minikube  Error: ErrImagePull
  Warning  Failed     19s (x6 over 113s)  kubelet, minikube  Error: ImagePullBackOff
  Normal   BackOff    4s (x7 over 113s)   kubelet, minikube  Back-off pulling image ""xxxxxxxxx.dkr.ecr.cn-north-1.amazonaws.com.cn/baopals:latest""

</code></pre>

<p>the main error being <code>no basic auth credentials</code></p>

<p>Now what I am confused about is that I can push images to my ECR fine and I can also push to my remote EKS cluster I feel like essentially the only thing I cant do right now is pull from my private repository that is hosted on ECR.</p>

<p>Is there something obvious that I'm missing here that is preventing me from pulling from private repos so i can use them on my local machine?</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","62455226","React/Spring/MySQL Containerisation: Frontend Unable To Start After Pulling Image","<reactjs><spring><docker><docker-compose><dockerfile>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>Still grasping with Docker and its concepts. I have a React front end, which I am trying to deploy to a backend REST API in Spring Boot and MySQL. Now, I'm able to build and push the images separately, i.e. one for the React application, and the other for the Spring Boot aplication. </p>

<p>This is my <code>Dockerfile</code> for the front-end React application.</p>

<pre><code>FROM node:13.12.0-alpine

WORKDIR /app

ENV PATH /app/node_modules/.bin:$PATH

COPY package.json ./
COPY package-lock.json ./
RUN npm install --silent
RUN npm install react-scripts@3.4.1 -g --silent

COPY . ./

CMD [""npm"", ""start""]
</code></pre>

<p>I'm able to run the front end on <code>localhost:3001</code> with this docker command: </p>

<p><code>docker run -it --rm -v ${PWD}:/app -v /app/node_modules -p 3001:3000 -e CHOKIDAR_USEPOLLING=true dockeruser/app-client</code></p>

<p>And this is for my Spring Boot backend: </p>

<pre><code>FROM openjdk:15-jdk-alpine

COPY app-server-web/target/app-server-web.jar /app-server-web.jar

ENTRYPOINT [""java"", ""-jar"", ""/app-server-web.jar""]
</code></pre>

<p>Now I am able to build both of these and push them to dockerhub. I want to compose them now, but for some reason the front end is not able to start on the port that I assigned it to. Here is my <code>docker-compose.yml</code> file: </p>

<pre><code>version: '3'
services:
  application-db:
    restart: always
    container_name: application-db
    image: 'mysql:5.7.30'
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: application_database
      MYSQL_USER: alee
      MYSQL_PASSWORD: anotherpassword
    ports:
      - '3308:3306'
    volumes:
      - './initial.sql:/docker-entrypoint-initdb.d/initial.sql'
      - null
  application-server:
    restart: on-failure
    image: 'dockeruser/app-server:latest'
    expose:
      - '8080'
    ports:
      - '8080:8080'
    environment:
      SPRING_DATASOURCE_URL: &gt;-
        jdbc:mysql://application-db:3306/application_database?useSSL=false&amp;allowPublicKeyRetrieval=true
      SPRING_DATASOURCE_USERNAME: alee
      SPRING_DATASOURCE_PASSWORD: anotherpassword
    depends-on:
      - application-db
    networks:
      - backend
      - frontend
  application-client:
    restart: on-failure
    image: 'dockeruser/app-client:latest'
    expose:
      - '3001'
    ports:
      - '3001:3000'
    depends-on:
      - application-server
    networks:
      - frontend
networks:
  backend: null
  frontend: null
</code></pre>

<p>These are the logs I get upon <code>docker-compose up</code> </p>

<pre><code>application-client_1  | ℹ ｢wds｣: Project is running at http://192.168.128.3/
application-client_1  | ℹ ｢wds｣: webpack output is served from 
application-client_1  | ℹ ｢wds｣: Content not from webpack is served from /app/public
application-client_1  | ℹ ｢wds｣: 404s will fallback to /
application-client_1  | Starting the development server...
application-client_1  | 
</code></pre>

<p>I'm pretty sure something is wrong in <code>docker-compose.yml</code> but I'm not sure what it is?</p>
"
"356100","How to wait in bash for several subprocesses to finish and return exit code !=0 when any subprocess ends with code !=0?","<bash><process><wait>","53429067","Return non-zero exit status if a child processes fails","<bash><performance><docker><continuous-integration>","<p>How to wait in a bash script for several subprocesses spawned from that script to finish and return exit code !=0 when any of the subprocesses ends with code !=0 ?</p>

<p>Simple script:</p>

<pre><code>#!/bin/bash
for i in `seq 0 9`; do
  doCalculations $i &amp;
done
wait
</code></pre>

<p>The above script will wait for all 10 spawned subprocesses, but it will always give exit status 0 (see <code>help wait</code>). How can I modify this script so it will discover exit statuses of spawned subprocesses and return exit code 1 when any of subprocesses ends with code !=0?</p>

<p>Is there any better solution for that than collecting PIDs of the subprocesses, wait for them in order and sum exit statuses?</p>
","<p>I have a docker build script which builds a series of libraries and runs test suits. In order to make my build script run faster on a many core server, I changed put the sequential order of commands (being passed to <code>RUN</code> to a bash invocation of several parallel commands:</p>

<pre><code>RUN /bin/bash -c -e '\
    cmd1 arg1 &amp; \
    cmd2 &amp; \
    cmd3 arg1 arg2=foo arg3 &amp; \
    wait'
</code></pre>

<p>This worked file when there was no error. Then I realized even if one of child processes return a non-zero exit status, the whole bash command returns <code>0</code> and docker continues to build... How can I make the bash call to return <code>0</code> iff all its children return <code>0</code> ? </p>
"
"3811908","Font is not available to the JVM with Jasper Reports","<jvm><jasper-reports><ubuntu-10.04><dynamic-jasper>","52996492","Jasper reports doesn't work on server in docker container (spring boot java application)","<java><docker><fonts><jasper-reports>","<p>I'm trying to generate report with DynamicJasper, but I'm getting the following error:</p>

<pre><code>net.sf.jasperreports.engine.util.JRFontNotFoundException:  
                                 Font 'Arial' is not available to the JVM.   
</code></pre>

<p><a href=""https://launchpad.net/ubuntu/lucid/+source/msttcorefonts"" rel=""noreferrer"">msttcorefonts</a> is installed, but I guess the JVM is not using any fonts from it. </p>

<p>I'm using Ubuntu 10.04. </p>

<p>How can I fix this?</p>
","<p>My pom file:</p>

<pre><code> &lt;!-- Jasper Dependency --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;net.sf.jasperreports&lt;/groupId&gt;
        &lt;artifactId&gt;jasperreports&lt;/artifactId&gt;
        &lt;version&gt;6.6.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Jasper fonts dependency --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;net.sf.jasperreports&lt;/groupId&gt;
        &lt;artifactId&gt;jasperreports-fonts&lt;/artifactId&gt;
        &lt;version&gt;6.0.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>And after running:</p>

<ol>
<li>mvn clean install -Pprod -DskipTest</li>
<li>docker-compose build</li>
<li>docker-compose push</li>
</ol>

<p>image is built and pushed on docker hub. Then with command: docker stack deploy .. i run 4 docker containers (database, front-end, back-end and network). When i try to use generating pdf APIs, i get errors like this:</p>

<pre><code>    2018-10-25 17:46:18.997 DEBUG 1 --- [ XNIO-2 task-14] n.s.j.e.fonts.FontExtensionsRegistry     : Loading font extensions from net/sf/jasperreports/fonts/fonts.xml
2018-10-25 17:46:18.999 DEBUG 1 --- [ XNIO-2 task-14] n.s.j.e.fonts.SimpleFontExtensionHelper  : Parsing font family DejaVu Sans
2018-10-25 17:46:18.999 DEBUG 1 --- [ XNIO-2 task-14] n.s.j.engine.fonts.SimpleFontFace        : Loading font net/sf/jasperreports/fonts/dejavu/DejaVuSans.ttf
2018-10-25 17:46:19.006 ERROR 1 --- [ XNIO-2 task-14] n.s.j.e.fonts.SimpleFontExtensionHelper  : Error loading font family

net.sf.jasperreports.engine.fonts.InvalidFontException: Error loading font ""net/sf/jasperreports/fonts/dejavu/DejaVuSans.ttf"".
    at net.sf.jasperreports.engine.fonts.SimpleFontFace.loadFont(SimpleFontFace.java:206)
    at net.sf.jasperreports.engine.fonts.SimpleFontFace.setTtf(SimpleFontFace.java:167)
    at net.sf.jasperreports.engine.fonts.SimpleFontExtensionHelper.parseFontFace(SimpleFontExtensionHelper.java:379)
    at net.sf.jasperreports.engine.fonts.SimpleFontExtensionHelper.parseFontFamily(SimpleFontExtensionHelper.java:311)
    at net.sf.jasperreports.engine.fonts.SimpleFontExtensionHelper.parseFontExtensions(SimpleFontExtensionHelper.java:259)
    at net.sf.jasperreports.engine.fonts.SimpleFontExtensionHelper.loadFontExtensions(SimpleFontExtensionHelper.java:230)
    at net.sf.jasperreports.engine.fonts.SimpleFontExtensionHelper.loadFontExtensions(SimpleFontExtensionHelper.java:187)
    at net.sf.jasperreports.engine.fonts.FontExtensionsRegistry.ensureFontExtensions(FontExtensionsRegistry.java:93)
    at net.sf.jasperreports.engine.fonts.FontExtensionsRegistry.getExtensions(FontExtensionsRegistry.java:57)
    at net.sf.jasperreports.extensions.DefaultExtensionsRegistry.getExtensions(DefaultExtensionsRegistry.java:130)
    at net.sf.jasperreports.engine.util.JRStyledTextParser.&lt;clinit&gt;(JRStyledTextParser.java:86)
    at net.sf.jasperreports.engine.fill.JRBaseFiller.&lt;init&gt;(JRBaseFiller.java:116)
    at net.sf.jasperreports.engine.fill.JRVerticalFiller.&lt;init&gt;(JRVerticalFiller.java:79)
    at net.sf.jasperreports.engine.fill.JRFiller.createBandReportFiller(JRFiller.java:251)
    at net.sf.jasperreports.engine.fill.JRFiller.createReportFiller(JRFiller.java:272)
    at net.sf.jasperreports.engine.fill.JRFiller.fill(JRFiller.java:156)
    at net.sf.jasperreports.engine.fill.JRFiller.fill(JRFiller.java:145)
    at net.sf.jasperreports.engine.JasperFillManager.fill(JasperFillManager.java:689)
    at net.sf.jasperreports.engine.JasperFillManager.fill(JasperFillManager.java:670)
    at net.sf.jasperreports.engine.JasperFillManager.fillReport(JasperFillManager.java:992)
    at com.springboot.jasper.utils.ReportUtil.exportReportToPDF(ReportUtil.java:402)
    at com.springboot.jasper.utils.ReportUtil.generateByteArray(ReportUtil.java:464)
    at com.logate.cumulus.service.ExportToFileService.generateDocumentPDFReport(ExportToFileService.java:94)
    at com.logate.cumulus.service.ExportToFileService.generateMultipleDocumentPDFReport(ExportToFileService.java:106)
    at com.logate.cumulus.service.ExportToFileService$$FastClassBySpringCGLIB$$f78d38a0.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:738)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
    at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:85)
    at com.logate.cumulus.aop.logging.LoggingAspect.logAround(LoggingAspect.java:72)
    at sun.reflect.GeneratedMethodAccessor407.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:629)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:618)
    at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:62)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:282)
    at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:673)
    at com.logate.cumulus.service.ExportToFileService$$EnhancerBySpringCGLIB$$89be9882.generateMultipleDocumentPDFReport(&lt;generated&gt;)
    at com.logate.cumulus.web.rest.DocumentResource.generateMultipleDocumentPDFs(DocumentResource.java:398)
    at com.logate.cumulus.web.rest.DocumentResource$$FastClassBySpringCGLIB$$5e55d698.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:738)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
    at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:85)
    at com.logate.cumulus.aop.logging.LoggingAspect.logAround(LoggingAspect.java:72)
    at sun.reflect.GeneratedMethodAccessor407.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:629)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:618)
    at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:62)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.security.access.intercept.aopalliance.MethodSecurityInterceptor.invoke(MethodSecurityInterceptor.java:69)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:673)
    at com.logate.cumulus.web.rest.DocumentResource$$EnhancerBySpringCGLIB$$ae51e2f3.generateMultipleDocumentPDFs(&lt;generated&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
    at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:85)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
    at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:111)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at com.logate.cumulus.security.jwt.JWTFilter.doFilter(JWTFilter.java:43)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:108)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
    at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
    at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
    at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:131)
    at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
    at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
    at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
    at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
    at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
    at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
    at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
    at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
    at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
    at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
    at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
    at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
    at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
    at io.undertow.server.Connectors.executeRootHandler(Connectors.java:211)
    at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:809)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Problem reading font data.
    at java.awt.Font.createFont0(Font.java:1000)
    at java.awt.Font.createFont(Font.java:877)
    at net.sf.jasperreports.engine.fonts.SimpleFontFace.loadFont(SimpleFontFace.java:198)
    ... 179 common frames omitted
</code></pre>

<p>I use <code>FROM openjdk:8u131-jre-alpine</code> for my back-end application in dockerfile.</p>
"
"7023052","Configure Flask dev server to be visible across the network","<python><flask><werkzeug>","53544122","Python Flask App Would not Serve any API Request","<python><api><docker><flask>","<p>I'm not sure if this is Flask specific, but when I run an app in dev mode (<code>http://localhost:5000</code>), I cannot access it from other machines on the network (with <code>http://[dev-host-ip]:5000</code>). With Rails in dev mode, for example, it works fine. I couldn't find any docs regarding the Flask dev server configuration. Any idea what should be configured to enable this?</p>
","<p>I have a Flask app and I'm trying to expose several API's via this app. I'm using Docker to start this app and here are the logs after I started the app:</p>

<pre><code>Joes-MacBook-Pro-78:my-flask-api joe$ docker run -p 5000:5000 --name my-flask-api my-flask-api
/usr/local/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:819: UserWarning: SQLALCHEMY_DATABASE_URI not set. Defaulting to ""sqlite:///:memory:"".
  'SQLALCHEMY_DATABASE_URI not set. Defaulting to '
/usr/local/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:839: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future.  Set it to True or False to suppress this warning.
  'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and '
[2018-11-29 16:54:58,812] [werkzeug] [INFO]   * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
[2018-11-29 16:54:58,813] [werkzeug] [INFO]   * Restarting with stat
/usr/local/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:819: UserWarning: SQLALCHEMY_DATABASE_URI not set. Defaulting to ""sqlite:///:memory:"".
  'SQLALCHEMY_DATABASE_URI not set. Defaulting to '
/usr/local/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:839: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future.  Set it to True or False to suppress this warning.
  'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and '
[2018-11-29 16:54:59,069] [werkzeug] [WARNING]   * Debugger is active!
[2018-11-29 16:54:59,070] [werkzeug] [INFO]   * Debugger PIN: 123-456-789
</code></pre>

<p>Is there anything that I could do to see what is wrong with this? How can I debug this to see more logs that would enable me to know what is happening in the background? Any ideas?</p>

<p>Right now when I hit the API, I get the following error in Chrome:</p>

<pre><code>This page isn’t working localhost didn’t send any data.
ERR_EMPTY_RESPONSE
</code></pre>
"
"10494431","Sticky and NON-Sticky sessions","<session><sticky-session>","53147032","Why reverse-proxy for persistent session in docker container?","<php><docker><docker-compose><dockerfile><docker-swarm>","<p>I want to know the difference between sticky- and non-sticky sessions. What I understood after reading from internet:</p>

<p><strong>Sticky</strong> : only single session object will be there.</p>

<p><strong>Non-sticky session</strong> : session object for each server node</p>
","<p>I am new to docker and dockerizing an existing application. The problem that I faced is that as I have multiple containers for the same application, the session variables will not be consistent across containers. So, on searching on the internet, I found one way to solve this problem is to implement reverse proxy to maintain persistent session. </p>

<p>One such article is:  <a href=""https://boxboat.com/2017/08/03/deploy-web-app-docker-swarm-sticky-sessions/"" rel=""nofollow noreferrer"">https://boxboat.com/2017/08/03/deploy-web-app-docker-swarm-sticky-sessions/</a></p>

<p>Can someone explain how the use of reverse proxy helps to maintain persistent session variable across containers?</p>
"
"13916132","What is the minimal agent install footprint for Delphi build automation?","<delphi><continuous-integration><delphi-xe3><build-server>","53295453","Installing Delphi via the command-line (i.e. non UI) for Docker image","<docker><delphi>","<p>When creating a build server that does clean version control check-outs and full system builds of everything in a given source repository or project, what is the minimum required Delphi install footprint for XE3 Win32/Win64 projects?  (Core system - not 3rd party components)</p>

<p>I'd prefer to have a small subset of files that can be included in a repository rather than a full Delphi install.  </p>
","<p>I am looking at converting our build machines to use Docker images, and although the Delphi compiler has a command line option, the actual installer itself does not seem to, and therefore requires a Windows user interface, meaning it can't be installer into Docker for Windows. Has anyone managed to build a Delphi compiler docker image? </p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","53245742","In which file (or path) are the Docker images and containers stored?","<linux><docker><dockerfile><boot2docker>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I installed Docker Toolbox on Windows 10. </p>

<p>I use the VM where my storage is:</p>

<p>C:\Users\ ***username*** \.docker\machine\machines\default\disk.vmdk</p>

<p>However, as I add and remove 100mb worth of images, the size of this file on my hard-drive is barely changing. boot2docker.iso is also same space.</p>

<p>So I was wondering, where are the Images and Containers really stored?</p>
"
"19537645","Get environment variable value in Dockerfile","<docker>","53858820","Get env variables from script and pass to Dockerfile","<bash><shell><docker><environment-variables><dockerfile>","<p>I'm building a container for a ruby app. My app's configuration is contained within environment variables (loaded inside the app with <a href=""http://github.com/bkeepers/dotenv"">dotenv</a>).</p>

<p>One of those configuration variables is the public ip of the app, which is used internally to make links.
I need to add a dnsmasq entry pointing this ip to 127.0.0.1 inside the container, so it can fetch the app's links as if it were not containerized.</p>

<p>I'm therefore trying to set an <code>ENV</code> in my Dockerfile which would pass an environment variable to the container.</p>

<p>I tried a few things.</p>

<pre><code>ENV REQUEST_DOMAIN $REQUEST_DOMAIN
ENV REQUEST_DOMAIN `REQUEST_DOMAIN`
</code></pre>

<p>Everything passes the ""REQUEST_DOMAIN"" string instead of the value of the environment variable though.
Is there a way to pass environment variables values from the host machine to the container?</p>
","<p>I have a <strong>script.sh</strong> like that</p>

<pre><code>DATE=""R_$(date +%Y_%m_%d__%H_%M_%S)""
export DISTDIR=""$BUILDDIR/$DATE""
</code></pre>

<p>and i wan't to pass this DISTDIR in my <strong>Dockerfile</strong> like that</p>

<pre><code>COPY build/$DISTDIR/ ""$CATALINA_HOME""/webapps/ws/js/
</code></pre>

<p>How i do that, i searched for various modes and none worked</p>
"
"20010199","How to determine if a process runs inside lxc/Docker?","<linux><bash><docker>","53062488","Find out the environment in which the bash script is running using Docker","<bash><docker>","<p>Is there any way to determine if a process (script) runs inside an lxc container (~ Docker runtime)? I know that some programs are able to detect whether they run inside a virtual machine, is something similar available for lxc/docker?</p>
","<p>What is the best way to find out in which environment the bash script is running (on the host machine or in a docker image)?</p>

<p>My bash script:</p>

<pre><code>#!/bin/bash
function PKG_ABSENT { return `which ""$1"" 2&gt;/dev/null | grep -cm1 ""$1""`; }

if ! PKG_ABSENT ""docker""; then
    # Docker package is installed
    echo ""running in the host machine""
else
    # Docker package is not installed
    echo ""running in a Docker image""
fi
</code></pre>

<p>Any Ideas?</p>
"
"20813486","Exploring Docker container's file system","<linux><docker><filesystems>","53243471","how to browse docker image without running it?","<docker><beyondcompare>","<p>I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.</p>

<p>What would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.</p>
","<p>Is there any why to browse a docker image using ssh or equivalent?
My motivation is to compare two docker images using beyond compare</p>

<p>edit: i am not interested in just listing file names, I need the content and I would like to be able compare two images, including content of the files.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","53481998","Flask in Docker container access localhost","<python><docker><python-requests>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>On my localhost I have a Django application running on port 8000. 
A Docker compose sets up different containers, among them a Flask application with the config:</p>

<pre><code>redirection-service:
  container_name: redirection-service
  build: 
    context: ""...""
  ports:
    - 5000:5000
  links:
    - redis
</code></pre>

<p>In the flask application I use a <code>requests</code> call to access an endpoint of the Django application on the localhost:</p>

<pre><code> backend_url = 'localhost:8000/...'
 requests.post(backend_url, data={}, allow_redirects=True, verify=False)
</code></pre>

<p>But I get the error </p>

<pre><code> requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /.../ (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f4d706f7588&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
</code></pre>
"
"31538314","Stopping docker container from inside","<docker>","53865557","How do I kill the container process (PID 1) from inside the container using bash/sh shell","<bash><docker>","<p>I have a cronjob running inside a docker container that checks whether all services are running as expected. If this cronjob determines that there is a problem I would like to stop the docker container (from inside...)</p>

<p>Unfortunately <code>exit</code> just stops my cronjob script</p>
","<p>I have a scenario where I have a shell script which is run using <code>crond</code>. I need to exit the container if that particular script fails. Seems like <code>SIGKILL</code> doesn't work with PID 1.</p>

<p>How do I kill the container process (PID 1) from inside the container using bash/sh shell?</p>

<p>Minimal example - </p>

<p>Dockerfile -</p>

<pre><code>FROM alpine:3.5

ENV LOGS_DIR=""/rest/logs/"" CRON_LOG_FILE=""${LOGS_DIR}/cron.log""

RUN apk add --update python py-pip zip bash &amp;&amp; \
    pip install awscli &amp;&amp; \
    mkdir -p ${LOGS_DIR} &amp;&amp; \
    touch ${CRON_LOG_FILE}

COPY ./lr-s3.sh ./lr-entry.sh ./install_crontab.txt ./files_to_rotate.txt ./

RUN chmod +x /lr-s3.sh /lr-entry.sh &amp;&amp; \
    crontab install_crontab.txt

ENTRYPOINT [""/lr-entry.sh""]
</code></pre>

<p>Entrypoint - </p>

<pre><code>#!/bin/bash

LOGS_DIR=""${LOGS_DIR:-/rest/logs}""
CRON_LOG_FILE=""${LOGS_DIR}/cron.log""

mkdir -p ${LOGS_DIR}
touch ${CRON_LOG_FILE}

ln -sf /proc/1/fd/1 ${CRON_LOG_FILE}

echo ""Cron [Starting]""
exec crond -c /var/spool/cron/crontabs -f -L ${CRON_LOG_FILE} ""$@""
</code></pre>

<p>Script to run via Cron - </p>

<pre><code>aws s3 cp ${LOGS_DIR}/${FL_NAME} s3://${BKTNAME}/${FL_NAME}

if [ ""$?"" -ne ""0"" ]; then
    echo ""S3 Backup Failed""
    pkill crond
    exit 1
fi
</code></pre>

<p><code>pkill crond</code> doesn't work inside the script, it has PID 1. </p>

<p>If container restarts or doesn't exist, we will come to know that there is an issue with the container or the script.</p>
"
"31885409","Why would a correct shell script give a wrapped/truncated/corrupted error message?","<bash><shell><sh><carriage-return>","53240682","Docker: Windows and Linux different output. Why?","<windows><docker><dockerfile><docker-machine><carriage-return>","<p>I have a shell script with a command that seems like it should work, but instead it fails with an odd wrapped/truncated/corrupted error message. Example:</p>

<pre><code>$ ls -l myfile
-rw-r----- 1 me me 0 Aug  7 12:36 myfile
$ cat myscript 
ls -l myfile
$ bash myscript
: No such file or directory
</code></pre>

<p>The file clearly exist, but even if I didn't, this is the kind of error message I would normally get:</p>

<pre><code>$ ls -l idontexist
ls: cannot access idontexist: No such file or directory
</code></pre>

<p>Notice how it includes the tool name <code>ls</code>, a message string and the filename while mine does not.</p>

<p>Here's what I get if I try to use <code>mysql</code> instead. The error message looks like it's been wrapped, and now starts with a quote:</p>

<pre><code>Command:  mysql -h myhost.example.com
Expected: ERROR 2005 (HY000): Unknown MySQL server host 'myhost.example.com' (0)
Actual:   ' (0) 2005 (HY000): Unknown MySQL server host 'myhost.example.com
</code></pre>

<p>And here's my trivial ssh command that should work, or at least give a normal error message, but which instead is wrapped to start with a colon and ends with strange clobbering:</p>

<pre><code>Command:  ssh myhost
Expected: ssh: Could not resolve hostname myhost: Name or service not known
Actual:   : Name or service not knownname myhost
</code></pre>

<p>Why does this happen, and how do I fix it?</p>
","<p>I discovered a really strange behaviour of my Dockerfile. It works perfectly under my linux machine but under windows I got a weird output running my container. Could somebody check my commands and tell me what is wrong with them? Why it works under ubuntu without any problems? I thought docker ensures that it can be started in the same way under different operations systems...</p>

<p>Windows commands:</p>

<p><code>git clone https://github.com/falent/googleHomeAssistantExpressNodeJS.git C:\Users\%username%\Documents\googleHomeAssistantExpressNodeJS</code></p>

<p><code>$ cd C:\Users\%username%\Documents\googleHomeAssistantExpressNodeJS</code></p>

<p><code>$ sudo build -t assistant .</code></p>

<p><code>$ docker run -v /C/Users/%username%/Documents/googleHomeAssistantExpressNodeJS:/skill -it --name myAssistant assistant</code></p>

<p>I got this output under windows:</p>

<pre><code>: not foundypoint.sh: 2: /skill/entrypoint.sh:
sleep: invalid time interval '10s\r'
Try 'sleep --help' for more information.
/skill/entrypoint.sh: 4: cd: can't cd to /skill
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM node:slim

RUN npm install -g nodemon@1.14.7

COPY package.json /skill/package.json

RUN mkdir -p /skill/
WORKDIR /skill/
RUN npm install

COPY entrypoint.sh /skill
RUN chmod +x /skill/entrypoint.sh

ENTRYPOINT [""sh"",""/skill/entrypoint.sh""]
</code></pre>

<p>entrypoint.sh</p>

<pre><code>#!/bin/sh

sleep 10s
cd /skill
npm install
nodemon 
</code></pre>
"
"34547637","Hibernate is generating auto increment alternating id for tables","<java><hibernate><auto-generate>","53732331","How can i access Postgres database running in docker-compose, and why my id behave so strange when adding data into Postgres?","<postgresql><docker><docker-compose>","<p>My environment : <code>Hibernate 5</code>, <code>Java 8</code>, <code>Phpmyadmin in WAMP</code></p>

<p>Problem: Hibernate creates auto increment id within a table, but the next sequence is given to a different table. </p>

<p><em>Expected</em></p>

<pre><code>Table 1        Table 2
1. Hello       1. Foo
2. World       2. Bar 
</code></pre>

<p><em>Instead it is creating</em></p>

<pre><code>Table 1        Table 2
1. Hello       2. Foo
3. World       4. Bar 
</code></pre>

<p><strong>Project Structure</strong></p>

<p><a href=""https://i.stack.imgur.com/bnLBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bnLBD.png"" alt=""enter image description here""></a></p>

<p><strong>hibernate.cfg.xml</strong></p>

<pre><code>  &lt;?xml version='1.0' encoding='utf-8'?&gt;
  &lt;!DOCTYPE hibernate-configuration PUBLIC
    ""-//Hibernate/Hibernate Configuration DTD 3.0//EN""
    ""http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd""&gt;

  &lt;hibernate-configuration&gt;

&lt;session-factory&gt;

    &lt;!-- Database connection settings --&gt;
    &lt;property name=""connection.driver_class""&gt;com.mysql.jdbc.Driver&lt;/property&gt;
    &lt;property name=""connection.url""&gt;jdbc:mysql://localhost:3306/ansarihibernate&lt;/property&gt;
    &lt;property name=""connection.username""&gt;localuser&lt;/property&gt;
    &lt;property name=""connection.password""&gt;&lt;/property&gt;

    &lt;!-- JDBC connection pool (use the built-in) --&gt;
    &lt;property name=""connection.pool_size""&gt;1&lt;/property&gt;

    &lt;!-- SQL dialect --&gt;
    &lt;property name=""dialect""&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt;

    &lt;!-- Disable the second-level cache  --&gt;
    &lt;property name=""cache.provider_class""&gt;org.hibernate.cache.internal.NoCacheProvider&lt;/property&gt;

    &lt;!-- Echo all executed SQL to stdout --&gt;
    &lt;property name=""show_sql""&gt;true&lt;/property&gt;

    &lt;property name=""hbm2ddl.auto""&gt;create&lt;/property&gt;   

    &lt;!-- Many To Many classes --&gt;
    &lt;mapping class=""org.ansari.hibernate.manytomany.StudentM2M""/&gt;       
    &lt;mapping class=""org.ansari.hibernate.manytomany.StudentM2MCertificates""/&gt;

&lt;/session-factory&gt;
</code></pre>

<p></p>

<p><strong>MainM2M.java</strong></p>

<pre><code>package org.ansari.hibernate.manytomany;

import org.hibernate.Session;
import org.hibernate.SessionFactory;
import org.hibernate.cfg.Configuration;

public class MainM2M {

public static void main(String[] args) {

    StudentM2MCertificates st1 = new StudentM2MCertificates();
    st1.setM2m_cert_det(""Oracle Cert"");
    StudentM2MCertificates st2 = new StudentM2MCertificates();
    st2.setM2m_cert_det(""Big Data Cert"");

    StudentM2M s1 = new StudentM2M();
    s1.setM2m_stu_name(""Ansari"");
    s1.getSetM2MCert().add(st1);

    StudentM2M s2 = new StudentM2M();
    s2.setM2m_stu_name(""Mohammed"");
    s2.getSetM2MCert().add(st2);                

    SessionFactory sessFac = new  Configuration().configure().buildSessionFactory();
    Session session = sessFac.openSession();
    session.beginTransaction();

    session.save(s1);
    session.save(s2);

    session.getTransaction().commit();
    session.close();
    sessFac.close();        
}
}
</code></pre>

<p><strong>StudentM2M.java</strong></p>

<pre><code>package org.ansari.hibernate.manytomany;

import java.util.HashSet;
import java.util.Set;

import javax.persistence.CascadeType;
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.Id;
import javax.persistence.ManyToMany;
import javax.persistence.Table;

@Entity
@Table(name=""STUDENTM2M"")
public class StudentM2M {

@Id
@GeneratedValue
private int m2m_id;

private String  m2m_stu_name;

@ManyToMany(cascade=CascadeType.ALL)
private Set&lt;StudentM2MCertificates&gt; setM2MCert = new HashSet&lt;StudentM2MCertificates&gt;(0);

public Set&lt;StudentM2MCertificates&gt; getSetM2MCert() {
    return setM2MCert;
}

public void setSetM2MCert(Set&lt;StudentM2MCertificates&gt; setM2MCert) {
    this.setM2MCert = setM2MCert;
}

public int getM2m_id() {
    return m2m_id;
}

public void setM2m_id(int m2m_id) {
    this.m2m_id = m2m_id;
}

public String getM2m_stu_name() {
    return m2m_stu_name;
}

public void setM2m_stu_name(String m2m_stu_name) {
    this.m2m_stu_name = m2m_stu_name;
}
}
</code></pre>

<p><strong>StudentM2MCertificates</strong></p>

<pre><code>package org.ansari.hibernate.manytomany;

import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.Id;
import javax.persistence.Table;

@Entity
@Table(name=""CERTM2M"")
public class StudentM2MCertificates {

@Id
@GeneratedValue
private int m2m_cert_id;

private String m2m_cert_det;

public int getM2m_cert_id() {
    return m2m_cert_id;
}

public void setM2m_cert_id(int m2m_cert_id) {
    this.m2m_cert_id = m2m_cert_id;
}

public String getM2m_cert_det() {
    return m2m_cert_det;
}

public void setM2m_cert_det(String m2m_cert_det) {
    this.m2m_cert_det = m2m_cert_det;
}
}
</code></pre>

<p><strong>Table Structure</strong></p>

<p><a href=""https://i.stack.imgur.com/CKY1i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKY1i.png"" alt=""enter image description here""></a></p>

<p><strong>certm2m</strong></p>

<p><a href=""https://i.stack.imgur.com/E9f6P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E9f6P.png"" alt=""enter image description here""></a></p>

<p><strong>hibernate_sequence</strong></p>

<p><a href=""https://i.stack.imgur.com/nvRJe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nvRJe.png"" alt=""enter image description here""></a></p>

<p><strong>studentm2m</strong></p>

<p><a href=""https://i.stack.imgur.com/ixbd7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ixbd7.png"" alt=""enter image description here""></a></p>

<p><strong>studentm2m_certm2m</strong></p>

<p><a href=""https://i.stack.imgur.com/bCgOh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bCgOh.png"" alt=""enter image description here""></a></p>
","<p>i have two tables. One table named <code>Cinema</code> contains all the cinemas, and other table named <code>Room</code> contains all the rooms belong to that cinema. In room there are id with reference to primary key in cinema table. </p>

<p>But when i add one cinema, its get id: 1, when i add another cinema its gets id: 2, but when i add a room with reference to primary key cinema id: 1, the room added gets id: 3?? </p>

<p>Why? should not room get id: 1 in their own tabel? Or did i have misunderstand how Postgres works?</p>

<p>My cinema entity:</p>

<pre><code>@Entity
class Cinema(

    @get:Id @get:GeneratedValue
    var id: Long? = null,

    @get:NotBlank @get:Size(max = 128)
    var name: String,

    @get:NotBlank @get:Size(max = 128)
    var location: String? = null,

    @get:OneToMany(mappedBy = ""cinema"", cascade = [CascadeType.ALL], fetch = FetchType.EAGER)
    var rooms: MutableSet&lt;Room&gt; = mutableSetOf()
)
</code></pre>

<p>And my room entity:</p>

<pre><code>@Entity
class Room(

    @get:Id @get:GeneratedValue
    var id: Long? = null,

    @get:NotBlank @get:Size(max = 128)
    var name: String,

    @get:ElementCollection
    @get:NotNull
    var seats: MutableSet&lt;String&gt;,

    @get:ManyToOne(fetch = FetchType.EAGER)
    @get:JoinColumn(name = ""cinema_id"")
    var cinema: Cinema? = null
)
</code></pre>

<p>I also run a <code>flyway</code> file to create database:</p>

<pre><code>create sequence hibernate_sequence start with 1 increment by 1;
create table user_entity_roles (user_entity_username varchar(255) not null, roles varchar(255));
create table users (username varchar(255) not null, enabled boolean not null, password varchar(255), primary key (username));
alter table user_entity_roles add constraint FKsn3bllbt5h2wue4tckbylorlj     
foreign key (user_entity_username) references users;
</code></pre>

<p>Now i want to try to access the Postgres database running in docker-compose to see how the database looks. How can i access it ?</p>

<p>This is my docker-compose logs from that database:</p>

<pre><code>postgres-cinema_1_aa7b4d8ccfa6 | The files belonging to this database system will be owned by user ""postgres"".
postgres-cinema_1_aa7b4d8ccfa6 | This user must also own the server process.
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | The database cluster will be initialized with locale ""en_US.utf8"".
postgres-cinema_1_aa7b4d8ccfa6 | The default database encoding has accordingly been set to ""UTF8"".
postgres-cinema_1_aa7b4d8ccfa6 | The default text search configuration will be set to ""english"".
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | Data page checksums are disabled.
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | fixing permissions on existing directory /var/lib/postgresql/data ... ok
postgres-cinema_1_aa7b4d8ccfa6 | creating subdirectories ... ok
postgres-cinema_1_aa7b4d8ccfa6 | selecting default max_connections ... 100
postgres-cinema_1_aa7b4d8ccfa6 | selecting default shared_buffers ... 128MB
postgres-cinema_1_aa7b4d8ccfa6 | selecting dynamic shared memory implementation ... posix
postgres-cinema_1_aa7b4d8ccfa6 | creating configuration files ... ok
postgres-cinema_1_aa7b4d8ccfa6 | running bootstrap script ... ok
postgres-cinema_1_aa7b4d8ccfa6 | performing post-bootstrap initialization ... ok
postgres-cinema_1_aa7b4d8ccfa6 | syncing data to disk ... ok
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | Success. You can now start the database server using:
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 |     pg_ctl -D /var/lib/postgresql/data -l logfile start
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | WARNING: enabling ""trust"" authentication for local connections
postgres-cinema_1_aa7b4d8ccfa6 | You can change this by editing pg_hba.conf or using the option -A, or
postgres-cinema_1_aa7b4d8ccfa6 | --auth-local and --auth-host, the next time you run initdb.
postgres-cinema_1_aa7b4d8ccfa6 | ****************************************************
postgres-cinema_1_aa7b4d8ccfa6 | WARNING: No password has been set for the database.
postgres-cinema_1_aa7b4d8ccfa6 |          This will allow anyone with access to the
postgres-cinema_1_aa7b4d8ccfa6 |          Postgres port to access your database. In
postgres-cinema_1_aa7b4d8ccfa6 |          Docker's default configuration, this is
postgres-cinema_1_aa7b4d8ccfa6 |          effectively any other container on the same
postgres-cinema_1_aa7b4d8ccfa6 |          system.
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 |          Use ""-e POSTGRES_PASSWORD=password"" to set
postgres-cinema_1_aa7b4d8ccfa6 |          it in ""docker run"".
postgres-cinema_1_aa7b4d8ccfa6 | ****************************************************
postgres-cinema_1_aa7b4d8ccfa6 | waiting for server to start....2018-12-11 20:47:19.832 UTC [44] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:19.916 UTC [45] LOG:  database system was shut down at 2018-12-11 20:47:19 UTC
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:19.933 UTC [44] LOG:  database system is ready to accept connections
postgres-cinema_1_aa7b4d8ccfa6 |  done
postgres-cinema_1_aa7b4d8ccfa6 | server started
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | waiting for server to shut down....2018-12-11 20:47:19.963 UTC [44] LOG:  received fast shutdown request
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:19.974 UTC [44] LOG:  aborting any active transactions
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:19.982 UTC [44] LOG:  worker process: logical replication launcher (PID 51) exited with exit code 1
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:19.984 UTC [46] LOG:  shutting down
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.050 UTC [44] LOG:  database system is shut down
postgres-cinema_1_aa7b4d8ccfa6 |  done
postgres-cinema_1_aa7b4d8ccfa6 | server stopped
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | PostgreSQL init process complete; ready for start up.
postgres-cinema_1_aa7b4d8ccfa6 |
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.139 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.139 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.147 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.203 UTC [53] LOG:  database system was shut down at 2018-12-11 20:47:20 UTC
postgres-cinema_1_aa7b4d8ccfa6 | 2018-12-11 20:47:20.235 UTC [1] LOG:  database system is ready to accept connections
</code></pre>
"
"36378615","Docker mysql cant connect to container","<php><mysql><docker><docker-compose>","53231517","My docker-compose not working properly when I try to connect mysql with nodejs","<docker><docker-compose>","<p>I've got docker-compose file for creating mysql image and expose port to 3306, but when I try to install CMS, it gives me error that it can't connect to Database. I try to scan port 3306 and it's showing me that it's open so mysql is running.</p>

<p>Why the two of docker containers can't see each other ?</p>

<p>Here is my docker-compose file:</p>

<pre><code>phpfpm:
  restart: always
  extends:
    file: php-fpm-5.6.yml
    service: phpfpm
  links:
    - db:db

nginx:
  restart: always
  image: nginx
  ports:
    - ""8000:80""
  links:
    - phpfpm:phpfpm
  volumes:
    - ./nginx/vhost.conf:/etc/nginx/conf.d/default.conf
    - ./app:/var/www/html
    - ./log/nginx:/var/log/nginx

db:
  restart: always
  image: mysql
  ports:
    - ""3306:3306""
  environment:
    MYSQL_ROOT_PASSWORD: 123456
    MYSQL_USER: user
    MYSQL_PASSWORD: password
    MYSQL_DATABASE: database
</code></pre>
","<p>here is my db.js file</p>

<pre><code>let connection = mysql.createConnection({
host: process.env.DATABASE_HOST || '127.0.0.1',
user: 'root',
database: 'bc2k19',
password: 'joeydash',
port: 33060
});
</code></pre>

<p>here is my docker-compose.yml file</p>

<pre><code>version: '3.2'
services:
  app:
    build: ./app
    ports:
    - ""3000:3000""
    depends_on:
    - db
    environment:
    - DATABASE_HOST=db
  db:
    build: ./db
    ports:
    - ""3306:3306""
</code></pre>

<p>here is my dockerfile for mysql </p>

<pre class=""lang-sh prettyprint-override""><code>FROM mysql:latest

ENV MYSQL_ROOT_PASSWORD joeydash
ENV MYSQL_DATABASE bc2k19
ENV MYSQL_USER joeydash
ENV MYSQL_PASSWORD joeydash

ADD setup.sql /docker-entrypoint-initdb.d
</code></pre>

<p>here is my dockerfile for my app </p>

<pre class=""lang-sh prettyprint-override""><code># Use Node v4 as the base

 image.
FROM node:latest

# Add everything in the current directory to our image, in the 'app' folder.
ADD . /app

# Install dependencies
RUN cd /app; \
    npm install --production

# Expose our server port.
EXPOSE 3000

# Run our app.
CMD [""node"", ""/app/bin/www""]
</code></pre>

<p>I don't know why everytime I try to do docker-compose up and connect it shows </p>

<pre><code>     Error: connect ECONNREFUSED 127.0.0.1:3306
app_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1117:14)
</code></pre>

<p>It shoes like the connection is not set but it is set</p>
"
"36930119","Remote API for docker exec commands","<docker>","53806182","How I can execute commands inside container by api","<linux><bash><api><docker>","<p>Could someone please help me with remote api for docker exec to run a command?</p>

<p>I am able to run it directly:</p>

<pre><code># docker exec 941430a3060c date
Fri Apr 29 05:18:03 UTC 2016
</code></pre>
","<p>I wonder if it is possible execute command (for example <strong>ls</strong>) inside container by docker api.</p>
"
"37366857","How to pass arguments to entrypoint in docker-compose.yml","<docker-compose>","53801558","How to specify container options in docker compose file?","<docker><docker-compose>","<p>I use this image: dperson/samba</p>

<p>The image is defining it's own entrypoint and I do not want to override it.</p>

<p>I need to pass arguments to the entrypoint, easy with docker only:</p>

<pre><code>docker run ... dperson/samba arg1 arg2 arg3
</code></pre>

<p>But how to do it with docker_compose.yml ?</p>

<p>Right now I use as a workaround: </p>

<pre><code>command: samba.sh arg1 arg2 arg3
</code></pre>

<p>But it is not satisfying as I force the redefinition of the entrypoint.</p>
","<p>I have deployed multiple containers using a docker-compose.yml file on a raspberry pi.
I would like to specify some options for one of those containers.
How can I do this ?</p>

<p>To be more specific:
I am using the docker image <a href=""https://github.com/dastrasmue/rpi-samba"" rel=""nofollow noreferrer"">dastrasmue/rpi-samba</a> and I want to set the options <code>-v /data:/share/data</code> and <code>-u ""pi:secret""</code> and <code>-s ""Pi (private):/share/data:rw:pi""</code> but I have no clue how I can do this in a composite docker application.</p>
"
"37733444","Mongodb: sharing a database between two computers","<mongodb>","53456277","MongoDB, two instances, same folder","<mongodb><docker>","<p>I created a database using MongoDB on Computer 1.
I am trying to load this DB from Computer 2.
Could you please help me to do that ?</p>

<p>I tried to set the --dbpath (data\db) in a shared disk F:\ but Computer 2 is not able to recognize the data.</p>
","<p>I do not find any good answer to my problem, so I ask you... hoping someone will find a solution...</p>

<p>I have to make two instance of mongoDB using the same /data/db folder...
There is my folders architecture : </p>

<ul>
<li>app/</li>
<li>app/docker-compose.yml</li>
</ul>

<pre>
  mongo:
    container_name: app_mongo
    image: mongo
    command: mongod --port 27017
    ports:
      - ""27017:27017""
    volumes:
      - /data/db:/data/db
</pre>

<ul>
<li>admin/</li>
<li>admin/docker-compose.yml</li>
</ul>

<pre>
  admin_mongo:
    container_name: admin_mongo
    image: mongo
    command: mongod --port 27027
    ports:
      - ""27027:27027""
    volumes:
      - /data/db:/data/db
</pre>

<p>So, how you can see, I am using stwo instancse of MongoDB, using two differents port (27017 and 27027) and sharing the same folder (/data/db)</p>

<p>But, an error occured when I try to run the second instance : </p>

<p><em>exception in initAndListen: DBPathInUse: Unable to lock the lock file: /data/db/mongod.lock (Unknown error). Another mongod instance is already running on the /data/db directory, terminating</em></p>

<p>My questions is : </p>

<ol>
<li>I split my app, one for the user, one for the admin but I want to share the same Data : Is there any solution, using this architecture ? </li>
<li>I was thinking maybe : I should export MongoDB to another docker-compose and Run it regardless of my applications... But how may I link it to my app, knowing the hostname of my mongodb will look like this below :

<ul>
<li>for this user app : <a href=""https://mongo:27017/"" rel=""nofollow noreferrer"">https://mongo:27017/</a></li>
<li>for this admin app : <a href=""https://admin_mongo:27027/"" rel=""nofollow noreferrer"">https://admin_mongo:27027/</a></li>
</ul></li>
</ol>

<p>Thanks a lot for reading</p>
"
"37945759","condas `source activate virtualenv` does not work within Dockerfile","<docker><anaconda><conda>","53350479","run a Python script with Conda dependencies on a Docker container","<python><docker><conda>","<h2>Scenario</h2>

<p>I'm trying to setup a simple docker image (<em>I'm quite new to docker, so please correct my possible misconceptions</em>) based on the public <a href=""https://hub.docker.com/r/continuumio/anaconda3/"" rel=""noreferrer"">continuumio/anaconda3</a> container.</p>

<p>The <code>Dockerfile</code>:</p>

<pre><code>FROM continuumio/anaconda3:latest

# update conda and setup environment
RUN conda update conda -y \
    &amp;&amp; conda env list \
    &amp;&amp; conda create -n testenv pip -y \
    &amp;&amp; source activate testenv \
    &amp;&amp; conda env list
</code></pre>

<p>Building and image from this by <code>docker build -t test .</code> ends with the error:</p>

<pre><code>/bin/sh: 1: source: not found
</code></pre>

<p>when activating the new virtual environment.</p>

<h2>Suggestion 1:</h2>

<p>Following <a href=""https://stackoverflow.com/a/25086628/3360982"">this answer</a> I tried:</p>

<pre><code>FROM continuumio/anaconda3:latest

# update conda and setup environment
RUN conda update conda -y \
    &amp;&amp; conda env list \
    &amp;&amp; conda create -y -n testenv pip \
    &amp;&amp; /bin/bash -c ""source activate testenv"" \
    &amp;&amp; conda env list
</code></pre>

<p>This seems to work at first, as it outputs: <code>prepending /opt/conda/envs/testenv/bin to PATH</code>, but <code>conda env list</code> as well ass <code>echo $PATH</code> clearly show that it doesn't:</p>

<pre><code>[...]
# conda environments:
#
testenv                  /opt/conda/envs/testenv
root                  *  /opt/conda

---&gt; 80a77e55a11f
Removing intermediate container 33982c006f94
Step 3 : RUN echo $PATH
---&gt; Running in a30bb3706731
/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
</code></pre>

<p>The docker files work out of the box as a MWE.
I appreciate any ideas. Thanks!</p>
","<p>I want to run a Python script from a Docker container that has dependencies set using Conda, i.e.:</p>

<p>Dockerfile:</p>

<pre><code>FROM continuumio/miniconda3

ADD environment.yml /environment.yml

RUN conda env create -f /environment.yml
# Pull the environment name out of the environment.yml
RUN echo ""source activate $(head -1 /environment.yml | cut -d' ' -f2)"" &gt; ~/.bashrc
ENV PATH /opt/conda/envs/$(head -1 /environment.yml | cut -d' ' -f2)/bin:$PATH

ADD hello.py /

CMD [""python"", ""./hello.py"" ]
</code></pre>

<p>The environment.yml contains all the dependencies and the python script for now it is just a hello world:</p>

<pre><code>import zeep


print(""hello"")
</code></pre>

<p>But when running the container I get:</p>

<pre><code>⇒  docker run  hello
Traceback (most recent call last):
  File ""./hello.py"", line 1, in &lt;module&gt;
    import zeep
ModuleNotFoundError: No module named 'zeep'
</code></pre>

<p>Why is that? If I start it interactively I can run the script fine.</p>

<pre><code>⇒  docker run -it hello /bin/bash
(smoke-test) root@1c593ac836b0:/# python hello.py
hello
</code></pre>
"
"39527571","Are shell scripts sensitive to encoding and line endings?","<bash><shell><sh>","53295617","Running docker build within a shell script","<bash><docker><sh><dockerfile>","<p>I am making a NW.js app on Mac, and want to run the app in dev mode by double-clicking on an icon. First step, I'm trying to make my shell script work.</p>

<p>Using VSCode on Windows (I wanted to gain time), I have created a <code>run-nw</code> file at the root of my project, containing this:</p>

<pre><code>#!/bin/bash

cd ""src""
npm install

cd ..
./tools/nwjs-sdk-v0.17.3-osx-x64/nwjs.app/Contents/MacOS/nwjs ""src"" &amp;
</code></pre>

<p>but I get this output:</p>

<pre><code>$ sh ./run-nw

: command not found  
: No such file or directory  
: command not found  
: No such file or directory  

Usage: npm &lt;command&gt;

where &lt;command&gt; is one of:  (snip commands list)

(snip npm help)

npm@3.10.3 /usr/local/lib/node_modules/npm  
: command not found  
: No such file or directory  
: command not found
</code></pre>

<p>I really don't understand:</p>

<ul>
<li>it seems that it takes empty lines as commands. In my editor (VSCode) I have tried to replace <code>\r\n</code> with <code>\n</code> (in case the <code>\r</code> creates problems) but it changes nothing.</li>
<li>it seems that it doesn't find the folders (with or without the <code>dirname</code> instruction), or maybe it doesn't know about the <code>cd</code> command ?</li>
<li>it seems that it doesn't understand the <code>install</code> argument to <code>npm</code></li>
<li>the part that really weirds me out, is that it still runs the app (if I did a <code>npm install</code> manually)...</li>
</ul>

<p>Not able to make it work properly, and suspecting something weird with the file itself, I created a new one directly on the Mac, using vim this time. I entered the exact same instructions, and... now it works without any issue.<br>
A diff on the two files reveals exactly zero difference.</p>

<p>What can be the difference? What can make the first script not work? How can I find out?</p>

<h2>Update</h2>

<p>Following the accepted answer's recommandations, after the wrong line endings came back, I checked multiple things. It turns out that since I copied my <code>~/.gitconfig</code> from my Windows machine, I had <code>autocrlf=true</code>, so every time I modified the bash file under Windows, it re-set the line endings to <code>\r\n</code>.<br>
So, in addition to running dos2unix (which you will have to install using Homebrew on mac), if you're using Git, check your config.</p>
","<p>I'm trying to set up a simple shell script that will do something along the following lines. Let's call it <code>docker_maker.sh</code>:</p>

<pre><code>do stuff
docker build -t my-registry.com/my_image:latest .
docker push my-registry.com/my_image:latest
do more stuff
</code></pre>

<p>But when I call the script:</p>

<pre><code>cd /where/my/dockerfile/is
sh docker_maker.sh
</code></pre>

<p>I get the following error:</p>

<pre><code>unable to prepare context: path "".\r"" not found
</code></pre>

<p>It seems that what is happening is that the docker build command is including the <code>.\r</code> linebreak characters as part of the build call. Is there a way that I could avoid this from happening? </p>
"
"41871950","Deploying docker-compose containers","<docker><docker-compose>","53417055","How to run the Docker image using docker-compose?","<docker><nginx><flask><docker-compose><dockerfile>","<p>I'm trying to deploy an app that's built with docker-compose, but it feels like I'm going in completely the wrong direction.</p>

<ol>
<li>I have everything working locally—<code>docker-compose up</code> brings up my app with the appropriate networks and hosts in place.</li>
<li>I want to be able to run the same configuration of containers and networks on a production machine, just using a different <code>.env</code> file.</li>
</ol>

<p>My current workflow looks something like this:</p>

<pre><code>docker save [web image] [db image] &gt; containers.tar
zip deploy.zip containers.tar docker-compose.yml
rsync deploy.zip user@server

ssh user@server
unzip deploy.zip ./
docker load -i containers.tar

docker-compose up
</code></pre>

<p>At this point, I was hoping to be able to run <code>docker-compose up</code> again when they get there, but that tries to rebuild the containers as per the <code>docker-compose.yml</code> file.</p>

<p>I'm getting the distinct feeling that I'm missing something. Should I be shipping over my full application then building the images at the server instead? How would you start composed containers if you were storing/loading the images from a registry?</p>
","<p>I have Flask application running under Docker Compose with 2 containers one for Flask and the other one for Nginx.</p>

<p>I am able to run the Flask successfully using <code>docker-compose up --build -d</code> command in my local machine.</p>

<p>What I want is, to save the images into .tar.gz file and move them to the production server and run them automatically. I have used below Bash script to save the Flask and Nginx into one image successfully.</p>

<pre><code>#!/bin/bash

for img in $(docker-compose config | awk '{if ($1 == ""image:"") print $2;}'); do
  images=""$images $img""
done

docker save $images | gzip -c &gt; flask_image.tar.gz
</code></pre>

<p>I then moved this image <code>flask_image.tar.gz</code> to my production server where Docker installed and used below command to load the image and run the containers.</p>

<pre><code>docker load -i flask_image.ta.gz
</code></pre>

<p>This command loaded every layer and loaded the image into my production server. But containers are not up which is expected, since I used only load command.</p>

<p>My question is, is there any command that can load the image and up the containers automatically?</p>

<p><code>docker-compose.yml</code></p>

<pre><code>version: '3'

services:

  api:
    container_name: flask
    image: flask_img
    restart: always
    build: ./app
    volumes:
      - ~/docker_data/api:/app/uploads
    ports:
      - ""8000:5000""
    command: gunicorn -w 1 -b :5000 wsgi:app -t 900

  proxy:
    container_name: nginx
    image: proxy_img
    restart: always
    build: ./nginx
    volumes:
      - ~/docker_data/nginx:/var/log/nginx/
    ports:
      - ""85:80""
    depends_on:
      - api
</code></pre>
"
"43654656","Dockerfile if else condition with external arguments","<docker><dockerfile>","53728418","Docker IF-ELSE with AND condition?","<docker>","<p>I have dockerfile</p>

<pre><code>FROM centos:7
ENV foo=42
</code></pre>

<p>then I build it </p>

<pre><code>docker build -t my_docker .
</code></pre>

<p>and run it.</p>

<pre><code>docker run -it -d  my_docker
</code></pre>

<p>Is it possible to pass arguments from command line and use it with if else in Dockerfile? I mean something like</p>

<pre><code>FROM centos:7
if (my_arg==42)
     {ENV=TRUE}
else:
     {ENV=FALSE}
</code></pre>

<p>and build with this argument.</p>

<pre><code> docker build -t my_docker . --my_arg=42
</code></pre>
","<p>Was following the conversation in this thread <a href=""https://stackoverflow.com/questions/31528384/conditional-copy-add-in-dockerfile"">Conditional COPY/ADD in Dockerfile?</a>. </p>

<p>I have a requirement to check AND conditions like this:</p>

<pre><code>IF Environment = development and Region = East : Do this
IF Environment = development and Region = West : Do this
IF Environment = production and Region = East : Do this
IF Environment = production and Region = East : Do this
</code></pre>

<p>How would I achieve this?</p>
"
"44722341","docker login unknown shorthand flag: 'e'","<docker><aws-ecr>","53865688","AWS Docker deployment fails with error: unknown shorthand flag: 'e' in -e and no basic auth credentials","<amazon-web-services><docker><amazon-ec2>","<p>I just updated my docker version and found out that command </p>

<pre><code>aws ecr get-login
</code></pre>

<p>is not working anymore. Got error: </p>

<blockquote>
  <p>unknown shorthand flag: 'e' in -e`. Seems that docker doesn't support -e flag anymore. </p>
</blockquote>

<p>Is there a way to fix this?</p>

<p>Installed versions:</p>

<pre><code>aws-cli/1.11.111 Python/2.7.10 Darwin/16.6.0 botocore/1.5.74

Docker version 17.06.0-ce-rc5, build b7e4173
</code></pre>
","<p>I try to deploy a docker container to AWS EC2 with AWS Codepipeline. </p>

<p>I get the following two erros:</p>

<pre><code>Container] 2018/12/19 16:42:43 Running command chmod +x buildspec_prebuild.sh &amp;&amp; ./buildspec_prebuild.sh
----------- AWS ECR Login -----------
unknown shorthand flag: 'e' in -e
See 'docker login --help'.
{
 ""failures"": [
 {
 ""failureCode"": ""ImageNotFound"",
 ""failureReason"": ""Requested image not found"",
 ""imageId"": {
 ""imageTag"": ""submissionmanager-backend""
 }
 }
 ],
 ""imageIds"": []
}
</code></pre>

<p>And I think as a following error I get this:</p>

<p><code>no basic auth credentials</code></p>

<p>My Files look like this:</p>

<p><strong>buildspec_build.sh</strong></p>

<pre><code>#!/bin/bash

# make code ready for docker
sbt docker:stage
cd target/docker/stage

# add port for aws to dockerfile
echo ""EXPOSE 9000"" &gt;&gt; Dockerfile

# generate docker image tag
docker build -t ""$(cat /tmp/build_tag.out)"" .
</code></pre>

<p><strong>buildspec_install.sh</strong></p>

<pre><code>#!/bin/bash
apt-get update

# install jdk
apt-get -y install software-properties-common
apt-get -y install -y python-software-properties debconf-utils
add-apt-repository -y ppa:openjdk-r/ppa
apt-get update
apt-get -y install openjdk-8-jdk
update-alternatives --config java
update-alternatives --config javac

sudo update-ca-certificates -f
sudo /var/lib/dpkg/info/ca-certificates-java.postinst configure

echo ""java installation ok""

# install sbt
echo ""starting sbt installation""

apt-get install -y apt-transport-https
echo ""deb https://dl.bintray.com/sbt/debian /"" | sudo tee -a /etc/apt/sources.list.d/sbt.list
apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823

# install python &amp; pip
echo ""Installing python &amp; pip""
apt-get install -y python3-pip

apt-get update
apt-get install -y sbt
echo ""sbt installation ok""

pip3 install --upgrade awscli
</code></pre>

<p><strong>buildspec_postbuild.sh</strong></p>

<pre><code>#!/bin/bash

# push docker image to aws ec2 container repository
docker push ""$(cat /tmp/build_tag.out)""
</code></pre>

<p><strong>buildspec_prebuild.sh</strong></p>

<pre><code>#!/bin/bash
printf ""%s:%s"" ""$REPOSITORY_URI"" ""$IMAGE_TAG"" &gt; /tmp/build_tag.out
echo ""----------- AWS ECR Login -----------""
$(aws ecr get-login)

#$(aws ecr get-login | sed 's|https://||')

# delete old docker image
aws ecr batch-delete-image --repository-name $REPOSITORY_NAME --image-ids imageTag=$IMAGE_TAG
</code></pre>

<p>What could be my issue in this case?</p>

<p>Thanks in advance.</p>
"
"44862100","Need to run docker run command inside python script","<python><docker>","53697261","Running docker-compose from python","<python><python-3.x><docker><docker-compose>","<p>My Docker Command is:</p>

<pre><code>docker run --rm wappalyzer/cli https://wappalyzer.com
</code></pre>

<p>When I run my python script:</p>

<pre><code>#!/usr/bin/python
from subprocess import call
import json
import os
import docker

docker run --rm wappalyzer/cli ""MYURL""
</code></pre>

<p>it says</p>

<pre><code>File ""t.py"", line 7
    docker run --rm wappalyzer/cli ""MYURL""
             ^
SyntaxError: invalid syntax
</code></pre>

<p>My os is ubuntu 14.04 and I am using ubuntu terminal.</p>
","<p>I am looking for a way to run a docker-compose file from python script. I looked in Docker SDK for python, but i didn't found anything about docker-compose.
So, is there a way to run a docker-compose file from python script?</p>
"
"46779471","find command with subprocess not working with out Shell=True","<python><find><subprocess><python-2.6>","53285088","'No such file or directory' when I use python subprocess.run('docker build ...')","<python><docker>","<p>I have below lines in my code. I have embedded a short a line which fetches the list of files that are older than 10 mins. My sub process have been failing with few errors. It seems to work when I give Shell=True, but I read that it is a very risky to use that option and I am very new to the Python, Don't want to mess up with something I am not understanding. I have tried changing quotes with in and around that find statement, but its not helping me. can you please suggest how could I get the list of files using find command.
I have looked into other questions with these find and subprocess combinations, I did not find any wildcard directory matches. I could not find solution for this.</p>

<pre><code>cmd = 'find /myapp/uat/aws/6.3/domains/*/appnodes/*/*/log/bwappnode.log -type f -mmin +10'

 apps_in_proc = subprocess.Popen(cmd,stdout=subprocess.PIPE, universal_newlines=True)
</code></pre>
","<p>In ubuntu18.04</p>

<p>I am writing a python script to build a docker image then do other things, the <code>main.py</code> is here:</p>

<pre><code>from subprocess import run
run('docker build --rm -f ""Dockerfile"" -t test:latest .')
...
</code></pre>

<p>Then error occurs:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/lib/python3.6/subprocess.py"", line 403, in run
with Popen(*popenargs, **kwargs) as process:
File ""/usr/lib/python3.6/subprocess.py"", line 709, in __init__
restore_signals, start_new_session)
File ""/usr/lib/python3.6/subprocess.py"", line 1344, in _execute_child
raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'docker build --rm -f ""Dockerfile"" -t test:latest .': 'docker build --rm -f ""Dockerfile"" -t test:latest .'
</code></pre>

<p>While the <code>Dockerfile</code> and <code>main.py</code> are put together and it is fine to run the build command in the shell directly.</p>
"
"48595829","How to pass environment variables to a frontend web application?","<docker><environment-variables><dockerfile><12factor>","53855305","Angular Docker Runtime Environments","<angular><typescript><docker>","<p>I am trying to containerize a frontend web application and I am having troubles to figure out how to pass environment variables. The application is a Angular application, so it is 100% client-side.</p>

<p>In a typical backend service, passing environment variables is easy, as everything is running on the same host, so the environment variables can be easily picked by the backend service. However, in a frontend application, this is different: the application is running in the browser of the client.</p>

<p>I want to configure my application via environment variables, as this makes deployment much easier. All configuration can be done in <code>docker-compose.yml</code> and there is no need to maintain several images, one for every possible environment. There is just one single immutable image. This follows the 12-factor application philosophy, as can be found on <a href=""https://12factor.net/config"" rel=""noreferrer"">https://12factor.net/config</a>.</p>

<p>I am building my application image as following:</p>

<pre><code>FROM node:alpine as builder
COPY package.json ./
RUN npm i &amp;&amp; mkdir /app &amp;&amp; cp -R ./node_modules ./app
WORKDIR /app
COPY . .
RUN $(npm bin)/ng build

FROM nginx:alpine
COPY nginx/default.conf /etc/nginx/conf.d/
RUN rm -rf /usr/share/nginx/html/*
COPY --from=builder /app/dist /usr/share/nginx/html
CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p>In <code>app/config.ts</code>, I have:</p>

<pre><code>export const config = {
    REST_API_URL: 'http://default-url-to-my-backend-rest-api'
};
</code></pre>

<p>Ideally, I want to do something like this in my <code>docker-compose.yml</code>:</p>

<pre><code>backend:
  image: ...
frontend:
  image: my-frontend-app
  environment:
    - REST_API_URL=http://backend:8080/api
</code></pre>

<p>So I believe I should alter this <code>app/config.ts</code> to replace <code>REST_API_URL</code> with the environment variable. As I prefer an immutable Docker image (so I do not want to do this replace during the build), I am quite puzzled how to progress here. I believe I should support to alter the <code>app/config.ts</code> at runtime before the nginx proxy is started. However, the fact that this file is minified and webpack-bundled, makes this more diffucult.</p>

<p>Any ideas how to tackle this?</p>
","<p>I have an <strong>angular</strong> app, which runs with nginx in a docker container. The docker image is publish in a private docker hub. Now I want to pass the environments from docker to <strong>compiled</strong> angular application. At time I create a <code>config.json</code> and a <code>ConfigService</code>, which reads out the file. The file can be change over docker volume. Unfortunately, I have no idea how to use the environment variable in the module.</p>

<pre><code>@NgModule({
  imports: [
    ...
    AgmCoreModule.forRoot({
      apiKey: environment.system.googleMaps.apiKey // old way
    }),
    ...
  ],
  declarations: [
    ...
  ],
  providers: [
    ...
    ConfigService
    ...
  ]
})
export class ContractsModule {}
</code></pre>

<p>Besides, it is only a workaround. Perfect would be to use the docker environment. Have some an idea?</p>

<p>I think this is a good way: <a href=""https://stackoverflow.com/a/49349963/8581092"">https://stackoverflow.com/a/49349963/8581092</a>.</p>
"
"50291827","Why can't I use the build arg again after FROM in a Dockerfile?","<docker><dockerfile><docker-build>","52873524","Why can't I print an ARG with echo in Dockerfile?","<docker>","<p>I'm using Docker <code>18.05.0~ce~3-0~ubuntu</code> and I'd like to pass a build argument to the FROM as well as other lines in my Dockerfile. You would expect the below to work:</p>

<pre><code>ARG FROM_IMAGE=ubuntu:bionic
FROM $FROM_IMAGE

COPY sources_list/$FROM_IMAGE /etc/apt/sources.list
</code></pre>

<p>It works for the second line (<code>FROM</code>), but it behaves like it is unset in the <code>COPY</code> line:</p>

<blockquote>
  <p>Step 1/3 : ARG FROM_IMAGE=ubuntu:bionic
  Step 2/3 : FROM $FROM_IMAGE
   ---> 8626492fecd3
  [...]
  Step 3/3 : COPY sources_list/${SOURCES_LIST_FILE} /etc/apt/sources.list
  failed to copy files: failed to copy directory: mkdir
  /var/lib/docker/overlay2/0536b4e280ddca2fec18db9d79fa625a8be86efdbaaea5b3dbbefcdaaab3f669/merged/etc/apt/sources.list:
  not a directory</p>
</blockquote>

<p>If add another, separate build arg, it works for the same <code>COPY</code> line:</p>

<pre><code>ARG FROM_IMAGE=ubuntu:bionic
FROM $FROM_IMAGE

ARG SOURCES_LIST_FILE
COPY sources_list/${SOURCES_LIST_FILE} /etc/apt/sources.list
</code></pre>

<blockquote>
  <p>Step 4/4 : COPY sources_list/${SOURCES_LIST_FILE} /etc/apt/sources.list
   ---> 7f974fffe929</p>
</blockquote>

<p>Why can't I use the <code>FROM_IMAGE</code> build arg twice, on and after a <code>FROM</code> line? I fail to find any documented restriction of this sort.</p>
","<p>Specifying <code>ARG</code> in a <code>Dockerfile</code> even allows me to change the image tag with <code>docker build --build-arg</code>. Why doesn't it work as argument to <code>echo</code>?:</p>

<pre><code>ARG VAR0
FROM ubuntu:18.04
RUN echo $VAR0
</code></pre>

<p>results in</p>

<pre><code>$ docker build -f Dockerfile-test --build-arg=ARG0=""Hello!"" --no-cache=true .
Sending build context to Docker daemon  19.97kB
Step 1/3 : ARG VAR0
Step 2/3 : FROM ubuntu:18.04
 ---&gt; cd6d8154f1e1
Step 3/3 : RUN echo $VAR0
 ---&gt; Running in 9724e22b51cc

Removing intermediate container 9724e22b51cc
 ---&gt; 9f6369d2de17
[Warning] One or more build-args [ARG0] were not consumed
Successfully built 9f6369d2de17
</code></pre>

<p>(where I expect <code>Hello!</code> to be printed to console for the third step) whereas</p>

<pre><code>ARG VAR0
FROM ubuntu:$VAR0
echo ""Hello Docker!""
</code></pre>

<p>even allows to make the Ubuntu version configurable at build-time.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","53825146","Can't publish to Kafka","<java><docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have the following Kafka producer:</p>

<pre><code>package dathanb;

import org.apache.kafka.clients.producer.*;

import java.util.Properties;

public class Producer {

    public static void main(String[] args){
        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:9092"");
        properties.put(ProducerConfig.ACKS_CONFIG, ""all"");
        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ""org.apache.kafka.common.serialization.StringSerializer"");
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ""org.apache.kafka.common.serialization.StringSerializer"");
        properties.put(ProducerConfig.LINGER_MS_CONFIG, 1000);
        properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 5000);

        try (KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(properties)) {
            System.out.println(kafkaProducer.partitionsFor(""kafka-test""));
            for (int i = 0; i &lt; 1000; i++) {
                System.out.println(i);
                var metadataFuture = kafkaProducer.send(new ProducerRecord&lt;&gt;(""kafka-test"", 0, null, ""test message - "" + i), callback());
                System.out.println(metadataFuture.get().partition());
                Thread.sleep(1000);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static Callback callback() {
        return (metadata, exception) -&gt; {
            System.out.println(metadata);
            System.out.println(exception);
        };
    }
}
</code></pre>

<p>Line 20 <code>System.out.println(kafkaProducer.partitionsFor(""kafka-test""));</code> prints out what looks like the right partition config: <code>[Partition(topic = kafka-test, partition = 0, leader = 0, replicas = [0], isr = [0], offlineReplicas = [])]</code>, but line 24 never prints anything, and <code>callback</code> is never called -- the application appears to hang indefinitely. I don't even get a timeout, which I would expect.</p>

<p>I'm running Kafka 2.1.0 on Scala 2.12 (<a href=""https://github.com/dathanb/docker-kafka/blob/master/kafka/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile</a>), and I'm using the kafka-clients 2.1.0 library. I'm running Kafka dockerized using Docker Compose. Here's my docker-compose.yml:</p>

<pre><code>version: ""2""
services:
  kafkaserver:
    image: ""kafka""
    container_name: kafka
    hostname: kafkaserver
    networks:
      - kafkanet
    ports:
      - ""2181:2181""
      - ""9092:9092""
    environment:
      ADVERTISED_HOST: ""kafkaserver""
      ADVERTISED_PORT: ""9092""
networks:
  kafkanet:
    driver: bridge
</code></pre>

<p>The repo is <a href=""https://github.com/dathanb/kafka-quickstart"" rel=""nofollow noreferrer"">hosted here</a>.</p>

<p>What would cause this behavior?</p>

<p><em>EDIT:</em></p>

<p>After adding log4j and the slf4j-log4j12 bridge, I now see the following error in my logs, over and over and over again:</p>

<pre><code>    [2018-12-17 17:39:44,885] ERROR [Producer clientId=producer-1] Uncaught error in kafka producer I/O thread:  (org.apache.kafka.clients.producer.internals.Sender)
    java.lang.IllegalStateException: No entry found for connection 0
        at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:330)
        at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:134)
        at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:921)
        at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:287)
        at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:335)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:308)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:233)
        at java.base/java.lang.Thread.run(Thread.java:834)
</code></pre>
"
"52979322","matplotlib 3.0.0, cannot import name 'get_backend' from 'matplotlib'","<python><matplotlib><anaconda>","52990572","Python3 + matplotlib on Alpine error: ImportError: cannot import name 'get_backend'","<python><docker><matplotlib><dockerfile>","<p>Using Windows 10, anaconda as a package manager. I have a base environment running python 3.7 where matplotlib works fine. When I create a new environment and install both keras and matplotlib, I start to run into problems:</p>

<pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 1111, in &lt;module&gt;
    rcParamsOrig = RcParams(rcParams.copy())
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 891, in __getitem__
    from matplotlib import pyplot as plt
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\pyplot.py"", line 32, in &lt;module&gt;
    import matplotlib.colorbar
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\colorbar.py"", line 40, in &lt;module&gt;
    import matplotlib._constrained_layout as constrained_layout
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\_constrained_layout.py"", line 52, in &lt;module&gt;
    from matplotlib.legend import Legend
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\legend.py"", line 43, in &lt;module&gt;
    from matplotlib.offsetbox import HPacker, VPacker, TextArea, DrawingArea
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\offsetbox.py"", line 33, in &lt;module&gt;
    from matplotlib.image import BboxImage
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\image.py"", line 19, in &lt;module&gt;
    from matplotlib.backend_bases import FigureCanvasBase
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\backend_bases.py"", line 46, in &lt;module&gt;
    from matplotlib import (
ImportError: cannot import name 'get_backend'
</code></pre>

<p>Any suggestions? This is a fresh installation of conda. All I've done to get here is run <code>conda create --name keras_env keras matplotlib</code>, enter the environment, and try to import matplotlib. These are the packages conda installs:</p>

<pre><code>## Package Plan ##

environment location: C:\...\Anaconda3\envs\keras_env

added / updated specs:
- keras
- matplotlib


The following NEW packages will be INSTALLED:

_tflow_select:       2.2.0-eigen
absl-py:             0.5.0-py36_0
astor:               0.7.1-py36_0
blas:                1.0-mkl
ca-certificates:     2018.03.07-0
certifi:             2018.10.15-py36_0
cycler:              0.10.0-py36h009560c_0
freetype:            2.9.1-ha9979f8_1
gast:                0.2.0-py36_0
grpcio:              1.12.1-py36h1a1b453_0
h5py:                2.8.0-py36h3bdd7fb_2
hdf5:                1.10.2-hac2f561_1
icc_rt:              2017.0.4-h97af966_0
icu:                 58.2-ha66f8fd_1
intel-openmp:        2019.0-118
jpeg:                9b-hb83a4c4_2
keras:               2.2.4-0
keras-applications:  1.0.6-py36_0
keras-base:          2.2.4-py36_0
keras-preprocessing: 1.0.5-py36_0
kiwisolver:          1.0.1-py36h6538335_0
libpng:              1.6.35-h2a8f88b_0
libprotobuf:         3.6.0-h1a1b453_0
markdown:            3.0.1-py36_0
matplotlib:          3.0.0-py36hd159220_0
mkl:                 2019.0-118
mkl_fft:             1.0.6-py36hdbbee80_0
mkl_random:          1.0.1-py36h77b88f5_1
numpy:               1.15.3-py36ha559c80_0
numpy-base:          1.15.3-py36h8128ebf_0
openssl:             1.0.2p-hfa6e2cd_0
pip:                 10.0.1-py36_0
protobuf:            3.6.0-py36he025d50_0
pyparsing:           2.2.2-py36_0
pyqt:                5.9.2-py36h6538335_2
python:              3.6.7-h33f27b4_0
python-dateutil:     2.7.3-py36_0
pytz:                2018.5-py36_0
pyyaml:              3.13-py36hfa6e2cd_0
qt:                  5.9.6-vc14h1e9a669_2
scipy:               1.1.0-py36h4f6bf74_1
setuptools:          40.4.3-py36_0
sip:                 4.19.8-py36h6538335_0
six:                 1.11.0-py36_1
sqlite:              3.25.2-hfa6e2cd_0
tensorboard:         1.11.0-py36he025d50_0
tensorflow:          1.11.0-eigen_py36h346fd36_0
tensorflow-base:     1.11.0-eigen_py36h45df0d8_0
termcolor:           1.1.0-py36_1
tornado:             5.1.1-py36hfa6e2cd_0
vc:                  14.1-h0510ff6_4
vs2015_runtime:      14.15.26706-h3a45250_0
werkzeug:            0.14.1-py36_0
wheel:               0.32.2-py36_0
wincertstore:        0.2-py36h7fe50ca_0
yaml:                0.1.7-hc54c509_2
zlib:                1.2.11-h8395fce_2
</code></pre>
","<p>I have a micro service which build stopped working, probably because I have no explicit versions in my Dockerfile or requirements file. I do need help.</p>

<pre><code>FROM python:3.6-alpine
RUN apk add --no-cache libpng freetype libstdc++ python3
RUN apk add --no-cache --virtual .build-deps \
    gcc \
    build-base \
    python3-dev \
    libpng-dev \
    musl-dev \
    freetype-dev
RUN python -m ensurepip --default-pip
RUN ln -s /usr/include/locale.h /usr/include/xlocale.h \
    &amp;&amp; pip install numpy \
    &amp;&amp; pip install matplotlib \
    &amp;&amp; apk del .build-deps
</code></pre>

<p>When I'm using matplotlib I get this error:</p>

<pre><code>from matplotlib import (
E   ImportError: cannot import name 'get_backend'
</code></pre>

<p>I saw info about it, but couldn't resolve this issue. Can someone help?</p>
"
"53326899","Why docker container name has an random number at the end?","<docker><docker-compose><containers>","53515963","How to adapt to new slug numering scheme in version 1.23?","<docker><docker-compose>","<p>I have a docker-compose.yml file as below (a piece of it):</p>

<pre><code>version: '3.5'
services:
    framework:
    image: ${DOCKER_REGISTRY}/gme/fmk:${COMPOSE_PROJECT_NAME}
    build: ./fmk
    ports:
      - ""2020:2020""
      - ""2025:2025""
      - ""4999:4999""
    volumes:
      - ${FOLDER_ENV}/workspace/logs/framework:/var/log/gcti
      - ${FOLDER_ENV}/..:/usr/local/genesys/gsg_qaart
</code></pre>

<p>what I got is:</p>

<pre><code>vagrant@docker:/repos/gsg_qaart/docker$ docker-compose ps
]              Name                             Command               State                                             
Ports
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
callback_framework_1_df361f67842c   /bootstrap.sh                    Up      0.0.0.0:2020-&gt;2020/tcp, 0.0.0.0:2025-&gt;2025/tcp, 0.0.0.0:4999-&gt;4999/tcp, 5432/tcp
</code></pre>

<p>As you can see the name is weird, it supposes to be ""callback_framwork_1"", why there is a random number at the end?</p>

<p>BTW, I'm using:</p>

<pre><code>vagrant@docker:/repos/gsg_qaart/docker$ docker -v
Docker version 18.09.0, build 4d60db4
vagrant@docker:/repos/gsg_qaart/docker$ docker-compose -v
docker-compose version 1.23.1, build b02f1306
</code></pre>

<p>Thanks.</p>
","<p>With version 1.23.0 of <a href=""https://github.com/docker/compose/releases"" rel=""nofollow noreferrer"">docker compose</a> the container naming scheme changed.</p>

<blockquote>
  <p>Important note</p>
  
  <p>The default naming scheme for containers created by Compose in this version
  has changed from project_service_index to
  project_service_index_slug, where slug is a randomly-generated
  hexadecimal string. Please make sure to update scripts relying on the old
  naming scheme accordingly before upgrading.</p>
</blockquote>

<p>Every time I call <code>docker-compose up -d</code> the <code>slug</code> is newly generated. </p>

<p>My problem: Inside on of my containers I have a PowerShell script (<code>build_container</code>) that performs SVN operations on a second container (<code>container_svn</code>). Since the container name is part of the SVN-URL my scripts are broken. Reason: The old SVN-URL was e.g. <a href=""http://container_svn_1/repos"" rel=""nofollow noreferrer"">http://container_svn_1/repos</a>. Based on the slug change my <code>build_container</code> is not able to connect to <code>container_svn_1</code> anymore, since the SVN container seems to be only reachable under <code>container_svn_1_someRandomSlug</code>.</p>

<p>With docker-compose &lt; 1.23.0 is was able to ping the <code>container_svn</code> from <code>build_container</code>via </p>

<pre><code>PS C:\&gt; ping container_svn_1
</code></pre>

<p>Now with docker-compose > 1.23.0 I've to use:</p>

<pre><code>PS C:\&gt; ping container_svn_1_c298f27bdf5c
</code></pre>

<p>How can I inject ""container_svn_1_c298f27bdf5c"" as the name of <code>container_svn</code> in the <code>build_container</code>? Is it possible to inject the name via an environment variable?</p>

<p>Thx</p>
"
"53326899","Why docker container name has an random number at the end?","<docker><docker-compose><containers>","53531228","Remove random string from docker container name","<docker><docker-compose><docker-container>","<p>I have a docker-compose.yml file as below (a piece of it):</p>

<pre><code>version: '3.5'
services:
    framework:
    image: ${DOCKER_REGISTRY}/gme/fmk:${COMPOSE_PROJECT_NAME}
    build: ./fmk
    ports:
      - ""2020:2020""
      - ""2025:2025""
      - ""4999:4999""
    volumes:
      - ${FOLDER_ENV}/workspace/logs/framework:/var/log/gcti
      - ${FOLDER_ENV}/..:/usr/local/genesys/gsg_qaart
</code></pre>

<p>what I got is:</p>

<pre><code>vagrant@docker:/repos/gsg_qaart/docker$ docker-compose ps
]              Name                             Command               State                                             
Ports
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
callback_framework_1_df361f67842c   /bootstrap.sh                    Up      0.0.0.0:2020-&gt;2020/tcp, 0.0.0.0:2025-&gt;2025/tcp, 0.0.0.0:4999-&gt;4999/tcp, 5432/tcp
</code></pre>

<p>As you can see the name is weird, it supposes to be ""callback_framwork_1"", why there is a random number at the end?</p>

<p>BTW, I'm using:</p>

<pre><code>vagrant@docker:/repos/gsg_qaart/docker$ docker -v
Docker version 18.09.0, build 4d60db4
vagrant@docker:/repos/gsg_qaart/docker$ docker-compose -v
docker-compose version 1.23.1, build b02f1306
</code></pre>

<p>Thanks.</p>
","<p>Even when I run ""docker-compose up -d"", containers fill a random string as suffix automatically (after _1). However, those strings did not appear few days ago.</p>

<p>I didn't change anything in docker-compose.yml. I am using Docker Desktop for Windows. </p>

<p>Anyone please tell me how to remove them? Thanks.</p>

<p><strong>nginx_1_874f33129390</strong> should be <strong>nginx_1</strong>.</p>

<pre><code>CONTAINER ID        IMAGE         NAMES
ddaca6e6aad3        nginx         nginx_1_874f33129390
c038bc23404c        php-fpm       php-fpm_1_efb57fda5fdb
e65d0468804a        php-worker    php-worker_1_d59af45d7431
660f98415824        workspace     workspace_1_67e1d0a5cc29
db4149e1df85        redis         redis_1_3defcbd0cae2
445f8f6e11b0        mysql         mysql_1_c2d15aaf6137
</code></pre>
"
"2960339","Unable to install pyodbc on Linux","<python><linux><centos><pyodbc>","59048087","Docker doesn't accept pyOBDC in image build","<python><docker>","<p>I am running Linux (2.6.18-164.15.1.el5.centos.plus) and trying to install pyodbc.  I am doing pip install pyodbc and get a very long list of errors, which end in </p>

<blockquote>
  <p>error: command 'gcc' failed with exit status 1</p>
</blockquote>

<p>I looked in  <strong>/root/.pip/pip.log</strong> and saw the following:</p>

<blockquote>
  <p>InstallationError: Command /usr/local/bin/python -c ""import setuptools; <strong>file</strong>='/home/build/pyodbc/setup.py'; execfile('/home/build/pyodbc/setup.py')"" install --single-version-externally-managed --record /tmp/pip-7MS9Vu-record/install-record.txt failed with error code 1</p>
</blockquote>

<p>Has anybody had a similar issue installing pyodbc?</p>
","<p>I'm trying to build q docker image of my python application. But every time i run Docker built it gives the next error.</p>

<pre><code>Building wheels for collected packages: pyodbc, matplotlib, fpdf, pandas, MarkupSafe
  Building wheel for pyodbc (setup.py): started
  Building wheel for pyodbc (setup.py): finished with status 'error'
  ERROR: Command errored out with exit status 1:
   command: /usr/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-_00fg6j6/pyodbc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-_00fg6j6/pyodbc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-gq580il1 --python-tag cp38
       cwd: /tmp/pip-install-_00fg6j6/pyodbc/
  Complete output (14 lines):
  running bdist_wheel
  running build
  running build_ext
  building 'pyodbc' extension
  creating build
  creating build/temp.linux-x86_64-3.8
  creating build/temp.linux-x86_64-3.8/src
  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DPYODBC_VERSION=4.0.27 -I/usr/local/include/python3.8 -c src/buffer.cpp -o build/temp.linux-x86_64-3.8/src/buffer.o -Wno-write-strings
  In file included from src/buffer.cpp:12:
  src/pyodbc.h:56:10: fatal error: sql.h: No such file or directory
   #include &lt;sql.h&gt;
            ^~~~~~~
  compilation terminated.
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for pyodbc
</code></pre>

<p>If i remove pyODBC from my requirements.txt it builds without problem.
My requirements.txt contains:</p>

<pre><code>Flask==1.1.1
pyodbc==4.0.27
matplotlib==3.1.1
fpdf==1.7.2
pandas==0.25.1
</code></pre>
"
"53328226","I'm unable to install opencv-contrib-python in docker","<python><docker><opencv>","53359649","from .cv2 import * ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory","<python><docker><opencv>","<p>I tried installing <strong>opencv-contrib-python</strong> but I'm unable to get it to work on docker. It says  <strong>Could not find a version that satisfies the requirement opencv-contrib-python</strong></p>

<p>I tried,</p>

<pre><code>pip install opencv-contrib-python-headless
</code></pre>

<p>Then, I tired 
<a href=""https://github.com/cassiobotaro/docker-opencv-contrib/blob/master/Dockerfile"" rel=""noreferrer"">https://github.com/cassiobotaro/docker-opencv-contrib/blob/master/Dockerfile</a>
 and I also tried,</p>

<pre><code>    FROM python:3.5-alpine

    COPY . /app
    WORKDIR /app


    RUN apk add --no-cache ca-certificates
    RUN apk add --no-cache git build-base musl-dev alpine-sdk cmake clang clang-dev make gcc g++ libc-dev linux-headers

    RUN mkdir /tmp/opencv
    WORKDIR /tmp/opencv
    RUN wget -O opencv.zip https://github.com/opencv/opencv/archive/3.4.1.zip
    RUN unzip opencv.zip
    RUN wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/3.4.1.zip
    RUN unzip opencv_contrib.zip
    RUN mkdir /tmp/opencv/opencv-3.4.1/build

    WORKDIR /tmp/opencv/opencv-3.4.1/build
    RUN cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D OPENCV_EXTRA_MODULES_PATH=/tmp/opencv/opencv_contrib-3.4.1/modules -D BUILD_DOCS=OFF BUILD_EXAMPLES=OFF -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D BUILD_opencv_java=OFF -D BUILD_opencv_python=OFF -D BUILD_opencv_python2=OFF -D BUILD_opencv_python3=OFF ..
    RUN make -j4
    RUN make install

    RUN rm -rf /tmp/opencv


    RUN pip3 install -r requirements.txt

CMD [""app.py""] 
</code></pre>

<p>But I cannot get either one of it to work. PLease let me know how can I install the above in docker by just the requirements file?</p>

<p>More references (Things that I've tried) :
<a href=""https://stackoverflow.com/questions/53350876/unable-to-install-run-docker-with-opencv"">Unable to install/run docker with opencv</a></p>

<p>and</p>

<p><a href=""https://stackoverflow.com/questions/53359649/from-cv2-import-importerror-libgthread-2-0-so-0-cannot-open-shared-object-f"">from .cv2 import * ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory</a></p>
","<p>I get this error when execute docker file for opencv:</p>

<p><strong>Error:</strong></p>

<pre><code>from .cv2 import * ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory
</code></pre>

<p><strong>Dockerfile:</strong></p>

<pre><code>FROM jjanzic/docker-python3-opencv
COPY . /app
WORKDIR /app

RUN pip3 install -r requirements.txt


ENTRYPOINT [""python3""]
CMD [""app.py""]
</code></pre>

<p><strong>Requirements.txt</strong></p>

<pre><code>opencv-contrib-python-headless==3.4.3.18
Click==7.0
cloudpickle==0.6.1
cycler==0.10.0
dask==0.20.1
decorator==4.3.0
Flask==1.0.2
imutils==0.5.1
itsdangerous==1.1.0
Jinja2==2.10
kiwisolver==1.0.1
MarkupSafe==1.1.0
networkx==2.2
numpy==1.15.4
Pillow==5.3.0
pyparsing==2.3.0
python-dateutil==2.7.5
PyWavelets==1.0.1
scikit-image==0.14.1
scipy==1.1.0
six==1.11.0
toolz==0.9.0
Werkzeug==0.14.1
</code></pre>

<p>Please tell me how to fix it?</p>
"
"6784463","Error - trustAnchors parameter must be non-empty","<java><hudson><jakarta-mail><jenkins>","58812873","The trustAnchors parameter must be non-empty error with Ubuntu 16.04 and openJDK 11 docker container","<java><docker><ubuntu><ubuntu-16.04><openjdk-11>","<p>I'm trying to configure my e-mail on Jenkins/Hudson, and I constantly receive the error:</p>

<pre><code>java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be
    non-empty
</code></pre>

<p>I've seen a good amount of information online about the error, but I have not gotten any to work. I'm using Sun's JDK on Fedora Linux (not OpenJDK).</p>

<p>Here are a few things I've tried. I tried following the advice from this <a href=""https://stackoverflow.com/questions/4764611/java-security-invalidalgorithmparameterexception-the-trustanchors-parameter-must"">post</a>, but copying the cacerts from Windows over to my Fedora box hosting Jenkins didn't work.  I tried following <a href=""http://g4j.sourceforge.net/faq.html"" rel=""noreferrer"">this guide</a> as I'm trying to configure Gmail as my SMTP server, but it didn't work either. I also tried to download and move those cacert files manually and move them over to my Java folder using a variation of the commands on <a href=""http://www.krzywanski.net/archives/546"" rel=""noreferrer"">this guide</a>.</p>

<p>I am open to any suggestions as I'm currently stuck right now. I have gotten it to work from a Windows Hudson server, but I am struggling on Linux.</p>
","<p>I am using a docker container to deploy/run my application, previously I was using <code>openjdk:11-slim</code> as the base image and everything was working fine.</p>

<p>Now due to some other requirement, I change base docker image to <code>ubuntu:xenial</code> and installed OpenJDK with apt-get, my docker image looks like </p>

<pre><code>FROM ubuntu:xenial
RUN apt-get update -y
RUN apt-get install software-properties-common -y
RUN add-apt-repository ppa:openjdk-r/ppa -y
RUN apt-get update -y
RUN apt install openjdk-11-jdk -y
#.......other commands.....
</code></pre>

<p>My application starts successfully, but when my application call AWS s3 APIs it is giving below error </p>

<pre><code>  com.amazonaws.SdkClientException: Unable to execute HTTP request: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1136)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1082)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:745)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:719)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:701)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:669)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:651)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:515)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4443)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4390)
    at com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2123)
    at com.amazonaws.services.s3.AmazonS3Client.deleteObject(AmazonS3Client.java:2108)
    at com.com.example.core.service.ExpertS3Service.deleteProfilePicture(ExpertS3Service.java:314)
    at com.com.example.core.service.profile.ExpertProfileService.deleteProfilePicture(ExpertProfileService.java:319)
    at com.com.example.core.service.profile.ExpertProfileService$$FastClassBySpringCGLIB$$19dd4604.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:750)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:295)
    at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:98)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:689)
    at com.com.example.core.service.profile.ExpertProfileService$$EnhancerBySpringCGLIB$$6dc5e9c9.deleteProfilePicture(&lt;generated&gt;)
    at com.com.example.rest.ExpertProfileResources.deleteProfilePicture(ExpertProfileResources.java:255)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:105)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:893)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:798)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1040)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:943)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
    at org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkServlet.java:931)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:666)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:741)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:88)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:94)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:114)
    at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:104)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at io.opentracing.contrib.web.servlet.filter.TracingFilter.doFilter(TracingFilter.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:202)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:526)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:860)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1589)
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: javax.net.ssl.SSLException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:133)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:320)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:258)
    at java.base/sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1314)
    at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:408)
    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436)
    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384)
    at com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:142)
    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374)
    at jdk.internal.reflect.GeneratedMethodAccessor953.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)
    at com.amazonaws.http.conn.$Proxy167.connect(Unknown Source)
    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1258)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1074)
    ... 88 common frames omitted
Caused by: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at java.base/sun.security.validator.PKIXValidator.&lt;init&gt;(PKIXValidator.java:89)
    at java.base/sun.security.validator.Validator.getInstance(Validator.java:181)
    at java.base/sun.security.ssl.X509TrustManagerImpl.getValidator(X509TrustManagerImpl.java:300)
    at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrustedInit(X509TrustManagerImpl.java:176)
    at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:189)
    at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:129)
    at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:625)
    at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:460)
    at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:360)
    at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392)
    at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443)
    at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:421)
    at java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:177)
    at java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:164)
    at java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1152)
    at java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1063)
    at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:402)
    ... 107 common frames omitted
Caused by: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty
    at java.base/java.security.cert.PKIXParameters.setTrustAnchors(PKIXParameters.java:200)
    at java.base/java.security.cert.PKIXParameters.&lt;init&gt;(PKIXParameters.java:120)
    at java.base/java.security.cert.PKIXBuilderParameters.&lt;init&gt;(PKIXBuilderParameters.java:104)
    at java.base/sun.security.validator.PKIXValidator.&lt;init&gt;(PKIXValidator.java:86)
    ... 123 common frames omitted
</code></pre>

<p>Is there any solution/workaround for this issue?</p>
"
"53900818","Passing environment variable to container is not working","<docker><environment-variables>","53902319","docker run: make the container command use it's own environment variable instead of the host's","<shell><docker><dockerfile><docker-run>","<p>I have this command:</p>

<pre><code>docker run -e ""DB_HOST=thehost"" --rm my_application echo $DB_HOST
</code></pre>

<p>but it shows nothing. I was expecting ""thehost"" to be shown.</p>

<p>I have tried with simple quotes, double quotes and without quotes.</p>

<p>What am I missing? Do I need to specify ENV parameter in my_aplication's Dockerfile?</p>

<p>If I do:</p>

<pre><code>docker run -e ""DB_HOST=thehost"" --rm my_application echo $PATH
</code></pre>

<p>It shows the PATH value properly. But it is ignoring my env var.</p>
","<p>Assuming that I set an environment variable in my <code>Dockerfile</code> like this:</p>

<pre><code>FROM ubuntu:latest
ENV MY_NAME=YOYO
</code></pre>

<p>When running <code>docker run -it my_image bash</code> I am able to access <code>MY_NAME</code> like this:</p>

<pre><code>root@dec16fb8dab1:/home $ echo $MY_NAME
YOYO
</code></pre>

<p>However, when I try <code>docker run my_image echo $MY_NAME</code>, not surprisingly I get nothing (since there is no <code>MY_NAME</code> variable on the host). How can I use docker run with a command to use the image's environement variables instead of the host's? </p>

<p>In a more general case, I need to see the output of <code>docker run my_image cmd --flag $CONTAINER_ENV_VARIABLE</code>.</p>
"
"11618898","pg_config executable not found","<python><pip><psycopg2>","59136580","Docker: pg_config is required to build psycopg2 from source","<python><postgresql><docker>","<p>I am having trouble installing psycopg2. I get the following error when I try to <code>pip install psycopg2</code>:</p>

<pre><code>Error: pg_config executable not found.

Please add the directory containing pg_config to the PATH

or specify the full executable path with the option:



    python setup.py build_ext --pg-config /path/to/pg_config build ...



or with the pg_config option in 'setup.cfg'.

----------------------------------------
Command python setup.py egg_info failed with error code 1 in /tmp/pip-build/psycopg2
</code></pre>

<p>But the problem is <code>pg_config</code> is actually in my <code>PATH</code>; it runs without any problem:</p>

<pre><code>$ which pg_config
/usr/pgsql-9.1/bin/pg_config
</code></pre>

<p>I tried adding the pg_config path to the <code>setup.cfg</code> file and building it using the source files I downloaded from their website (<a href=""http://initd.org/psycopg/"" rel=""noreferrer"">http://initd.org/psycopg/</a>) and I get the following error message!</p>

<pre><code>Error: Unable to find 'pg_config' file in '/usr/pgsql-9.1/bin/'
</code></pre>

<p>But it is actually THERE!!!</p>

<p>I am baffled by these errors. Can anyone help please?</p>

<p>By the way, I <code>sudo</code> all the commands. Also I am on RHEL 5.5.</p>
","<p>I'm currently looking to build an SSL out-of-the-box Django application, with the help of Lua Open Resty SSL and the <code>valian/docker-nginx-auto-ssl</code> image (See here for reference: <a href=""https://github.com/Valian/docker-nginx-auto-ssl"" rel=""nofollow noreferrer"">https://github.com/Valian/docker-nginx-auto-ssl</a>).</p>

<p>I want to merge the Dockerfile from the above project with one of mine, and I am hitting a snag with installing the python dependecy from my <code>requirements.txt</code> file:</p>

<p><code>psycopg2-binary==2.8.4</code></p>

<p>The base OS specified in the valian Dockerfile is Alpine, so OS level dependencies are <code>apk</code> sourced. (<code>FROM openresty/openresty:alpine-fat</code>).</p>

<p>The specific error I am receiving is this:</p>

<pre><code>Collecting psycopg2-binary==2.8.2
  Downloading https://files.pythonhosted.org/packages/dc/93/bb5655730913b88f9068c6b596177d1df83be0d476671199e17b06ea8436/psycopg2-binary-2.8.2.tar.gz (369kB)
    ERROR: Command errored out with exit status 1:
     command: /usr/local/lib/pyenv/versions/3.8.0/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-6t5fbphg/psycopg2-binary/setup.py'""'""'; __file__='""'""'/tmp/pip-install-6t5fbphg/psycopg2-binary/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info
         cwd: /tmp/pip-install-6t5fbphg/psycopg2-binary/
    Complete output (23 lines):
    running egg_info
    creating /tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info/psycopg2_binary.egg-info
    writing /tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info/psycopg2_binary.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info/psycopg2_binary.egg-info/dependency_links.txt
    writing top-level names to /tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info/psycopg2_binary.egg-info/top_level.txt
    writing manifest file '/tmp/pip-install-6t5fbphg/psycopg2-binary/pip-egg-info/psycopg2_binary.egg-info/SOURCES.txt'

    Error: pg_config executable not found.

    pg_config is required to build psycopg2 from source.  Please add the directory
    containing pg_config to the $PATH or specify the full executable path with the
    option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    If you prefer to avoid building psycopg2 from source, please install the PyPI
    'psycopg2-binary' package instead.

    For further information please check the 'doc/src/install.rst' file (also at
    &lt;http://initd.org/psycopg/docs/install.html&gt;).

    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
ERROR: Service 'web' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>

<p>This also happens if I use <code>psycopg2==2.8.4</code>.</p>

<p>So, I guess, what would I need to do to provide the container the <code>pg_config</code> executable? Is this something that comes with <code>apk add postgresql</code>?</p>

<p><strong>Related question</strong>: <a href=""https://stackoverflow.com/questions/11618898/pg-config-executable-not-found"">pg_config executable not found</a></p>
"
"19585028","I lose my data when the container exits","<docker>","59032338","Docker container file getting lost after I stop the container","<docker><dockerfile><containers>","<p>Despite Docker's <a href=""http://www.docker.io/gettingstarted/"" rel=""noreferrer"">Interactive tutorial</a> and <a href=""http://docs.docker.io/en/latest/faq/#do-i-lose-my-data-when-the-container-exits"" rel=""noreferrer"">faq</a> I lose my data when the container exits.</p>

<p>I have installed Docker as described here: <a href=""http://docs.docker.io/en/latest/installation/ubuntulinux"" rel=""noreferrer"">http://docs.docker.io/en/latest/installation/ubuntulinux</a>
without any problem on ubuntu 13.04.</p>

<p>But it loses all data when exits.</p>

<pre><code>iman@test:~$ sudo docker version
Client version: 0.6.4 
Go version (client): go1.1.2 
Git commit (client): 2f74b1c 
Server version: 0.6.4 
Git commit (server): 2f74b1c 
Go version (server): go1.1.2 
Last stable version: 0.6.4 


iman@test:~$ sudo docker run ubuntu ping
2013/10/25 08:05:47 Unable to locate ping 
iman@test:~$ sudo docker run ubuntu apt-get install ping
Reading package lists... 
Building dependency tree... 
The following NEW packages will be installed: 
  iputils-ping 
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. 
Need to get 56.1 kB of archives. 
After this operation, 143 kB of additional disk space will be used. 
Get:1 http://archive.ubuntu.com/ubuntu/ precise/main iputils-ping amd64 3:20101006-1ubuntu1 [56.1 kB] 
debconf: delaying package configuration, since apt-utils is not installed 
Fetched 56.1 kB in 0s (195 kB/s) 
Selecting previously unselected package iputils-ping. 
(Reading database ... 7545 files and directories currently installed.) 
Unpacking iputils-ping (from .../iputils-ping_3%3a20101006-1ubuntu1_amd64.deb) ... 
Setting up iputils-ping (3:20101006-1ubuntu1) ... 
iman@test:~$ sudo docker run ubuntu ping
2013/10/25 08:06:11 Unable to locate ping 
iman@test:~$ sudo docker run ubuntu touch /home/test
iman@test:~$ sudo docker run ubuntu ls /home/test
ls: cannot access /home/test: No such file or directory 
</code></pre>

<p>I also tested it with interactive sessions with the same result. Did I forget something?</p>

<h2>EDIT: IMPORTANT FOR NEW DOCKER USERS</h2>

<p>As @mohammed-noureldin and others said, actually this is <strong>NOT</strong> a <strong>container exiting</strong>. Every time it just creates a new container.</p>
","<p>I pulled Ubuntu image using <code>docker pull</code>.</p>

<p>I connect to the container using <code>docker exec</code> and then create a file and then exit.</p>

<p>Again, when I execute docker exec file is lost.</p>

<p>How to maintain the file in that container, I have tried dockerfile and tagging docker images, it works.</p>

<p>But, is there any other way to maintain the files in docker container for a longer time?</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","58989178","docker container ports confusion","<docker><dockerfile><containers>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I have some confusion on Docker ports.As i am not aware of the IP of server i cannot check myself.</p>

<p>In Dockerfile i can see below</p>

<pre><code>EXPOSE 8080:8080
</code></pre>

<p>But to run the container is ran using the same image below command is used</p>

<pre><code>docker run -d --restart=unless-stopped --name image1 -p 3000:8080 image1:latest
</code></pre>

<p>My confusion is wile creating image host port was given as 8080 in Dockerfile but when running the container from same image host port given was 3000.So in which port of host this container will run and why ?</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","58959919","Error response from daemon: the daemon on this operating system does not support exporting Windows containers","<docker><docker-for-windows><docker-container><docker-command>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>Is there another way to transport a container image between 2 private hosts, without using docker hub?</p>

<p>I am running docker Desktop 2.1 on windows 10 (latest version).</p>

<p><strong>Update</strong></p>

<p>Note: I look for a solution for windows containers, running on windows 10 and windows server. Also, i don't intent to use a docker hub for this purpose. So, solutions must not use linux available functions or hubs.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","58766661","How to make my docker container call my API that is im my docker host?","<docker><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have an angular application in a container and want to make it call my API that is in my docker host. I want to do it so I could debug my API and make some tests.</p>

<p>This is my dockerfile:</p>

<pre><code>FROM node:6.9.5-alpine
COPY . .
RUN npm install

CMD [""npm"", ""start""]
</code></pre>

<p>How can I make it call my API that is in my docker host using a HTTP request?</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","59025587","Docker build: runnable is in parent folder","<docker><dockerfile><docker-build>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have multiple Dockerfiles in a different folder but the source for all the dockers is the same.</p>

<pre><code>FROM openjdk:11.0.5-stretch

COPY my.zip /home/my.zip

RUN unzip /home/target.zip -d /home/ &amp;&amp; rm -f /home/target.zip

ENTRYPOINT [""java"", ""-jar"", ""/home/my/{a.jar, b.jar, ...}""
</code></pre>

<p>Either one of the jar will be used in one Docker file.
I have a python script which runs all the docker build commands, whenever I update something in code and recompile, I have to copy my.zip in almost 20 folders. Is it possible we ADD the file from the root folder as </p>

<pre><code>ADD ../my.zip /home/my.zip
</code></pre>

<p>Is it possible?</p>
"
"28996907","docker: ""build"" requires 1 argument. See 'docker build --help'","<docker><containers>","59086261","Unable to build docker image","<docker>","<p>Trying to follow the instructions for building a docker image from the docker website.</p>

<p><a href=""https://docs.docker.com/examples/running_redis_service/"">https://docs.docker.com/examples/running_redis_service/</a></p>

<p>this is the error I get will following the instructions on the doc and using this Dockerfile</p>

<pre><code>FROM        ubuntu:14.04
RUN         apt-get update &amp;&amp; apt-get install -y redis-server
EXPOSE      6379
ENTRYPOINT  [""/usr/bin/redis-server""]


sudo docker build -t myrepo/redis
docker: ""build"" requires 1 argument. See 'docker build --help'.
</code></pre>

<p>How do  resolve?</p>
","<p>i want to build a docker image. I do this:</p>

<pre><code>$ docker build -f Dockerfile -t $IMAGE_TAG
</code></pre>

<p>I have this error:</p>

<blockquote>
  <p>""docker build"" requires exactly 1 argument. See 'docker build --help'.
  Usage:  docker build [OPTIONS] PATH | URL | - Build an image from a
  Dockerfile ERROR: Job failed: exit code 1</p>
</blockquote>
"
"32758793","How to show the run command of a docker container","<docker>","59127760","How to derive ""docker run"" syntax from docker container","<docker><docker-run>","<p>I use a third party GUI (Synology Docker package) to setup a docker container. However, it's limitation makes me need to run the container from the command line. (I want to map another host ip to bind the port) </p>

<p>Now, since there are lots of settings that already done, I would like to retrieve the original run command that start this container, then I can change the port mapping port to new one. eg. ""<code>docker run -p 80:8080 gitlab</code>""</p>

<p>I can't find the way to do so, event use ""docker inspect"", no such information provided.</p>

<p>Please provide some advice to solve this problem.</p>
","<p>I ran a <code>docker run</code> on a docker image with a long convoluted command syntax that I have since forgotton and lost. Is there any way to derive the syntax that ran a container given the container ID? </p>

<p>I know <code>docker inspect</code> can give you information about the container but I was hoping the <code>docker run</code> syntax would be saved somewhere.</p>
"
"33913020","Docker remove <none> TAG images","<docker>","59181071","'docker image ls' gives huge <none> image list","<docker>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>I'm kinda new to docker but has some fair knowledge about it. Recently I wanted to list down the my docker images, so I ran <code>docker image ls</code>, which gives me a bit of large list of pulled images..</p>

<p><a href=""https://i.stack.imgur.com/YZs9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YZs9u.png"" alt=""Part of the image list""></a></p>

<p>Above list shows the part of the image list in the machine. I have some knowledge about this <code>&lt;none&gt;</code> images which are known as intermediate images. But the problem is they are huge some images are almost 1GB. To my knowledge I never ran a single huge container other than gitlab. When I was done with it I removed the image. Can someone explain why these intermediate images are large and way to get rid of it without damaging other image layers?</p>
"
"36666246","Docker look at the log of an exited container","<docker>","58668681","Docker view the logs of failed / crashed container","<docker><containers>","<p>Is there any way I can see the log of a container that has exited?</p>

<p>I can get the container id of the exited container using <code>docker ps -a</code> but I want to know what happened when it was running.</p>
","<p>I know you can check the logs of a running container with:</p>

<p><strong>Command to list containers</strong></p>

<p><code>docker container ls</code></p>

<p><strong>Command to view the logs</strong></p>

<p><code>docker logs [container-id]</code></p>

<p><strong>Question:</strong></p>

<p>Can you also check the logs of a failed container that is not listed, because the container failed?</p>
"
"36813690","Connection refused on docker container","<docker>","59056385","Cannot connect to Elasticsearch from Go server running in Docker","<docker><go><elasticsearch><kibana>","<p>I'm new to Docker and trying to make a demo Rails app. I made a dockerfile that looks like this: </p>

<pre><code>FROM ruby:2.2
MAINTAINER marko@codeship.com

# Install apt based dependencies required to run Rails as 
# well as RubyGems. As the Ruby image itself is based on a 
# Debian image, we use apt-get to install those.
RUN apt-get update &amp;&amp; apt-get install -y \
build-essential \
nodejs

    # Configure the main working directory. This is the base 
    # directory used in any further RUN, COPY, and ENTRYPOINT 
    # commands.
RUN mkdir -p /app
WORKDIR /app

    # Copy the Gemfile as well as the Gemfile.lock and install 
    # the RubyGems. This is a separate step so the dependencies 
    # will be cached unless changes to one of those two files 
    # are made.
COPY Gemfile Gemfile.lock ./
RUN gem install bundler &amp;&amp; bundle install --jobs 20 --retry 5

# Copy the main application.
COPY . ./

# Expose port 8080 to the Docker host, so we can access it 
# from the outside.
EXPOSE 8080

# The main command to run when the container starts. Also 
# tell the Rails dev server to bind to all interfaces by 
# default.
CMD [""bundle"", ""exec"", ""rails"", ""server"", ""-b"", ""0.0.0.0"", ""-p"", ""8080""]
</code></pre>

<p>I then built it like so: </p>

<pre><code>docker build -t demo . 
</code></pre>

<p>And call a command to start the server which does start the server on port 8080: </p>

<pre><code>Johns-MacBook-Pro:demo johnkealy$ docker run -it demo
=&gt; Booting WEBrick
=&gt; Rails 4.2.5 application starting in development on http://0.0.0.0:8080
=&gt; Run `rails server -h` for more startup options
=&gt; Ctrl-C to shutdown server
[2016-04-23 16:50:34] INFO  WEBrick 1.3.1
[2016-04-23 16:50:34] INFO  ruby 2.2.4 (2015-12-16) [x86_64-linux]
[2016-04-23 16:50:34] INFO  WEBrick::HTTPServer#start: pid=1 port=8080
</code></pre>

<p>I then try to find the correct IP to navigate to: </p>

<pre><code>Johns-MacBook-Pro:demo johnkealy$ docker-machine ip default
192.168.99.100
</code></pre>

<p>I navigate to <a href=""http://192.168.99.100:8080"" rel=""noreferrer"">http://192.168.99.100:8080</a> and get the error This site can’t be reached 192.168.99.100 refused to connect.</p>

<p>What could I be doing wrong ? </p>
","<p>I have set up an environment today that runs a <code>golang:1.13-alpine</code> image, along with the latest images for Elasticsearch and Kibana.</p>

<p>Elasticsearch and Kibana are running fine when accessing from my local machine, but I cannot connect to Elasticsearch through the Go server. I have put this together from guides I have found and followed. </p>

<p>I am still a bit green using Docker. I have an idea that I am pointing at the wrong ip address in the container, but I am unsure how to fix it. Hope someone can guide me in the right direction.</p>

<p><strong>docker-compose.yml:</strong></p>

<pre><code>version: ""3.7""

services:
  web:
    image: go-docker-webserver
    build: .
    ports:
      - ""8080:8080""

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.2
    environment:
      node.name: elasticsearch
      cluster.initial_master_nodes: elasticsearch
      cluster.name: docker-cluster
      bootstrap.memory_lock: ""true""
      ES_JAVA_OPTS: -Xms256m -Xmx256m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - ""9200:9200""

  kibana:
    image: docker.elastic.co/kibana/kibana:7.4.2
    ports:
      - ""5601:5601""
    links:
      - elasticsearch
</code></pre>

<p><strong>Dockefile:</strong></p>

<pre><code>FROM golang:1.13-alpine as builder

RUN apk add --no-cache --virtual .build-deps \
    bash \
    gcc \
    git \
    musl-dev

RUN mkdir build
COPY . /build
WORKDIR /build

RUN go get
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-extldflags ""-static""' -o webserver .
RUN adduser -S -D -H -h /build webserver
USER webserver

FROM scratch
COPY --from=builder /build/webserver /app/
WORKDIR /app
EXPOSE 8080
EXPOSE 9200
CMD [""./webserver""]
</code></pre>

<p><strong>main.go</strong>:</p>

<pre><code>func webserver(logger *log.Logger) *http.Server {
    router := http.NewServeMux()
    router.HandleFunc(""/"", func(w http.ResponseWriter, r *http.Request) {

        es, err := elasticsearch.NewDefaultClient()
        if err != nil {
            log.Fatalf(""Error creating the client: %s"", err)
        }

        res, err := es.Info()
        if err != nil {
            log.Fatalf(""Error getting response: %s"", err)
        }

        log.Println(res)
    })

    return &amp;http.Server{
        Addr:         listenAddr,
        Handler:      router,
        ErrorLog:     logger,
        ReadTimeout:  5 * time.Second,
        WriteTimeout: 10 * time.Second,
        IdleTimeout:  15 * time.Second,
    }
}
</code></pre>

<p>When I boot the server, everything is running fine and I can access Kibana and query the data that I have indexed, but as soon as I hit <code>localhost:8080</code> in Postman, the server dies and outputs: </p>

<pre><code>web_1            | 2019/11/26 16:40:40 Error getting response: dial tcp 127.0.0.1:9200: connect: connection refused
go-api_web_1 exited with code 1
</code></pre>
"
"36996046","How to prevent Dockerfile caching git clone","<git><docker><dockerfile>","59201409","How do I remove a particular item from the docker cache?","<docker>","<p>I have a Dockerfile trying to package and deploy a web app to a container. The code of app fetches from git repository during Docker image building.
Here's the Dockerfile snapshot:</p>

<pre><code>........
RUN git clone --depth=1 git-repository-url $GIT_HOME/
RUN mvn package -Dmaven.test.skip
........
</code></pre>

<p>I want the docker do not cache the step of <code>RUN git clone --depth=1 git-repository-url $GIT_HOME/</code> so that the on-going updated on the the repository can be reflected on the Docker image building. Is it possible to a achieve that?</p>
","<p>I run a docker build with command </p>

<pre><code>sudo docker build -t catskills-xview2-0.0.0 .
</code></pre>

<p>I have previously built this tag.  It has a lot of cached build steps, like this:</p>

<pre><code>Step 15/20 : RUN pip3 install matplotlib tqdm libtiff scipy Pillow scikit-image opencv-python imgaug IPython geopandas keras imantics simplification scikit-learn chainer tensorboard tensorboardX
 ---&gt; Using cache
 ---&gt; 2af652c17995
Step 16/20 : RUN git clone https://github.com/xview2/xview2-baseline.git ~/code/xview-2
 ---&gt; Using cache
 ---&gt; 8ea290c99ee8
</code></pre>

<p>Most of the time I want the cache.  If I've updated source for a particular cache item, I want to delete that item so it will fetch it from scratch.  For example, cache item 8ea290c99ee8.</p>

<p>I know that I can completely rebuild with --no_cache.  This is slow.</p>

<p><strong>Question:</strong> How do I delete a single cache item using it's key, so that the rebuild will re-fetch on that step?</p>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","58583067","Docker container crontab not running","<linux><docker><ubuntu><dockerfile>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>I have a dockerfile image based on ubuntu. Iam trying to make a bash script run each day but the cron never runs. When the container is running, i check if cron is running and it is. the bash script works perfectly and the crontab command is well copied inside the container. i can't seem to find where the problem is coming from. </p>

<p>Here is the Dockerfile:</p>

<pre><code>FROM snipe/snipe-it:latest

ENV TZ=America/Toronto

RUN apt-get update \
    &amp;&amp; apt-get install awscli -y \
    &amp;&amp; apt-get clean \
    &amp;&amp; apt-get install cron -y \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN mkdir /var/www/html/backups_scripts /var/www/html/config/scripts

COPY config/crontab.txt /var/www/html/backups_scripts
RUN /usr/bin/crontab /var/www/html/backups_scripts/crontab.txt

COPY config/scripts/backups.sh /var/www/html/config/scripts

CMD [""cron"",""-f""]
</code></pre>

<p>The last command CMD doesn't work. And as soon as i remove the cmd command i get this message when i check the cron task inside the container:</p>

<pre><code>root@fcfb6052274a:/var/www/html# /etc/init.d/cron status
 * cron is not running
</code></pre>

<p>Even if i start the cron process before the crontab, the crontab is still not launched</p>

<p>How can i tackle this problem ??? Thank you</p>
"
"37871540","How many CPUs does a docker container use?","<docker>","58888534","Will Python multiprocessing work inside a single docker container?","<python><multithreading><docker><multiprocessing>","<p>Lets say I am running a <a href=""https://docs.python.org/2/library/multiprocessing.html"" rel=""noreferrer"">multiprocessing</a> service inside a docker container spawning multiple processes, would docker use all/multiple cores/CPUs of the host or just one?</p>
","<p>I have a Django web app served by apache2 and mod_wsgi in daemon mode, the same way as described in Django's <a href=""https://docs.djangoproject.com/en/1.11/howto/deployment/wsgi/modwsgi/#using-mod-wsgi-daemon-mode"" rel=""nofollow noreferrer"">docs</a>. The app is hosted in AWS and is not dockerized. </p>

<p>However, now we need to do an on-prem deployment on one of our customer’s server and we decided to use Docker for easier deployment. I have been using Docker on this project for a while but only for development and never for production.</p>

<p>Since the app is using both Python's multithreading for blocking operations and multiprocessing for CPU-bound tasks. Also, Apache2 is spawning multiple processes to serve multiple requests at the same time. </p>

<p>Will there be any problem with multiprocessing if the app is running in a <strong>single docker container</strong>? Will it be able to spawn multiple processes and utilize <strong>multiple cores</strong> of the host machine? </p>
"
"39054411","[Docker]: Connecting PHPMyAdmin to MySQL doesnt work","<mysql><docker><phpmyadmin>","58601409","How to communicate mysql & mysqladmin docker containers","<docker>","<p>I'm trying to connect a PHPMyAdmin-Container to a MySQL-Container to view the databases.</p>

<p>I have started the MySQL container via <code>$ docker run --name databaseContainer -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql</code> </p>

<p>and the PHPMyAdmin-Container via <code>$ docker run --name myadmin -d --link databaseContainer:mysql -p 8080:8080 phpmyadmin/phpmyadmin</code></p>

<p>When trying to login on PHPMyAdmin, I get:
 mysqli_real_connect(): php_network_getaddresses: getaddrinfo failed: Name does not resolve</p>

<p>and</p>

<p>mysqli_real_connect(): (HY000/2002): php_network_getaddresses: getaddrinfo failed: Name does not resolve</p>

<p>By the way, I have also started a wordpress container and also linked it to mysql, there it works...</p>
","<p>I want to spawn MySQL &amp; PHPMyAdmin docker containers. Mysql container can be accessed via 3306 port &amp; PHPMyAdmin can be accessed through 8280 port. </p>

<p>My question is, how a PHP application can be configured to access the MySQL docker container on 3306 port
and the PHPMyAdmin can be configured for MySQL.</p>

<p>Thanks.</p>
"
"45271420","Docker: where is docker volume located for this compose file","<docker><docker-compose>","58926469","What does {} in docker-compose.yml map to on host system?","<docker><docker-compose>","<p>I was setting up some materials for a trainning, when I came around this sample compose file:</p>

<p><a href=""https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml"" rel=""noreferrer"">https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml</a></p>

<p>and I couldn't find out how this volume is mounted, on lines 48 and 49 of the file:</p>

<pre><code>volumes:
  db-data:
</code></pre>

<p>Can someone explain me where is this volume on the host? Couldn't find it and I wouldn't like to keep any postgresql data dangling around after the containers are gone. Similar thing happens to the networks:</p>

<pre><code>networks:
  front-tier:
  back-tier:
</code></pre>

<p>Why docker compose accepts empty network definitions like this?</p>
","<p>In the following docker-compose.yml setting, how would one find out the path on the host system where solr_data variable is mapped to. It is only mapped to {}. Is there any default path that {} maps to?</p>

<pre><code>volumes:
  solr_data: {}
services:
  solr:
    build:
      context: ${DEV_ENV_ROOT}/solr/docker
      dockerfile: Dockerfile.development
      args:
        DEV_ENV_USER_ID: ${DEV_ENV_USER_ID}
    networks:
      - default
    ports:
      - ""8983:8983""
    environment:
      - SOLR_HEAP=2048m
    volumes:
      - solr_data:/var/data/solr
</code></pre>
"
"46166293","How to measure Docker build steps duration?","<docker><dockerfile>","59038819","Docker build timestamps","<docker><console><docker-multi-stage-build>","<p>Is it possible to configure Docker to output timing for the build of a Dockerfile?</p>

<p>We run a medium sized development team and would like to collect statistics on the average build times for our developers' development containers.</p>

<p>Ideally, it should measure the duration of individual steps.</p>
","<p>Is it possible to get step timestamps in a console/log file while running docker build command?</p>

<pre class=""lang-sh prettyprint-override""><code>Step 28/49 : RUN find dist -name ""*.map"" -delete 
 ---&gt; Running in 5e1ee1063999 
Removing intermediate container 5e1ee1063999 
 ---&gt; a02191df133c 
Step 29/49 : FROM dotnet-install AS web-build 
 ---&gt; e0ecdc23b275 
Step 30/49 : COPY src src 
 ---&gt; d33c7da41fb9
</code></pre>

<p>I would like to identify the slowest steps and I would expect something like:</p>

<pre class=""lang-sh prettyprint-override""><code>2019-11-25T12:00:00Z : Step 28/49 : RUN find dist -name ""*.map"" -delete 
 ---&gt; Running in 5e1ee1063999 
Removing intermediate container 5e1ee1063999 
 ---&gt; a02191df133c 
2019-11-25T12:10:00Z : Step 29/49 : FROM dotnet-install AS web-build 
 ---&gt; e0ecdc23b275 
2019-11-25T12:20:00Z : Step 30/49 : COPY src src 
 ---&gt; d33c7da41fb9
</code></pre>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","58577879","Error: pg_config executable not found in Postgres Docker container","<python><postgresql><docker><docker-container>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>Probably my question is repeated</p>

<p>I am using Postgres with Docker on MacOs </p>

<p>I am not doing it with <code>Dockerfile</code></p>

<pre><code>docker pull postgres:10
docker run -p 5432:5432 --name postgres10 -v /Users/myuser/Volumes/postgres10:/var/lib/postgresql/data -e POSTGRES_PASSWORD=123 -d postgres:10
</code></pre>

<p>I am trying to install <code>psycopg2</code> in a flask application with<code>Python 3.7</code></p>

<p>But it doesn't find <code>pg_config</code></p>

<p>I know that in a traditional installation it is located in <code>PATH=""/Applications/Postgres.app/Contents/Versions/latest/bin:$PATH""</code></p>

<p>Any idea how to fix?</p>
"
"51134630","Is it possible to combine ""docker logs"" output and ""docker exec"" output?","<docker>","58963571","How to get logs from multiple detached commands on an already running Docker container","<docker>","<p>Is it possible that merging <code>docker logs -f</code> output and <code>docker exec</code> result?
I already tried to redirect <code>docker exec</code> results to <code>docker logs</code> file. My environment that host is MacOS and run ubuntu docker image.  </p>
","<p>I can use <code>docker exec -d CONTAINER_NAME sh -c ""COMMAND""</code> to make a <strong>running</strong> container execute multiple concurrent commands.</p>

<p>However, when I run <code>docker logs CONTAINER_NAME</code> I only get the logs from the container's initial process.</p>

<p>How do I see the logs from COMMAND?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","59144778","Connecting Kafka Producer to Kafka broker in Docker container through Java libraries","<java><docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I want to make a Kafka producer send messages to a topic in a Kafka cluster through org.apache.kafka Java library. Here's what I've done:</p>

<p>I started a Kafka and a Zookeeper server using <code>docker-compose up -d</code> with this <code>docker-compose.yml</code> file:</p>

<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""
  kafka:
    # build: .
    image: wurstmeister/kafka
    ports:
      - ""9092""
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 192.168.99.100
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: ""testTopic:1:1""
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>

<p>Result:</p>

<pre><code>          Name                         Command               State                         Ports
-----------------------------------------------------------------------------------------------------------------------
dockertoolbox_kafka_1       start-kafka.sh                   Up      0.0.0.0:32768-&gt;9092/tcp
dockertoolbox_zookeeper_1   /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181-&gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp
</code></pre>

<p>Then, I took the code from <a href=""https://www.tutorialkart.com/apache-kafka/producer-example-in-apache-kafka/"" rel=""nofollow noreferrer"">https://www.tutorialkart.com/apache-kafka/producer-example-in-apache-kafka/</a> and changed the port number:</p>

<pre><code>public class SampleProducer extends Thread {
    private final KafkaProducer&lt;Integer, String&gt; producer;
    private final String topic;
    private final Boolean isAsync;

    public static final String KAFKA_SERVER_URL = ""localhost"";
    public static final int KAFKA_SERVER_PORT = 32768;
    public static final String CLIENT_ID = ""SampleProducer"";

    public SampleProducer(String topic, Boolean isAsync) {
        final Properties properties = new Properties();
        properties.put(""bootstrap.servers"", KAFKA_SERVER_URL + "":"" + KAFKA_SERVER_PORT);
        properties.put(""client.id"", CLIENT_ID);
        properties.put(""key.serializer"", ""org.apache.kafka.common.serialization.IntegerSerializer"");
        properties.put(""value.serializer"", ""org.apache.kafka.common.serialization.StringSerializer"");
        producer = new KafkaProducer&lt;&gt;(properties);
        this.topic = topic;
        this.isAsync = isAsync;
    }

    @Override
    public void run() {
        int messageNo = 1;
        while (true) {
            final String messageStr = ""Message_"" + messageNo;
            final long startTime = System.currentTimeMillis();
            if (isAsync) { // Send asynchronously
                producer.send(new ProducerRecord&lt;&gt;(topic,
                        messageNo,
                        messageStr), new DemoCallBack(startTime, messageNo, messageStr));
            } else { // Send synchronously
                try {
                    producer.send(new ProducerRecord&lt;&gt;(topic,
                            messageNo,
                            messageStr)).get();
                    System.out.println(""Sent message: ("" + messageNo + "", "" + messageStr + "")"");
                } catch (InterruptedException | ExecutionException e) {
                    e.printStackTrace();
                    // handle the exception
                }
            }
            ++messageNo;
        }
    }
}

class DemoCallBack implements Callback {

    private final long startTime;
    private final int key;
    private final String message;

    public DemoCallBack(long startTime, int key, String message) {
        this.startTime = startTime;
        this.key = key;
        this.message = message;
    }

    /**
     * onCompletion method will be called when the record sent to the Kafka Server has been acknowledged.
     *
     * @param metadata  The metadata contains the partition and offset of the record. Null if an error occurred.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    @Override
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        final long elapsedTime = System.currentTimeMillis() - startTime;
        if (metadata != null) {
            System.out.println(
                    ""message("" + key + "", "" + message + "") sent to partition("" + metadata.partition() +
                    ""), "" +
                    ""offset("" + metadata.offset() + "") in "" + elapsedTime + "" ms"");
        } else {
            exception.printStackTrace();
        }
    }
}
</code></pre>

<pre><code>public class KafkaProducerDemo {
    public static final String TOPIC = ""testTopic"";

    public static void main(String[] args) {
        final boolean isAsync = false;
        final SampleProducer producerThread = new SampleProducer(TOPIC, isAsync);
        // start the producer
        producerThread.start();

    }
}
</code></pre>

<p>But when I run KafkaProducerDemo, it continues to print:</p>

<pre><code>[kafka-producer-network-thread | SampleProducer] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=SampleProducer] Connection to node -1 (localhost/127.0.0.1:32768) could not be established. Broker may not be available.
</code></pre>

<p>What should I change?</p>
"
"52492611","Failed to build: Error parsing reference: ""microsoft/dotnet:2.1-runtime AS base"" is not a valid repository/tag:","<c#><docker><.net-core>","58693737","The FROM ... AS Command gives parsing error in linux","<linux><docker><dockerfile>","<p>I got the below error when build docker for console application using .net core.</p>

<blockquote>
  <p>Step 1/15 : FROM microsoft/dotnet:2.1-runtime AS base
  Error parsing reference: ""microsoft/dotnet:2.1-runtime AS base"" is not a valid repository/tag: invalid reference format</p>
</blockquote>

<p>My Dockerfile looks below </p>

<pre><code>FROM microsoft/dotnet:2.1-sdk AS build
WORKDIR /src
COPY ConsoleApp2/ConsoleApp2.csproj ConsoleApp2/
RUN dotnet restore ConsoleApp2/ConsoleApp2.csproj
COPY . .
WORKDIR /src/ConsoleApp2
RUN dotnet build ConsoleApp2.csproj -c Release -o /app

FROM build AS publish
RUN dotnet publish ConsoleApp2.csproj -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""ConsoleApp2.dll""]
</code></pre>

<p>My docker version is </p>

<blockquote>
  <p>Docker version 17.03.1-ce-rc1, build 3476dbf</p>
</blockquote>

<p>Can some one suggest what I am doing wrong in this.
Thanks in advance.</p>
","<p>The Docker example for .NET Core's documentation has the first statement as:</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:2.2 AS build-env
</code></pre>

<p>This commands works on the windows system but giving problems on linux:</p>

<pre class=""lang-none prettyprint-override""><code>Step 1/10 : FROM mcr.microsoft.com/dotnet/core/sdk AS build-env
Error parsing reference: ""mcr.microsoft.com/dotnet/core/sdk AS build-env"" is not a valid repository/tag: invalid reference format
</code></pre>

<p>I tried removing the <code>AS &lt;name&gt;</code> which worked, but now have a scenario where the name needs to be used.</p>

<p>Below is the code snippet for the basic example</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:2.2 AS build-env
WORKDIR /app

COPY *.csproj ./
RUN dotnet restore

COPY . ./
RUN dotnet publish -c Release -o out

FROM mcr.microsoft.com/dotnet/core/aspnet:2.2
WORKDIR /app
COPY --from=build-env /app/out .
ENTRYPOINT [""dotnet"", ""aspnetapp.dll""]
</code></pre>
"
"52701772","How to implement Dockerfile inheritance?","<docker>","58645433","Docker & Python, Speed up when your requirements.pip list is huge?","<python><django><docker>","<p>I have a Dockerfile which installs production &amp; test dependencies. I want to have separate image for tests, so production image is smaller, without to much code duplication. Maybe there is something like <code>FROM</code> statement for referencing other Dockerfiles?</p>

<p>Dockerfile has following lines:</p>

<pre><code>ADD requirements.txt ${PROJECT_DIR}/requirements.txt
RUN pip install --no-cache --process-dependency-links --trusted-host github.com -r requirements.txt
ADD requirements-test.txt ${PROJECT_DIR}/requirements-test.txt
RUN pip install --no-cache --process-dependency-links --trusted-host github.com -r requirements-test.txt
</code></pre>

<p>First two install depencencies for project, second two - install dependencies for testing (pytest, pylint, etc.). </p>

<p>I also have docker-compose that starts database, redis cache, etc. This is how I run service and run tests:</p>

<pre><code>run:
    docker-compose -f docker-compose.yaml run
test:
    docker-compose -f docker-compose-dev.yaml run py.test tests/
</code></pre>

<p>Inside both <code>docker-compose.yaml</code> has this build config for my container:</p>

<pre><code>build:
  context: .
  dockerfile: ./Dockerfile
</code></pre>

<p>So, I could reference different Dockerfiles from my <code>docker-compose.yaml</code>, but I don't want them to be complete copies that have only two lines difference.</p>
","<p>Say you have a rather large requirements.pip</p>

<p>when you modify a requirements.pip, it takes ages to build Docker image because it has to download and install all packages in <code>requirements.pip</code></p>

<p>Is there a way to speed up the docker image building process when you modify a length <code>requirements.pip</code> ?</p>

<p>MY docker</p>

<pre><code>....




 COPY ./momsite/requirements.pip /requirements.pip
 RUN pip install -r /requirements.pip \
   &amp;&amp; rm -rf /root/.cache

 COPY ./momsite /app/momsite

 COPY ./compose/production/web/etc /etc

 COPY ./compose/production/web/start.sh /
 RUN chmod +x /start.sh


 WORKDIR /app
 RUN chown www-data:www-data /app/momsite/reload

 CMD [""/start.sh""]
</code></pre>

<p>requirements.pip</p>

<pre><code> amqp==2.1.4
 anyjson==0.3.3
 apns2==0.3.0
 argh==0.24.1
 arrow==0.5.4
 asgi-redis==0.14.1
 asgiref==3.2.3
 asn1crypto==0.22.0
 astroid==1.2.1
 async-timeout==3.0.1
 attrs==19.3.0
 autobahn==19.10.1
 Automat==0.7.0
 autopep8==1.2.1
 awscli==1.10.46
 Babel==2.3.4
 BabelDjango==0.2.2
 backcall==0.1.0
 backports-abc==0.4
 backports.csv==1.0.7
 backports.shutil-get-terminal-size==1.0.0
 backports.ssl-match-hostname==3.4.0.2
 bcdoc==0.12.2
 beautifulsoup4==4.6.0
 billiard==3.5.0.2
 bleach==2.1.1
 boto==2.42.0
 boto3==1.9.146
 botocore==1.12.146
 braintree==3.24.0
 cachetools==3.1.0
 celery==4.0.2
 certifi==2018.11.29
 cffi==1.10.0
 channels==2.3.1
 chardet==3.0.4
 cluster==1.1.2
 colorama==0.3.3
 confusable-homoglyphs==3.2.0
 constantly==15.1.0
 contextlib2==0.4.0
 coreapi==2.3.3
 coreschema==0.0.4
 cron-descriptor==1.2.5
 croniter==0.3.12
 cryptography==2.8
 cssselect==0.9.1
 cssutils==1.0.1
 cycler==0.10.0
 Cython==0.24
 daphne==2.3.0
 dateutils==0.6.6
 decorator==4.1.2
 defusedxml==0.5.0
 Delorean==0.4.1
 derpconf==0.7.3
 diff-match-patch==20121119
 dj-database-url==0.3.0
 dj-email-url==0.0.4
 Django==2.2.5
 django-absolute==0.3
 django-admin-rangefilter==0.3.8
 django-advanced-filters==1.1.1
 django-annoying==0.9.0
 django-appconf==1.0.2
 django-autocomplete-light==3.3.5
 -e git://github.com/justinmayer/django-autoslug.git@4dc75083d84265e019a900d636273c731457193d#egg=django_autoslug
 django-avatar==2.0
 django-baton==1.3.6
 django-bower==5.0.1
 django-braces==1.13.0
 django-cache-url==1.0.0
 django-cacheops==3.2.1
 django-categories==1.6.1
 django-celery-results==1.0.1
 django-classy-tags==0.5
 django-compat==1.0.15
 django-configurations==0.8
 django-constance==2.0.0
 django-cors-headers==0.12
 django-countries==3.4.1
 django-crispy-forms==1.7.2
 django-dashing==0.3
 django-db-readonly==0.3.2
 django-debug-panel==0.8.3
 django-debug-toolbar==2.0
 django-debug-toolbar-line-profiler==0.4.0

... total 383 line
</code></pre>
"
"53225029","Configuring Spring boot Docker and Mysql","<mysql><docker><spring-boot>","58851404","docker-compse.yml spring boot mysql configuration","<mysql><spring-boot><docker><docker-compose>","<p>I am new to Spring Boot and Docker.</p>

<p>I am trying to create a Spring Boot application connecting to mysql and using Docker to run both.</p>

<p><strong>Steps I followed</strong>
<em>Step1</em> - Created mysql image and started running it.</p>

<pre><code>docker run --name=docker-mysql --env=""MYSQL_ROOT_PASSWORD=root"" --env=""MYSQL_PASSWORD=root"" --env=""MYSQL_DATABASE=test"" mysql
</code></pre>

<p><em>Step2</em> Created a SpringBoot application </p>

<pre><code>docker build -f Dockerfile -t gradle-springboot-docker .
</code></pre>

<p><em>Step3</em> Ran the Spring Boot app and linked with Mysql</p>

<pre><code>docker run -t --name gradle-springboot-docker --link docker-mysql:mysql -p 8080:8080 gradle-springboot-docker
</code></pre>

<p>It gives basic connection error to mysql. I have listed the below application.properties. Is my connection information correct since I am using Docker. What would be the host for mysql?</p>

<pre><code>SSL properties
server.port=8080

#DataSource
datasource.driver = com.mysql.jdbc.Driver
datasource.url= jdbc:mysql://localhost:3306/test? 
autoReconnect=true&amp;useSSL=false            
datasource.username=root
datasource.password=root

# Hibernate
hibernate.dialect = org.hibernate.dialect.MySQL5Dialect
hibernate.show_sql = true
hibernate.lazy = true
hibernate.max_fetch_depth = 3
hibernate.packagesToScan = com.springboot.poc
# Once DB is created change below property to 'update'
hibernate.hbm2ddl.auto = update
</code></pre>
","<p>I have been trying to configure docker for my spring boot application with MySQL. But, I keep getting <strong>communications link failure</strong> error after running </p>

<blockquote>
  <p>docker compose up</p>
</blockquote>

<p>Here's a snapshot of the error</p>

<pre><code>spring-batch_1  | 2019-11-14 06:23:43.713  INFO 1 --- [           main] 
com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
spring-batch_1  | 2019-11-14 06:23:43.918 ERROR 1 --- [           main] 
com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool 
initialization.
spring-batch_1  | 
spring-batch_1  | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link 
failure
spring-batch_1  | 
spring-batch_1  | The last packet sent successfully to the server was 0 milliseconds ago. The 
driver has not received any packets from the server.
spring-batch_1  |       at 
com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[ . 
mysql-connector-java-8.0.18.jar!/:8.0.18]
spring-batch_1  |  at
com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException
(SQLExceptionsMapping.java:64 ) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
spring-batch_1  |       at 
com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836) ~[mysql-connector-java- 
8.0.18.jar!/:8.0.18]
spring-batch_1  |       at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:456) ~ 
[mysql-connector-java-8.0.18.jar!/:8.0.18]
spring-batch_1  |       at 
com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246) ~[mysql-connector-java- 
8.0.18.jar!/:8.0.18]
spring-batch_1  |       at 
com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql- 
connector-java-8.0.18.jar!/:8.0.18]
spring-batch_1  |       at 
com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP- 
3.4.1.jar!/:na]
spring-batch_1  |       at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~ 
[HikariCP-3.4.1.jar!/:na]
spring-batch_1  |       at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~ 
[HikariCP-3.4.1.jar!/:na]
spring-batch_1  |       at 
com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP- 
3.4.1.jar!/:na]
spring-batch_1  |       at 
com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP- 
3.4.1.jar!/:na]
spring-batch_1  |       at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~ . 
[HikariCP-3.4.1.jar!/:na]
spring-batch_1  |       at 
com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP- 
3.4.1.jar!/:na]
spring-batch_1  |       at 
com.zaxxer.hikari.HikariDataSource$$FastClassBySpringCGLIB$$eeb1ae86.invoke(&lt;generated&gt;) ~ . 
[HikariCP-3.4.1.jar!/:na]
spring-batch_1  |       at 
org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) ~[spring-core- 
5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
spring-batch_1  |       at 
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint
(CglibAopProxy.java:769) ~[spring-aop-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
spring-batch_1  |       at 
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed
(ReflectiveMethodInvocation.java:163) ~[spring-aop-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
spring-batch_1  |       at 
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed
(CglibAopProxy.java:747) ~[spring-aop-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
spring-batch_1  |       at 
org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed
(DelegatingIntroductionInterceptor.java:136) ~[spring-aop-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
</code></pre>

<p>Here's my <strong>docker-compose.yml</strong> file</p>

<pre><code>version: ""3.3""
services:
  mysql:
    restart: always
    image: mysql:latest
    ports:
      - 6033:3306
    expose:
      - 6033
    volumes:
      - db_data:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD=root@123
      - MYSQL_DATABASE=micro_services
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root@123
  spring-batch:
    restart: always
    image: ioesandeep/product-service:0.0.1-SNAPSHOT
    ports:
      - ""8082:8082""
    environment:
       SPRING_DATASOURCE_URL: jdbc:mysql://127.0.0.1:6033/micro_services
       SPRING_DATASOURCE_USERNAME: root
       SPRING_DATASOURCE_PASSWORD: root@123
       SPRING_JPA_DATABASE: mysql
    depends_on:
      - mysql
volumes:
  db_data: {}
</code></pre>

<p>and here's my <code>dockerfile</code> which is used to build my <strong>spring-batch image</strong></p>

<pre><code>FROM openjdk:11-jdk
VOLUME /tmp
ARG JAR_FILE
ADD ${JAR_FILE} product.jar
ENTRYPOINT [""java"",""-Djava.security.egd=file:/dev/./urandom"",""-jar"",""/product.jar""]
</code></pre>

<p>I am using <strong>dockerfile-maven-plugin</strong> from <strong>com.spotify</strong> to build the image.
Here's my <strong>application.properties</strong> file for the spring boot and hibernate</p>

<pre><code>spring.application.name=product
server.port=8082
spring.jpa.hibernate.ddl-auto=create-drop
spring.datasource.url=jdbc:mysql://127.0.0.1:6033/micro_services
spring.datasource.username=root
spring.datasource.password=root@123
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.jpa.database=mysql
spring.batch.initialize-schema=always
spring.batch.job.enabled=false
</code></pre>

<p><code>Docker version 18.09.2, build 6247962</code></p>

<p><code>mysql  Ver 8.0.18 for macos10.14 on x86_64 (MySQL Community Server - GPL)</code></p>

<p><strong>Images</strong></p>

<p><code>mysql                          latest                    c8ee894bd2bd        4 weeks ago</code></p>

<p><code>openjdk                        11-jdk                    a7e47afa852b        3 weeks ago</code></p>

<p>I have tried a lot many configurations for the docker-compose file but every time I get the same error. I am not sure what am I missing in the configuration. I have been trying to get this up for over 2 days now. I would really appreciate a quick help.
Thanks</p>
"
"53539807","Why docker in docker (dind) containers mount volumes with host path?","<docker><dind>","59065649","Docker mount host directory not container when using Docker inside Docker by docker.sock","<docker><jenkins>","<p>I have a setup with docker in docker and try to mount folders.</p>

<p>Let's say I have those folders that I wish to share with his parent. On the host, I created a file in /tmp/dind called <code>foo</code>. Host starts container 1, which starts container 2. This is the result I want to have.</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;     &lt;-------&gt;
</code></pre>

<p>Instead, I get</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;
      &lt;-----------------------&gt;
</code></pre>

<p>Code here:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind2:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>This outputs nothing, while the next command gives foo as result. I changed the mounted volume:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>The question is, what do I need to do in order to use Container 1 path and not host? Or do I misunderstand something about docker here?</p>
","<p>I'm using Docker inside docker by mount docker.sock into container for Jenkins docker agents. In my Jenkins container, when I execute</p>

<pre><code>docker run -t -d -v /var/data:/var:data image_name
</code></pre>

<p>it will not create and mount /var/data folder inside Jenkins container. Instead it create a folder in my host and mounted into agent container.</p>

<p>Does this behavior expected?</p>
"
"54272997","Access Docker postgres container from another container","<postgresql><docker>","59071799","psycopg2.OperationalError: could not connect to server: Connection refused Is the server running on host ""localhost""","<python><docker><flask><docker-compose><psycopg2>","<p>I am trying to make a portable solution to having my application container connect to a postgres container.  By 'portable' I mean that I can give the user two <code>docker run</code> commands, one for each container, and they will always work together.</p>

<p>I have a postgres docker container running on my local PC, and I run it like this,</p>

<pre><code>docker run -p 5432:5432 -v $(pwd)/datadir:/var/lib/postgresql/data -e POSTGRES_PASSWORD=qwerty -d postgres:11
</code></pre>

<p>and I am able to access it from a python flask app, using the address <code>127.0.0.1:5432</code>.</p>

<p>I put the python app in a docker container as well, and I am having trouble connecting to the postgres container.</p>

<p>Address <code>127.0.0.1:5432</code> does not work.</p>

<p>Address <code>172.17.0.2:5432</code> <strong>DOES</strong> work (172.17.0.2 is the address of the docker container running postgres).  However I consider this not portable because I can't guarantee what the postgres container IP will be.</p>

<p>I am aware of the <code>--add-host</code> flag, but it is also asking for the host-ip, which I want to be the localhost (127.0.0.1).  Despite several hits on <code>--add-host</code> I wasn't able to get that to work so that the final docker run commands can be the same on any computer they are run on.</p>

<p>I also tried this: <a href=""https://stackoverflow.com/questions/42318770/docker-container-port-accessed-from-another-container"">docker container port accessed from another container</a></p>

<p>My situation is that the postgres and myApp will be containers running on the same computer.  I would prefer a non-Docker compose solution.</p>
","<p>Got Error while doing docker-compose up.</p>

<pre><code>conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
Is the server running on host ""localhost"" (127.0.0.1) and accepting
TCP/IP connections on port 5433?
could not connect to server: Cannot assign requested address
Is the server running on host ""localhost"" (::1) and accepting
TCP/IP connections on port 5433?
</code></pre>

<p>docker-compose.yml:-</p>

<pre><code>version: '3'

services:
  dcs_web:
    build: .
    depends_on:
      - db
    ports:
      - ""5000:5000""
  db:
    image: postgres:latest
    volumes:
      - db-data:/var/lib/postgresql/data
    ports:
      - ""5433:5433""
    environment:
      - 'POSTGRES_DB:dcmDB'
      - 'POSTGRES_USER:postgres'
      - 'POSTGRES_PASSWORD:admin'

volumes:
  db-data:
</code></pre>

<p>In App config.ini:</p>

<pre><code>[DEFAULT]
DB_NAME = user
DB_PASSWORD = admin
DB_USER = postgres
DB_HOST = localhost
DB_PORT = 5433
DEBUG = True
</code></pre>

<p>I have gone throught '/var/lib/postgresql/data' location 'listen adress = *' is there . Dont know how to deal with this.</p>
"
"55689701","How to use Tor with Chrome browser through Selenium","<python-3.x><selenium><google-chrome><proxy><tor>","59159994","Selenium webdriver.Remote driver does not work with tor proxy(webdriver.Chrome does)","<python><selenium><docker><selenium-chromedriver><tor>","<p>I'm trying to run my selenium driver on Tor. Note that the script already runs with no errors without Tor.</p>

<p>This is what I've done so far:</p>

<p>1) I called the Tor framework</p>

<pre><code>import socks
import socket
from stem.util import term    


import stem.process

SOCKS_PORT=7000 

socks.setdefaultproxy(proxy_type=socks.PROXY_TYPE_SOCKS5,
                      addr = ""127.0.0.1"", 
                      port = SOCKS_PORT)
socket.socket = socks.socksocket

# Perform DNS resolution through the socket
def getaddrinfo(*args):   return [(socket.AF_INET, socket.SOCK_STREAM, 6, '', (args[0], args[1]))]

socket.getaddrinfo = getaddrinfo

def print_bootstrap_lines(line):   
    if ""Bootstrapped "" in line:
      print(term.format(line, term.Color.GREEN))

tor_process = stem.process.launch_tor_with_config(
    tor_cmd = ""C:/Users/my-username\Desktop/Tor Browser/Browser/TorBrowser/Tor//tor.exe"" ,
    config = { 'SocksPort': str(SOCKS_PORT),},
    init_msg_handler = print_bootstrap_lines,
)
</code></pre>

<ol start=""2"">
<li><p>After calling the Tor framework which is like a container to my understanding, I then called the Chrome driver:</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
options = Options()
options.binary_location = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
driver = webdriver.Chrome(options=options, executable_path = r'C:\Users\my-username\chromedriver')
</code></pre></li>
</ol>

<p>3) At this point I insert the scraping script.</p>

<p>4) Close driver and kill the Tor process:</p>

<pre><code>driver.close()   
tor_process.kill()
</code></pre>

<p>The output I get is the following:</p>

<pre><code>Apr 15 14:31:20.000 [notice] Bootstrapped 0%: Starting
Apr 15 14:31:23.000 [notice] Bootstrapped 10%: Finishing handshake with directory server
Apr 15 14:31:23.000 [notice] Bootstrapped 80%: Connecting to the Tor network
Apr 15 14:31:23.000 [notice] Bootstrapped 90%: Establishing a Tor circuit
Apr 15 14:31:24.000 [notice] Bootstrapped 100%: Done
Traceback (most recent call last):

  File ""&lt;ipython-input-2-2b2233fc0ae4&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/my-username-folder/FireStarter_All_1Step_2.py', wdir='C:/Users/my-username-folder')

  File ""C:\Users\my-username\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\my-username\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/my-username-folder/FireStarter_All_1Step_2.py"", line 94, in &lt;module&gt;
    driver = webdriver.Chrome(options=options, executable_path = r'C:\Users\my-username-folder\chromedriver')

  File ""C:\Users\my-username\AppData\Local\Continuum\anaconda3\lib\site-packages\selenium\webdriver\chrome\webdriver.py"", line 73, in __init__
    self.service.start()

  File ""C:\Users\my-username\AppData\Local\Continuum\anaconda3\lib\site-packages\selenium\webdriver\common\service.py"", line 104, in start
    raise WebDriverException(""Can not connect to the Service %s"" % self.path)

WebDriverException: Can not connect to the Service C:\Users\my-username-folder\chromedriver
</code></pre>

<p>What am I doing wrong?</p>

<p>Update:
I am looking to use Tor with Chrome browser.</p>
","<p>I'm trying to use socks5 proxy on my <a href=""https://hub.docker.com/r/selenium/standalone-chrome"" rel=""nofollow noreferrer"">remote driver which is hosted as a docker container on port 4444</a>. </p>

<p>here is code-sample:</p>

<pre><code>from selenium import webdriver

opts = webdriver.ChromeOptions()
opts.add_argument(""--no-sandbox"")
opts.add_argument(""--disable-dev-shm-usage"")
opts.add_argument(""--proxy-server=socks5://127.0.0.1:9050"")
driver = webdriver.Remote(command_executor=""http://localhost:4444/wd/hub"", desired_capabilities=opts.to_capabilities())
</code></pre>

<p>Then, when I try to open any page, I get error stating <code>Check your proxy settings or contact your network administrator</code>. </p>

<p>While using same code sample on regular proxy – it works just fine.
When I do bold request through <code>9050</code> port – it works just fine. </p>

<p>And finally, when I use same code sample with <code>webdriver.Chrome</code> instead of <code>webdriver.Remote</code> it works fine!</p>

<p>I would appreciate any suggestions to make things work through Remote webdriver. </p>

<p><strong>UPDATE:</strong>
I'm using <code>selenium==3.14.0</code> and RemoteDriver is getting docker image <code>selenium/node-chrome-debug:3.141.59-radium</code>. </p>
"
"57660451","Extending CouchDB Docker image","<docker><dockerfile><couchdb>","58738959","How do you extend a CouchDB Docker Image to include schema + seed data","<docker><dockerfile><couchdb>","<p>I’m trying to extend CouchDB docker image to pre-populate CouchDB (with initial databases, design documents, etc.). </p>

<p>In order to create a database named <code>db</code>, I first tried this initial <code>Dockerfile</code>:</p>

<pre><code>FROM couchdb
RUN curl -X PUT localhost:5984/db
</code></pre>

<p>but the build failed since couchdb service is not yet started at build time. So I changed it into this:</p>

<pre><code>FROM couchdb
RUN service couchdb start &amp;&amp; \ 
  sleep 3 &amp;&amp; \                 
  curl -s -S -X PUT localhost:5984/db &amp;&amp; \
  curl -s -S localhost:5984/_all_dbs
</code></pre>

<p>Note:</p>

<ul>
<li>the <code>sleep</code> was the only way I found to make it work, since it did not work with curl option <code>--connect-timeout</code>,</li>
<li>the second <code>curl</code> is only to check that the database was created.</li>
</ul>

<p>The build seems to work fine:</p>

<pre><code>$ docker build . -t test3 --no-cache
Sending build context to Docker daemon  6.656kB
Step 1/2 : FROM couchdb
 ---&gt; 7f64c92d91fb
Step 2/2 : RUN service couchdb start &amp;&amp;   sleep 3 &amp;&amp;   curl -s -S -X PUT localhost:5984/db &amp;&amp;   curl -s -S localhost:5984/_all_dbs
 ---&gt; Running in 1f3b10080595
Starting Apache CouchDB: couchdb.
{""ok"":true}
[""db""]
Removing intermediate container 1f3b10080595
 ---&gt; 7d733188a423
Successfully built 7d733188a423
Successfully tagged test3:latest
</code></pre>

<p>What is weird is that now when I start it as a container, database <code>db</code> does not seem to be saved into <code>test3</code> image:</p>

<pre><code>$ docker run -p 5984:5984 -d test3
b34ad93f716e5f6ee68d5b921cc07f6e1c736d8a00e354a5c25f5c051ec01e34

$ curl localhost:5984/_all_dbs
[]
</code></pre>
","<p>I’m trying to extend CouchDB docker image in order to pre-populate CouchDB. I have a second image that, when started, looks for a CouchDB instance, sets up schema, and then runs a seed script that populates the DB.</p>

<p>I know how to orchestrate this with docker-compose, but ideally, I would like to create and publish a custom CouchDB docker image that has the schema + data already seeded to speed things up.</p>

<p>I think I need to use a docker ""builder"" to accomplish this, but I'm not sure how to spin up the CouchDB image within the builder to do the seed.</p>
"
"57731428","How do I prevent root access to my docker container","<shell><docker><docker-compose><dockerfile><hardening>","59025732","is there a way to stop a user with dockerfile / docker-compose from assuming root inside of a container?","<docker><dockerfile><root><superuser>","<p>I am working on hardening our docker images, which I already have a bit of a weak understanding of.  With that being said, the current step I am on is preventing the user from running the container as root.  To me, that says ""when a user runs 'docker exec -it my-container bash', he shall be an unprivileged user"" (correct me if I'm wrong).</p>

<p>When I start up my container via docker-compose, the start script that is run needs to be as root since it deals with importing certs and mounted files (created externally and seen through a volume mount).  After that is done, I would like the user to be 'appuser' for any future access.  This question seems to match pretty well what I'm looking for, but I am using docker-compose, not docker run: <a href=""https://stackoverflow.com/questions/43814416/how-to-disable-the-root-access-of-a-docker-container"">How to disable the root access of a docker container?</a></p>

<p>This seems to be relevant, as the startup command differs from let's say tomcat.  We are running a Spring Boot application that we start up with a simple 'java -jar jarFile', and the image is built using maven's dockerfile-maven-plugin.  With that being said, should I be changing the user to an unprivileged user before running that, or still after?</p>

<p>I believe changing the user inside of the Dockerfile instead of the start script will do this... but then it will not run the start script as root, thus blowing up on calls that require root.  I had messed with using ENTRYPOINT as well, but could have been doing it wrong there.  Similarly, using ""user:"" in the yml file seemed to make the start.sh script run as that user instead of root, so that wasn't working.</p>

<p>Dockerfile:</p>

<pre><code>FROM parent/image:latest

ENV APP_HOME                            /apphome
ENV APP_USER                            appuser
ENV APP_GROUP                           appgroup

# Folder containing our application, i.e. jar file, resources, and scripts.
# This comes from unpacking our maven dependency
ADD target/classes/app ${APP_HOME}/

# Primarily just our start script, but some others
ADD target/classes/scripts /scripts/

# Need to create a folder that will be used at runtime
RUN mkdir -p ${APP_HOME}/data &amp;&amp; \
    chmod +x /scripts/*.sh &amp;&amp; \
    chmod +x ${APP_HOME}/*.*

# Create unprivileged user
RUN groupadd -r ${APP_GROUP} &amp;&amp; \
    useradd -g ${APP_GROUP} -d ${APP_HOME} -s /sbin/nologin  -c ""Unprivileged User"" ${APP_USER} &amp;&amp; \
    chown -R ${APP_USER}:${APP_GROUP} ${APP_HOME}

WORKDIR $APP_HOME

EXPOSE 8443

CMD /opt/scripts/start.sh
</code></pre>

<p>start.sh script:</p>

<pre><code>#!/bin/bash

# setup SSL, modify java command, etc

# run our java application
java -jar ""boot.jar""

# Switch users to always be unprivileged from here on out? 
# Whatever ""hardening"" wants...  Should this be before starting our application?
exec su -s ""/bin/bash"" $APP_USER
</code></pre>

<p>app.yml file:</p>

<pre><code>version: '3.3'

services:
  app:
    image: app_image:latest
    labels:
      c2core.docker.compose.display-name: My Application
      c2core.docker.compose.profiles: a_profile
    volumes:
      - ""data_mount:/apphome/data""
      - ""cert_mount:/certs""
    hostname: some-hostname
    domainname: some-domain
    ports:
    - ""8243:8443""
    environment:
      - some_env_vars
    depends_on:
    - another-app
    networks:
      a_network:
        aliases:
          - some-network
networks:
  a_network:
    driver: bridge
volumes:
  data_mount:
  cert_mount:
</code></pre>

<p>docker-compose shell script:</p>

<pre><code>docker-compose -f app.yml -f another-app.yml $@
</code></pre>

<p>What I would expect is that anyone trying to access the container internally will be doing so as appuser and not root.  The goal is to prevent someone from messing with things they shouldn't (i.e. docker itself).</p>

<p>What is happening is that the script will change users after the app has started (proven via an echo command), but it doesn't seem to be maintained.  If I exec into it, I'm still root.</p>
","<p>Is there any way to stop a user with dockerfile and docker-compose from assuming root inside of a docker container? The concern is, of course, security and least privilege and I would like to see if anyone has solved that issue. Thankyou ahead for your feedback.</p>
"
"58298774","standard_init_linux.go:211: exec user process caused ""exec format error""","<python><docker><kubernetes><dockerfile><minikube>","59000007","standard_init_linux.go:207: exec user process caused ""exec format error""","<python><linux><azure><docker>","<p>I am building the Dockerfile for python script which will run in minikube windows 10 system below is my Dockerfile</p>

<p>Building the docker using the below command
<code>docker build -t python-helloworld .</code></p>

<p>and loading that in minikube docker demon
<code>docker save python-helloworld | (eval $(minikube docker-env) &amp;&amp; docker load)</code></p>

<p>Docker File</p>

<pre><code>FROM python:3.7-alpine
#add user group and ass user to that group
RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup

#creates work dir   
WORKDIR /app

#copy python script to the container folder app
COPY helloworld.py /app/helloworld.py

#user is appuser
USER appuser

ENTRYPOINT  [""python"", ""/app/helloworld.py""]
</code></pre>

<p>pythoncronjob.yml file (cron job file)</p>

<pre><code>apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: python-helloworld
spec:
  schedule: ""*/1 * * * *""
  jobTemplate:
    spec:
      backoffLimit: 5
      template:
        spec:
          containers:
          - name: python-helloworld
            image: python-helloworld
            imagePullPolicy: IfNotPresent
            command: [/app/helloworld.py]
          restartPolicy: OnFailure
</code></pre>

<p>Below is the command to run this Kubernetes job
<code>kubectl create -f pythoncronjob.yml</code></p>

<p>But getting the below  error job is not running scuessfully but when u ran the Dockerfile alone its work fine </p>

<p>standard_init_linux.go:211: exec user process caused ""exec format error""</p>
","<p>I am currently trying to deploy my docker application to the container registry Azure. I am able to run my docker image locally but when I deploy it to azure, it gives me this error:</p>

<p>standard_init_linux.go:207: exec user process caused ""exec format error""
 Here is my dockerfile:</p>

<pre><code>*Pull a pre-built alpine docker image with nginx and python3 installed

*this image is from docker community, its small so our upload to contain will be faster

FROM tiangolo/uwsgi-nginx-flask:python3.7

FROM ubuntu:latest

ENV LISTEN_PORT=8400

EXPOSE 8400

RUN apt-get update &amp;&amp; apt-get install -y /

    curl apt-utils apt-transport-https debconf-utils gcc build-essential g++-5\

    &amp;&amp; rm -rf /var/lib/apt/lists/*

*adding custom MS repository

RUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -

RUN curl https://packages.microsoft.com/config/ubuntu/19.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list

RUN apt-get update &amp;&amp; ACCEPT_EULA=Y apt-get install -y msodbcsql17

*install SQL Server drivers

RUN apt-get update &amp;&amp; ACCEPT_EULA=Y apt-get -f install -y unixodbc-dev

*install SQL Server tools

RUN apt-get update &amp;&amp; ACCEPT_EULA=Y apt-get install -y mssql-tools

RUN echo 'export PATH=""$PATH:/opt/mssql-tools/bin""' &gt;&gt; ~/.bashrc

RUN /bin/bash -c ""source ~/.bashrc""

RUN apt-get update &amp;&amp; apt-get install -y python3-pip

RUN apt-get update &amp;&amp; apt-get install -y libpq-dev

*install additional requirements from a requirements.txt file

COPY requirements.txt /

RUN pip3 install --no-cache-dir -r /requirements.txt

COPY app/. /.

CMD python3 wsgi.py 

</code></pre>

<p>Because I do not understand how azure calls my Docker images, I kept on trying different CMD versions such as:</p>

<p>CMD [""python3"", ""wsgi.py"", ""runserver"", ""0.0.0.0:8400""]</p>

<p>But to no avail. I looked up on internet for solutions but really could not find any. Is there anyone there that has insights on what I do wrong? Is it essential to create a .sh file ? I am new to linux so any insights will help!</p>

<p>Thanks again!</p>
"
"58622131","Docker-compose - setting environment variables that are not literals","<docker><docker-compose>","58624912","use ssh keyfile as environment variable in docker-compose","<docker><jenkins><docker-compose><jenkins-plugins><ssh-keys>","<p>I have setup Jenkins within a Docker container and I am trying to access that my private Bitbucket repo with that server. I need to copy my SSH key into that container so that Bitbucket recognizes it and I can have my Jenkins server access the repo then.</p>

<p>I have in my docker-compose.yml file the following:</p>

<pre><code>services:
  jenkins:
    build: .
    volumes:
      - jenkins-data:/var/jenkins_home
    environment:
      - SSH_PRIVATE_KEY=$(cat ~/.ssh/id_rsa)
    ports:
      - ""8080:8080""
      - ""50000:50000""

volumes:
  jenkins-data:
</code></pre>

<p>However, <code>echo $SSH_PRIVATE_KEY</code> gives <code>/.ssh/id_rsa</code> literally instead of the value stored inside. I have heard the problem with doing this inside the Dockerfile instead would be that it still can be viewed in one of the layers of the image that will be pushed.</p>

<p>My question is how can I set the value of <code>SSH_PRIVATE_KEY</code> to the value of the contents of my file?</p>

<p>I believe this could be a duplicate of <a href=""https://stackoverflow.com/questions/50238621/how-to-set-environment-variable-into-docker-container-using-docker-compose"">How to set environment variable into docker container using docker-compose</a> however that solution does not appear to change anything for me.</p>
","<p>I've run into the problem that I need to get a ssh keyfile as environment variable within my docker container.</p>

<p>My thing is, that I don't want to include the value hardcoded at build time of the container but at runtime as environment variable.</p>

<p><em>My tl;dr question is:</em></p>

<p><strong>Is there any way to assign the contents of a file (in my case a ssh key) to an environment variable in docker-compose?</strong></p>

<p>I'd imagine something like this:</p>

<pre><code>version: ""3.1""
services:
  jenkinspink:
    image: jenkinspink:latest
    ports:
      - 8080:8080

    # Here I want to pass a ssh keyfile as environment variable to the docker container
    environment:
      EC2_PRIVATE_KEY=&lt;ssh_keyfile_on_host_system&gt;

</code></pre>

<p>In my specific use case I need the ssh key to configure the EC2-Plugin in Jenkins. I'm doing this via JCasC (Jenkins Configuration as Code). Neither the EC2-Plugin nor the JCasC configuration allow the usage of a file path for the ssh key. Both want it as environment variable.
Now I see only 2 options:</p>

<ul>
<li>Either I do it the clean way by forking the EC2-Plugin repository and adapt the <em>ssh key</em> section in a way that allows file paths to be injected and then do a pull request.</li>
<li>Or I do the quick way my question refers to by injecting the file via docker-compose as environment variable. But I don't know how.</li>
</ul>

<p>Any hints appreciated :-D</p>
"
"3368683","How to compile .c file with OpenSSL includes?","<c><linux><compiler-construction><openssl><compiler-errors>","45818962","Installing OpenSSL inside a docker image","<c++><ubuntu><docker><openssl><dockerfile>","<p>I am trying to compile a small .c file that has the following includes:</p>
<pre><code>#include &lt;openssl/ssl.h&gt;
#include &lt;openssl/rsa.h&gt;
#include &lt;openssl/x509.h&gt;
#include &lt;openssl/evp.h&gt;
</code></pre>
<p>In the same folder where I have the .c file I have a /openssl with all those files (and more), also in synaptic package manager I see OpenSSL installed, I am trying to compile with this:</p>
<pre><code>gcc -o Opentest Opentest.c -lcrypto
</code></pre>
<p>but I always get the errors:</p>
<pre><code>error: openssl/ssl.h: No such file or directory
error: openssl/rsa.h: No such file or directory
error: openssl/x509.h: No such file or directory
error: openssl/evp.h: No such file or directory
</code></pre>
<p>The file I want to compile is only a .c file, doesn't have Makefile or ./configure.</p>
<p>I already tried:</p>
<pre><code>env CFLAGS=-I/path/to/openssl/
</code></pre>
<p>and tried to compile again but I get the same errors.</p>
<p>What should I do in order to compile with OpenSSL includes?</p>
","<p>I'm working with Docker and I have the following Dockerfile</p>

<pre><code>FROM ubuntu:16.04

RUN dpkg --add-architecture i386 &amp;&amp; apt update &amp;&amp; apt install -y \
        bc \
        build-essential \
        cpio \
        dosfstools \
        g++-multilib \
        gdisk \
        git-core \
        libncurses5-dev \
        libncurses5-dev:i386 \
        python \
        squashfs-tools \
        sudo \
        unzip \
        wget \
        locales \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN locale-gen en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LANGUAGE en_US:en
ENV LC_ALL en_US.UTF-8

WORKDIR ""/sedutil""

CMD [""/sedutil/images/autobuild.sh"",""-h""]
</code></pre>

<p>I added to the list ""libssl-dev"" and ""openssl"" . Now I'm tryng to complie some c++ code inside the docker. It seems that the compiler does not find the openssl headers because it returns </p>

<pre><code>fatal error: openssl/evp.h: No such file or directory
#include &lt;openssl/evp.h&gt;
</code></pre>

<p>I think that adding ""libssl-dev"" is not enough. Any suggestion ?</p>
"
"22049212","Docker: Copying files from Docker container to host","<docker><docker-container><file-copying>","63993955","using docker how to copy files from docker to host using docker run","<docker>","<p>I'm thinking of using Docker to build my dependencies on a Continuous Integration (CI) server, so that I don't have to install all the runtimes and libraries on the agents themselves. </p>

<p>To achieve this I would need to copy the build artifacts that are built inside the container back into the host. Is that possible?</p>
","<p>I have a docker image which has all the required files when I say docker run I would like for it to copy the contents of a folder in docker image to a host directory. After which the docker is not needed and stops</p>
"
"24309526","How to change the docker image installation directory?","<docker>","63993702","Run Docker image on /dev/shm partition","<docker><filesystems>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>Is there any way to run Docker image on <code>/dev/shm</code> partition? when running docker by default, the &quot;overlay&quot; entry is added in filesystem which has the same size as <code>/dev/sda3</code> partition, where the root is mounted. But since size of <code>/dev/shm</code> is bigger on the linux server I use, I was wondering if I could somehow run the image on that partition instead of <code>/dev/sda3</code>.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63828252","How to use services such as Kafka, Zookeeper and Postgres of the host sever inside a docker using docker compose?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am using docker compose and trying to use services such as Kafka, Zookeeper and Postgres of the host sever inside the container.</p>
"
"31210973","How do I seed a mongo database using docker-compose?","<mongodb><docker><docker-compose>","63845803","How can I use my database in a container after push the container into Dockerhub?","<database><docker>","<p>I am trying to distribute a set of connected applications running in several linked containers that includes a mongo database that is required to:</p>

<ul>
<li>be distributed containing some seed data;</li>
<li>allow users to add additional data.</li>
</ul>

<p>Ideally the data will also be persisted in a linked data volume container.</p>

<p>I can get the data into the <code>mongo</code> container using a <code>mongo</code> base instance that doesn't mount any volumes (dockerhub image: <code>psychemedia/mongo_nomount</code> - this is essentially the base mongo Dockerfile without the <code>VOLUME /data/db</code> statement) and a <code>Dockerfile</code> config along the lines of:</p>

<pre><code>ADD . /files
WORKDIR /files
RUN mkdir -p /data/db &amp;&amp; mongod --fork --logpath=/tmp/mongodb.log &amp;&amp; sleep 20 &amp;&amp; \
mongoimport  --db testdb --collection testcoll  --type csv --headerline --file ./testdata.csv  #&amp;&amp; mongod --shutdown
</code></pre>

<p>where <code>./testdata.csv</code> is in the same directory (<code>./mongo-with-data</code>) as the Dockerfile.</p>

<p>My docker-compose config file includes the following:</p>

<pre><code>mongo:
  #image: mongo
  build: ./mongo-with-data
  ports:
    - ""27017:27017""
  #Ideally we should be able to mount this against a host directory
  #volumes:
  #  - ./db/mongo/:/data/db
  #volumes_from:
  #  - devmongodata

#devmongodata:
#    command: echo created
#    image: busybox
#    volumes: 
#       - /data/db
</code></pre>

<p>Whenever I try to mount a VOLUME it seems as if the original seeded data - which is stored in <code>/data/db</code> - is deleted. I guess that when a volume is mounted to <code>/data/db</code> it replaces whatever is there currently. </p>

<p>That said, the <a href=""https://docs.docker.com/userguide/dockervolumes/"" rel=""noreferrer"">docker userguide</a> suggests that: <em>Volumes are initialized when a container is created. If the container’s base image contains data at the specified mount point, that existing data is copied into the new volume upon volume initialization</em>? So I expected the data to persist if I placed the VOLUME command after the seeding <code>RUN</code> command?</p>

<p>So what am I doing wrong?</p>

<p>The long view is that I want to automate the build of several linked containers, and then distribute a <code>Vagrantfile</code>/docker-compose YAML file that will fire up a set of linked apps, that includes a pre-seeded <code>mongo</code> database with a (partially pre-populated) persistent data container.</p>
","<p>I'm trying to save a database into one container. How can I save my database when I push the container to Dockerhub?</p>
"
"33001750","Connect to mysql in a docker container from the host","<mysql><docker><dockerfile>","63904851","In Javascript, how would I connect to a MySQL database that is inside a docker?","<javascript><mysql><docker>","<p><em>(It's probably a dumb question due to my limited knowledge with Docker or mysql administration, but since I spent a whole evening on this issue, I dare to ask it.)</em></p>

<p><strong>In a nutshell</strong></p>

<p>I want to run mysql in a docker container and connect to it from my host. So far, the best I have achieved is:</p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
</code></pre>

<p><strong>More details</strong></p>

<p>I'm using the following <code>Dockerfile</code>:</p>

<pre><code>FROM ubuntu:14.04.3
RUN apt-get update &amp;&amp; apt-get install -y mysql-server

# Ensure we won't bind to localhost only
RUN grep -v bind-address /etc/mysql/my.cnf &gt; temp.txt \
  &amp;&amp; mv temp.txt /etc/mysql/my.cnf

# It doesn't seem needed since I'll use -p, but it can't hurt
EXPOSE 3306

CMD /etc/init.d/mysql start &amp;&amp; tail -F /var/log/mysql.log
</code></pre>

<p>In the directory where there is this file, I can succesfully build the image and run it with:</p>

<pre><code>&gt; docker build -t my-image .
&gt; docker run -d -p 12345:3306 my-image
</code></pre>

<p>When I attach to the image, it seems to work just fine:</p>

<pre><code># from the host
&gt; docker exec -it &lt;my_image_name&gt; bash

#inside of the container now
$ mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
[...]
</code></pre>

<p>However I don't have that much success from the host:</p>

<pre><code>&gt; mysql -P 12345 -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
</code></pre>

<p><strong>Even more details</strong></p>

<ul>
<li>I've seen that there's a question which <a href=""https://stackoverflow.com/q/25069860/1796345"">looks like mine</a>. However, it isn't the same (and it doesn't have any answers anyway)

<ul>
<li>I've seen that there are images <a href=""https://github.com/sameersbn/docker-mysql"" rel=""noreferrer"">dedicated to mysql</a>, but I didn't have more success with them</li>
<li>My <code>grep -v</code> may feel weird. Admittedly, there may be cleaner way to do it. But when I attach my image, I can observe it actually worked as expected (ie: removed the <code>bind-address</code>). And I can see in the container <code>/var/log/mysql/error.log</code>:</li>
</ul></li>
</ul>

<blockquote>
  <p>Server hostname (bind-address): '0.0.0.0'; port: 3306
        - '0.0.0.0' resolves to '0.0.0.0';
      Server socket created on IP: '0.0.0.0'.</p>
</blockquote>
","<p>I've written scripts to connect to mysql databases in Javascript. One I've done is like:</p>
<pre><code>function connectToSQL() {
    return new Promise((resolve,reject)=&gt;{
        try {
            var connection = mysql.createConnection({
                host:&quot;0.0.0.0&quot;,
                user: &quot;username&quot;,
                password: &quot;password&quot;,
                database: &quot;mydb&quot;
            });
        } catch(e) {
            reject(e);
        }
        if(!connection) {
            reject(new Error(&quot;No Connection&quot;));
        }
        connection.connect(function(err) {
            if(err) reject(err);
            resolve(connection);
        });
    });
}
</code></pre>
<p>However, that was a standalone Mysql, not inside a docker container, I was connecting to. Inside my Docker, the Sql container can be found:</p>
<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                     NAMES
fcd8a5277248        mysql:5.6           &quot;docker-entrypoint...&quot;   2 weeks ago         Up 2 weeks          0.0.0.0:3306-&gt;3306/tcp                                                    db-mysql
</code></pre>
<p>How would I write a JS script to get into this to run queries?</p>
"
"34178556","How to set a Spring Boot property with an underscore in its name via Environment Variables?","<java><spring><spring-boot>","45440862","Properties with underscores in docker-compose.yml","<java><spring><hibernate><docker><docker-compose>","<p>I want to set <code>hibernate.format_sql</code> in a Spring Boot app. I want to set it using environment variables.</p>

<p>Spring Boot rather handily converts all environment variables from, for example, <code>FOO_BAR_BAZ</code> to properties called <code>foo.bar.baz</code> inside the Spring context.</p>

<p>How can I set a property that has an underscore in the target name, in Spring Boot, using environment variables? Presumably <code>HIBERNATE_FORMAT_SQL</code> will be translated to <code>hibernate.format.sql</code>?</p>
","<p>I have webbapp based on Spring. To configure it I use .yml files. 
To convert environmental variable from spring/backend .yml format to docker-compose.yml environment section I have to change indents into underscores. Example:</p>

<p>application.yml</p>

<pre><code>spring:
  mail:
    properties.mail.smtp:
      auth: true
</code></pre>

<p>docker-compose.yml</p>

<pre><code>    environment:
      SPRING_MAIL_PROPERTIES.MAIL.SMTP_AUTH: true
</code></pre>

<p>But some property names contain underscores itself. Like this:</p>

<pre><code>spring:
  jpa:
    properties:
      hibernate:
        temp:
          use_jdbc_metadata_defaults: false
</code></pre>

<p>How should I translate such properties (with underscores in names) to the environment section in docker-compose.yml?</p>

<p><strong>This question is not a duplicate</strong>. Mentioned similar question does not answer my question. Question <a href=""https://stackoverflow.com/questions/34178556/how-to-set-a-spring-boot-property-with-an-underscore-in-its-name-via-environment"">How to set a Spring Boot property with an underscore in its name via Environment Variables?</a> is about how to represent properties with underscores in Spring Boot and accepted answer shows how property file entry is interpreted by Spring. There is a link to <code>SystemEnvironmentPropertySource.java</code> class which is responsible for translation and interpretation of properties in Spring. My question asks about .yml file interpretation in <strong>Docker-Compose</strong>. So I need basically similar answer to the accepted one in the mentioned thread but about how <strong>Docker-Compose</strong> interprets .yml properties to environment variables.</p>
"
"34324277","How to pass ARG value to ENTRYPOINT?","<docker><dockerfile><docker-compose><docker-registry>","45880367","Docker Passing an argument Docker Entrypoint with entrypoint.sh","<docker><devops>","<p>Docker 1.9 allows to pass arguments to a dockerfile. See link: <a href=""https://docs.docker.com/engine/reference/builder/#arg"" rel=""noreferrer"">https://docs.docker.com/engine/reference/builder/#arg</a></p>

<p>How can i pass the same arugments within ENTRYPOINT Instruction??  </p>

<p>My dockerfile has  </p>

<blockquote>
  <p>ARG $Version=3.1<br>
  ENTRYPOINT /tmp/folder-$Version/sample.sh start  </p>
</blockquote>

<p>I am getting an error while creating container with above dockerfile.
Please suggest what is the correct way to specify the argument within ENTRYPOINT instruction??</p>
","<p>I tried to pass an argument to my docker entry point , but it fails ,
these are steps i followed </p>

<pre><code>Docker Build Command : docker build -t ""DBDNS"" --build-arg  db=sample
</code></pre>

<p>In Dockerfile </p>

<pre><code>ARG db
ENV database ${db}
ENTRYPOINT [""/docker/entrypoint.sh"", ${db}]
</code></pre>

<p>Error for this
bash: 1: bash: [/var/www/html/.docker/entrypoint.sh,: not found</p>

<p>Actually file exists and passing an argument for entrypoint.sh causing issue.
Any clues for this </p>

<pre><code>-----------ENTRYPOINT---------------------
#!/usr/bin/env bash

echo ""Entrypoint stuff""
echo ""----------------""
echo ""NEW APP DB CLONE FROM  $1""
echo ""sites/files permission changes""
echo ""--------------------------------------""
</code></pre>
"
"41093812","How to get Docker containers to talk to each other while running on my local host?","<docker>","45481943","Connecting two docker containers","<docker><docker-compose><dockerfile><docker-swarm>","<p>I have a Webapp running completely locally on my MacBook.</p>

<p>The Webapp has a Front End (Angular/Javascript) and a Back End (Python/Django) which implements a RESTful API.</p>

<p>I have Dockerized the Back End so that it is completely self-contained in a Docker Container and exposes port 8000. I map this port locally to 4026.</p>

<p>Now I need to Dockerize the Front End. But if I have these two docker containers running on my localhost, how can I get the FE to send HTTP requests to the BE? The FE container won't know anything that exists outside of it. Right?</p>

<p>This is how I run the FE:</p>

<pre><code>$ http-server
Starting up http-server, serving ./
Available on:
  http://127.0.0.1:8080
  http://192.168.1.16:8080
Hit CTRL-C to stop the server
</code></pre>

<p>Please provide references explaining how I can achieve this.</p>
","<p>I have two existing docker container web and db. I want to link these two container, so that they will communicate with each other. If i go with --link command means it will link web to a new image and not to the db.</p>
"
"45271420","Docker: where is docker volume located for this compose file","<docker><docker-compose>","45381623","Where docker named volumes are stored?","<docker><docker-compose><dockerfile>","<p>I was setting up some materials for a trainning, when I came around this sample compose file:</p>

<p><a href=""https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml"" rel=""noreferrer"">https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml</a></p>

<p>and I couldn't find out how this volume is mounted, on lines 48 and 49 of the file:</p>

<pre><code>volumes:
  db-data:
</code></pre>

<p>Can someone explain me where is this volume on the host? Couldn't find it and I wouldn't like to keep any postgresql data dangling around after the containers are gone. Similar thing happens to the networks:</p>

<pre><code>networks:
  front-tier:
  back-tier:
</code></pre>

<p>Why docker compose accepts empty network definitions like this?</p>
","<p>I have this <code>docker-compose.yml</code> file</p>

<pre><code>version: '3'

volumes:
  jenkins_home:

services:

  registry:
     image: registry:2
     ports:
       - ""5000:5000""

  jenkins:
    image: jenkins/jenkins
    ports:
      - ""9090:8080""
    volumes:
      - jenkins_home:/var/jenkins_home
</code></pre>

<p>As you can see, there is a named volume called <code>jenkins_home</code>, now I wonder, where does the data get really persisted?</p>

<p>running <code>docker inspect infra_jenkins</code> i got this:</p>

<pre><code> ...
 ""Mounts"": [
    {
      ""Type"": ""volume"",
      ""Source"": ""infra_jenkins_home"",
      ""Target"": ""/var/jenkins_home"",
      ""VolumeOptions"": {
      ""Labels"": {
      ""com.docker.stack.namespace"": ""infra""
              }
          }
      }
],
...
</code></pre>

<p>I am running those services on a local docker swarm cluster using <code>docker stack deploy</code> command, the cluster is composed of three VirtualBox instances.</p>
"
"45682010","docker : invalid reference format","<docker>","63870803","How to build and run a Docker image?","<docker><machine-learning>","<p>I'm following this tutorial:
<a href=""https://medium.com/towards-data-science/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c"" rel=""noreferrer"">https://medium.com/towards-data-science/number-plate-detection-with-supervisely-and-tensorflow-part-1-e84c74d4382c</a></p>
<p>and they use docker. When I tried to run docker (inside the run.sh script):</p>
<pre><code>docker run \
    -p 8888:8888 
    -v `pwd`/../src:/src \
    -v `pwd`/../data:/data -w /src supervisely_anpr \
    --rm \
    -it \
    bash
</code></pre>
<p>I got the error:</p>
<pre><code>docker: invalid reference format.
</code></pre>
<p>I spent 2 hours and I can't really understand what's wrong. Any idea really appreciated.</p>
","<p>I'm <em>new</em> to Docker, and I would like to reproduce a project from a <a href=""https://github.com/Zhongdao/Towards-Realtime-MOT"" rel=""nofollow noreferrer"">https://github.com/Zhongdao/Towards-Realtime-MOT</a>.</p>
<p>In order to run the project examples, I need to build an image, so I first cloned the repository locally.</p>
<p>The documentation tells me to run the following commands:</p>
<pre><code>docker build -t towards-realtime-mot docker/

docker run --rm --gpus all -v $(pwd)/:/Towards-Realtime-MOT -ti towards-realtime-mot /bin/bash
cd /Towards-Realtime-MOT;
python demo.py --input-video path/to/your/input/video --weights path/to/model/weights
               --output-format video --output-root path/to/output/root
</code></pre>
<p>I'm running on macOS (without gpu support), and I've done:</p>
<pre><code>➜ docker build -t towards-realtime-mot docker/
➜ docker run --rm -v -ti towards-realtime-mot /bin/bash
➜ cd /Towards-Realtime-MOT
cd: no such file or directory: /Towards-Realtime-MOT
</code></pre>
<p>I've successfully built the docker image, but I'm stuck from the second command onwards. I know that I should create a folder called <code>Towards-Realtime-MOT</code>, but <code>$(pwd)</code> confuses me since it gives me erros (docker: invalid reference format. See 'docker run --help'.). What is the correct way?</p>
"
"45700361",".NET Core Docker Image for SPA Applications","<node.js><docker><asp.net-core><asp.net-core-mvc><.net-core>","45880460","Enable Docker support for Angular project","<angular><docker><asp.net-core><visual-studio-2017>","<p>What is the correct Docker image to use when creating a new ASP.NET Core MVC app, specifically with the React/Redux (or other Node.js required) template? If not a specific image, what commands or process should be followed in the Dockerfile for a Node.js app backed by ASP.NET Core MVC?</p>

<p>I don't require the SDK version of the framework for anything other than running the backing MVC site.</p>

<p><code>dotnet new reactredux</code></p>

<p>The runtime image does not have Node.js installed, and will error when trying to run the container.</p>

<p>Dockerfile:</p>

<pre><code>FROM microsoft/aspnetcore:latest

ARG source=./bin/Debug/netcoreapp2.0/publish/
WORKDIR /app
COPY $source .

EXPOSE 80
ENTRYPOINT [""dotnet"", ""Project.dll""]
</code></pre>

<p>Error:</p>

<pre><code>Unhandled Exception: System.AggregateException: One or more errors occurred. (Failed to start Node process. To resolve this:.

[1] Ensure that Node.js is installed and can be found in one of the PATH directories.
    Current PATH enviroment variable is: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    Make sure the Node executable is in one of those directories, or update your PATH.
</code></pre>

<p>The project I am working with is being upgraded from ASP.NET MVC for .NET Standard 1.1 (standalone), to a new .NET Standard 2.0 React/Redux project.</p>
","<p>Is there a way in Visual Studio 2017 to automatically enable Docker support for an asp.net-core <strong>Angular</strong> project? The option is disabled when creating a new one. It is only available for <em>Web Application (MVC)</em>.</p>

<p>I am able to Enable Docker support (Project->Add->Docker Support) for the angular project after I have created the project, but when I start the application I get an exception that node.js is not available.</p>

<blockquote>
  <p>System.AggregateException occurred   HResult=0x80131500   Message=One
  or more errors occurred. (Failed to start Node process. To resolve
  this:.</p>
  
  <p>[1] Ensure that Node.js is installed and can be found in one of the
  PATH directories.
      Current PATH enviroment variable is: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      Make sure the Node executable is in one of those directories, or update your PATH.</p>
  
  <p>[2] See the InnerException for further details of the cause.)<br>
  Source=   StackTrace:    at
  System.Threading.Tasks.Task`1.GetResultCore(Boolean
  waitCompletionNotification)    at
  Microsoft.AspNetCore.Builder.WebpackDevMiddleware.UseWebpackDevMiddleware(IApplicationBuilder
  appBuilder, WebpackDevMiddlewareOptions options)    at
  WebApplication3.Startup.Configure(IApplicationBuilder app,
  IHostingEnvironment env) in
  C:\Users\temp\Documents\Visual Studio
  2017\Projects\WebApplication3\Startup.cs:line 34</p>
  
  <p>Inner Exception 1: InvalidOperationException: Failed to start Node
  process. To resolve this:.</p>
  
  <p>[1] Ensure that Node.js is installed and can be found in one of the
  PATH directories.
      Current PATH enviroment variable is: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      Make sure the Node executable is in one of those directories, or update your PATH.</p>
  
  <p>[2] See the InnerException for further details of the cause.</p>
  
  <p>Inner Exception 2: Win32Exception: No such file or directory</p>
</blockquote>

<p>If it is not possible to create it automatically how can I manually get the Docker support enabled for my Angular project?</p>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","63883482","Docker build getting errors after I tried to install pip install -r requirements.txt file","<python><django><docker><dockerfile><docker-image>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>This is the error I get after I installed</p>
<p>RUN pip install -r requirements.txt</p>
<p>RROR: Service 'web' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
➜  django-docker pip install psycopg2
Defaulting to user installation because normal site-packages is not writeable
Collecting psycopg2
Using cached psycopg2-2.8.6.tar.gz (383 kB)
ERROR: Command errored out with exit status 1:
command: /usr/bin/python3.6 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-7xuxd9cm/psycopg2/setup.py'&quot;'&quot;'; <strong>file</strong>='&quot;'&quot;'/tmp/pip-install-7xuxd9cm/psycopg2/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(<strong>file</strong>);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, <strong>file</strong>, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-uon18vv4
cwd: /tmp/pip-install-7xuxd9cm/psycopg2/
Complete output (23 lines):
running egg_info
creating /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info
writing /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/PKG-INFO
writing dependency_links to /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/dependency_links.txt
writing top-level names to /tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/top_level.txt
writing manifest file '/tmp/pip-pip-egg-info-uon18vv4/psycopg2.egg-info/SOURCES.txt'</p>
<pre><code>Error: pg_config executable not found.

pg_config is required to build psycopg2 from source.  Please add the directory
containing pg_config to the $PATH or specify the full executable path with the
option:

    python setup.py build_ext --pg-config /path/to/pg_config build ...

or with the pg_config option in 'setup.cfg'.

If you prefer to avoid building psycopg2 from source, please install the PyPI
'psycopg2-binary' package instead.

For further information please check the 'doc/src/install.rst' file (also at
&lt;https://www.psycopg.org/docs/install.html&gt;).

----------------------------------------
</code></pre>
<p>ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.<a href=""https://i.stack.imgur.com/GkScl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GkScl.png"" alt=""enter image description here"" /></a></p>
"
"52570093","What is the practical purpose of VOLUME in Dockerfile?","<docker><dockerfile>","63845709","What is the actual advantage of declaring a VOLUME in a Dockerfile?","<docker><dockerfile><docker-volume>","<p>First of all, I want to make it clear I've done due diligence in researching this topic. Very closely related is <a href=""https://stackoverflow.com/questions/34809646/what-is-the-purpose-of-volume-in-dockerfile"">this SO question</a>, which doesn't really address my confusion.</p>

<p>I understand that when <code>VOLUME</code> is specified in a Dockerfile, this instructs Docker to create an unnamed volume for the duration of the container which is mapped to the specified directory inside of it. For example:</p>

<pre><code># Dockerfile
VOLUME [""/foo""]
</code></pre>

<p>This would create a volume to contain any data stored in <code>/foo</code> inside the container. The volume (when viewed via <code>docker volume ls</code>) would show up as a random jumble of numbers.</p>

<p>Each time you do <code>docker run</code>, this volume is not reused. This is the key point causing confusion here. To me, the goal of a volume is to contain state persistent across <em>all instances</em> of an image (all containers started from it). So basically if I do this, <em>without explicit volume mappings</em>:</p>

<pre><code>#!/usr/bin/env bash
# Run container for the first time
docker run -t foo

# Kill the container and re-run it again. Note that the previous 
# volume would now contain data because services running in `foo`
# would have written data to that volume.
docker container stop foo
docker container rm foo

# Run container a second time
docker run -t foo
</code></pre>

<p>I expect the unnamed volume to be reused between the 2 <code>run</code> commands. However, this is not the case. Because I did not explicitly map a volume via the <code>-v</code> option, a new volume is created for each <code>run</code>.</p>

<p>Here's important part number 2: Since I'm required to explicitly specify <code>-v</code> to share persistent state between <code>run</code> commands, why would I ever specify <code>VOLUME</code> in my Dockerfile? Without <code>VOLUME</code>, I can do this (using the previous example):</p>

<pre><code>#!/usr/bin/env bash
# Create a volume for state persistence
docker volume create foo_data

# Run container for the first time
docker run -t -v foo_data:/foo foo

# Kill the container and re-run it again. Note that the previous 
# volume would now contain data because services running in `foo`
# would have written data to that volume.
docker container stop foo
docker container rm foo

# Run container a second time
docker run -t -v foo_data:/foo foo
</code></pre>

<p>Now, truly, the second container will have data mounted to <code>/foo</code> that was there from the previous instance. I can do this without <code>VOLUME</code> in my Dockerfile. From the command line, I can turn any directory inside the container into a mount to either a bound directory on the host or a volume in Docker.</p>

<p>So my question is: What is the point of <code>VOLUME</code> when you have to explicitly map named volumes to containers via commands on the host anyway? Either I'm missing something or this is just confusing and obfuscated.</p>

<p>Note that all of my assertions here are based on my observations of how docker behaves, as well as what I've gathered from the documentation.</p>
","<p>What is the unique value of using the <code>VOLUME</code> keyword in a Dockerfile given that it's possible to mount volumes to images that don't use it?</p>
"
"53681522","Share variable in multi-stage Dockerfile: ARG before FROM not substituted","<docker><dockerfile><docker-multi-stage-build>","63957831","passing arguments to docker","<bash><docker><continuous-integration>","<p>I'm writing a multi-stage Dockerfile for the <a href=""https://www.mcs.anl.gov/research/projects/darshan/"" rel=""noreferrer"">darshan utils</a>:</p>

<pre><code>ARG DARSHAN_VER=3.1.6

FROM fedora:29 as build
RUN dnf install -y \
        gcc \
        make \
        bzip2 bzip2-devel zlib zlib-devel
RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz"" \
    &amp;&amp; tar ...


FROM fedora:29
COPY --from=build ""/usr/local/darshan-${DARSHAN_VER}"" ""/usr/local/darshan-${DARSHAN_VER}""
...
</code></pre>

<p>I build it with <code>docker build -t darshan-util:3.6.1 .</code> and the error I get is:</p>

<pre><code>Step 5/10 : RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...

 ---&gt; Running in 9943cce1669c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
...
curl: (78) RETR response: 550
The command '/bin/sh -c curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...' returned a non-zero code: 78
</code></pre>

<p>I'd like to reuse the same ARG in both stages, so that I can define a default build variable just once.
If I duplicate ARG in both stages, just below the two FROMs, it builds correctly.</p>

<p>What is the correct way to define a ""global"" multi-stage ARG variable with a default?</p>
","<p>I am trying to pass an argument to docker. For that, I use build-arg in my docker build command.</p>
<pre><code>echo &quot;[INFO] Hadoop version is ${HDP_VERSION}&quot;
docker build --no-cache=true --squash \
--build-arg HDP_VERSION=${HDP_VERSION} \
</code></pre>
<p>This is my docker</p>
<pre><code>ARG HDP_VERSION
FROM host:5000/runner-hadoop:${HDP_VERSION}
RUN echo &quot;HDP_VERSION=&quot;${HDP_VERSION}
COPY oozie/${HDP_VERSION} ${PATH_UNIX_PROJECT}
</code></pre>
<p>The first and second row execute without failing, but after that, I see that <code>HDP_VERSION</code> is actually empty. So at step 4, the wrong directory is taken.</p>
<p><a href=""https://i.stack.imgur.com/QZ4fb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QZ4fb.png"" alt=""enter image description here"" /></a></p>
<p>Why is that and how do you correct it?</p>
<p>EDIT</p>
<p>This is the result of echo</p>
<p><a href=""https://i.stack.imgur.com/HxSo5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HxSo5.png"" alt=""enter image description here"" /></a></p>
<p>This is what I get if I use</p>
<pre><code>--build-arg HDP_VERSION=1 \
</code></pre>
<p><a href=""https://i.stack.imgur.com/A4ATn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A4ATn.png"" alt=""enter image description here"" /></a></p>
<p>EDIT2</p>
<p>This is what happens if I use ENV. The result is the same.</p>
<pre><code>ENV HDP_VERSION=${HDP_VERSION}
RUN echo &quot;HDP_VERSION=&quot;${HDP_VERSION}
</code></pre>
<p><a href=""https://i.stack.imgur.com/Kpvwh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kpvwh.png"" alt=""enter image description here"" /></a></p>
"
"617182","How can I suppress all output from a command using Bash?","<bash><shell><scripting><echo>","55890945","Silence docker rm command in bash","<bash><docker><stdout><stderr><io-redirection>","<p>I have a Bash script that runs a program with parameters.  That program outputs some status (doing this, doing that...). There isn't any option for this program to be quiet. How can I prevent the script from displaying anything?</p>

<p>I am looking for something like Windows' <a href=""https://ss64.com/nt/echo.html"" rel=""noreferrer"">""echo off""</a>.</p>
","<p>I tried <code>docker container rm container-name &gt; /dev/null</code> but this still prints <code>Error: No such container: container-name</code>. I'm trying to silence this error in my bash script.</p>
"
"1819592","Warning: PDO::__construct(): [2002] No such file or directory (trying to connect via unix:///tmp/mysql.sock) in","<php><mysql><pdo>","56072027","Drupal docker image not able to connect to MySQL database","<mysql><docker><nginx><drupal>","<p>My code was working all fine yesterday and today it suddenly just don't want to connect to my database. I have changed no settings on it or on the code and I haven't updated any software either. All I do is this:</p>

<pre class=""lang-php prettyprint-override""><code>new PDO('mysql:host=localhost;port=3306;dbname=test', 'username', 'password');
</code></pre>

<p>And I get a nice exception message saying this:</p>

<blockquote>
  <p>Warning: PDO::__construct(): [2002] No such file or directory (trying to connect via unix:///tmp/mysql.sock) in ...</p>
</blockquote>

<p>The thing is: I'm clearly not trying to connect using a unix socket but using TCP/IP. What am I doing wrong? Is there something I'm missing here?</p>

<p>Thanks for any help.</p>
","<p>I am using docker on Ubuntu 18.04 having Drupal 8 and MySQL images. Also using Nginx as a web server (not an image in docker). Now during the installation of Drupal 8 on port 8080, it's not connecting to MySQL server. I am getting the error as:
<a href=""https://i.stack.imgur.com/PciZz.png"" rel=""nofollow noreferrer"">Error by drupal</a></p>

<p>I am entering the correct database username and password.
But I am not sure about the <a href=""https://i.stack.imgur.com/1dOKU.png"" rel=""nofollow noreferrer"">advanced settings</a>.</p>

<p>I have tried the host as my-ip:8080, 127.0.0.1:8080 but it didn't work.</p>
"
"15933493","pygame.error: No available video device","<python><pygame>","55799766","How to fix ""pygame.error: No available video device"" error related with pygame on docker?","<python-3.x><docker><pygame><dockerfile>","<p>I have this setup:</p>

<pre><code> $ python -V
Python 2.7.2+
 $ python -c ""import pygame; print pygame.__version__""
1.9.1release
</code></pre>

<p>When I run a pygame script, I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/santosh/tmp/pygameHelloWorld.py"", line 8, in &lt;module&gt;
    windowSurface = pygame.display.set_mode((500, 400), 0, 32)
pygame.error: No available video device
</code></pre>

<p>I am using Ubuntu and have install pygame with apt-get. Additionally I have install all dependencies mentioned on this <a href=""http://www.pygame.org/wiki/CompileUbuntu"" rel=""noreferrer"">pygame wiki page</a>.</p>
","<p>I'm trying to run an image with a pygame script, but i get the error:
<strong>pygame.error: No available video device</strong></p>

<p>The pygame script works good on my computer (Linux, Ubuntu 16.04, 32 bits, i installed docker using these steps: <a href=""https://stackoverflow.com/a/48881019/6796652"">https://stackoverflow.com/a/48881019/6796652</a>), the problem happens when i include it in the dockerfile image.</p>

<p>I read some posts related as this <a href=""https://stackoverflow.com/questions/32533585/pygame-projects-wont-run-pygame-error-no-available-video-device"">pygame projects won&#39;t run: &quot;pygame.error: No available video device&quot;</a>, but i couldn't find the same issue in a dockerfile.</p>

<p>This is a part of the pygame script:</p>

<pre><code>import pygame

def main():
    pygame.init()
    screen=pygame.display.set_mode((400,600))

    # Here are more code...
    # And finally

    pygame.quit()

main() 
</code></pre>

<p>I tried including this part, as this post says <a href=""https://stackoverflow.com/a/53623914/6796652"">https://stackoverflow.com/a/53623914/6796652</a></p>

<pre><code>from pygame.locals import *
</code></pre>

<p>This is my Dockerfile:</p>

<pre><code>FROM python:3
WORKDIR /myfolder
COPY main.py someimage.png wallpaper.jpg requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
CMD [ ""python"", ""./main.py"" ]
</code></pre>

<p>I tried also with an Ubuntu image, but the result was exactly the same:</p>

<pre><code>FROM ubuntu:14.04
WORKDIR /myfolder
COPY main.py someimage.png wallpaper.jpg requirements.txt ./
RUN apt-get update
RUN apt-get install python-pip python-dev build-essential -y
RUN pip install --upgrade pip 
RUN pip install --no-cache-dir -r requirements.txt
CMD [ ""python"", ""./main.py"" ]
</code></pre>

<p>Even, installing dependencies for Python 3.x as the documentation says: <a href=""http://www.pygame.org/wiki/CompileUbuntu"" rel=""nofollow noreferrer"">http://www.pygame.org/wiki/CompileUbuntu</a> the result was the same</p>

<pre><code>FROM python:3
WORKDIR /DefeatTheBat
COPY bat.png main.py wallpaper.jpg requirements.txt ./
RUN apt-get update &amp;&amp; apt-get install -y \
    python3-dev -y \ 
    python3-setuptools -y \ 
    python3-numpy -y \ 
    python3-opengl -y  \ 
    libsdl-image1.2-dev -y \ 
    libsdl-mixer1.2-dev -y \ 
    libsdl-ttf2.0-dev -y \ 
    libsmpeg-dev -y \ 
    libsdl1.2-dev -y \ 
    libportmidi-dev -y \ 
    libswscale-dev -y \ 
    libavformat-dev -y \ 
    libavcodec-dev -y \ 
    libtiff5-dev -y \ 
    libx11-6 -y \ 
    libx11-dev -y \ 
    fluid-soundfont-gm -y \ 
    timgm6mb-soundfont -y \ 
    xfonts-base -y \ 
    xfonts-100dpi -y \ 
    xfonts-75dpi -y \ 
    xfonts-cyrillic -y \ 
    fontconfig -y \ 
    fonts-freefont-ttf -y \ 
    libfreetype6-dev -y
RUN pip install --no-cache-dir -r requirements.txt
CMD [ ""python"", ""./main.py"" ]
</code></pre>

<p>And the requirements.txt:</p>

<pre><code>pygame==1.9.5
</code></pre>

<p>I'm getting this error:</p>

<pre><code>    screen=pygame.display.set_mode((400,600)) 
pygame.error: No available video device
</code></pre>

<p>When it should display the windows. If i try that part in my interactive python shell, it works good.</p>

<p><strong>How can i fix this dockerfile image?</strong></p>
"
"18012930","How can I redirect all output to /dev/null?","<bash><pipe><output-redirect><dev-null>","56224989","How to hide Docker-compose messages to stderr?","<bash><docker><docker-compose>","<p>I want to run a program (<code>google-chrome</code>) in the background, but prevent it from outputting any messages to the terminal.</p>

<p>I tried doing this:</p>

<pre><code>google-chrome 2&gt;&amp;1 1&gt;/dev/null &amp;
</code></pre>

<p>However, the terminal still fills up without messages like:</p>

<blockquote>
  <p>[5746:5746:0802/100534:ERROR:object_proxy.cc(532)] Failed to call method: org.chromium.Mtpd.EnumerateStorag...</p>
</blockquote>

<p>What am I doing wrong? How do I redirect <strong>all</strong> the output to <code>/dev/null</code>?</p>
","<p>I have a very simple script that I use to wait for a process to complete. The script is used preceding calls to tasks, commands that depend on the process completion. If the process is ready the desired command will run. The format is:</p>

<pre><code>await_script_while_condition_not_met &amp;&amp; cmd2
</code></pre>

<p>Which means that <code>await_script_while_condition_not_met</code> will loop until the condition is met and only after the cmd2 runs.</p>

<p>Docker-compose appears to output messages to stderr. In Bash <code>&gt; /dev/null</code> redirects stdout and not stdeer. To redirect both to <code>/dev/null</code>, I have to redirect stderr to stdout.</p>

<pre><code>#!/usr/bin/env bash

is_npm_install_running () {
    docker-compose run node test -d node_modules/.staging 2&gt;&amp;1 &gt; /dev/null
}

main () {
    while is_npm_install_running; do
        sleep 30
    done
}

main &amp; PID=$!

echo ''
echo ''
echo 'This may take a while, please be patient!'
echo 'Npm install is still running...'
echo ''

while kill -0 $PID 2&gt; /dev/null; do 
    printf  ""▓""
    sleep 2
done

echo 'Done!'
</code></pre>

<p>The <code>2&gt;&amp;1 &gt; /dev/null</code> does not prevent Docker-compose to output the initialization messages you can see in the image screenshot bellow <code>Starting docker_XXXX...done!, etc</code>:</p>

<p><a href=""https://i.stack.imgur.com/6vOjJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6vOjJ.png"" alt=""enter image description here""></a></p>

<p>If the process is long there'll be a big number of repeated messages that are not necessary and I'd like to hide. The script I have is quite simple and it's only used to <code>await</code> for the process to finish before allowing the user to proceed to run certain services, tasks or commands: a nice to have, but not absolutely necessary! A sort of prettifier.</p>

<p><a href=""https://www.youtube.com/watch?v=GI8Jy-qqFwI"" rel=""nofollow noreferrer"">This video demo</a> demonstrates the script in action and working for the case <code>&gt; /dev/null</code> (have in mind the messages I want to hide are present and keep increasing for each iteration), the case for the source code above <code>2&gt;&amp;1 &gt; /dev/null</code> works similarly.</p>

<h2>Edit</h2>

<p>This is marked as duplicate but the answers present in the suggested posts would not work as exposed in the current message; also <a href=""https://mywiki.wooledge.org/BashFAQ/055"" rel=""nofollow noreferrer"">as documented here</a>.</p>
"
"19104847","How to generate a Dockerfile from an image?","<image><repository><docker>","55954188","If I have locally docker image. How Could I list what is installed on it?","<image><docker>","<p>Is it possible to generate a Dockerfile from an image?  I want to know for two reasons:</p>

<ol>
<li><p>I can download images from the repository but would like to see the recipe that generated them.</p></li>
<li><p>I like the idea of saving snapshots, but once I am done it would be nice to have a structured format to review what was done.</p></li>
</ol>
","<p>For instance I have this image locally <a href=""https://github.com/topflight-technology/docker-images/tree/master/go-testing"" rel=""nofollow noreferrer"">https://github.com/topflight-technology/docker-images/tree/master/go-testing</a>. Then system lost all info about it. So how we could recreate the image docker file? Or at list get the list what tools are installed. </p>

<pre><code>docker ps -a
</code></pre>

<pre><code>CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS                      PORTS               NAMES
f047b64e79d1        topflighttech/go-testing   ""/bin/sh""           28 seconds ago      Exited (0) 26 seconds ago                       friendly_leakey
</code></pre>

<p>So I need to investigate docker container or strait forward image?</p>
"
"21738944","How to close a SQLAlchemy session?","<python><session><sqlalchemy>","56152160","SQLAlchemy connection dropping","<python><docker><sqlalchemy>","<p>Following what we commented in <a href=""https://stackoverflow.com/questions/8645250/how-to-close-sqlalchemy-connection-in-mysql/8705750?noredirect=1#comment32872652_8705750"">How to close sqlalchemy connection in MySQL</a>, I am checking the connections that SQLAlchemy creates into my database and I cannot manage to close them without exiting from Python.</p>

<p>If I run this code in a python console, it keeps the session opened until I exit from python:</p>

<pre><code>from sqlalchemy.orm import sessionmaker
from models import OneTable, get_engine

engine = get_engine(database=""mydb"")
session = sessionmaker(bind=engine)()

results = session.query(OneTable.company_name).all()

# some work with the data #

session.close()
</code></pre>

<p>and the only workaround I found to close it is to call <code>engine.dispose()</code> at the end.</p>

<p>As per the comments in the link I gave above, my question are now:</p>

<ul>
<li>Why is <code>engine.dispose()</code> necessary to close sessions?</li>
<li>Doesn't <code>session.close()</code> suffice?</li>
</ul>
","<p>I wonder if someone had this issue before.</p>

<p>I have an application running with Python 3.7.3, SqlAlchemy and Falcon running in a Docker container.</p>

<p>My database is MariaDB also running in a docker container.</p>

<p>I setup SqlAlchemy <code>pool_recycle</code> to 3600 but it still drops connection from time to time.</p>

<p>I can't see anything in the logs but the SQLAlchemy log stating connection has been lost after trying to perform some query.</p>

<p>Where could be the issue? SQLAlchemy configuration? MariaDB? Maybe Docker?
How can I inspect that?</p>

<p>My repository:</p>

<pre><code>class NfeRepositorio:

    def __init__(self, db_session: sessionmaker):
        self.session_factory = scoped_session(db_session)
        self._session = None

    def __enter__(self):
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        self.session_factory.remove()

    @property
    def session(self):
        if not self._session:x
            self._session = self.session_factory()
        return self._session
</code></pre>

<p>My engine setup:</p>

<pre><code>orm_engine = create_engine(
    get_config().ORM_ENGINE_TEMPLATE.format(
        get_config().DB_PROTOCOL,
        get_config().DB_USERNAME,
        get_config().DB_PASSWORD,
        get_config().DB_HOST,
        get_config().DB_NAME
    ),
    echo=get_config().ORM_ECHO,
    pool_recycle=3600
)
</code></pre>

<p><strong>Edit 1</strong>:
Just found a log entry and did some research on Google. Several people having the same issue. 
<code>[Warning] Aborted connection to db (Got timeout reading communication packets)</code></p>

<p>Looks like the issue relates with the difference between SqlAlchemy session and MySql session. The SO post below explains better.
<a href=""https://stackoverflow.com/questions/21738944/how-to-close-a-sqlalchemy-session"">How to close a SQLAlchemy session?</a></p>

<p>I'll give it a try and post the results.</p>
"
"23439126","How to mount a host directory in a Docker container","<docker><mount><boot2docker>","55951217","Accessing file from host system in Docker","<docker><file>","<p>I am trying to mount a host directory into a Docker container so that any updates done on the host is reflected into the Docker containers.</p>

<p>Where am I doing something wrong. Here is what I did:</p>

<pre><code>kishore$ cat Dockerfile

FROM ubuntu:trusty
RUN apt-get update
RUN apt-get -y install git curl vim
CMD [""/bin/bash""]
WORKDIR /test_container
VOLUME [""/test_container""]
</code></pre>

<p><pre><code>kishore$ tree
.
├── Dockerfile
└── main_folder
    ├── tfile1.txt
    ├── tfile2.txt
    ├── tfile3.txt
    └── tfile4.txt</p>

<p>1 directory, 5 files
kishore$ pwd
/Users/kishore/tdock
</code></pre><pre><code>kishore$ docker build --tag=k3_s3:latest .</p>

<pre><code>Uploading context 7.168 kB
Uploading context
Step 0 : FROM ubuntu:trusty
 ---&gt; 99ec81b80c55
Step 1 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 1c7282005040
Step 2 : RUN apt-get -y install git curl vim
 ---&gt; Using cache
 ---&gt; aed48634e300
Step 3 : CMD [""/bin/bash""]
 ---&gt; Running in d081b576878d
 ---&gt; 65db8df48595
Step 4 : WORKDIR /test_container
 ---&gt; Running in 5b8d2ccd719d
 ---&gt; 250369b30e1f
Step 5 : VOLUME [""/test_container""]
 ---&gt; Running in 72ca332d9809
 ---&gt; 163deb2b1bc5
Successfully built 163deb2b1bc5
Removing intermediate container b8bfcb071441
Removing intermediate container d081b576878d
Removing intermediate container 5b8d2ccd719d
Removing intermediate container 72ca332d9809
</code></pre>

<p>kishore$ docker run -d -v /Users/kishore/main_folder:/test_container k3_s3:latest
<code>c9f9a7e09c54ee1c2cc966f15c963b4af320b5203b8c46689033c1ab8872a0ea</code></code></pre><pre><code>kishore$ docker run -i -t k3_s3:latest /bin/bash</p>

<pre><code>root@0f17e2313a46:/test_container# ls -al
total 8
drwx------  2 root root 4096 Apr 29 05:15 .
drwxr-xr-x 66 root root 4096 Apr 29 05:15 ..
</code></pre>

<p>root@0f17e2313a46:/test_container# exit
exit</code></pre><pre><code>kishore$ docker -v
Docker version 0.9.1, build 867b2a9</code></pre></p>

<ul>
<li>I don't know how to check boot2docker version</li>
</ul>

<p>Questions, issues facing:</p>

<ol>
<li>How do I need to link the main_folder to the test_container folder present inside the docker container?</li>
<li>I need to make this automatically. How do I to do that without really using the <code>run -d -v</code> command?</li>
<li>What happens if the boot2docker crashes? Where are the Docker files stored (apart from Dockerfile)?</li>
</ol>
","<p>I want to use graph-tools to manage some data, however, i am unable to install the package on my ubuntu 18.04.</p>

<p>So i downloaded the docker file from the official site, and i was wondering how to access my .csv files on the host system from docker.</p>

<p>Thank you</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","55721573","Is there a way to seamlessly transfer docker-compose containers to another host without building?","<docker><docker-compose>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>I'm setting up a ci/cd solution. I would like to run a docker application on a production machine that has no access to the internet. </p>

<p>The constraints are as follows:</p>

<p><code>Build needs to happen on machine A</code></p>

<p><code>Resulting image/container needs to be exported and transported to machine B</code></p>

<p>Optionally: <code>Run the container again with a docker-compose file</code></p>

<p>I know about docker commit and repos, but this is sadly not an option, as the resulting server does not have access to the internet.</p>

<p>Here's the docker-compose.yaml; this is not set in stone and can change however necessary</p>

<pre><code>version: '2'
services:
  test_dev_app:
    image: testdevapp:latest
    container_name: test_dev_app
    hostname: test_dev_app
    environment:
      DJANGO_SETTINGS_MODULE: ""settings.production""
      APPLICATION_RUN_TYPE: ""uwsgi""
    volumes:
      - ./:/data/application
    ports:
      - ""8000:8000""
      - ""8080:8080""
</code></pre>

<p>I'd expect to be able to properly transport a container or image and use the same image on a different machine with <code>docker-compose up</code></p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","56433222","Connecting docker container to mongodb on localhost","<mongodb><docker><docker-compose><pymongo><airflow>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I'm trying to use Airflow to schedule web scraping tasks, dumping the results to MongoDB on my local machine. I am using the puckel/docker-airflow image, modifying it to include MongoDB as an additional service. </p>

<p>I have tried various solutions posted on here, including:</p>

<ol>
<li>Use links/networks;</li>
<li>Use the mongo container name with MongoClient;</li>
<li>Set the mongod.conf bind_ip to 0.0.0.0 and map the mongo container to port 172.17.0.1;</li>
</ol>

<p>but I still face the same problem. </p>

<p>I'm doing something wrong but I'm not entirely sure what it is. </p>

<p>Here is the modified docker-compose file:</p>

<pre><code>version: '3'                                                                      
services:                                                                                 
    postgres:                                                                     
        image: postgres:9.6                                                       
        environment:                                                              
            - POSTGRES_USER=airflow                                               
            - POSTGRES_PASSWORD=airflow                                           
            - POSTGRES_DB=airflow                                                 
        # link to common network                                                                               
        networks:                                                                 
          - app_tier                                                              

    # Custom mongo db                                                             
    mongo:                                                                                                     
        image: mongo:3.6.3                                                                                     
        restart: always                                                                                        
        volumes:                                                                                               
            - /data/db:/data/db                                                                                
        ports:                                                                                                 
            - ""172.17.0.1:27017:27017""                                                                           
        networks:                                                                                              
            - app_tier                                                                                         

    webserver:                                                                                                 
        image: puckel/docker-airflow:1.10.2                                                                    
        restart: always                                                                                        
        depends_on:                                                                                            
            - postgres                                                                                         
        environment:                                                                                           
            - LOAD_EX=n                                                                                        
            - EXECUTOR=Local                                                                                   
        volumes:                                                                                               
            - ./dags:/usr/local/airflow/dags                                                                   
            # Uncomment to include custom plugins                               
            # - ./plugins:/usr/local/airflow/plugins                            
            # Custom python package                                                                            
            - ./requirements.txt:/requirements.txt                              
            # FIFA file path                                                                                   
            - ~/FIFA:/FIFA                                                                                     
            # Mongo DB path                                                                                    
            - /data/db:/data/db                                                                                
        # link to common network                                                                               
        networks:                                                                                              
            - app_tier                                                                                         
        ports:                                                                                                 
            - ""8080:8080""                                                                                      
        command: webserver                                                                                     
        healthcheck:                                                                                           
            test: [""CMD-SHELL"", ""[ -f /usr/local/airflow/airflow-webserver.pid ]""]
            interval: 30s                                                                                      
            timeout: 30s                                                                                       
            retries: 3                                                                                         

networks:                                                                                                      
  app_tier:                                                                                                    
    driver: bridge 
</code></pre>

<p>I am using mongodb://mongo:27017 to connect to MongoDB. </p>

<p>In my logs, I get the following error:</p>

<p>pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused</p>

<p>Any idea what I am doing wrong? </p>

<p>TIA!</p>

<p><strong>NOTE:</strong> I have looked at the answers in this section: </p>

<p><a href=""https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach"">From inside of a Docker container, how do I connect to the localhost of the machine?</a> </p>

<p>but I am having difficulty implementing it in a docker-compose file. </p>

<p>Running the individual containers is challenging, as the entrypoint.sh script for the puckel/docker-airflow image depends on postgres running (and I don't know how to make it run the same way on my local machine). Even so, running each service individually is a bit tedious. I tried running a personal python image and successfully dumped the results from the container into my local machine, but I don't know how to do the same with the puckel/docker-airflow image, hence I am stuck. </p>

<p>Is there a solution for this but with docker-compose?</p>

<p><strong>EDIT:</strong> It seems like docker can read from my local machine, but cannot write to it. If I have mongod running on my local machine, I get logs indicating that a connection has been made to my docker container, and that data was sent to it:</p>

<pre><code>2019-06-04T15:51:34.299-0400 I NETWORK  [listener] connection accepted from 172.23.0.3:48768 #8 (8 connections now open)
2019-06-04T15:51:34.299-0400 I NETWORK  [conn8] received client metadata from 172.23.0.3:48768 conn: { driver: { name: ""PyMongo"", version: ""3.8.0"" }, os: { type: ""Linux"", na    me: ""Linux"", architecture: ""x86_64"", version: ""4.15.0-48-generic"" }, platform: ""CPython 3.6.8.final.0"" }
2019-06-04T15:51:34.550-0400 I COMMAND  [conn8] command agents_proxies.user_agents command: getMore { getMore: 20847821675, collection: ""user_agents"", lsid: { id: UUID(""69b1    fd25-36f8-49a4-8a14-bafc83483abb"") }, $db: ""agents_proxies"", $readPreference: { mode: ""primary"" } } originatingCommand: { find: ""user_agents"", filter: { $and: [ { $or: [ { O    S: ""Windows"" }, { OS: ""Mac OS X"" }, { OS: ""macOS"" }, { OS: ""Linux"" } ] }, { $or: [ { hardware_type: ""Computer"" }, { hardware_type: ""Windows"" }, { hardware_type: ""Linux"" }, {     hardware_type: ""Mac"" } ] }, { $or: [ { popularity: ""Very common"" }, { popularity: ""Common"" } ] } ] }, projection: { _id: 0, user_agent: 1 }, lsid: { id: UUID(""69b1fd25-36f8    -49a4-8a14-bafc83483abb"") }, $db: ""agents_proxies"", $readPreference: { mode: ""primaryPreferred"" } } planSummary: COLLSCAN cursorid:20847821675 keysExamined:0 docsExamined:10    9441 cursorExhausted:1 numYields:855 nreturned:1188 reslen:163454 locks:{ Global: { acquireCount: { r: 1712 } }, Database: { acquireCount: { r: 856 } }, Collection: { acquir    eCount: { r: 856 } } } protocol:op_msg 248ms
</code></pre>

<p>However, when my python script attempts to store the data I receive the connection refused message from pymongo. I'm starting to think this has something to do with the airflow Dockerfile or the entrypoint.sh script.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","56455611","Connect between client docker and postgres server on host","<postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a postgres server on my host machine and I want to make a docker container that connects to this postgres server.</p>

<p>So I guess I need to expose the postgres server on a connection IP:5432 to docker. Expose 5432 on the docker and specify the correct connection information inside the docker something like:</p>

<pre><code>SQLALCHEMY_DATABASE_URI = ""postgresql+psycopg2://username:password@IP/db_name""
</code></pre>

<p>The host docker IP's are:</p>

<pre><code>docker0   Link encap:Ethernet  HWaddr 02:42:b3:d9:eb:e2  
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0
          inet6 addr: fe80::42:b3ff:fed9:ebe2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:213512 errors:0 dropped:0 overruns:0 frame:0
          TX packets:351284 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:9157933 (9.1 MB)  TX bytes:826914241 (826.9 MB)

docker_gwbridge Link encap:Ethernet  HWaddr 02:42:5c:b9:3b:0a  
          inet addr:172.18.0.1  Bcast:172.18.255.255  Mask:255.255.0.0
          inet6 addr: fe80::42:5cff:feb9:3b0a/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:436 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:64397 (64.3 KB)
</code></pre>

<p>What am I missing and how does I expose the postgres server to the relavant ports both on host side. </p>
"
"25981703","pip install fails with ""connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)""","<python><windows><ssl><pip>","56131677","RUN pip install: There was a problem confirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed","<python><docker><ssl>","<p>I am very new to Python and trying to <code>&gt; pip install linkchecker</code> on Windows 7. Some notes:</p>

<ul>
<li>pip install is failing no matter the package. For example, <code>&gt; pip install scrapy</code> also results in the SSL error.</li>
<li>Vanilla install of Python 3.4.1 included pip 1.5.6. The first thing I tried to do was install linkchecker. Python 2.7 was already installed, it came with ArcGIS. <code>python</code> and <code>pip</code> were not available from the command line until I installed 3.4.1.</li>
<li><code>&gt; pip search linkchecker</code> works. Perhaps that is because pip search does not verify the site's SSL certificate.</li>
<li>I am in a company network but we do not go through a proxy to reach the Internet.</li>
<li>Each company computer (including mine) has a Trusted Root Certificate Authority that is used for various reasons including enabling monitoring TLS traffic to <a href=""https://google.com"">https://google.com</a>. Not sure if that has anything to do with it.</li>
</ul>

<p>Here are the contents of my <strong>pip.log</strong> after running <code>pip install linkchecker</code>:</p>

<pre><code>Downloading/unpacking linkchecker
  Getting page https://pypi.python.org/simple/linkchecker/
  Could not fetch URL https://pypi.python.org/simple/linkchecker/: connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)
  Will skip URL https://pypi.python.org/simple/linkchecker/ when looking for download links for linkchecker
  Getting page https://pypi.python.org/simple/
  Could not fetch URL https://pypi.python.org/simple/: connection error: HTTPSConnectionPool(host='pypi.python.org', port=443): Max retries exceeded with url: /simple/ (Caused by &lt;class 'http.client.CannotSendRequest'&gt;: Request-sent)
  Will skip URL https://pypi.python.org/simple/ when looking for download links for linkchecker
  Cannot fetch index base URL https://pypi.python.org/simple/
  URLs to search for versions for linkchecker:
  * https://pypi.python.org/simple/linkchecker/
  Getting page https://pypi.python.org/simple/linkchecker/
  Could not fetch URL https://pypi.python.org/simple/linkchecker/: connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)
  Will skip URL https://pypi.python.org/simple/linkchecker/ when looking for download links for linkchecker
  Could not find any downloads that satisfy the requirement linkchecker
Cleaning up...
  Removing temporary dir C:\Users\jcook\AppData\Local\Temp\pip_build_jcook...
No distributions at all found for linkchecker
Exception information:
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\pip\basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""C:\Python34\lib\site-packages\pip\commands\install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""C:\Python34\lib\site-packages\pip\req.py"", line 1177, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""C:\Python34\lib\site-packages\pip\index.py"", line 277, in find_requirement
    raise DistributionNotFound('No distributions at all found for %s' % req)
pip.exceptions.DistributionNotFound: No distributions at all found for linkchecker
</code></pre>
","<p>Following the lab from [GitHub][1] to learn more about Docker containers, I felt in this problem:</p>

<pre><code>No matching distribution found for Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1))
  Could not fetch URL https://pypi.python.org/simple/flask/: There was a problem confirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:726) - skipping```


  [1]: https://github.com/docker/labs/blob/master/beginner/chapters/webapps.md
</code></pre>
"
"26153686","How do I run a command on an already existing Docker container?","<docker>","56328857","Updating rails while working with Docker?","<ruby-on-rails><docker><docker-compose>","<p>I created a container with <code>-d</code> so it's not interactive.</p>

<pre><code>docker run -d shykes/pybuilder bin/bash
</code></pre>

<p>I see that the container has exited:</p>

<pre><code>CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS                      PORTS               NAMES
d6c45e8cc5f0        shykes/pybuilder:latest   ""bin/bash""          41 minutes ago      Exited (0) 2 seconds ago                        clever_bardeen
</code></pre>

<p>Now I would like to run occasional commands on the machine and exit. Just to get the response.</p>

<p>I tried to start the machine. I tried attaching. I thought I could call <code>run</code> with a container, but that does not seem to be allowed. Using <code>start</code> just seems to run and then exist quickly.</p>

<p>I'd like to get back into interactive mode after exiting.</p>

<p>I tried:</p>

<pre><code>docker attach d6c45e8cc5f0
</code></pre>

<p>But I get:</p>

<pre><code>2014/10/01 22:33:34 You cannot attach to a stopped container, start it first
</code></pre>

<p>But if I start it, it exits anyway. Catch 22. I can't win.</p>
","<p>I'm trying to update some gems including <code>rails</code>.</p>

<p>How can a run <code>bundle install</code> while working with <code>Docker</code>.</p>

<p>My Docker files looks like this:</p>

<pre><code> FROM ruby:2.4.1
 RUN apt-get update -qq &amp;&amp; apt-get install -y build-essential libpq-dev nodejs
 RUN mkdir /myapp
 WORKDIR /myapp
 ADD Gemfile /myapp/Gemfile
 ADD Gemfile.lock /myapp/Gemfile.lock
 RUN gem install bundler --no-ri --no-rdoc &amp;&amp; bundle install
 ADD . /myapp
</code></pre>
"
"26608117","ngrok not working correctly to test HTTPs","<laravel><ngrok>","55708124","Facebook setting get wrong request uri","<php><symfony><docker><facebook-login><ngrok>","<p>I downloaded ngrok so i can test my site for http and https requests (if someone is trying to get in my site specific url and it will be a simple http request, i will deny it),</p>

<p>first, my localhost is working in 8080 port</p>

<p>I start ngrok, it gives me the following:</p>

<p><img src=""https://i.stack.imgur.com/G9Afg.png"" alt=""enter image description here""></p>

<p>both at the same port, it's a problem i think, because if i do such simple route configuration in laravel:</p>

<pre><code>Route::filter('force.ssl', function()
{
    if( ! Request::secure())
    {
        return 'unsecured';
    }

});
</code></pre>

<p>and i have this route:</p>

<pre><code>Route::get('survey/payment/secured', array('before' =&gt; 'force.ssl', function(){
   return 'secured!';
}));
</code></pre>

<p>and i do the following request:</p>

<p><a href=""https://75fdaa96.ngrok.com/survey/payment/secured"" rel=""nofollow noreferrer"">https://75fdaa96.ngrok.com/survey/payment/secured</a></p>

<p>it thinks it unsecured and returns 'unsecured', how can i fix this?</p>
","<p>I have installed docker on my machine + symfony 4. I try to implement facebook login and for that I installed ngrok because I have some problems with https, so ngrok generated this link : <a href=""https://f7f8d7d3.ngrok.io"" rel=""nofollow noreferrer"">https://f7f8d7d3.ngrok.io</a>. I put in facebook developper app this url </p>

<p>When I push on button Login with facebook I get the error : 
Insecure Login Blocked: You can’t get an access token or log in to this app from an insecure page. Try re-loading the page as https://</p>

<p>But I saw in url that the request_uri is the old host : 
<a href=""https://www.facebook.com/v2.8/dialog/oauth?response_type=code&amp;client_id=634818730276116&amp;state=73109d9bebdfd1670ccfff86702e001c&amp;redirect_uri=http%3A%2F%project.loc%2Fsecured%2Flogin_facebook&amp;display=popup&amp;auth_type=rerequest"" rel=""nofollow noreferrer"">https://www.facebook.com/v2.8/dialog/oauth?response_type=code&amp;client_id=634818730276116&amp;state=73109d9bebdfd1670ccfff86702e001c&amp;redirect_uri=http%3A%2F%project.loc%2Fsecured%2Flogin_facebook&amp;display=popup&amp;auth_type=rerequest</a></p>

<p>redirect_uri=http%3A%2F%project.loc</p>

<p>I don't understand why.</p>

<p>My implimentation</p>


        <li>
            
                Connect with Facebook
            
        </li>
        <li>
            
                Connect with Google
            
        </li>
    

<pre><code>&lt;/form&gt;

&lt;div id=""fb-root""&gt;&lt;/div&gt;
&lt;script
        src=""https://code.jquery.com/jquery-3.2.1.min.js""
        integrity=""sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=""
        crossorigin=""anonymous""&gt;&lt;/script&gt;
&lt;script&gt;
    window.fbAsyncInit = function() {
        // init the FB JS SDK
        FB.init({
            appId      : '634818730276116',                        // App ID from the app dashboard
            channelUrl : '//mysite.com/channel.html',      // Channel file for x-domain comms
            status     : true,                                 // Check Facebook Login status
            xfbml      : true                                  // Look for social plugins on the page
        });
    };

    // Load the SDK asynchronously
    (function(d, s, id){
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) {return;}
        js = d.createElement(s); js.id = id;
        js.src = ""//connect.facebook.net/en_US/all.js"";
        fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));

    function fb_login() {
        console.log(""Facebook Login"");
        FB.getLoginStatus(function(response) {
            if (response.status === 'connected') {
                // connected
                document.location = ""{{ url(""hwi_oauth_service_redirect"", {service: ""facebook""}) }}"";
            } else {
                // not_authorized
                FB.login(function(response) {
                    if (response.authResponse) {
                        document.location = ""{{ url(""hwi_oauth_service_redirect"", {service: ""facebook""}) }}"";
                    }
                }, {scope: 'email'});
            }
        });
    }
&lt;/script&gt;
</code></pre>

<p>So the idea is how to change this request_uri, and why is the old one ? I will appreciate your help. Thx in advance and sorry for my english. No one idea how to solve that ? :(</p>
"
"26711103","Docker changing /var/lib/docker/aufs/diff location","<docker><diskspace>","56125057","Is there any way to move the docker running target disk to a specific disk","<docker><docker-compose>","<p>Docker folder  /var/lib/docker/aufs/diff grows too much and I would like to move it on an other partition.
Is there a way to configure Docker to use another location for this foder?</p>
","<p>I'm setting up my own project on a server using docker. OS have installed on the hard-disk and docker too and because of that i get into problem to use disk. I read/write with a high speed on the disk so hard disk can't provide what i need. I plugged SSD to my server and moved the swap and ramdisk location to the SSD device.</p>

<p>Docker: latest
CentOS: 7</p>

<p>Now i need to run docker on the SSD device, not on hard-disk.</p>

<p>Is there any way to change move it to the SSD device to increase the read/write speed?</p>
"
"33054369","How to change the default docker registry from docker.io to my private registry?","<docker><docker-registry>","56289208","Docker registry : how to avoid hostname and directory name when I pull or push docker image from local/3rd party docker registry","<docker><dockerfile>","<p>By default, if I issue command: </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from the docker.io offical site by default.</p>

<pre><code>Pulling repository docker.io/library/ruby
</code></pre>

<p>How do I change it to my private registry. That means if I issue </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from my own private registry, the output is something like:</p>

<pre><code>Pulling repository my_private.registry:port/library/ruby
</code></pre>
","<p>I have configured my local docker registry using nexus. But I had to add docker registry URL into .docker/daemon.json insecure registry like below:</p>

<pre><code>{
  ""registry-mirrors"": [],
  ""insecure-registries"": [""xx.xxxxx.xxxx.xxxx:8080""],
  ""debug"": true,
  ""experimental"": false
} 
</code></pre>

<p>But when I need to mention complete url  to pull or push image like below:</p>

<pre><code>docker image push hostname/[new_directory]/[image_name]: [tag] 

docker push 11.111.11.1234:8012/dockerrepository/imagename:latest
</code></pre>

<p>So how to avoid hostname and directory name from here?</p>

<p>for example:  </p>

<pre><code>docker push imagename:latest
</code></pre>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","56472886","Docker endpoint other compose file","<docker><docker-compose><endpoint>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I've two docker-compose files, one contains the solution with multiple API's the other contains some background running programs.</p>

<p>The problem is that one background program uses the API that runs in the other compose file.
I was hoping I could use  cotnainername as <a href=""http://[CONTAINERNAME]:80"" rel=""nofollow noreferrer"">http://[CONTAINERNAME]:80</a> but that isn't working, the url is not known. I think it is because it's not known in that compose file? What is the solution for this case?</p>

<p>Below the simplified (removed the other services) docker-compose.yml's</p>

<p>file1:</p>

<pre><code>version: '3.3'
services:
    firstapi:
        image: firstapi:latest
        container_name: firstapi
        ports:
            - ""4005:80""
</code></pre>

<p>file2:</p>

<pre><code>version: '3.3'
services:
    firsttool:
        image: firsttool:latest
        container_name: firsttool
        environment:
            - Endpoint_FirstAPI=http://firstapi:80
</code></pre>
"
"38438933","How to make a build arg mandatory during Docker build?","<docker>","56029559","Fail if --build-arg is not passed","<docker><docker-build>","<p>Is there any way to make a build argument mandatory during <code>docker build</code>? The expected behaviour would be for the build to fail if the argument is missing.</p>

<p>For example, for the following Dockerfile:</p>

<pre><code>FROM ubuntu

ARG MY_VARIABLE
ENV MY_VARIABLE $MY_VARIABLE

RUN ...
</code></pre>

<p>I would like the build to fail at <code>ARG MY_VARIABLE</code> when built with <code>docker build -t my-tag .</code> and pass when built with <code>docker build -t my-tag --build-arg MY_VARIABLE=my_value .</code>.</p>

<p>Is there any way to achieve that behaviour? Setting a default value doesn't really do the trick in my case. </p>

<p>(I'm running Docker <code>1.11.1</code> on <code>darwin/amd64</code>.)</p>

<p><strong>EDIT</strong>:
One way of doing that I can think of is to run a command that fails when <code>MY_VARIABLE</code> is empty, e.g.:</p>

<pre><code>FROM ubuntu

ARG MY_VARIABLE
RUN test -n ""$MY_VARIABLE""
ENV MY_VARIABLE $MY_VARIABLE

RUN ...
</code></pre>

<p>but it doesn't seem to be a very idiomatic solution to the problem at hand.</p>
","<p>I have this in a Dockerfile:</p>

<pre><code>ARG aws_access_key_id
ARG aws_secret_access_key
</code></pre>

<p>even though I did not use any --build-arg options, it still built successfully, how can I make it fail if those arguments are missing?</p>
"
"38982807","Are a WSGI server and HTTP server required to serve a Flask app?","<python><nginx><flask><uwsgi>","56172747","Flask application with nginx and gunicorn on AWS","<amazon-web-services><docker><nginx><flask><amazon-elastic-beanstalk>","<p>Setting up Flask with uWSGI and Nginx can be difficult. I tried following <a href=""https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uwsgi-and-nginx-on-ubuntu-14-04"" rel=""nofollow noreferrer"">this DigitalOcean tutorial</a> and still had trouble. Even with buildout scripts it takes time, and I need to write instructions to follow next time.</p>
<p>If I don't expect a lot of traffic, or the app is private, does it make sense to run it without uWSGI? Flask can listen to a port. Can Nginx just forward requests?</p>
<p>Does it make sense to not use Nginx either, just running bare Flask app on a port?</p>
","<p>I have simple Flask application (simply shows ""Hello world""), I would like to deploy it on AWS Elastic BeanStalk. Multiple tutorial show deployment with nginx and gunicorn. 
1) I don't understand why we need to use nginx, gunicorn is already a web-server to replace Flask build-in web server.
2) Tutorials show how to build two Docker containers: one for Flask and gunicorn and another for nginx. Why do I need two containers, can I package all in one? With two containers I cannot use Single Container Docker, I need to use Multicontainer Docker.</p>

<p>Any thoughts?</p>
"
"39337254","Java FTPS fails to retrieve file list (FileZilla Client works fine)","<java><ssl><ftp><apache-commons-net><ftps>","56464536","How to configure Docker to use a FTP Client in Java?","<java><docker><ftp>","<p>I am using Apache Commons Net (v3.5) with a Java 8 to connect to a remote FTPS site (i.e. out on the internet).  I am able to easily connect with a FileZilla client on my Windows 10 machine, but my Java program is unable to complete the same steps.  I've googled high and low, but cannot find the root cause.  Here are things that I have confirmed:</p>

<ul>
<li>I ensured the Java FTP commands are in the exact same order as the FileZilla client.</li>
<li>I disabled Windows Firewall and Anti-Virus on the PC</li>
<li>I re-enabled Windows Firewall and enabled logging.  When using FileZilla, the Windows Firewall Log lists the TCP connection when the passive mode connection is established.  I see no such entry with the Java program. </li>
<li>I installed a FileZilla server on my PC.  The java program worked after I un-checked ""Require TLS session resumption on data connection when using PROT P.""  The Java exception was different, so I do not believe this is a smoking gun. </li>
<li>I successfully ran this same code against test.rebex.com server.  </li>
</ul>

<p>Below is the code and any thoughts are greatly appreciated:</p>

<pre><code>import java.io.IOException;
import java.io.PrintWriter;
import org.apache.commons.net.PrintCommandListener;
import org.apache.commons.net.ftp.FTP;
import org.apache.commons.net.ftp.FTPFile;
import org.apache.commons.net.ftp.FTPReply;
import org.apache.commons.net.ftp.FTPSClient;

public class testProgram {

  public static void main(String[] args) {

    String ftpServer = ""ftp.domain.com"";
    String ftpUsername = ""user@domain.com"";
    String ftpPassword = ""********"";

    FTPSClient ftp = null;

    // CONNECT TO THE SERVER
    try {
        // I have tried ""SSL"" as the argument, but same result
        ftp = new FTPSClient(); 
        ftp.addProtocolCommandListener(new PrintCommandListener(new PrintWriter(System.out)));

        ftp.connect(ftpServer,21);

        int reply = ftp.getReplyCode();

        if (!FTPReply.isPositiveCompletion(reply)) {
            ftp.disconnect();
            System.err.println(""----------&gt;FTP server refused connection.\n"");

        } 

    } catch (Exception e) {
        System.out.println(e.getMessage());
        e.printStackTrace();

    }

    // LOGIN INTO SERVER
    try {
        if (!ftp.login(ftpUsername, ftpPassword)) {
            ftp.logout();

        } else {

            ftp.sendCommand(""OPTS UTF8 ON"");            
            ftp.execPBSZ(0);            
            ftp.execPROT(""P"");
            ftp.pwd();
            ftp.setFileType(FTP.BINARY_FILE_TYPE);      
            ftp.enterLocalPassiveMode();

            /* The next command always fails.

               The FTP Server responds with ""150 Accepted data connection"" then:

                org.apache.commons.net.ftp.FTPConnectionClosedException: Connection closed without indication.
                at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:316)
                at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:292)
                at org.apache.commons.net.ftp.FTP.getReply(FTP.java:712)
                at org.apache.commons.net.ftp.FTPClient.completePendingCommand(FTPClient.java:1857)
                at org.apache.commons.net.ftp.FTPClient.listNames(FTPClient.java:2919)
                at org.apache.commons.net.ftp.FTPClient.listNames(FTPClient.java:2952)
                at myPackage.testProgram.main(testProgram.java:78)

                I have tried other commands, but it disconnects here...
             */

            FTPFile[] ftpFiles = ftp.listFiles();
            System.out.println(""----------&gt;Number of Files = "" + ftpFiles.length);
            ftp.logout();

        }
    } catch (Exception e) {

        e.printStackTrace();
    } 

    //Ensure Disconnected at the end.
    if (ftp.isConnected()) {
        try {
            ftp.disconnect();
        } catch (IOException f) {
            // do nothing
        }

    }
  }
}
</code></pre>

<p>Here is the FileZilla Client log from my PC:</p>

<pre><code>2016-09-06 09:09:50 4756 1 Status: Resolving address of ftp.domain.com
2016-09-06 09:09:51 4756 1 Status: Connecting to h1.h2.h3.h4:21...
2016-09-06 09:09:51 4756 1 Status: Connection established, waiting for welcome message...
2016-09-06 09:09:51 4756 1 Response: 220---------- Welcome to Pure-FTPd [privsep] [TLS] ----------
2016-09-06 09:09:51 4756 1 Response: 220-You are user number 2 of 50 allowed.
2016-09-06 09:09:51 4756 1 Response: 220-Local time is now 13:09. Server port: 21.
2016-09-06 09:09:51 4756 1 Response: 220-This is a private system - No anonymous login
2016-09-06 09:09:51 4756 1 Response: 220-IPv6 connections are also welcome on this server.
2016-09-06 09:09:51 4756 1 Response: 220 You will be disconnected after 15 minutes of inactivity.
2016-09-06 09:09:51 4756 1 Command: AUTH TLS
2016-09-06 09:09:51 4756 1 Response: 234 AUTH TLS OK.
2016-09-06 09:09:51 4756 1 Status: Initializing TLS...
2016-09-06 09:09:51 4756 1 Status: Verifying certificate...
2016-09-06 09:09:51 4756 1 Status: TLS connection established.
2016-09-06 09:09:51 4756 1 Command: USER user@domain.com
2016-09-06 09:09:51 4756 1 Response: 331 User user@domain.com OK. Password required
2016-09-06 09:09:51 4756 1 Command: PASS *************
2016-09-06 09:09:51 4756 1 Response: 230 OK. Current restricted directory is /
2016-09-06 09:09:51 4756 1 Command: SYST
2016-09-06 09:09:51 4756 1 Response: 215 UNIX Type: L8
2016-09-06 09:09:51 4756 1 Command: FEAT
2016-09-06 09:09:51 4756 1 Response: 211-Extensions supported:
2016-09-06 09:09:51 4756 1 Response:  EPRT
2016-09-06 09:09:51 4756 1 Response:  IDLE
2016-09-06 09:09:51 4756 1 Response:  MDTM
2016-09-06 09:09:51 4756 1 Response:  SIZE
2016-09-06 09:09:51 4756 1 Response:  MFMT
2016-09-06 09:09:51 4756 1 Response:  REST STREAM
2016-09-06 09:09:51 4756 1 Response:  MLST type*;size*;sizd*;modify*;UNIX.mode*;UNIX.uid*;UNIX.gid*;unique*;
2016-09-06 09:09:51 4756 1 Response:  MLSD
2016-09-06 09:09:51 4756 1 Response:  AUTH TLS
2016-09-06 09:09:51 4756 1 Response:  PBSZ
2016-09-06 09:09:51 4756 1 Response:  PROT
2016-09-06 09:09:51 4756 1 Response:  UTF8
2016-09-06 09:09:51 4756 1 Response:  TVFS
2016-09-06 09:09:51 4756 1 Response:  ESTA
2016-09-06 09:09:51 4756 1 Response:  PASV
2016-09-06 09:09:51 4756 1 Response:  EPSV
2016-09-06 09:09:51 4756 1 Response:  SPSV
2016-09-06 09:09:51 4756 1 Response:  ESTP
2016-09-06 09:09:51 4756 1 Response: 211 End.
2016-09-06 09:09:51 4756 1 Command: OPTS UTF8 ON
2016-09-06 09:09:51 4756 1 Response: 200 OK, UTF-8 enabled
2016-09-06 09:09:51 4756 1 Command: PBSZ 0
2016-09-06 09:09:51 4756 1 Response: 200 PBSZ=0
2016-09-06 09:09:51 4756 1 Command: PROT P
2016-09-06 09:09:52 4756 1 Response: 200 Data protection level set to ""private""
2016-09-06 09:09:52 4756 1 Status: Logged in
2016-09-06 09:09:52 4756 1 Status: Retrieving directory listing...
2016-09-06 09:09:52 4756 1 Command: PWD
2016-09-06 09:09:52 4756 1 Response: 257 ""/"" is your current location
2016-09-06 09:09:52 4756 1 Command: TYPE I
2016-09-06 09:09:52 4756 1 Response: 200 TYPE is now 8-bit binary
2016-09-06 09:09:52 4756 1 Command: PASV
2016-09-06 09:09:52 4756 1 Response: 227 Entering Passive Mode (h1,h2,h3,h4,133,150)
2016-09-06 09:09:52 4756 1 Command: MLSD
2016-09-06 09:09:52 4756 1 Response: 150 Accepted data connection
2016-09-06 09:09:52 4756 1 Response: 226-Options: -a -l 
2016-09-06 09:09:52 4756 1 Response: 226 6 matches total
</code></pre>

<p>Using Mike's suggesting, I turned on the TLS debugging.  It appears the program goes through the TLS handshake again.  The output is very long, but after issuing the list command, I see ""*** ClientHello, TLSv1.2"" and what looks like the same commands as initiating the FTP connection.  </p>

<p>The difference appears to come at the end:</p>

<pre><code>%% Cached client session: [Session-2, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256]
main, received EOFException: ignored
main, called closeInternal(false)
main, SEND TLSv1.2 ALERT:  warning, description = close_notify
main, WRITE: TLSv1.2 Alert, length = 26
main, called closeSocket(false)
main, called close()
main, called closeInternal(true)
main, called close()
main, called closeInternal(true)
main, received EOFException: ignored
main, called closeInternal(false)
main, SEND TLSv1.2 ALERT:  warning, description = close_notify
main, WRITE: TLSv1.2 Alert, length = 26
main, called closeSocket(false)
org.apache.commons.net.ftp.FTPConnectionClosedException: Connection closed without indication.
</code></pre>
","<p>I am trying to run a simple Java FTP Client (org.apache.commons.net.ftp.FTPClient) program with Docker containerization to obtain some file from a remote FTP server. </p>

<p>Running the app locally (without docker) works fine. The containerized app however does not return any data. Example program:</p>

<pre><code>ftpClient.connect(host, port);
ftpClient.login(usr, pwd);
System.out.println(ftpClient.getReplyString());
filenameList = ftpClient.listNames();
System.out.println(ftpClient.getReplyString());
</code></pre>

<p>With Docker, I get the following error message: Connection closed without indication. </p>

<p>Interesting however is that I also receive: 
230 User logged in. Any hints?</p>
"
"39901311","Docker - Ubuntu - bash: ping: command not found","<ubuntu><docker><ping>","56211124","Ping won't install inside a Docker container","<docker>","<p>I've got a Docker container running Ubuntu which I did as follows:</p>

<pre><code>docker run -it ubuntu /bin/bash
</code></pre>

<p>however it doesn't seem to have <code>ping</code>. E.g.</p>

<pre><code>bash: ping: command not found
</code></pre>

<p>Do I need to install that?</p>

<p>Seems a pretty basic command to be missing. I tried <code>whereis ping</code> which doesn't report anything.</p>
","<p>I pulled the <a href=""https://hub.docker.com/_/httpd"" rel=""nofollow noreferrer"">Apache httpd image</a> and ran a container. However, once, I'm inside a container, ping doesn't work and I see an error: ""bash: ping: command not found"". Can't even ping 127.0.0.1. Ping is working outside of a container.</p>

<p><code>cat /etc/resolv.conf</code> shows the same nameserver, inside and outside of the container. I restarted the Docker service, but, it doesn't solve the problem either. </p>

<pre><code>docker pull httpd
docker run -d --name testweb httpd
docker exec -it testweb /bin/bash
ping google.com

root@fb1ce4bccc11:/usr/local/apache2# ping google.com

bash: ping: command not found
</code></pre>

<p>I tried to install Ping, but the package manager can't find it:</p>

<pre><code>root@fb1ce4bccc11:/usr/local/apache2# yum -y install iputils-20160308-10.el7.x86_64
bash: yum: command not found
</code></pre>
"
"40827870","Constant expression contains invalid operations","<php><class><properties><syntax-error><compile-time-constant>","56441748","Use environment variable in Cakephp 2 database.php","<docker><cakephp><environment-variables>","<p>I have the following code, where I get the error ""PHP Fatal Error: Constant expression contains invalid operations"". It works fine when I define the variable in the constructor. I am using Laravel framework.</p>

<pre><code>&lt;?php

namespace App;

class Amazon
{
    protected $serviceURL = config('api.amazon.service_url');

    public function __construct()
    {
    }

}
</code></pre>

<p>I have seen this question: <a href=""https://stackoverflow.com/questions/40171546/php-error-fatal-error-constant-expression-contains-invalid-operations"">PHP Error : Fatal error: Constant expression contains invalid operations</a>
But my code does not declare anything as static, so that did not answer my question.</p>
","<p>I want to make my project more scalable using docker and environment variables. I am using the latest version of CakePhp 2 and my project is also running well in docker. Now I want to create an .env file where information like the database name,  database user, database password, Salt value and so on is stored and later on created by the configuration of the docker container. Creating the .env file and reading the values in CakePhp is easy. I created a file called .env and put it into the app folder. It is looking like </p>

<pre><code>//.env in app folder
{
    ""DB_USER"" : ""user"",
    ""DB_PASS"" : ""password""
}
</code></pre>

<p>Then I am putting the information from the .env file into the environment in the bootstrap.php using this function: </p>

<pre><code>//bootstrap.php Putting env infos to the env from the .env file
if(is_file(APP . DS . '.env')) {
    $vars = json_decode(file_get_contents(APP . DS . '.env'), true);
    foreach ($vars as $name =&gt; $val) {
        putenv(""$name=$val"");
    }
 }
</code></pre>

<p>Finally, I want to call the env information from the database.php</p>

<pre><code>//database.php Get value from env
public $default = array(
    'datasource' =&gt; 'Database/Mysql',
    'persistent' =&gt; false,
    'host' =&gt; 'host.docker.internal',
    'login' =&gt; env('DB_USER'),
    'password' =&gt; env('DB_PASS'),
    'database' =&gt; 'dbname',
    'prefix' =&gt; '',
    'encoding' =&gt; 'utf8',
);
</code></pre>

<p>Unluckily CakePhp does not wants the env() inside the array of the database.php and returns the message <code>Fatal error: Constant expression contains invalid operations in /var/www/html/test/app/Config/database.php on line 70</code>. What can I do to use the .env file and call the environment information for e.g. the database configuration?</p>

<p>If I try to call the env values from somewhere else, I can fetch them without any problems.</p>
"
"40873165","Use docker run command to pass arguments to CMD in Dockerfile","<node.js><docker><dockerfile>","56421323","Overriding docker cmd when docker run","<docker><dockerfile>","<p>I'm new to Docker and I'm having a hard time to setup the docker container as I want. I have a nodejs app can take two parameters when start. For example, I can use</p>

<p><code>node server.js 0 dev</code> </p>

<p>or</p>

<p><code>node server.js 1 prod</code></p>

<p>to switch between production mode and dev mode and determine if it should turn the cluster on. Now I want to create docker image with arguments to do the similar thing,  the only thing I can do so far is to adjust the Dockerfile to have a line </p>

<p><code>CMD [ ""node"", ""server.js"", ""0"", ""dev""]</code> </p>

<p>and </p>

<p><code>docker build -t me/app .</code> to build the docker.</p>

<p>Then <code>docker run -p 9000:9000 -d me/app</code> to run the docker.</p>

<p>But If I want to switch to prod mode, I need to change the Dockerfile CMD to be </p>

<p><code>CMD [ ""node"", ""server.js"", ""1"", ""prod""]</code> , </p>

<p>and I need to kill the old one listening on port 9000 and rebuild the image.
I wish I can have something like </p>

<p><code>docker run -p 9000:9000 environment=dev cluster=0 -d me/app</code> </p>

<p>to create an image and run the nodejs command with ""environment"" and ""cluster"" arguments, so I don't need to change the Dockerfile and rebuild the docker any more. How can I accomplish this?</p>
","<p>I am dockerizing a node application.</p>

<p>This is the Dockerfile that I am using:</p>

<pre><code>FROM node:10-slim

# Sets environment variable
ENV NODE_ENV production

# Sets work directory
WORKDIR /usr/src/app

# Copy package.json
COPY [""package.json"", ""./""]

# Installs dependencies 
RUN npm install

# Copy working files
COPY . /usr/src/app

EXPOSE 80

# Starts run command
CMD npm start
</code></pre>

<p>But then, since I have several .env files, I would like to pass an argument to choose which env file I will use.</p>

<p>Like this</p>

<pre><code>npm start -- --env=""test"" 
</code></pre>

<p>So what I ultimately want is </p>

<pre><code>docker run -p 8080:8080 test/nodeapp:1.0 -- -evn=""test"" 
</code></pre>

<p>How should I override the <code>CMD</code> on docker run?</p>
"
"42370404","Pass optional arguments when running a Docker image","<docker><asp.net-core><dockerfile>","56304449","Can I pass Docker run arguments to optional Golang flags?","<docker><go>","<p>I have a Docker file based on windowsservercore, which I am hosting an ASP.net Core web application.</p>

<p>Snippet from docker file</p>

<pre><code>ENTRYPOINT [""my.exe""]
ENV ASPNETCORE_URLS http://+:5000
EXPOSE 5000
</code></pre>

<p>When running the docker image with the below command, I'm trying to pass optional arguments that will be passed down to my exe endpoint.</p>

<pre><code>docker rm myapp
docker run --net=""host"" --name myapp -p 5000:5000 myappservice
</code></pre>
","<p>I want to run a simple go script in a docker container. The script has some flags like the two examples bellow.</p>

<p>Flags:</p>

<pre><code>dataSource := flag.String(""input"", """", ""Path"")
...
concurrency := flag.Int(""concurrency"", 10, ""Concurrency"")
flag.Parse()
</code></pre>

<p>Some flags have defaults set and are optional. Others are mandatory to be set by the user. How would I pass arguments in the docker run command to the go script without having the user to enter all arguments?</p>

<p>Dockerfile:</p>

<pre><code>FROM golang:alpine AS builder
RUN apk update &amp;&amp; apk add --no-cache git
WORKDIR $GOPATH/go/src/app
ENV GOBIN=/usr/local/bin
COPY . .
RUN go get github.com/lib/pq
RUN go build -o /go/bin/Import
FROM scratch
COPY --from=builder /go/bin/Import /go/bin/Import
ENTRYPOINT [""/go/bin/Import""""]
</code></pre>

<p>I've read the docs about <a href=""https://docs.docker.com/v17.09/engine/reference/builder/#entrypoint"" rel=""noreferrer"">ENTRYPOINT</a> but could not find anything suitable for me. Is it even possible or doesn't it make sense?</p>

<p>I've read through posts like this one: <a href=""https://stackoverflow.com/questions/42370404/pass-optional-arguments-when-running-a-docker-image"">Pass optional arguments when running a Docker image</a> . They simply pass single arguments, how would I define the default value for those?</p>

<p>Thanks for any help</p>
"
"44342741","Auto reloading flask server on Docker","<python><docker><flask>","56127235","Flask docker compose reload when changing source code?","<python><docker><flask>","<p>I want my flask server to detect changes in code and reload automatically.
I'm running this on docker container.
Whenever I change something, I have to build and up again the container. I have no idea where's wrong. This is my first time using flask.</p>

<p>Here's my tree</p>

<pre><code>├── docker-compose.yml
└── web
    ├── Dockerfile
    ├── app.py
    ├── crawler.py
    └── requirements.txt
</code></pre>

<p>and code(app.py)</p>

<pre><code>from flask import Flask 
import requests
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello Flask!!'

if __name__ == '__main__':
    app.run(debug = True, host = '0.0.0.0')
</code></pre>

<p>and docker-compose</p>

<pre><code>version: '2'
services:

  web:
    build: ./web
    ports:
     - ""5000:5000""
    volumes:
     - ./web:/code
</code></pre>

<p>Please give me some advice. Thank you in advance.</p>
","<p>I'm used to python reloading flask in debug mode automatically when changes have been detected in the source code. I started moving over to docker composer, I can't get it so that it reloads automatically in the container. </p>

<p>my dockerfile:</p>

<pre><code>WORKDIR /app
COPY . /app
RUN pip install --trusted-host pypi.python.org -r requirements.txt
EXPOSE 80
ENV NAME World
CMD [""python"", ""app.py""]
</code></pre>

<p>my docker compose file:</p>

<pre><code>version: '3'
services:
  web:
    stdin_open: true
    tty: true
    build: .
    ports:
     - ""80:80""
  redis:
    image: ""redis:alpine""

</code></pre>
"
"48228534","Kubernetes Dashboard access using config file Not enough data to create auth info structure.","<kubernetes>","56142206","How to Configure Kubeconfig based login in Kubernetes Dashboard","<docker><kubernetes><google-cloud-platform><devops><kubernetes-dashboard>","<p>I am trying to access the kubernetes Dashboard using the config file. From the authentication when I select config file its giving ‘<code>Not enough data to create auth info structure</code>.’  Bu the same config file work for kubectl command.</p>

<p><a href=""https://i.stack.imgur.com/VEzb9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VEzb9.png"" alt=""enter image description here""></a></p>

<p>here is my config file.</p>

<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://kubemaster:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
</code></pre>

<p>Any help to resolve this issue?</p>

<p>Thanks
SR</p>
","<p>I have configured Kubernetics has 4 node cluster and configured Dashboard from Kubernetics Documentation but able to login with different Token based service account which have different role bind on that account .</p>

<p>But my point is I want to login with Kubeconfig options but I am unable to do so . So help me the steps how to do that. 
<a href=""https://i.stack.imgur.com/ijTpR.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
"
"48850813","Where to put the php artisan migrate command","<php><laravel><docker><docker-stack>","56382638","Getting ""php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution"" error when using docker-compose to build containers","<php><mysql><laravel><docker><dns>","<blockquote>
  <p>Trying to deploy the laravel application on docker stack .What I am
  confused or not able to figure out is where can I run this php artisan
  migrate:fresh to generate the tables required in mysql.</p>
</blockquote>

<p>The services and the task are running well</p>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3.3'

networks:
  smstake: 
    ipam:
      config:
        - subnet: 10.0.10.0/24

services:

    db:
        image: mysql:5.7
        networks:
          - smstake
        ports:
          - ""3306:3306""
        volumes:
          - db_data:/var/lib/mysql
        environment:
          MYSQL_ROOT_PASSWORD: password
          MYSQL_DATABASE: smstake
          MYSQL_USER: root
          MYSQL_PASSWORD: password
        deploy:
          mode: replicated
          placement:
            constraints:
              - node.role == manager
    app:

        image: smstake:latest          
        ports:
          - 8000:80
        networks:
          - smstake

        command: docker-compose exec app php artisan migrate --seed
        deploy:
          mode: replicated
          replicas: 1
          placement:
            constraints:
              - node.role == manager
volumes:
    db_data:
</code></pre>

<p><strong>Here is the dockerfile with which the image is generated</strong></p>

<pre><code>FROM alpine

ENV \
  APP_DIR=""/app"" \
  APP_PORT=""80""

# the ""app"" directory (relative to Dockerfile) containers your Laravel app...
COPY app/ $APP_DIR
# or we can make the volume in compose to say use this directory 

RUN apk update &amp;&amp; \
    apk add curl \
    php7 \
    php7-opcache \
    php7-openssl \
    php7-pdo \
    php7-json \
    php7-phar \
    php7-dom \
    php7-curl \
    php7-mbstring \
    php7-tokenizer \
    php7-xml \
    php7-xmlwriter \
    php7-session \
    php7-ctype \
    php7-mysqli \
    php7-pdo \
    php7-pdo_mysql\
    &amp;&amp; rm -rf /var/cache/apk/*

RUN curl -sS https://getcomposer.org/installer | php -- \
  --install-dir=/usr/bin --filename=composer

RUN cd $APP_DIR &amp;&amp; composer install

WORKDIR $APP_DIR

RUN chmod -R 775 storage
RUN chmod -R 775 bootstrap

#CMD php artisan migrate:fresh
CMD php artisan serve --host=0.0.0.0 --port=$APP_PORT
</code></pre>

<p><strong>Tried adding to the Dockerfile as is commented but didn't solve the problem</strong></p>

<p><strong>Tried adding on docker-compose as command: php artisan migrate:fresh too</strong> </p>

<p><strong>Previously was doing this in jenkins to make it work Now dont want it via jenkins</strong></p>

<pre><code>docker-compose up -d --force-recreate --build 

#Running commands on already running service 
docker-compose exec -T app php artisan migrate:fresh --seed --force
</code></pre>
","<p>It seems as if my php container cannot resolve the host ""db"" of my database container when I run <code>php artisan migrate</code>, I'm wondering if I've set something up wrong in the config.</p>

<p>Tried messing with the depends_on: and links: options on the dockerfile.</p>

<p>docker-compose.yml</p>

<pre><code>version: '3.3'

services:
  db:
    container_name: tasks-db
    image: mariadb:latest
    volumes:
      - dbdata:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=tasks
      - MYSQL_USER=root
      - MYSQL_PASSWORD=password
    restart: always

  webserver:
    container_name: tasks-webserver
    image: nginx:latest
    ports:
      - ""8080:80""
      - ""443:443""
    volumes:
      - .:/app
      - ./provision/nginx/:/etc/nginx/conf.d/
    depends_on:
      - php
    restart: always

  php:
    container_name: tasks-php
    build:
      context: .
      dockerfile: provision/php.docker
    depends_on:
      - db
    restart: always
    env_file:
      - .env

volumes:
  dbdata:
    driver: local
</code></pre>

<p>provision/db.docker</p>

<pre><code>FROM php:7.3-fpm

# Add dependencies
RUN apt-get update -y &amp;&amp; apt-get install -y openssl libpng-dev libxml2-dev curl cron git libzip-dev zip unzip

# Install php extensions
RUN docker-php-ext-install pdo mbstring gd xml pdo_mysql zip

# Install composer
RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer

COPY . /app/

WORKDIR /app

RUN chown -R $USER:www-data /app/storage
RUN chown -R $USER:www-data /app/bootstrap/cache

RUN chmod -R 775 /app/storage
RUN chmod -R 775 /app/bootstrap/cache

# Install composer dependencies
RUN composer install

RUN php artisan optimize

#RUN php artisan migrate --seed

RUN crontab -l | { cat; echo ""* * * * * php /app/artisan schedule:run &gt;&gt; /dev/null 2&gt;&amp;1""; } | crontab -

STOPSIGNAL SIGTERM

CMD [""php-fpm""]
</code></pre>

<p>db.docker</p>

<pre><code>FROM mariadb:latest

CMD [""mysqld"", ""--user=mysql""]
</code></pre>

<p>I'm getting this error:</p>

<pre><code>Step 13/16 : RUN php artisan migrate --no-interaction --seed
 ---&gt; Running in ccbfcde9324d

   Illuminate\Database\QueryException  : SQLSTATE[HY000] [2002] php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution (SQL: select * from information_schema.tables where table_schema = tasks and table_name = migrations and table_type = 'BASE TABLE')

  at /app/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|

  Exception trace:

  1   PDOException::(""PDO::__construct(): php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution"")
      /app/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70

  2   PDO::__construct(""mysql:host=db;port=3306;dbname=tasks"", ""root"", ""password"", [])
      /app/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70

  Please use the argument -v to see more details.
ERROR: Service 'php' failed to build: The command '/bin/sh -c php artisan migrate --no-interaction --seed' returned a non-zero code: 1
</code></pre>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","55790221","Docker image white labelling","<docker><dockerfile><docker-image>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I am creating docker images for my python application.I have to share it with others.</p>

<p>When we run </p>

<pre><code>docker save
</code></pre>

<p>using this command we can create a tar of docker image.Tar file contain of the layer of images and inside folder whole application code may expose.</p>

<p>is there a way to hide code or white label the docker image or i have to do it from python side.  </p>
"
"53521104","ASP.NET Core: ""The project doesn't know how to run the profile Docker."" on Visual Studio 2017","<visual-studio><docker><asp.net-core>","56184618","this project doesn't know how to run the profile docker","<.net><docker><.net-core><dockerfile><visual-studio-2019>","<p>I have set up an ASP.Net Core Web application - this application runs <code>Angular</code> using <code>.Net Core 2.2</code> -> <code>2.2.0-preview3</code>. After initializing that project I thought I would have added working docker-support when clicking on ""Add"" -> ""Docker Support"" for ""Linux Container"" - but running this would prompt me with the following error-message:</p>

<p><a href=""https://i.stack.imgur.com/8cYbC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8cYbC.png"" alt=""enter image description here""></a></p>

<p>Since I <em>do</em> have the option to add docker support there should be a way to run Angular in docker, right?</p>

<pre><code>Docker Version: 18.09.0
</code></pre>
","<p>I pull a project from git with dockerfile, I am able to build the image via command line, but the visual studio says ""this project doesn't know how to run the profile docker"" I am using vs2019. Any pieces of advice?</p>

<p>I have checked this <a href=""https://stackoverflow.com/a/54485107/1219916"">https://stackoverflow.com/a/54485107/1219916</a> however did not fix my problem </p>

<p>this is my dockerfile</p>

<pre><code>FROM mcr.microsoft.com/dotnet/core/aspnet:2.2-nanoserver-1803 AS base WORKDIR /app EXPOSE 80 EXPOSE 443

FROM mcr.microsoft.com/dotnet/core/sdk:2.2-nanoserver-1803 AS build WORKDIR /src COPY [""WebApi/WebApi.csproj"", ""WebApi/""] RUN dotnet restore ""WebApi/WebApi.csproj"" COPY . . WORKDIR ""/src/WebApi"" RUN dotnet build ""WebApi.csproj"" -c Release -o /app

FROM build AS publish RUN dotnet publish ""WebApi.csproj"" -c Release -o /app

FROM base AS final WORKDIR /app COPY --from=publish /app . ENTRYPOINT [""dotnet"", ""WebApi.dll""]
</code></pre>
"
"55271912","Flask CLI throws 'OSError: [Errno 8] Exec format error' when run through docker-compose","<python><windows><docker><flask><docker-compose>","56298768","Dockerize ubuntu with docker for windows","<python><docker><flask>","<p>I'm running a Flask application with a <a href=""http://flask.pocoo.org/docs/0.12/cli/#custom-scripts"" rel=""noreferrer"">Custom Script</a>. Or trying to, anyway.</p>

<p>I'm on Windows 10 and the application ought to run in a linux Docker container with the command:</p>

<pre><code>docker-compose up api
</code></pre>

<p>Docker-compose is <code>version 1.23.2</code>. In the dockerfile, the <code>api</code> service runs via the command:</p>

<pre><code>command: python manage.py run --host ""0.0.0.0"" --with-threads
</code></pre>

<p>As it tries to start up, I see the exception</p>

<pre><code>OSError: [Errno 8] Exec format error: '/api/manage.py'
</code></pre>

<p>I initially thought this would be the Dreaded Windows Line Endings, come for me once more, but running <code>dos2unix</code> on all my source files has not resolved the problem.</p>

<p>How can I avoid this error?</p>

<hr>

<p><strong>manage.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>    import click
    from flask.cli import FlaskGroup

    from my_app_api import create_app


    def create_my_app(info):
        return create_app()


    @click.group(cls=FlaskGroup, create_app=create_my_app)
    def cli():
        pass


    if __name__ == ""__main__"":
        cli()
</code></pre>

<p><strong>Full traceback</strong></p>

<pre><code>api_1          | Traceback (most recent call last):
api_1          |   File ""manage.py"", line 22, in &lt;module&gt;
api_1          |     cli()
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
api_1          |     return self.main(*args, **kwargs)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 380, in main
api_1          |     return AppGroup.main(self, *args, **kwargs)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 717, in main
api_1          |     rv = self.invoke(ctx)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
api_1          |     return _process_result(sub_ctx.command.invoke(sub_ctx))
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
api_1          |     return ctx.invoke(self.callback, **ctx.params)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
api_1          |     return callback(*args, **kwargs)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/decorators.py"", line 64, in new_func
api_1          |     return ctx.invoke(f, obj, *args, **kwargs)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
api_1          |     return callback(*args, **kwargs)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 438, in run_command
api_1          |     use_debugger=debugger, threaded=with_threads)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/werkzeug/serving.py"", line 988, in run_simple
api_1          |     run_with_reloader(inner, extra_files, reloader_interval, reloader_type)
api_1          |   File ""/usr/local/lib/python3.6/site-packages/werkzeug/_reloader.py"", line 332, in run_with_reloader
api_1          |     sys.exit(reloader.restart_with_reloader())
api_1          |   File ""/usr/local/lib/python3.6/site-packages/werkzeug/_reloader.py"", line 176, in restart_with_reloader
api_1          |     exit_code = subprocess.call(args, env=new_environ, close_fds=False)
api_1          |   File ""/usr/local/lib/python3.6/subprocess.py"", line 287, in call
api_1          |     with Popen(*popenargs, **kwargs) as p:
api_1          |   File ""/usr/local/lib/python3.6/subprocess.py"", line 729, in __init__
api_1          |     restore_signals, start_new_session)
api_1          |   File ""/usr/local/lib/python3.6/subprocess.py"", line 1364, in _execute_child
api_1          |     raise child_exception_type(errno_num, err_msg, err_filename)
api_1          | OSError: [Errno 8] Exec format error: '/api/manage.py'
</code></pre>
","<p>I've create</p>

<ul>
<li>Dockerfile</li>
<li>app.py</li>
<li>requirements.txt</li>
</ul>

<p>here my code</p>

<p><strong>app.py</strong></p>

<pre><code>import os
from flask import Flask
app = Flask(__name__)

@app.route(""/"")
def main():
    return ""Welcome!""

@app.route('/how are you')
def hello():
    return 'I am good, how about you?'

if __name__ == ""__main__"":
    app.run(debug=True, host=""0.0.0.0"", port=8080)
</code></pre>

<p><strong>requirements.txt</strong></p>

<pre><code> Flask
</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code> FROM ubuntu:latest
 RUN apt-get update -y
 RUN apt-get install -y python-pip python-dev build-essential
 COPY . /app
 WORKDIR /app
 RUN pip install -r requirements.txt
 ENTRYPOINT [""python""]
 CMD [""app.py""]
</code></pre>

<p>Then i build the image</p>

<pre><code>docker build . -t dede-flask:latest
</code></pre>

<p>But i got an error with run script</p>

<ol>
<li>first i've try with this code</li>
</ol>

<pre><code>  docker run dede-flask:latest
</code></pre>

<p>i got error</p>

<pre><code> * Serving Flask app ""app"" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)
 * Restarting with stat
Traceback (most recent call last):
  File ""app.py"", line 14, in &lt;module&gt;
    app.run(debug=True, host=""0.0.0.0"", port=8080)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 944, in run
    run_simple(host, port, self, **options)
  File ""/usr/local/lib/python2.7/dist-packages/werkzeug/serving.py"", line 1007, in run_simple
    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)
  File ""/usr/local/lib/python2.7/dist-packages/werkzeug/_reloader.py"", line 332, in run_with_reloader
    sys.exit(reloader.restart_with_reloader())
  File ""/usr/local/lib/python2.7/dist-packages/werkzeug/_reloader.py"", line 176, in restart_with_reloader
    exit_code = subprocess.call(args, env=new_environ, close_fds=False)
  File ""/usr/lib/python2.7/subprocess.py"", line 172, in call
    return Popen(*popenargs, **kwargs).wait()
  File ""/usr/lib/python2.7/subprocess.py"", line 394, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1047, in _execute_child
    raise child_exception
OSError: [Errno 8] Exec format error
</code></pre>

<ol start=""2"">
<li>i've too try with this code</li>
</ol>

<pre><code>docker run -d -p 5000:5000 dede-flask:latest
</code></pre>

<p>i got an error</p>

<pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint friendly_bhaskara (990535df1272811a3991c8a11e06476267e027484b850c244ca195e9d5b8a220): Error starting userland proxy: mkdir /port/tcp:0.0.0.0:5000:tcp:172.17.0.2:5000: input/output error.
</code></pre>
"
"55921914","How to source a script with environment variables in a docker build process?","<docker><dockerfile>","55998422",".env file not getting sourced in Alpine Linux Docker","<bash><docker>","<p>I'm creating an image that has a similar problem like the following docker project:</p>

<h3>Dockerfile</h3>

<pre><code>FROM alpine:3.9.3

COPY ./env.sh /env.sh
RUN source /env.sh
CMD env
</code></pre>

<h3>env.sh</h3>

<pre><code>TEST=test123
</code></pre>

<p>I built the image with </p>

<pre><code>docker build -t sandbox .
</code></pre>

<p>and run it with</p>

<pre><code>docker run --rm sandbox
</code></pre>

<p>The output is</p>

<pre><code>HOSTNAME=72405c43801b
SHLVL=1
HOME=/root
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
</code></pre>

<p>My environment variable is missing.</p>

<p>In the real project I have to source a longer complex script for the installation for IBM DB2 client that also sets environment variables. How can I achieve it without reading the whole installation process and setting all variables with <code>ENV</code> in the dockerfile?</p>

<p>EDIT: 
In the real project the file <code>env.sh</code> is created as part of the installation process and it is not available from outside of the container. The environment variables are set depending on the system it is executed on. If I run it on the host it will set wrong variables in the guest.</p>

<p>Part of the real script is</p>

<pre><code>if [ -f ${INST_DIR?}/tools/clpplus.jar ]; then
    AddRemoveString CLASSPATH ${INST_DIR?}/tools/clpplus.jar a
fi

if [ -f ${INST_DIR?}/tools/antlr-3.2.jar ]; then
    AddRemoveString CLASSPATH ${INST_DIR?}/tools/antlr-3.2.jar a
fi

if [ -f ${INST_DIR?}/tools/jline-0.9.93.jar ]; then
    AddRemoveString CLASSPATH ${INST_DIR?}/tools/jline-0.9.93.jar a
fi

if [ -f ${INST_DIR?}/java/db2jcc.jar ]; then
    AddRemoveString CLASSPATH ${INST_DIR?}/java/db2jcc.jar a
fi

if [ -f ${INST_DIR?}/java/db2jcc_license_cisuz.jar ]; then
    AddRemoveString CLASSPATH ${INST_DIR?}/java/db2jcc_license_cisuz.jar a
fi
</code></pre>

<p>It checks the installation and sets the variables depending on this. Since on the host is no DB2 installation the variables wouldn't be set.</p>
","<p><code>.env</code> file is in project root.
I am using a docker file as follows </p>

<pre class=""lang-sh prettyprint-override""><code>FROM alpine:3.7
WORKDIR /app
COPY . /app
RUN apk update &amp;&amp; apk add build-base python3 python3-dev --no-cache bash &amp;&amp;  \
   pip3 install --upgrade pip &amp;&amp; \
   pip3 install --trusted-host pypi.python.org --no-cache-dir -e. &amp;&amp; \
   ./scripts/install.sh

EXPOSE 5000 3306
CMD [""myserver"", ""run""]
</code></pre>

<p>and the install.sh file as follows</p>

<pre class=""lang-sh prettyprint-override""><code>#!/usr/bin/env bash

source .env
</code></pre>

<p>When I log in to the docker container I noticed that the <code>.env</code> file not getting sourced. How ever <code>.env</code> file is in the <code>folder</code>.  How to source the <code>.env</code> file in docker container by using the docker file? </p>
"
"56059921","How can I have a host and container read/write the same files with Docker?","<javascript><python><linux><docker>","56184987","docker-compose user mapping","<docker><docker-compose>","<p>I would like to volume mount a directory from a Docker container to my work station, so when I edit the content in the volume mount from my work station it updated in the container as well. It would be very useful for testing and develop web applications in general.</p>

<p>However I get a permission denied in the container, because the UID's in the container and host isn't the same. Isn't the original purpose of Docker that it should make development faster and easier?</p>

<p><a href=""https://stackoverflow.com/a/48536224/256439"">This answer</a> works around the issue I am facing when volume mounting a Docker container to my work station. But by doing this, I make changes to the container that I won't want in production, and that defeats the purpose of using Docker during development.</p>

<p>The container is <a href=""https://en.wikipedia.org/wiki/Alpine_Linux"" rel=""nofollow noreferrer"">Alpine Linux</a>, work station <a href=""https://en.wikipedia.org/wiki/Fedora_%28operating_system%29"" rel=""nofollow noreferrer"">Fedora</a> 29, and editor <a href=""https://en.wikipedia.org/wiki/Atom_(text_editor)"" rel=""nofollow noreferrer"">Atom</a>.</p>

<p><strong>Question</strong></p>

<p>Is there another way, so both my work station and container can read/write the same files?</p>
","<p>I'm using a very simple docker-compose file:</p>

<pre><code>version: '2'
services:

db:
  image: mysql:5.7 # https://hub.docker.com/_/mysql/ - or mariadb https://hub.docker.com/_/mariadb
  container_name: prestashop_db
  ports:
    - 3306:3306 # change ip if required
  volumes:
    # for init 
    - ./db-init:/docker-entrypoint-initdb.d # IMPORTANT ! comment after the first docker-compose up 
    - ./db-data:/var/lib/mysql
  environment:
    MYSQL_ROOT_PASSWORD: root
    MYSQL_DATABASE: prestashop
    MYSQL_USER: docker
    MYSQL_PASSWORD: docker     
  restart: unless-stopped

prestashop:
  image: prestashop/prestashop:1.7-7.2-apache
  container_name: prestashop_www
  ports:
    - 127.0.0.1:80:80 # change ip if required
  volumes:
    # sites directory: add the sources
    - ./src:/var/www/html
  environment:
    DB_SERVER: ""db:3306""
    DB_USER: docker
    DB_PASSWD: docker
  depends_on:
    - db
  links:
    - db:db 
</code></pre>

<p>All the content generated in my src directory, which actually corresponds to prestashop code, has the www-data:www-data owner. This means I can't actually edit it. If I change permissions or create any files as the host user, I get permission errors in the service because my user is not mapped.</p>

<p>I'd like to ask docker to map my host user to the www-data user in the prestashop service and specify it in the docker-compose file. Is it possible? This is something I used to do with LXC containers and was very useful.</p>

<p>I'd like not to:
* Run any scripts to change permissions or add my user to the www-data group in my host machine</p>

<p>Thanks for your help</p>
"
"56184154","Can I pass --max-concurrent-downloads as a flag?","<docker>","56184665","dockerd --max-concurrent-downloads 1 command not found","<docker>","<p>I'm working with a poor internet connection and trying to pull and run a image.</p>

<p>I wanted to download one layer at a time and per documentation tried adding a flat --max-concurrent-downloads like so:</p>

<pre><code>docker run --rm -p 8787:8787 -e PASSWORD=blah --max-concurrent-downloads=1 rocker/verse
</code></pre>

<p>But this gives an error:</p>

<blockquote>
  <p>unknown flag: --max-concurrent-downloads See 'docker run --help'.</p>
</blockquote>

<p>I tried typing <code>docker run --help</code> and interestingly did not see the option --max-concurrent-downloads.</p>

<p>I'm using Docker Toolbox since I'm on a old Mac.</p>

<p>Over here under l there's an option for --max-concurrent-downloads however this doesn't appear on my terminal when typing <code>docker run --help</code></p>

<p><a href=""https://i.stack.imgur.com/Pxv4Q.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Pxv4Q.png"" alt=""enter image description here""></a></p>

<p>How can I change the default of downloading 3 layers at a time to just one?</p>
","<p>I was trying to use the suggestions int he comments on <a href=""https://stackoverflow.com/questions/41303784/how-to-pull-layers-one-by-one-in-docker"">this post</a> in order to pull one image layer at a time.</p>

<p>Per the discussion on that post I typed:</p>

<pre><code>dockerd --max-concurrent-downloads 1
</code></pre>

<p>Which gave</p>

<blockquote>
  <p>bash: dockerd: command not found</p>
</blockquote>

<p>I'm using Docker Toolbox since I'm on a old Mac.</p>

<p>I tried looking for a directory /etc/docker too since that came up in the comments on the linked post but this directory does not exist in my hierarchy.</p>

<p>Since I'm using Docker Toolbox I'm apparently using a VM.</p>

<p>My docker info below. </p>

<p>How can I configure to only download one layer at a time when using run or pull?</p>

<pre><code>Macs-MacBook:~ macuser$ docker info
Containers: 1
 Running: 0
 Paused: 0
 Stopped: 1
Images: 2
Server Version: 18.09.6
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host macvlan null overlay
 Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84
runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30
init version: fec3683
Security Options:
 seccomp
  Profile: default
Kernel Version: 4.14.116-boot2docker
Operating System: Boot2Docker 18.09.6 (TCL 8.2.1)
OSType: linux
Architecture: x86_64
CPUs: 1
Total Memory: 1.951GiB
Name: default
ID: XMCE:OBLV:CKEX:EGIB:PHQ7:MLHF:ZJSA:PGYN:OIMM:JI67:ETCI:JKBH
Docker Root Dir: /mnt/sda1/var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Labels:
 provider=virtualbox
Experimental: false
Insecure Registries:
 127.0.0.0/8
Live Restore Enabled: false
</code></pre>
"
"560749","How do I configure Apache 2 to run Perl CGI scripts?","<linux><perl><apache><cgi>","50849120","How to setup cgi for php5-apache","<php><perl><docker><cgi><fastcgi>","<p>I would like to configure Apache 2 running on <code>Kubuntu</code> to execute Perl CGI scripts. I've tried some steps that I came across by googling, but nothing seems to work.</p>

<p>What is the right way of achieving this?</p>
","<p>My Dockerfile looks like this:</p>

<pre><code>FROM php:5-apache

RUN docker-php-source extract \
    &amp;&amp; docker-php-ext-install mysql mysqli pdo pdo_mysql \
    &amp;&amp; docker-php-source delete
</code></pre>

<p>Unfortunately, when I click on a cgi file I see the actual cgi script rather than the HTML file inside the browser: </p>

<pre><code>#!/usr/bin/perl -w
print ""Content-type: text/html\n\n"";
... 
</code></pre>

<p>How is it possible to configure cgi?</p>

<p>Thank you in advance,</p>
"
"5725296","Difference between sh and bash","<bash><shell><unix><sh>","50649104","/bin/sh: 1: Bad substitution Makefile","<bash><shell><docker><makefile>","<p>When writing shell programs, we often use <code>/bin/sh</code> and <code>/bin/bash</code>. I usually use <code>bash</code>, but I don't know what's the difference between them. </p>

<p>What's main difference between <code>bash</code> and <code>sh</code>?</p>

<p>What do we need to be aware of when programming in <code>bash</code> and <code>sh</code>?</p>
","<p>I have written a script to find all running docker containers with a certain name and it works when I directly type it into my terminal but as soon as I put it a Makefile it throws an error </p>

<blockquote>
  <p>/bin/sh: 1: Bad substitution</p>
</blockquote>

<p>This is the script in makefile:</p>

<pre><code>remote: FORCE
   docker ps -q --filter name=$$(tmp=$${PWD##*/} &amp;&amp; printf ""%s_workspace"" ""$${tmp//./}"")
</code></pre>

<p>To clarify what that chunk after name= is doing, it's trying to get the current folder name and remove all .'s and append it to my container name which is workspace. </p>
"
"5725296","Difference between sh and bash","<bash><shell><unix><sh>","51118502","Bad substitution when running shell script","<bash><docker>","<p>When writing shell programs, we often use <code>/bin/sh</code> and <code>/bin/bash</code>. I usually use <code>bash</code>, but I don't know what's the difference between them. </p>

<p>What's main difference between <code>bash</code> and <code>sh</code>?</p>

<p>What do we need to be aware of when programming in <code>bash</code> and <code>sh</code>?</p>
","<p>When I try to run a init and run script for my docker, it gives me an error: </p>

<pre><code>docker_init.sh: 5: docker_init.sh: Bad substitution
</code></pre>

<p>Two files are written as follows:</p>

<p><strong>init.sh</strong></p>

<pre><code>#!/bin/bash
#
DOCKER_PROCESS=$(docker ps | head -1)

if [ ${DOCKER_PROCESS:0:9} == ""CONTAINER"" ]
then
    echo ""Docker is up!""

    echo ""docker stop all""
    docker stop $(docker ps -q -a)

    echo ""docker rm all""
    docker rm $(docker ps -q -a)

    echo ""docker rmi all""
    docker rmi $(docker images -q)

    echo ""docker build backend""
    docker build -t backend .
fi

###############################
</code></pre>

<p><strong>run.sh</strong></p>

<pre><code>#!/bin/bash

DOCKER_PROCESS=$(docker ps | head -1)

if [ ${DOCKER_PROCESS:0:9} == ""CONTAINER"" ]
then
    echo ""Docker is up!""

    echo ""docker run -p 5000:5000 -it backend""
    docker run -p 5000:5000 -it backend 
fi

###############################
</code></pre>

<p>When I searched this up, most answers told be to add <code>#!/bin/bash</code> on the top, but that was not the solution.</p>
"
"14392525","Passing arguments to an interactive program non-interactively","<bash><interactive><non-interactive>","50604004","Sending commands to console installer in docker","<bash><docker><installation>","<p>I have a bash script that employs the <code>read</code> command to read arguments to commands interactively, for example yes/no options. Is there a way to call this script in a non-interactive script passing default option values as arguments?</p>

<p>It's not just one option that I have to pass to the interactive script.</p>
","<p>I am trying to install inspectIT in my docker container, however it seems as if they only package an installer that I must go through.
This seems rather difficult but I was thinking: is it possible to send commands to a console application?</p>

<p>for example if I start the installer it will get to a point like this</p>

<pre><code>Press 1 to continue, 2 to quit, 3 to redisplay
</code></pre>

<p>Can I send a command to it or script it in some way in order to install it on docker?</p>
"
"18050071","PHP parse/syntax errors; and how to solve them","<php><parsing><debugging><syntax-error>","50657741","Parse error: syntax error, unexpected ';' in /etc/zabbix/web/zabbix.conf.php on line 24","<php><docker><docker-compose>","<p>Everyone runs into syntax errors. Even experienced programmers make typos. For newcomers, it's just part of the learning process. However, it's often easy to interpret error messages such as:</p>
<blockquote>
<p>PHP Parse error: syntax error, unexpected '{' in index.php on line 20</p>
</blockquote>
<p>The unexpected symbol isn't always the real culprit. But the line number gives a rough idea of where to start looking.</p>
<blockquote>
<p>Always look at the <strong>code context</strong>. The syntax mistake often hides in the mentioned <em>or</em> in <strong>previous code lines</strong>. Compare your code against syntax examples from the manual.</p>
</blockquote>
<p>While not every case matches the other. Yet there are some <a href=""https://stackoverflow.com/a/18050072"">general steps to <strong>solve syntax mistakes</strong></a>.
This references summarized the common pitfalls:</p>
<ul>
<li><p><a href=""https://stackoverflow.com/a/18092277"">Unexpected T_STRING</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/a/18092267"">Unexpected T_VARIABLE <br> Unexpected '$varname' (T_VARIABLE)</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/a/18092288"">Unexpected T_CONSTANT_ENCAPSED_STRING <br> Unexpected T_ENCAPSED_AND_WHITESPACE</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/18050071/php-parse-syntax-errors-and-how-to-solve-them#29500670"">Unexpected $end</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/3723748/php-version-5-2-14-parse-error-syntax-error-unexpected-t-function-expecting"">Unexpected T_FUNCTION</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/a/18092308"">Unexpected <code>{</code><br>Unexpected <code>}</code><br>Unexpected <code>(</code><br>Unexpected <code>)</code></a></p>
</li>
<li><p><a href=""https://stackoverflow.com/a/29505827"">Unexpected <code>[</code><br>Unexpected <code>]</code></a></p>
</li>
<li><p><a href=""https://stackoverflow.com/a/18092318"">Unexpected T_IF <br> Unexpected T_FOREACH <br> Unexpected T_FOR <br> Unexpected T_WHILE <br> Unexpected T_DO <br> Unexpected T_PRINT <br> Unexpected T_ECHO</a></p>
</li>
<li><p><a href=""//stackoverflow.com/a/47202089"">Unexpected T_LNUMBER</a></p>
</li>
<li><p><a href=""//stackoverflow.com/a/48670368"">Unexpected ?</a></p>
</li>
<li><p><a href=""//stackoverflow.com/a/51786865"">Unexpected continue (T_CONTINUE)<br>Unexpected continue (T_BREAK)<br>Unexpected continue (T_RETURN)</a></p>
</li>
<li><p><a href=""//stackoverflow.com/a/53037930"">Unexpected '='</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/4934754/t-inline-html-whats-wrong-with-this"">Unexpected T_INLINE_HTML</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/1966010/what-does-this-mean-parse-error-syntax-error-unexpected-t-paamayim-nekudotay"">Unexpected T_PAAMAYIM_NEKUDOTAYIM</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/3990212/help-with-this-error-message-unexpected-t-object-operator"">Unexpected T_OBJECT_OPERATOR</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/2622624/parse-error-syntax-error-unexpected-t-double-arrow-php"">Unexpected T_DOUBLE_ARROW</a>…</p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/11208725/parse-error-syntax-error-unexpected-t-sl-php-heredoc"">Unexpected T_SL</a>…</p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/4419095/syntax-error-unexpected-t-boolean-or"">Unexpected T_BOOLEAN_OR</a>…
<br>
<a href=""https://stackoverflow.com/questions/11500935/parse-error-syntax-error-unexpected-t-boolean-and-expecting-in"">Unexpected T_BOOLEAN_AND</a>…</p>

</li>
<li><p><a href=""https://stackoverflow.com/a/30142092/345031"">Unexpected T_IS_EQUAL <br>
Unexpected T_IS_GREATER_OR_EQUAL <br>
Unexpected T_IS_IDENTICAL <br>
Unexpected T_IS_NOT_EQUAL <br>
Unexpected T_IS_NOT_IDENTICAL <br>
Unexpected T_IS_SMALLER_OR_EQUAL <br>
Unexpected <code>&lt;</code> <br>
Unexpected <code>&gt;</code></a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/6263105/parsing-error-syntax-error-unexpected-t-ns-separator"">Unexpected T_NS_SEPARATOR</a>…</p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/17156251/unexpected-character-in-input-ascii-92-state-1"">Unexpected character in input: '<code>\</code>' (ASCII=92) state=1</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/13341378/php-parse-error-syntax-error-unexpected-t-public"">Unexpected 'public' (T_PUBLIC) <br> Unexpected 'private' (T_PRIVATE) <br> Unexpected 'protected' (T_PROTECTED) <br> Unexpected 'final' (T_FINAL)</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/4668557/parse-error-syntax-error-unexpected-t-static"">Unexpected T_STATIC</a>…</p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/32205590/laravel-parse-error-syntax-error-unexpected-t-class-expecting-t-string-or-t-v"">Unexpected T_CLASS</a>…</p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/33342994/unexpected-use-t-use-when-trying-to-use-composer"">Unexpected 'use' (T_USE)</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/27783613/parse-error-syntax-error-unexpected-t-dnumber-in-home-a3206525-public-html-ea"">Unexpected T_DNUMBER</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/29241208/php-parse-error-syntax-error-unexpected-in"">Unexpected <code>,</code></a> <em>(comma)</em></p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/10969342/parse-error-syntax-error-unexpected-expecting-or"">Unpexected <code>.</code></a> <em>(period)</em></p>

</li>
<li><p><a href=""https://stackoverflow.com/questions/12961248/php-string-parse-error-with-necessary-semicolon-after-variable"">Unexpected <code>;</code></a> <em>(semicolon)</em></p>
</li>
<li><p><a href=""https://stackoverflow.com/q/32905365/3933332"">Unexpected <code>*</code></a> <em>(asterisk)</em></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/33178974/parse-error-unexpected-works-fine-in-localhost"">Unexpected <code>:</code></a> <em>(colon)</em></p>
</li>
<li><p><a href=""https://stackoverflow.com/a/65539862"">Unexpected ':', expecting ',' or ')'</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/4665782/php-warning-call-time-pass-by"">Unexpected <code>&amp;</code></a> (call-time pass-by-reference)</p>
</li>
<li><p><a href=""https://stackoverflow.com/a/61407635/250259"">Unexpected <code>.</code></a></p>
</li>
</ul>
<p>Closely related references:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/12769982/reference-what-does-this-error-mean-in-php"">What does this error mean in PHP? (runtime errors)</a>
<ul>
<li><a href=""https://stackoverflow.com/questions/12769982/reference-what-does-this-error-mean-in-php/12770089/#12770089"">Parse error: syntax error, unexpected T_XXX</a></li>
<li><a href=""https://stackoverflow.com/questions/12769982/reference-what-does-this-error-mean-in-php/13935532#13935532"">Parse error: syntax error, unexpected T_ENCAPSED_AND_WHITESPACE</a></li>
<li><a href=""https://stackoverflow.com/questions/12769982/reference-what-does-this-error-mean-in-php/15539535#15539535"">Parse error: syntax error, unexpected T_VARIABLE</a></li>
</ul>
</li>
<li><a href=""https://stackoverflow.com/questions/3737139/reference-what-does-this-symbol-mean-in-php"">What does this symbol mean in PHP? (language tokens)</a></li>
<li><a href=""https://stackoverflow.com/questions/14303353/double-quotes-are-not-copied-normally-how-can-i-edit-them"">Those <code>“”</code> smart  <code>‘’</code> quotes mean nothing to PHP</a></li>
</ul>
<p>And:</p>
<ul>
<li>The <a href=""http://www.php.net/manual/en/"" rel=""noreferrer"">PHP manual on php.net</a> and its various <a href=""http://php.net/tokens"" rel=""noreferrer"">language tokens</a></li>
<li>Or Wikipedia's <a href=""http://en.wikipedia.org/wiki/PHP_syntax_and_semantics"" rel=""noreferrer"">syntax introduction on PHP</a>.</li>
<li>And lastly our <a href=""https://stackoverflow.com/tags/php/info""><strong>php</strong> tag-wiki</a> of course.</li>
</ul>
<p>While Stack Overflow is also welcoming rookie coders, it's mostly targetted at professional programming questions.</p>
<ul>
<li>Answering everyone's coding mistakes and narrow typos is considered mostly off-topic.</li>
<li>So please take the time to follow the <a href=""https://stackoverflow.com/a/18050072"">basic steps</a>, before posting syntax fixing requests.</li>
<li>If you still have to, please show your own solving initiative, attempted fixes, and your thought process on what looks or might be wrong.</li>
</ul>
<p>If your <em>browser</em> displays error messages such as &quot;SyntaxError: illegal character&quot;, then it's not actually <a href=""/questions/tagged/php"" class=""post-tag"" title=""show questions tagged &#39;php&#39;"" rel=""tag"">php</a>-related, but a <a href=""/questions/tagged/javascript"" class=""post-tag"" title=""show questions tagged &#39;javascript&#39;"" rel=""tag"">javascript</a>-<a href=""https://stackoverflow.com/questions/2120093/how-to-find-javascript-syntax-errors"">syntax error</a>.</p>
<hr />
<p><strong>Syntax errors raised on vendor code:</strong> Finally, consider that if the syntax error was not raised by editing your codebase, but after an external vendor package install or upgrade, it could be due to PHP version incompatibility, so check the vendor's requirements against your platform setup.</p>
","<p>I am using docker-compose to create a zabbix container operating with Nginx and PostgreSQL but it generates the following error Parse error: </p>

<blockquote>
  <p>syntax error, unexpected ';' in /etc/zabbix/web/zabbix.conf.php on
  line 24.</p>
</blockquote>

<p>I do not understand this because it was working correctly a few minutes ago.</p>

<p>docker-compose.yaml</p>

<pre><code>version: '3.1'
services:
postgres:
image: postgres
restart: always
environment:
  POSTGRES_USER: zabbix
  POSTGRES_PASSWORD: zabbix
  POSTGRES_DB: zabbix
zabbix-server:
image: zabbix/zabbix-server-pgsql
restart: always
environment:
  DB_SERVER_HOST: postgres
  POSTGRES_USER: zabbix
  POSTGRES_PASSWORD: zabbix
  POSTGRES_DB: zabbix
depends_on:
  - postgres
zabbix-web:
image: zabbix/zabbix-web-nginx-pgsql
restart: always
environment:
  ZBX_SERVER_HOST: zabbix-server
  DB_SERVER_HOST: postgres
  POSTGRES_USER: zabbix
  POSTGRES_PASSWORD: zabbix
  POSTGRES_DB: zabbix
depends_on:
  - postgres
  - zabbix-server
ports:
  - 8081:80
</code></pre>
"
"18497564","Assigning vhosts to Docker ports","<nginx><proxy><dns><docker>","51684215","How to handle http requests to container","<docker><centos>","<p>I have a wildcard DNS set up so that all web requests to a custom domain (*.foo) map to the IP address of the Docker host.  If I have multiple containers running Apache (or Nginx) instances, each container maps the Apache port (80) to some external inbound port.</p>

<p>What I would like to do is make a request to container-1.foo, which is already mapped to the correct IP address (of the Docker host) via my custom DNS server, but proxy the default port 80 request to the correct Docker external port such that the correct Apache instance from the specified container is able to respond based on the custom domain.  Likewise, container-2.foo would proxy to a second container's apache, and so on.</p>

<p>Is there a pre-built solution for this, is my best bet to run an Nginx proxy on the Docker host, or should I write up a node.js proxy with the potential to manage Docker containers (start/stop/reuild via the web), or...?  What options do I have that would make using the Docker containers more like a natural event and not something with extraneous ports and container juggling?</p>
","<p>I have a centos7 server with docker installed. Currently i have 2 dockerised website in different containers, one (website1.com) listening on port 8081 and the other one (website2.com) on 8082. So port 80 of the server is not serving anything at them moment.</p>

<p>To access those websites i have to specify the right port number (e.g. website1.com:8081) so i want to accept any requests on port 80 and then serve the right website (e.g. website1.com should serve 0.0.0.0:8081)</p>

<p>How can i achieve that? And if possible, i need a solution that also offers a GUI.</p>
"
"18967441","Add a prefix to all Flask routes","<python><routes><flask>","50702471","Run flask app in container with url path","<docker><nginx><flask><dockerfile><uwsgi>","<p>I have a prefix that I want to add to every route.  Right now I add a constant to the route at every definition.  Is there a way to do this automatically?</p>

<pre><code>PREFIX = ""/abc/123""

@app.route(PREFIX + ""/"")
def index_page():
  return ""This is a website about burritos""

@app.route(PREFIX + ""/about"")
def about_page():
  return ""This is a website about burritos""
</code></pre>
","<p>I have a Flask app that I want to run as a container, and be able to load it on url path of my server, something like <code>http://example.com/flask</code>. I managed to get a Flask container up and running on the root url <code>http://example.com</code>, but when setting on the path all the urls within the pages remain like <code>&lt;a href=""/login""&gt;&lt;/a&gt;</code> instead of being <code>&lt;a href=""/flask/login""&gt;&lt;/a&gt;</code>. My setup is:</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM ubuntu:latest

RUN apt-get -y update

RUN apt-get -y install build-essential python3-dev python3-pip python3-setuptools python3-wheel uwsgi-plugin-python3 python3-cffi libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 libffi-dev shared-mime-info libmysqlclient-dev

RUN mkdir -p /etc/logs/webapp

ENV FLASK_APP=main.py
ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8

WORKDIR /app
ADD requirements.txt /app/requirements.txt
RUN python3 -m pip install -r requirements.txt
ADD . /app

ADD app.fcgi /app/app.fcgi
RUN chmod +x /app/app.fcgi
RUN python3 -m pip install flup

RUN apt-get -y install nginx
ADD nginx.conf /etc/nginx/conf.d/default.conf
RUN rm /etc/nginx/sites-enabled/*

ADD entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT [ ""/entrypoint.sh"" ]
</code></pre>

<p><strong>entrypoint.sh</strong></p>

<pre><code>#!/bin/sh
service nginx start
/app/app.fcgi &gt; /app/logs/app.log
</code></pre>

<p><strong>app.fcgi</strong></p>

<pre><code>#!/usr/bin/python3
from flup.server.fcgi import WSGIServer
from main import app

if __name__ == '__main__':
    WSGIServer(app, bindAddress='/var/run/app-fcgi.sock').run()
</code></pre>

<p><strong>nginx.conf</strong></p>

<pre><code>server {
  listen 80 default;

  server_name _;

  location /css {
    root /app/static/;
  }
  location /js {
    root /app/static/;
  }
  location /assets {
    root /app/static/;
  }

  location / { try_files $uri @app; }
  location @app {
    include uwsgi_params;
    uwsgi_pass unix:/var/run/app-fcgi.sock;
  }
}
</code></pre>

<p>I'm using nginx to serve the static files directly and uwsgi to bind the app to a socket. I'm new to Flask and trying to run a project someone else did so I have no idea if this is a good approach, but it doesn't work</p>
"
"20635472","Using the RUN instruction in a Dockerfile with 'source' does not work","<bash><shell><docker>","50749966","Sourcing (""dotting"") shell script from Docker","<linux><bash><shell><docker><dockerfile>","<p>I have a Dockerfile that I am putting together to install a vanilla python environment (into which I will be installing an app, but at a later date).</p>

<pre><code>FROM ubuntu:12.04

# required to build certain python libraries
RUN apt-get install python-dev -y

# install pip - canonical installation instructions from pip-installer.org
# http://www.pip-installer.org/en/latest/installing.html
ADD https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py /tmp/ez_setup.py
ADD https://raw.github.com/pypa/pip/master/contrib/get-pip.py /tmp/get-pip.py
RUN python /tmp/ez_setup.py
RUN python /tmp/get-pip.py
RUN pip install --upgrade pip 

# install and configure virtualenv
RUN pip install virtualenv 
RUN pip install virtualenvwrapper
ENV WORKON_HOME ~/.virtualenvs
RUN mkdir -p $WORKON_HOME
RUN source /usr/local/bin/virtualenvwrapper.sh
</code></pre>

<p>The build runs ok until the last line, where I get the following exception:</p>

<pre><code>[previous steps 1-9 removed for clarity]
...
Successfully installed virtualenvwrapper virtualenv-clone stevedore
Cleaning up...
 ---&gt; 1fc253a8f860
Step 10 : ENV WORKON_HOME ~/.virtualenvs
 ---&gt; Running in 8b0145d2c80d
 ---&gt; 0f91a5d96013
Step 11 : RUN mkdir -p $WORKON_HOME
 ---&gt; Running in 9d2552712ddf
 ---&gt; 3a87364c7b45
Step 12 : RUN source /usr/local/bin/virtualenvwrapper.sh
 ---&gt; Running in c13a187261ec
/bin/sh: 1: source: not found
</code></pre>

<p>If I <code>ls</code> into that directory (just to test that the previous steps were committed) I can see that the files exist as expected:</p>

<pre><code>$ docker run 3a87 ls /usr/local/bin
easy_install
easy_install-2.7
pip
pip-2.7
virtualenv
virtualenv-2.7
virtualenv-clone
virtualenvwrapper.sh
virtualenvwrapper_lazy.sh
</code></pre>

<p>If I try just running the <code>source</code> command I get the same 'not found' error as above. If I RUN an interactive shell session however, source does work:</p>

<pre><code>$ docker run 3a87 bash
source
bash: line 1: source: filename argument required
source: usage: source filename [arguments]
</code></pre>

<p>I can run the script from here, and then happily access <code>workon</code>, <code>mkvirtualenv</code> etc.</p>

<p>I've done some digging, and initially it looked as if the problem might lie in the difference between <strong>bash</strong> as the Ubuntu <em>login shell</em>, and <strong>dash</strong> as the Ubuntu <em>system shell</em>, <strong>dash</strong> not supporting the <code>source</code> command.</p>

<p>However, the answer to this appears to be to use <strong>'.'</strong> instead of <code>source</code>, but this just causes the Docker runtime to blow up with a go panic exception.</p>

<p>What is the best way to run a shell script from a Dockerfile RUN instruction to get around this (am running off the default base image for Ubuntu 12.04 LTS).</p>
","<p>I am trying to set up a docker image that downloads <code>Phantomjs</code> and sets up the alias in <code>~/.bash_rc</code> and path in <code>~/.profile</code>. Below is the <code>Dockerfile</code> that does this.</p>

<pre><code>FROM node:8.9.4

RUN mkdir -p /usr/src/app

RUN wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 \
    &amp;&amp; tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2

RUN mv phantomjs-2.1.1-linux-x86_64 /usr/src/app

WORKDIR /usr/src/app

COPY package*.json /usr/src/app/

RUN npm install

COPY . /usr/src/app

RUN  . usr/src/app/test.sh
RUN . ~/.bashrc
RUN . ~/.profile

EXPOSE 4219

CMD [""node_modules/casperjs/bin/casperjs"",""selftest""]
</code></pre>

<p>Also the script test.sh sets up the environment variable. Here are the contents of the script.</p>

<pre><code>  #!/bin/sh

   echo ""alias phantomjs='/usr/src/app/phantomjs-2.1.1-linux-x86_64/bin/phantomjs'"" &gt;&gt; ~/.bashrc

   echo ""PATH=\""$HOME/bin:$HOME/.local/bin:$PATH:/usr/src/app/phantomjs-2.1.1-linux-x86_64/bin\"""" &gt;&gt; ~/.profile
</code></pre>

<p>But it returns this error now</p>

<pre><code> Step 10/16 : RUN . ~/.profile
    ---&gt; Running in 87cdccc4fef7
    stdin: is not a tty
</code></pre>

<p>What should I do to make this script run?</p>
"
"22697688","How to cat <<EOF >> a file containing code?","<linux><unix><sh><heredoc>","50650401","How to exec a local binary in a script in docker image bash?","<bash><shell><docker>","<p>I want to print code into a file using <code>cat &lt;&lt;EOF &gt;&gt;</code>:</p>

<pre><code>cat &lt;&lt;EOF &gt;&gt; brightup.sh
!/bin/bash
curr=`cat /sys/class/backlight/intel_backlight/actual_brightness`
if [ $curr -lt 4477 ]; then
   curr=$((curr+406));
   echo $curr  &gt; /sys/class/backlight/intel_backlight/brightness;
fi
EOF
</code></pre>

<p>but when I check the file output, I get this:</p>

<pre><code>!/bin/bash
curr=1634
if [  -lt 4477 ]; then
   curr=406;
   echo   &gt; /sys/class/backlight/intel_backlight/brightness;
fi
</code></pre>

<p>I tried putting single quotes but the output also carries the single quotes with it. How can I avoid this issue?</p>
","<p>I want to use the official bash to run my unit tests but can't exec any local file in a script. Here is a minimal example what I want to do:</p>
<pre><code>$ docker run -it bash
bash-4.4# cd /usr/local/bin/
bash-4.4# ./bashbug --version
GNU bashbug, version 4.4.19-release
bash-4.4# echo \#\!/usr/local/bin/bash \
&gt; ./bashbug --version &gt; script.sh
bash-4.4# chmod a+x script.sh
bash-4.4# ./script.sh
/usr/local/bin/bash: ./bashbug --version: No such file or directory
bash-4.4#
</code></pre>
<p>I one of the installed programs (<code>/usr/local/bin/bashbug</code>) to illustrate the problem. As you can see, you can exec <code>./bashbug</code> directly, but not when it is executed by a bash script. How can I call it within <code>script.sh</code>?</p>
<h1>Edit</h1>
<p>The example above has a bug. This is fixed by the accepted answer.</p>
<p>I found out that my original problem was a problem with link dependencies, which outputs:</p>
<pre><code>./exec_test.sh: line 30: ./test: No such file or directory
</code></pre>
<p>When I call <code>ldd</code> on <code>test</code> I get:</p>
<pre><code>ldd test
    /lib64/ld-linux-x86-64.so.2 (0x7fe7ad476000)
    libdl.so.2 =&gt; /lib64/ld-linux-x86-64.so.2 (0x7fe7ad476000)
    libpthread.so.0 =&gt; /lib64/ld-linux-x86-64.so.2 (0x7fe7ad476000)
    libstdc++.so.6 =&gt; /path/bin/gcc-8.1.0/debug/libstdc++.so.6 (0x7fe7ac855000)
    libm.so.6 =&gt; /lib64/ld-linux-x86-64.so.2 (0x7fe7ad476000)
    libgcc_s.so.1 =&gt; /path/bin/gcc-8.1.0/debug/libgcc_s.so.1 (0x7fe7ac63d000)
    libc.so.6 =&gt; /lib64/ld-linux-x86-64.so.2 (0x7fe7ad476000)
    ld-linux-x86-64.so.2 =&gt; /path/bin/gcc-8.1.0/debug/ld-linux-x86-64.so.2 (0x7fe7ab49b000)
    libboost_system.so.1.67.0 =&gt; /path/bin/gcc-8.1.0/debug/libboost_system.so.1.67.0 (0x7fe7ab291000)
Error relocating /path/bin/gcc-8.1.0/debug/libstdc++.so.6: __cxa_thread_atexit_impl: symbol not found
Error relocating /path/bin/gcc-8.1.0/debug/libgcc_s.so.1: __cpu_indicator_init: symbol not found
Error relocating /path/bin/gcc-8.1.0/debug/libgcc_s.so.1: __cpu_model: symbol not found
</code></pre>
<p>Probably a <code>glibc</code> version issue.</p>
"
"23111631","Cannot download Docker images behind a proxy","<proxy><docker>","51440888","Cannot download a base image declared in Dockerfile due to proxy settings","<docker><dockerfile>","<p>I installed Docker on my Ubuntu 13.10 (Saucy Salamander) and when I type in my console:</p>

<pre><code>sudo docker pull busybox
</code></pre>

<p>I get the following error:</p>

<pre><code>Pulling repository busybox
2014/04/16 09:37:07 Get https://index.docker.io/v1/repositories/busybox/images: dial tcp: lookup index.docker.io on 127.0.1.1:53: no answer from server
</code></pre>

<p>Docker version:</p>

<pre><code>$ sudo docker version

Client version: 0.10.0
Client API version: 1.10
Go version (client): go1.2.1
Git commit (client): dc9c28f
Server version: 0.10.0
Server API version: 1.10
Git commit (server): dc9c28f
Go version (server): go1.2.1
Last stable version: 0.10.0
</code></pre>

<p>I am behind a proxy server with no authentication, and this is my <code>/etc/apt/apt.conf</code> file:</p>

<pre><code>Acquire::http::proxy ""http://192.168.1.1:3128/"";
Acquire::https::proxy ""https://192.168.1.1:3128/"";
Acquire::ftp::proxy ""ftp://192.168.1.1:3128/"";
Acquire::socks::proxy ""socks://192.168.1.1:3128/"";
</code></pre>

<p>What am I doing wrong?</p>
","<p>This might be a simple question, but anyway.</p>

<p>I have created a <code>Dockerfile</code>, where the first line is: <code>FROM python:2.7-slim</code>. This image is not in my local system, so it would download it from the repo, right? </p>

<p>However, my proxy settings do not allow me to download the image. So, my question is, is there any chance I could declare the proxy settings in order to download the base image?</p>

<p>The host machine runs Ubuntu 16.04.</p>
"
"23439126","How to mount a host directory in a Docker container","<docker><mount><boot2docker>","50775835","How to mount a folder from my host to a custom docker image, setup using Dockerfile in a directory?","<linux><docker><debian><containers>","<p>I am trying to mount a host directory into a Docker container so that any updates done on the host is reflected into the Docker containers.</p>

<p>Where am I doing something wrong. Here is what I did:</p>

<pre><code>kishore$ cat Dockerfile

FROM ubuntu:trusty
RUN apt-get update
RUN apt-get -y install git curl vim
CMD [""/bin/bash""]
WORKDIR /test_container
VOLUME [""/test_container""]
</code></pre>

<p><pre><code>kishore$ tree
.
├── Dockerfile
└── main_folder
    ├── tfile1.txt
    ├── tfile2.txt
    ├── tfile3.txt
    └── tfile4.txt</p>

<p>1 directory, 5 files
kishore$ pwd
/Users/kishore/tdock
</code></pre><pre><code>kishore$ docker build --tag=k3_s3:latest .</p>

<pre><code>Uploading context 7.168 kB
Uploading context
Step 0 : FROM ubuntu:trusty
 ---&gt; 99ec81b80c55
Step 1 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 1c7282005040
Step 2 : RUN apt-get -y install git curl vim
 ---&gt; Using cache
 ---&gt; aed48634e300
Step 3 : CMD [""/bin/bash""]
 ---&gt; Running in d081b576878d
 ---&gt; 65db8df48595
Step 4 : WORKDIR /test_container
 ---&gt; Running in 5b8d2ccd719d
 ---&gt; 250369b30e1f
Step 5 : VOLUME [""/test_container""]
 ---&gt; Running in 72ca332d9809
 ---&gt; 163deb2b1bc5
Successfully built 163deb2b1bc5
Removing intermediate container b8bfcb071441
Removing intermediate container d081b576878d
Removing intermediate container 5b8d2ccd719d
Removing intermediate container 72ca332d9809
</code></pre>

<p>kishore$ docker run -d -v /Users/kishore/main_folder:/test_container k3_s3:latest
<code>c9f9a7e09c54ee1c2cc966f15c963b4af320b5203b8c46689033c1ab8872a0ea</code></code></pre><pre><code>kishore$ docker run -i -t k3_s3:latest /bin/bash</p>

<pre><code>root@0f17e2313a46:/test_container# ls -al
total 8
drwx------  2 root root 4096 Apr 29 05:15 .
drwxr-xr-x 66 root root 4096 Apr 29 05:15 ..
</code></pre>

<p>root@0f17e2313a46:/test_container# exit
exit</code></pre><pre><code>kishore$ docker -v
Docker version 0.9.1, build 867b2a9</code></pre></p>

<ul>
<li>I don't know how to check boot2docker version</li>
</ul>

<p>Questions, issues facing:</p>

<ol>
<li>How do I need to link the main_folder to the test_container folder present inside the docker container?</li>
<li>I need to make this automatically. How do I to do that without really using the <code>run -d -v</code> command?</li>
<li>What happens if the boot2docker crashes? Where are the Docker files stored (apart from Dockerfile)?</li>
</ol>
","<p>My Dockerfile path: <code>/home/kshitij/docker/php_apache_5.6/Dockerfile</code></p>

<p>I have apache installed on my host pc and it has files in <code>/var/www/html</code> directory.</p>

<p>Now, I want to mount this directory from <strong>host pc</strong> (<code>/var/www/html</code>) to my <strong>docker container's</strong> <code>/var/www/html</code> directory.</p>

<p>How can this be done? Thanks!</p>
"
"29286307","x509: certificate signed by unknown authority - both with docker and with github","<github><docker><x509>","50766086","Docker don't run with proxy","<docker><authentication><virtualbox><linux-mint>","<p><code>docker build -t oreng/iojs .</code></p>

<pre><code>INFO[0000] Get https://index.docker.io/v1/repositories/library/iojs/images: x509: certificate signed by unknown authority. 
</code></pre>

<p>my Dockerfile is</p>

<pre><code>FROM iojs:latest
RUN useradd -ms /bin/bash developer
WORKDIR /home/developer
USER developer
</code></pre>

<p>Also <code>hub create</code> (using <a href=""https://github.com/github/hub"" rel=""noreferrer"">https://github.com/github/hub</a>)</p>

<pre><code>Post https://api.github.com/user/repos: x509: certificate signed by unknown authority 
</code></pre>
","<p>i need your help,</p>

<p>My docker don't run on my enterprise, I do not know what to do</p>

<pre><code>kaue default # docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
9bb5a5d4561a: Pulling fs layer 
docker: error pulling image configuration: Get https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/e3/e38bc07ac18ee64e6d59cf2eafcdddf9cec2364dfe129fe0af75f1b0194e0c96/data?verify=1528483070-KGbywXnskgTKu5B9AuTdFPQdYjs%3D: x509: certificate signed by unknown authority.
See 'docker run --help'
</code></pre>

<p>.</p>

<p>I have a Windows 7, and Authenticated proxy in my job...</p>
"
"31726407","Mount directory in Container and share with Host","<docker>","50780431","Sharing Docker Process Executables with hosts","<linux><docker><docker-compose>","<p>I thought I understood the docs, but maybe I didn't. I was under the impression that the <code>-v /HOST/PATH:/CONTAINER/PATH</code> flag is bi-directional. If we have file or directories in the container, they would be mirrored on the host giving us a way to retain the directories and files even after removing a docker container.</p>

<p>In the official MySQL docker images, this works. The <code>/var/lib/mysql</code> can be bound to the host and survive restarts and replacement of container while maintaining the data on the host.</p>

<p>I wrote a docker file for sphinxsearch-2.2.9 just as a practice and for the sake of learning and understanding, here it is:</p>

<pre><code>FROM debian

ENV SPHINX_VERSION=2.2.9-release

RUN apt-get update -qq &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -yqq\
    build-essential\
    wget\
    curl\
    mysql-client\
    libmysql++-dev\
    libmysqlclient15-dev\
    checkinstall

RUN wget http://sphinxsearch.com/files/sphinx-${SPHINX_VERSION}.tar.gz &amp;&amp; tar xzvf sphinx-${SPHINX_VERSION}.tar.gz &amp;&amp; rm sphinx-${SPHINX_VERSION}.tar.gz

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; ./configure --prefix=/usr/local/sphinx

EXPOSE 9306 9312

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; make

RUN cd sphinx-${SPHINX_VERSION} &amp;&amp; make install

RUN rm -rf sphinx-${SPHINX_VERSION}

VOLUME /usr/local/sphinx/etc
VOLUME /usr/local/sphinx/var
</code></pre>

<p>Very simple and easy to get your head wrapped around while learning. I am assigning the /etc &amp; /var directories from the sphinx build to the VOLUME command thinking that it will allow me to do something like <code>-v ~/dev/sphinx/etc:/usr/local/sphinx/etc -v ~/dev/sphinx/var:/usr/local/sphinx/var</code>, but it's not, instead it's overwriting the directories inside the container and leaving them blank. When i remove the -v flags and create the container, the directories have the expected files and they are not overwritten.</p>

<p>This is what I run to create the docker file after navigating to the directory that it's in: <code>docker build -t sphinxsearch .</code></p>

<p>And once I have that created, I do the following to create a container based on that image: <code>docker run -it --hostname some-sphinx --name some-sphinx --volume ~/dev/docker/some-sphinx/etc:/usr/local/sphinx/etc -d sphinxsearch</code></p>

<p>I really would appreciate any help and insight on how to get this to work. I looked at the MySQL images and don't see anything magical that they did to make the directory bindable, they used VOLUME.</p>

<p>Thank you in advance.</p>
","<p>This question is related to docker-compose not related to 
<a href=""https://stackoverflow.com/questions/31726407/mount-directory-in-container-and-share-with-host"">Mount directory in Container and share with Host</a>
, <a href=""https://stackoverflow.com/questions/40043887/how-to-mount-a-directory-inside-a-docker-container-on-linux-host"">How to mount a directory inside a docker container on Linux host</a> 
What i want is sharing docker executables in hosts machine (Mac), Suppose I have a path in docker container <code>/usr/bin/aria2c</code>, Want to share it to host, so it can be globally used by terminal or any app.</p>

<p>My Docker-compose.yml</p>

<pre><code>version: ""2.0""
services:
    slate:
        container_name: Aria
        image: abcminiuser/docker-aria2-with-webui
        restart: always
        ports:
          - 6800:6800
          - 6880:80
        environment:
          - UID=100
          - GID=1001
        volumes:
          - /Users/rahul/Downloads:/data  #Where Real Downloads Happen
          - /Users/rahul:/usr/bin/aria2c  #point exectable to home dir
</code></pre>

<p>Output :</p>

<pre><code>ERROR: for slate  Cannot start service slate: b'OCI runtime create failed: container_linux.go:348: starting container process caused ""process_linux.go:402: container init caused \\""rootfs_linux.go:58: mounting \\\\\\""/Users/rahul\\\\\\"" to rootfs \\\\\\""/var/lib/docker/overlay2/7e712f0c3f26b123000d74b9d7e4f6109e724eb8d30329c22925ba728793db12/merged\\\\\\"" at \\\\\\""/var/lib/docker/overlay2/7e712f0c3f26b123000d74b9d7e4f6109e724eb8d30329c22925ba728793db12/merged/usr/bin/aria2c\\\\\\"" caused \\\\\\""not a directory\\\\\\""\\"""": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type
</code></pre>

<p>I know file is not available in the hosts, But Is there a method to let boot the container and then symlink container executable to host.</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","51470803","Is there a docker-compose service initialization available in version 3?","<docker><docker-compose><devops><docker-swarm>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I was looking to set a initialization sequence for a docker-compose services to start. Like service 'web' should wait for 'db' to complete in docker-compose. </p>

<p>I guess previously 'depends_on' was used for this purpose but since version 3, 'depends_on' no longer imposes the docker services start-up sequence due to docker wants to start services at any moment when they fail independently. I guess that's a good reason to remove the previous waiting feature. </p>

<p>But, if that's true, what is the alternative for implementing initialization sequence where I don't want to start 'web' service until 'db' is fully initialization and completes the startup.</p>

<p>Please correct me if I'm wrong in my understanding about this topic.</p>
"
"33521891","From JPG to b64encode to cv2.imread()","<python><opencv><numpy>","50684393","How can I send an OpenCV-Image via json to an Flask Docker-Container?","<python><json><opencv><docker><flask>","<p>For a program I am writing, I am transferring an image from one computer - using base64.b64encode(f.read(image)) - and trying to read it in the receiving script without saving it to hard drive (in an effort to minimize process time). I'm having a hard time figuring out how to read the image into OpenCV without saving it locally.</p>

<p>Here is what my code for sending the image looks like:</p>

<pre><code>f = open(image.jpg)
sendthis = f.read()
f.close()
databeingsent = base64.b64encode(sendthis)
client.publish('/image',databeingsent,0) 
# this is an MQTT publish, details for SO shouldn't be relevant
</code></pre>

<p>Meanwhile, here is the code receiving it. (This is in an on_message function, since I'm using MQTT for the transfer.)</p>

<pre><code>def on_message(client, userdata, msg): # msg.payload is incoming data
    img = base64.b64decode(msg.payload)
    source = cv2.imread(img)
    cv2.imshow(""image"", source)
</code></pre>

<p>After the message decodes, I have the error:
""TypeError: Your input type is not a numpy array"". </p>

<p>I've done some searching, and I can't seem to find a relevant solution - some exist regarding converting from text files to numpy using b64, but none really relate to using an image and immediately reading that decoded data into OpenCV without the intermediary step of saving it to the harddrive (using the inverse process used to read the file in the ""send"" script).</p>

<p>I'm still pretty new to Python and OpenCV, so if there's a better encoding method to send the image - whatever solves the problem. How the image is sent is irrelevant, so long as I can read it in on the receiving end without saving it as a .jpg to disk.</p>

<p>Thanks!</p>
","<p>I followed Miguel Grinbergs Tutorial to have a flask video streaming server with OpenCV (see here <a href=""https://blog.miguelgrinberg.com/post/flask-video-streaming-revisited"" rel=""nofollow noreferrer"">https://blog.miguelgrinberg.com/post/flask-video-streaming-revisited</a>). I got it up and running in a Docker Container and it works fine. Now I wanted to have to video source in another Docker-Container, because I don't want to have OpenCV in the flask web container. 
Now I've got a problem with the Json-Serialization. 
What I thought is the best way, is to encode the OpenCV-image to bytes and base64encode it, then dump it in json and send it to the flask-container, where I load the json and base64-decode the image. 
But that doesn't work. In some frames I get an padding-error, in some no errors, but the frame isn't shown in the browser. </p>

<p>Does someone has an idea? </p>

<pre><code>def frames():
    camera = cv2.VideoCapture(Camera.video_source)
    _, img = camera.read()
    img_bytes = cv2.imencode('.jpg', img)[1].tobytes()
    img_as_text = base64.b64encode(img_bytes)
    data = {""image"": str(img_as_text)}
    data_json = json.dumps(data)
    new_json = json.loads(data_json)
    im = new_json['image']
    img_original = base64.b64decode(im)
    yield img_original
</code></pre>
"
"35296541","NameError when splitting app and views code","<python><flask>","51484207","Flask development environment breaks when importing views.py","<python><docker><ubuntu><flask>","<p>I want to divide my code into two parts, the app initialization and the view definitions.  I import my views and define my app, but I get <code>NameError: name 'manager' is not defined</code>.  Why am I getting this error?  How do I split up my code correctly?</p>

<p><code>manage.py</code>:</p>

<pre><code>from flask import Flask,render_template
from flask.ext.script import Manager

import viewports

manager = Flask(__name__)

if __name__=='__main__':
     manager.run()
</code></pre>

<p><code>viewports.py</code></p>

<pre><code>@manager.route('/')
def Home():
     return render_template('Home.html', title='FrontPage')
</code></pre>
","<p>So I have this web app running in a Ubuntu 16.04 VM using Docker Compose, and it has four containers:
Elasticsearch, Node (react fronend), Nginx, Flask (backend)</p>

<p>For Flask, I have to files: <strong>init</strong>.py and views.py</p>

<p>When I run the Flask container in development mode, everything works. If I make a change to any of the files, it reloads properly. The moment I add this line to <strong>init</strong>.py: ""import flaskr.views"" or anything similar that imports views.py, the reloader breaks. It never reloads, just freezes and crashes. </p>

<p>Flask is the latest version and Python is 3.6.2.</p>

<p>Any help would be appreciated! Thanks</p>
"
"35864227","How to populate zone tables in mysql database within ubuntu with xampp","<mysql><ubuntu><xampp>","51154996","Docker MySQL server failed","<mysql><linux><docker><arm><debian>","<p>I am trying to import time zones according to this document: <a href=""http://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html"" rel=""nofollow"">http://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html</a>.</p>

<p>When I try hitting even first command through terminal i.e. </p>

<pre><code>mysql_tzinfo_to_sql tz_dir
</code></pre>

<p>it says</p>

<pre><code>There were fatal errors during processing of zoneinfo directory 'tz_dir'
</code></pre>

<p>When I run:</p>

<pre><code>mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysql
</code></pre>

<p>then it returns </p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock'
</code></pre>

<p><strong>Operating System:</strong> Ubuntu</p>

<p><strong>Server:</strong> XAMPP (having mariadb)</p>

<p>So, I am not able to import timezone data into the zone tables of <code>mysql</code> database.</p>
","<p>I have installed MySQL Server, but when I am running container, I will get error:</p>

<pre><code>docker run -it \
-e 'MYSQL_ROOT_PASSWORD=password' \
-e 'MYSQL_DATABASE=identity' \
-e 'MYSQL_USER=identity' \
-e 'MYSQL_PASSWORD=password' \
-p '3306:3306' \
--name example_mysql \
beercan1989/arm-mysql:latest\
</code></pre>

<p>The output is ""There were fatal errors during processing of zoneinfo directory"".</p>

<p>I am using debian OS.</p>

<p>Does someone had the same?
Thanks.</p>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","51431604","Best way to run cron in docker container","<linux><docker><cron><docker-compose>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>Suppose we have to run a script every 15 mins with tools that already exist in our app container (e.g. php, node, etc.) What is the best way to do this?</p>

<p>For example:</p>

<ol>
<li>Run cron within the app container </li>
<li>Run cron as a separate container</li>
<li>Run cron on the host (using docker exec)</li>
</ol>

<p><strong><em>UPDATES</em></strong></p>

<p>Most of the answers to this question seem to only address <code>SYSTEM crons</code> which are installed either in <code>/etc/cron.d/</code> or <code>/etc/crontab</code> on the Ubuntu 16.04 base image however I'm more interested in USER crons which normally reside in <code>/var/spool/cron/crontabs</code> for ALL <code>users crons</code> or I believe in <code>/home/user/crontab(not too sure)</code></p>

<p>I am running my container as a <code>non-root user</code> and need to be able to do <code>RUN crontab myCrontab</code> and have <code>myCrontab</code> under that <code>non-root user within my container</code> and NOT the usual system path listed earlier above.</p>

<p>Hope that's clearer and not identified as a duplicate of the other popular question. </p>

<p>Any ideas ?</p>
"
"42130132","Can Cargo download and build dependencies without also building the application?","<rust><rust-cargo>","50832375","How do I cache Rust crates locally when using Docker?","<docker><rust><rust-cargo>","<p>Is there a way to tell <a href=""http://doc.crates.io/guide.html"" rel=""noreferrer"">Cargo</a> to install and build all my dependencies, but not attempt to build my application?</p>

<p>I thought <code>cargo install</code> would do that, but it actually goes all the way to building my app too. I want to get to a state where <code>cargo build</code> would find all dependencies ready to use, but without touching the <code>/src</code> directory.</p>

<hr>

<p><strong>What I'm really trying to accomplish:</strong></p>

<p>I'm trying to build a Docker image for a Rust application, where I'd like to do the following steps:</p>

<p>Build time (<code>docker build .</code>):</p>

<ol>
<li>import a docker image with rust tooling installed</li>
<li>add my Cargo.toml and Cargo.lock files</li>
<li>download and build all dependencies</li>
<li>add my source directory to the image</li>
<li>build my source code</li>
</ol>

<p>Run time (<code>docker run ...</code>):</p>

<ol>
<li>run the application</li>
</ol>

<p>I've tried the following <code>Dockerfile</code>, but the indicated step builds my application as well (which of course fails since the source directory isn't there yet):</p>

<pre><code>FROM jimmycuadra/rust

ADD Cargo.toml /source
ADD Cargo.lock /source

RUN cargo install # &lt;-- failure here

ADD src /source/src
RUN cargo build

ENTRYPOINT cargo run
</code></pre>

<p>The reason I want to separate the install dependencies step from actually building my application, is that if I don't change the dependencies, I want Docker to be able use a cached image with all dependencies already installed and built. Thus, I can't <code>ADD /src /source/src</code> until <em>after</em> installing the dependecies, as that would invalidate the cached image when I change my own code.</p>
","<p>When using Cargo to build my Rust project, it downloads and compiles all the crates each time I trigger  <code>cargo build</code>.</p>

<p>Is there no way to cache these libraries and speedup my build process?</p>

<p>I am running this on an Ubuntu 16.04 machine and using Docker to run my builds in. I guess there needs to be some mounted directory to share across builds which could solve my problem. </p>
"
"42238934","Ansible shows error: ""One or more undefined variables: 'item' is undefined"" when using 'with_items'","<ansible>","51112326","ansible docker mount multiple port","<docker><ansible>","<p>I am trying to count the instances inside an elb. This is my Ansible playbook:</p>

<pre><code>- name: Get elb facts
  local_action:
    module: ec2_elb_facts
    name: ""{{elb}}""
    region: ""{{ansible_ec2_placement_region}}""
  environment: creds
  register: elb_facts

- debug:
    var: elb_facts
    verbosity: 2

- debug:
    msg: ""Instance: {{ item.instances }}""
    with_items: ""{{ elb_facts.elbs }}""
</code></pre>

<p>and my output (sensitive data removed):</p>

<pre><code>TASK: [debug ] ****************************************************************
ok: [10.0.0.0] =&gt; {
    ""elb_facts"": {
        ""changed"": false,
        ""elbs"": [
            {
                ""availability_zones"": [
                    ""ap-southeast-2b"",
                    ""ap-southeast-2a""
                ],
                ""dns_name"": ""elbname123.ap-southeast-2.elb.amazonaws.com"",
                ""health_check"": {
                    ""healthy_threshold"": 2,
                    ""interval"": 10,
                    ""target"": ""TCP:0000"",
                    ""timeout"": 5,
                    ""unhealthy_threshold"": 2
                },
                ""instances"": [
                    {
                        ""id"": ""i-000000000000000"",
                        ""state"": null
                    }
                ],
                ""name"": ""accessgateway"",
                ""scheme"": ""internal"",
                ""security_groups"": [
                    ""sg-00000000""
                ],
                ""subnet"": [
                    ""subnet-0000000"",
                    ""subnet-1111111""
                ],
                ""vpc_id"": ""vpc-000000""
            }
        ],
        ""invocation"": {
            ""module_args"": """",
            ""module_name"": ""ec2_elb_facts""
        }
    }
}

TASK: [debug ] ****************************************************************
fatal: [10.0.0.0] =&gt; One or more undefined variables: 'item' is undefined

FATAL: all hosts have already failed -- aborting
</code></pre>

<p>So what im trying to do is just loop through and print everything inside the elb_facts, instances variable. From what I can tell it's a hash, containing a list of hashes.</p>

<p>I am using <a href=""http://docs.ansible.com/ansible/playbooks_loops.html#looping-over-subelements"" rel=""nofollow noreferrer"">http://docs.ansible.com/ansible/playbooks_loops.html#looping-over-subelements</a> as a reference. I cannot for the life of mine figure out why this is not working.</p>
","<p>ansible version</p>

<pre><code>ansible --version
ansible 2.5.5
</code></pre>

<p>docker version</p>

<pre><code>docker --version
Docker version 18.03.1-ce, build 9ee9f40
</code></pre>

<p>my Examples</p>

<pre><code>- name: start container
  docker_container:
    name: ""tomcat-container""
    image: ""tomcat-images""
    state: started
    ports:
      - ""{{ item[0]}}:{{ item[1] }}""
    with_nested:
      - [8080,8080]
      - [8081,8081]
</code></pre>

<p>FAILED! => {""msg"": ""The task includes an option with an undefined variable. The error was: 'item' is undefined\n\nThe error appears to have been in '/home/playbook/roles/ts-docker/tasks/main.yml': line 81, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: start container\n  ^ here\n""}</p>

<p>...'item' is undefined </p>

<p>How to solve the problem of mounting multiple port?</p>

<p>-_-||  English is not good, forgive me</p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","51035442","Docker volumn path in windows","<docker>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I am relatively new to docker. I want to use a database with volume to persist. I am in windows 10.</p>

<p>I want to check where the volumns are created in my machine.</p>

<p>When i run the command</p>

<pre><code>C:\Users\satul&gt;docker volume inspect 368984d12c3525d8752d249347cfd563afb46c847e1c109afa9785bf54b89701 [
    {
        ""CreatedAt"": ""2018-06-25T22:43:29Z"",
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/368984d12c3525d8752d249347cfd563afb46c847e1c109afa9785bf54b89701/_data"",
        ""Name"": ""368984d12c3525d8752d249347cfd563afb46c847e1c109afa9785bf54b89701"",
        ""Options"": null,
        ""Scope"": ""local""
    } ]
</code></pre>

<p>Since this is a windows box, i donot have folder /var/lib/docker/volumes/. Where exactly is the volumn folder in windows so that i can backit up if required.</p>
"
"45712122","connection string for sqlserver in Docker container","<macos><docker><connection-string><asp.net-core-1.1>","51180104","How to setup .Net Core database connection to use MS SQL Server running in Docker container?","<.net><sql-server><macos><docker><asp.net-core-2.0>","<p>I'm using Visual Studio 2017 for mac with dotnet Core and EF Core. After setting up the mssql image in Docker container , I was trying to add the connection string  but throwing connection error. I tried with different options such as ip address , container name , host name etc. as server name but none of them worked. </p>

<pre><code> ""Default"": ""Server=172.17.0.2; Database=ERPDb; User=sa; Password =******;""
</code></pre>

<p>with container name </p>

<pre><code> ""Default"": ""Server=ecstatic_hermann; Database=ERPDb; User=sa; Password=******;""
</code></pre>

<p>with hostname :</p>

<pre><code> ""Default"": ""Server=f45840a59623; Database=ERPDb; User=sa; Password=******;""
</code></pre>

<p>While connecting through using localhost in Terminal its successfully connecting </p>

<pre><code>$ mssql -s localhost -p Technocrat123
Connecting to localhost...done

sql-cli version 0.6.2
Enter "".help"" for usage hints.
</code></pre>

<p>But when running the application the connection fails.</p>

<p>Appreciate any help. Thanks in advance.</p>

<p>If using localhost then error is </p>

<pre><code>Login failed for user ''. Reason: An attempt to login using SQL authentication failed. Server is configured for Integrated authentication only.
</code></pre>
","<p>I have an existing .Net Core 2.0 application using LocalDB for database. I work on macOS and cannot use LocalDB as far as I understand. I have seen recommendation to use MS SQL Server running in Docker container, but I haven't managed to configure connection string properly.</p>

<p>Existing connection string:</p>

<pre><code>""DefaultConnection"": ""Server=(localdb)\\mssqllocaldb;Database=My.Application;Trusted_Connection=True;MultipleActiveResultSets=true"",
</code></pre>

<p>I am trying to connect to MSSQL on Docker with:</p>

<pre><code>""DefaultConnection"": ""Data Source=localhost; Initial Catalog=My.Application;User id=foo; password=bar;"",
</code></pre>

<p>or</p>

<pre><code>""DefaultConnection"": ""Server=localhost;Database=My.Application;User=foo;Password=bar;"",
</code></pre>

<p>resulting in error:</p>

<pre><code>Unhandled Exception: System.PlatformNotSupportedException: LocalDB is not supported on this Platform.
   at System.Data.SqlClient.SNI.LocalDB.GetLocalDBConnectionString(String localDbInstance)
   at System.Data.SqlClient.SNI.SNIProxy.GetLocalDBDataSource(String fullServerName, Boolean&amp; error)
   at System.Data.SqlClient.SNI.SNIProxy.CreateConnectionHandle(Object callbackObject, String fullServerName, Boolean ignoreSniOpenTimeout, Int64 timerExpire, Byte[]&amp; instanceName, Byte[]&amp; spnBuffer, Boolean flushCache, Boolean async, Boolean parallel, Boolean isIntegratedSecurity)
   at System.Data.SqlClient.SNI.TdsParserStateObjectManaged.CreatePhysicalSNIHandle(String serverName, Boolean ignoreSniOpenTimeout, Int64 timerExpire, Byte[]&amp; instanceName, Byte[]&amp; spnBuffer, Boolean flushCache, Boolean async, Boolean parallel, Boolean isIntegratedSecurity)
   ...
</code></pre>

<p>How do I make it work? Do I need to use different packages?</p>
"
"47837149","Build Docker with Go app: cannot find package","<docker><go>","50865880","Building golang project in docker - cannot find package in any of $GOPATH or $GOROOT","<docker><go><dockerfile><grafana>","<p>I have my Dockerfile in the root of directory with <code>src/myapp</code> folder, <code>myapp</code> contains <code>myapp.go</code> with main package.</p>

<p><code>Dockerfile</code> looks like following:</p>

<pre><code>FROM golang:1.9.2

ADD . /
RUN go build myapp;

ENTRYPOINT [""/go/bin/myapp""]
</code></pre>

<p>I get following error:</p>

<pre><code>can't load package: package myapp: cannot find package ""myapp"" in any of:
    /usr/local/go/src/myapp (from $GOROOT)
    /go/src/myapp (from $GOPATH)
</code></pre>

<p>What am I doing wrong? Can I log <code>ls</code> command after docker has done <code>ADD</code>?</p>
","<p>I have a project with the path <code>/Users/me/Documents/dev/grafana/src/github.com/grafana/grafana</code>. This project uses several other projects, for example:</p>

<pre><code>/Users/me/Documents/dev/grafana/src/github.com/BurntSushi/toml
/Users/me/Documents/dev/grafana/src/github.com/Unknwon/com
</code></pre>

<p>I can build everything fine on my machine, but when I try to build within Docker, I get a bunch of <code>cannot find package</code> errors.</p>

<pre><code>go install -v ./pkg/cmd/grafana-server
pkg/login/ldap_settings.go:7:2: cannot find package ""github.com/BurntSushi/toml"" in any of:
/usr/local/go/src/github.com/BurntSushi/toml (from $GOROOT)
/go/src/github.com/BurntSushi/toml (from $GOPATH)
pkg/services/notifications/codes.go:9:2: cannot find package ""github.com/Unknwon/com"" in any of:
/usr/local/go/src/github.com/Unknwon/com (from $GOROOT)
/go/src/github.com/Unknwon/com (from $GOPATH)
</code></pre>

<p>When I build myself, I have <code>$GOPATH=/Users/me/Documents/dev/grafana/</code> -- in my Dockerfile I have:</p>

<pre><code>FROM golang:latest AS build

RUN go version

ENV SRC_DIR=/go/src/github.com/grafana/grafana/
ENV GIT_SSL_NO_VERIFY=1

COPY . $SRC_DIR
WORKDIR $SRC_DIR

[... dependency installations ...]

# Building of Grafana
RUN npm run build
RUN go run build.go setup
RUN go run build.go build
</code></pre>

<p>I can't figure out why this step (Wich starts in the <code>RUN go run build.go setup</code> step) keeps reporting that it can't access the packages.</p>

<p>I've looked around for similar questions, but almost everything related doesn't specify building in Docker (and the ones that do aren't super helpful for this scenario).</p>
"
"48749200","run apt-get with proxy in Dockerfile","<docker><proxy><environment-variables><aptitude>","50812033","How to distinguish between Build and Run in the Dockerfile?","<docker><docker-compose><dockerfile>","<p>I am behind a proxy an i need to install something via <code>apt-get</code>.</p>

<p>The best I came with is this</p>

<pre><code>ARG PROXY
ENV http_proxy=$PROXY
ENV https_proxy=$PROXY
RUN apt-get update -y &amp;&amp; apt-get -y install ...
ENV http_proxy=
ENV https_proxy=
</code></pre>

<p>The thing is that I need to unset those environment variables afterwards.</p>

<p>Any idea how to do it in less then 5 layers?</p>
","<p>I need to set a proxy environment variable during the build of a container. But the proxy environment variable must not be set, when the container runs.</p>

<p>I have used the ENV command in the Dockerfile to set the variable. But this sets the variable during the build and the run of the container. As far as I can see the Dockerfile does not support a unset for ENV commands.</p>

<p>How to specify an environment for the build but not for the run?</p>
"
"49019652","not able to connect to mysql docker from local","<mysql><docker>","50616051","peewee connect docker-mysql failed","<python><mysql><docker><peewee>","<p>I am trying to connect to mysql database from docker image. However it's throwing errors. </p>

<p>following is the docker image I am using. 
<a href=""https://hub.docker.com/_/mysql/"" rel=""noreferrer"">https://hub.docker.com/_/mysql/</a></p>

<p>And following is the command I have used to run the docker image. </p>

<pre><code>docker run -p 3306:3306 --name mysql_80 -e MYSQL_ROOT_PASSWORD=password -d mysql:8
</code></pre>

<p>Following is the output of <code>docker ps</code> command </p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                              NAMES
9f35d2e39476        mysql:8             ""docker-entrypoint.s…""   5 minutes ago       Up 5 minutes        0.0.0.0:3306-&gt;3306/tcp
</code></pre>

<p>if I check the IP using docker inspect and ping that IP, it shows IP is not reachable. </p>

<pre><code>docker inspect 9f35d2e39476 | grep -i ipaddress
</code></pre>

<p>And if i try to connect using <code>localhost</code> and <code>127.0.0.1</code> I am getting following error. </p>

<blockquote>
  <p>Unable to load authentication plugin 'caching_sha2_password'.</p>
</blockquote>
","<p>I created a mysql docker image as:</p>

<pre><code>docker run \
        --name mysql \
        -v /Users/pivdet/Downloads/backup/docker_mysql/conf.d:/etc/mysql/conf.d \
        -v /Users/pivdet/Downloads/backup/docker_mysql/initdb.d:/docker-entrypoint-initdb.d \
        -v /Users/pivdet/Downloads/backup/docker_mysql/data:/var/lib/mysql \
        -e MYSQL_ROOT_PASSWORD=piv123 \
        -p 3306:3306 \
        -d mysql
</code></pre>

<p>And started the server by <code>docker start mysql</code></p>

<p>I could connect to the server in terminal: <code>$ mysql -h127.0.0.1 -P3306 -uroot -ppiv123</code></p>

<p>But when I tried to connect it by using peewee:</p>

<pre><code>import peewee

mysql_db = peewee.MySQLDatabase(""websdl"", host=""127.0.0.1"", port=3306, user=""root"", passwd=""piv123"")

if __name__ == ""__main__"":
    mysql_db.connect()
</code></pre>

<p>There's an error:</p>

<pre><code>Traceback (most recent call last):
  File ""models.py"", line 6, in &lt;module&gt;
    mysql_db.connect()
...
peewee.OperationalError: (1045, u""Access denied for user 'root'@'172.17.0.1' (using password: NO)"")
</code></pre>

<p>The user information of my db would be:</p>

<pre><code>mysql&gt; select Host, User from user;
+-----------+------------------+
| Host      | User             |
+-----------+------------------+
| %         | root             |
| localhost | mysql.infoschema |
| localhost | mysql.session    |
| localhost | mysql.sys        |
| localhost | root             |
+-----------+------------------+
5 rows in set (0.01 sec)
</code></pre>
"
"50278632","What does localhost means inside a Docker container?","<linux><docker>","51680070","docker-compose and localhost","<docker><docker-compose>","<p>Say, if I use this command inside a docker container.</p>

<pre><code>/opt/lampp/bin/mysql -h localhost -u root -pThePassword
</code></pre>

<p>What would the localhost here refer to? The host machine's IP or the docker container's own IP? </p>
","<p>I have a java project and I am using docker-compose to run some tests.</p>

<p>In this project, I have a sample.properties file that are being used during the test and it has the following fields:</p>

<pre><code>database_name = some.address
another_db_name = localhost
</code></pre>

<p>My compose file looks like this:</p>

<pre><code> services:
        some.address:
           # this is my database
        ...
        networks:
         - network1

        localhost:
           # this does not work

         test:
           # runs test
         networks:
          - network1
</code></pre>

<p>In this set up, my test will correctly use the database_name for test, but not for localhost. From what I searched, localhost refers container itself in this context.</p>

<p>I have tried creating aliases but it does not work. In addition, having networks and network_mode = host together in a service is not allowed.</p>

<p>The only way I could get my test to work is to change localhost in properties file to something else, eg) localhost1. Is it possible to somehow refer to a service as ""localhost"" while being connected to network1? (eg, by not changing my property file).</p>

<p>Thank you.</p>
"
"50387952","How to resolve Unable to load authentication plugin 'caching_sha2_password' issue","<java><mysql><eclipse><maven><spring-boot>","50552319","caching_sha2_password Connection closed by foreign host","<java><mysql><docker>","<p>In eclipse when i started my application i got this - Could not discover the dialect to use. java.sql.SQLException: Unable to load authentication plugin 'caching_sha2_password'.</p>

<blockquote>
  <p>at java.sql.SQLException: Unable to load authentication plugin
  'caching_sha2_password'.  at  at
  com.mysql.jdbc.SQLError.createSQLException(SQLError.java:868)     at  at
  com.mysql.jdbc.SQLError.createSQLException(SQLError.java:864)     at  at
  com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1746)
    at  at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1226)    at  at
  com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2191)
    at  at
  com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2222)
    at  at
  com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2017)
    at  at com.mysql.jdbc.ConnectionImpl.(ConnectionImpl.java:779)
    at  at com.mysql.jdbc.JDBC4Connection.(JDBC4Connection.java:47)
    at  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
  Method)   at  at
  sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at  at
  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown
  Source)   at  at java.lang.reflect.Constructor.newInstance(Unknown
  Source)   at  at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at  at
  com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389)    at
    at
  com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330)
    at  at java.sql.DriverManager.getConnection(Unknown Source)     at  at
  java.sql.DriverManager.getConnection(Unknown Source)  at  at
  ch.qos.logback.core.db.DriverManagerConnectionSource.getConnection(DriverManagerConnectionSource.java:54)
    at  at
  ch.qos.logback.core.db.ConnectionSourceBase.discoverConnectionProperties(ConnectionSourceBase.java:46)
    at  at
  ch.qos.logback.core.db.DriverManagerConnectionSource.start(DriverManagerConnectionSource.java:38)
    at  at
  ch.qos.logback.core.joran.action.NestedComplexPropertyIA.end(NestedComplexPropertyIA.java:161)
    at  at
  ch.qos.logback.core.joran.spi.Interpreter.callEndAction(Interpreter.java:309)
    at  at
  ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:193)
    at  at
  ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:179)
    at  at
  ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:62)
    at  at
  ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:165)
    at  at
  ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:152)
    at  at
  ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:110)
    at  at
  ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:53)
    at  at
  ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:75)
    at  at
  ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:150)
    at  at
  org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:84)    at
    at
  org.slf4j.impl.StaticLoggerBinder.(StaticLoggerBinder.java:55)
    at  at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)     at  at
  org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
    at  at
  org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)     at
    at
  ch.qos.logback.classic.util.StatusViaSLF4JLoggerFactory.addStatus(StatusViaSLF4JLoggerFactory.java:32)
    at  at
  ch.qos.logback.classic.util.StatusViaSLF4JLoggerFactory.addInfo(StatusViaSLF4JLoggerFactory.java:20)
    at  at
  ch.qos.logback.classic.servlet.LogbackServletContainerInitializer.onStartup(LogbackServletContainerInitializer.java:32)
    at  at
  org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5245)
    at  at
  org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
    at  at
  org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1421)
    at  at
  org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1411)
    at  at java.util.concurrent.FutureTask.run(Unknown Source)  at  at
  java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)     at
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at  at java.lang.Thread.run(Unknown Source)</p>
</blockquote>
","<p>I create the mysql container in docker with url:</p>

<blockquote>
  <p>jdbc:mysql://172.17.0.2:3306/'databaseName'</p>
</blockquote>

<p>172.17.0.2 is the ip address of docker container.
I create the user with:</p>

<pre><code> CREATE USER 'saman'@'%' IDENTIFIED BY 'password';
 GRANT ALL PRIVILEGES ON *.* TO 'saman'@'%'  WITH GRANT OPTION;
</code></pre>

<p>when i want to connect from intllje Ide i connect successfully, but when i want to connect from my java application i got the exception :</p>

<pre><code> Caused by: com.mysql.cj.core.exceptions.WrongArgumentException: Unable 
to load authentication plugin 'caching_sha2_password'
</code></pre>

<p>also i changed the default authentication password from <strong>caching_sha2_password</strong> to <strong>mysql_native_password</strong> in mysql users.
for more information when i use telnet command:</p>

<blockquote>
  <p><code>telnet 172.17.0.2 3306</code> <strong>return:</strong>
  caching_sha2_passwordConnection closed by foreign host.</p>
</blockquote>

<p>Thanks in advanced.</p>
"
"50580736","DOCKER: Linux Container on Windows 10, how to use nmap to scan device's mac address","<docker><nmap><docker-networking>","50621105","Way to assign Linux container to the same LAN as host?","<docker><docker-networking><docker-for-windows><linux-containers>","<p>I am trying to setup the docker which can successfully scan the subnet device's mac address by using nmap. And I've spent 3 days to figure out how to do it but still failed.</p>

<p>For example:</p>

<ul>
<li>The host IP: 10.19.201.123</li>
<li>The device IP: 10.19.201.101</li>
</ul>

<p>I've setup docker container which can ping 10.19.201.123 and 10.19.201.101 both successfully. But when I use nmap to scan mac address from docker container, I got below:</p>

<pre><code>~$sudo nmap -sP 10.19.201.101
 Starting Nmap 7.01 ( https://nmap.org ) at 2018-05-29 08:57 UTC
 Nmap scan report for 10.19.201.101
 Host is up (0.00088s latency).
 Nmap done: 1 IP address (1 host up) scanned in 0.39 seconds
</code></pre>

<p>However, if I use nmap to scan mac address from VM (10.19.201.100), I got:</p>

<pre><code>~$sudo nmap -sP 10.19.201.101
 Starting Nmap 7.01 ( https://nmap.org ) at 2018-05-29 17:16 CST
 Nmap scan report for 10.19.201.101
 Host is up (0.00020s latency).
 MAC Address: 0F:01:H5:W3:0G:J5(ICP Electronics)
 Nmap done: 1 IP address (1 host up) scanned in 0.32 seconds
</code></pre>

<p>PLEASE, who can help or give prompts of how to do it?</p>
","<p>My goal is to make my Linux container live on the same lan as host and other devices.
Because I need to use nmap frequently to scan the devices mac address on the lan. Unfortunately, the nmap scanning is only working when these machines all live on the same subnet.</p>

<p>I've tried several ways to make it happen, but all failed.
Although there are lots of instructions about how to do this, seem like they are all for Docker for Linux.</p>

<p>For example, a very detailed instructions from stackoverflow:
<a href=""https://stackoverflow.com/questions/43240377/docker-on-centos-with-bridge-to-lan-network"">Docker on CentOS with bridge to LAN network</a> is also not working for me.</p>

<hr>

<p>Things I've tried:</p>

<h1>Macvlan:</h1>

<p>it seems like Docker for Windows 10 doesn't support macvlan due to I have no way to make Windows network adapter as parent..</p>

<h1>Pipework:</h1>

<p>which is only working on Linux system but I am using Windows 10..</p>

<h1>Modify bip from daemon.json:</h1>

<p>I tried, which will set docker0 to static IP then container is still not able to ping devices on the LAN. I guess it's because the container is placed at NAT and change docker0 bridge ip won't be able to achieve my goal.</p>

<h1>Run image with --net host:</h1>

<p>which ifconfig shows:</p>

<pre><code>docker0   Link encap:Ethernet  HWaddr 02:42:2d:b8:0b:7c
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0
          inet6 addr: fe80::42:2dff:feb8:b7c/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:540 (540.0 B)

eth0      Link encap:Ethernet  HWaddr 02:50:00:00:00:01
          inet addr:192.168.65.3  Bcast:192.168.65.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:111 errors:0 dropped:0 overruns:0 frame:0
          TX packets:147 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:9701 (9.7 KB)  TX bytes:10384 (10.3 KB)

hvint0    Link encap:Ethernet  HWaddr 00:15:5d:0d:52:27
          inet addr:10.0.75.2  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::215:5dff:fe0d:5227/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:359819 errors:0 dropped:1303 overruns:0 frame:0
          TX packets:1157 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:54740692 (54.7 MB)  TX bytes:103676 (103.6 KB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:57 errors:0 dropped:0 overruns:0 frame:0
          TX packets:57 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:5732 (5.7 KB)  TX bytes:5732 (5.7 KB)
</code></pre>

<p>It's able to ping everything on my subnet, but the IP is still not from my subnet but 192.168.65.3. </p>

<p>Then I was trying to change the eth0 ip to static IP by editing /etc/network/interface.d/eth0, after restart networking service, the eth0 ip is changed to static ip from my subnet, but the network is not working anymore.</p>

<p>PLEASE, if anyone here knows how to place Windows 10's Linux Container on the LAN as same as host's.</p>

<hr>

<p>My Docker Version</p>

<pre><code>Client:
 Version:      18.03.1-ce
 API version:  1.37
 Go version:   go1.9.5
 Git commit:   9ee9f40
 Built:        Thu Apr 26 07:12:48 2018
 OS/Arch:      windows/amd64
 Experimental: false
 Orchestrator: swarm

Server:
 Engine:
  Version:      18.03.1-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.5
  Git commit:   9ee9f40
  Built:        Thu Apr 26 07:22:38 2018
  OS/Arch:      linux/amd64
  Experimental: false
</code></pre>
"
"50608301","Docker mounted volume adds ;C to end of windows path when translating from linux style path","<docker>","51216995","Why does "";C"" get added to the end of my strings in bash when running docker on Windows 10?","<windows><bash><docker>","<p>I've found some interesting weirdness when trying to mount a docker image on windows.</p>

<p>I created a <code>.sh</code> script that does a mount of the project folder to run our developer environment image. I want one script that every dev can run, regardless of their machine. All it does is runs docker with the current project folder.</p>

<pre><code>#!/usr/bin/env bash
docker run -it --rm -v D:\my\project\folder:/wkDir $IMAGE_TAG yarn dev
</code></pre>

<p>Runs okay. Now the plan is to call this script from <code>npm</code>, so I'd like this to work relative to the current folder. Let's try another version.</p>

<p><code>docker run -it --rm -v $PWD:/wkDir $IMAGE_TAG yarn dev</code></p>

<p>Fails with:</p>

<pre><code>C:\Program Files\Docker\Docker\Resources\bin\docker.exe: Error response from 
daemon: Mount denied:
The source path ""D:/my/project/folder;C""
doesn't exist and is not known to Docker.
</code></pre>

<p>Wat. What's <code>;C</code> and where did it come from?</p>

<p>So I do <code>echo $PWD</code> which gives me <code>/d/my/project/folder</code>.</p>

<p>Interesting, so <code>$PWD</code> resolves to the correct path in linux path format, and it seems like docker is trying to translate from that to the correct windows path, except there's this <code>;C</code> that appears out of nowhere. And the <code>\</code> are <code>/</code>...</p>

<p>What exactly is going on here?</p>

<p>I get the same result in VSCode's terminal git bash and powershell.</p>

<p>Update: I noticed that running the <code>.sh</code> in VSCode's powershell terminal, opens a separate <code>cmd.exe</code> console window which seems to run the script in git bash. So this might be a git bash issue.</p>
","<p>I have a bash script that I am trying to run, which includes the following:</p>

<pre><code>#!/bin/bash

docker run \
    --env-file .env.local \
    -p ${Z_PORT:-8001}:8000 \
    --entrypoint /bin/bash \
    --hostname forms-api.local \
    --name forms-api \
    --net lasernet \
    --rm \
    --volume ""$(pwd)"":/opt/application \
    --volume ""$(pwd)/logs"":/var/log/application \
    dockerhub.zad.com/z-project
</code></pre>

<p>However, when I use git-bash to run it, I get the following error:</p>

<pre><code>The source path ""C:/Users/[...]/Documents/GitHub/ZProject;C""
doesn't exist and is not known to Docker.
</code></pre>

<p>That <code>;C</code> definitely shouldn't be there, but the rest is otherwise correct. When I add some like <code>echo $(pwd)</code>, the correct path is printed, so it seems to be related to the docker command itself. It also seems to be specific to Windows, as my Mac-using coworkers haven't had any issues running this. What would cause that addition?</p>
"
"50787082","Sending JSON from Python to Node via child_process gets truncated if too long, how to fix?","<python><node.js><stdout><child-process>","51103206","Reaching a limit when executing python scripts from node.js","<python><node.js><docker><spawn><alpine>","<p>My Node &amp; Python backend is running just fine, but I now encountered an issue where if a JSON I'm sending from Python back no Node is too long, it gets split into two and my JSON.parse at the Node side fails.</p>

<p>How should I fix this? For example, the first batch clips at</p>

<pre><code>... [1137.6962355826706, -100.78015825640887], [773.3834338399517, -198
</code></pre>

<p>and the second one has the remaining few entries</p>

<pre><code>.201506231888], [-87276.575065248, -60597.8827676457], [793.1850250453127, 
-192.1674702207991], [1139.4465453979683, -100.56741252031816], 
[780.498416769341, -196.04064849430705]]}
</code></pre>

<p>Do I have to create some logic on the Node side for long JSONs or is this some sort of a buffering issue I'm having on my Python side that I can overcome with proper settings? Here's all I'm doing on the python side:</p>

<pre><code>outPoints, _ = cv2.projectPoints(inPoints, np.asarray(rvec), 
np.asarray(tvec), np.asarray(camera_matrix), np.asarray(dist_coeffs))

# flatten the output to get rid of double brackets per result before JSONifying
flattened = [val for sublist in outPoints for val in sublist]
print(json.dumps({'testdata':np.asarray(flattened).tolist()}))
sys.stdout.flush()
</code></pre>

<p>And on the Node side:</p>

<pre><code>// Handle python data from print() function
  pythonProcess.stdout.on('data', function (data){

    try {
      // If JSON handle the data
      console.log(JSON.parse(data.toString()));
    } catch (e) {
      // Otherwise treat as a log entry
      console.log(data.toString());
    }
  });
</code></pre>
","<p>I am executing python scripts through node.js using express and spawn (I am executing them within an Alpine Container).</p>

<p>Python scripts write json data within the stdout using:</p>

<pre><code>print(json_data, file=sys.stdout)
</code></pre>

<p>Whereas the node.js script read the input with:</p>

<pre><code>var child = spawn(...);

child.stdout.on('data', function(data) {
      res.json(JSON.parse(data.toString()).data);
});
</code></pre>

<p>Unfortunately, I reach a limit at <code>2^16</code> bytes with an <em>Unexpected end of JSON</em> input error message.</p>

<p>When redirecting the output fto a file from Python script, the file is complete.</p>

<p>I haven't seen any limit neither from <code>spawn</code> nor from <code>write</code> within both documention.</p>

<p>Moreover, <code>sys.maxsize</code>returns <code>2^32</code>. Any explanation?</p>
"
"50907911","Can't pull image from docker, ProcessUtilityVMImage cannot find the path specified","<docker>","50883581","failed to register layer: re-exec error: exit status 1: output: ProcessUtilityVMImage","<docker-registry><docker-for-windows><docker>","<p>I have made a .net core app and it is uploaded to docker hub</p>

<p>When I try to pull it to my own machine, (win 10) it just works</p>

<p>When I try to pull it to the server (server 2016) I get an error:</p>

<pre><code>docker pull arrivaflg/flg:20180618104928

....

failed to register layer: re-exec error: exit status 1: output: ProcessUtilityVMImage \\?\C:\ProgramData\docker\windowsfilter\cf1f49a6508aaa657768d667c58779e571392a80be0ba7519fe0835ac2476402\UtilityVM: The system cannot find the path specified.
</code></pre>

<p>But the really interesting part is when I try to pull a specific microsoft image, I get the SAME error message. (this is the version 1709 visual studio uses in the docker file on my machine)</p>

<pre><code>c:\tmp&gt;docker pull microsoft/nanoserver:1709
1709: Pulling from microsoft/nanoserver
407ada6e90de: Extracting [==================================================&gt;]  81.04MB/81.04MB
85710d780d68: Download complete
failed to register layer: re-exec error: exit status 1: output: ProcessUtilityVMImage \\?\C:\ProgramData\docker\windowsfilter\cf1f49a6508aaa657768d667c58779e571392a80be0ba7519fe0835ac2476402\UtilityVM: The system cannot find the path specified.
</code></pre>

<p>If I don't specify the version number (and it just defaults to latest) there is no problem with getting the nano server on the server </p>

<p>But still a problem with getting mine image to the server.</p>

<p>So I'm guessing I should use a specific version of the nano server.</p>

<p>I have tried with these in my dockerfile:</p>

<pre><code>FROM microsoft/aspnetcore:2.0-nanoserver-1709 AS base
and
FROM microsoft/aspnetcore:2.0-nanoserver-1803 AS base
</code></pre>

<p>My server information:</p>

<pre><code>C:\Windows\system32&gt;docker info
Containers: 3
 Running: 0
 Paused: 0
 Stopped: 3
Images: 3
Server Version: 17.06.2-ee-11
Storage Driver: windowsfilter
 Windows:
Logging Driver: json-file
Plugins:
 Volume: local
 Network: l2bridge l2tunnel nat null overlay transparent
 Log: awslogs etwlogs fluentd json-file logentries splunk syslog
Swarm: inactive
Default Isolation: process
Kernel Version: 10.0 14393 (14393.2312.amd64fre.rs1_release.180607-1919)
Operating System: Windows Server 2016 Datacenter
OSType: windows
Architecture: x86_64
CPUs: 2
Total Memory: 4GiB
Name: AWS1twAROS001
ID: IVVQ:GK2Q:DNJ7:PW6W:GYZ7:WYQM:65VV:Q4JM:6BEL:5CGQ:ISXY:AWEF
Docker Root Dir: C:\ProgramData\docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Experimental: false
Insecure Registries:
 127.0.0.0/8
Live Restore Enabled: false
</code></pre>
","<p>I have a private docker registry on Windows Server 2016 Datacenter and from local machine (Windows 10 Pro) I pushed few images to it. When I try to pull image from server it starts pulling and later throws an error </p>

<p><strong>failed to register layer: re-exec error: exit status 1: output: ProcessUtilityVMImage \?\C:\ProgramData\docker\windowsfilter\f9613d03a4fae39c0cebca07bc51aef2931756f5e674cda8f6b78729ba5ebac8\UtilityVM: The system cannot find the path specified.</strong></p>

<p>I used following image and steps to start a registry <a href=""https://hub.docker.com/r/stefanscherer/registry-windows/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/stefanscherer/registry-windows/</a></p>

<p>I tested with hello-world image, pushed it from local machine and pulled it from server, it worked.</p>

<p>But, I am unable pull images like microsoft/wcf, microsoft/iis which are base images to my local built images.</p>

<p>Other way around, if I already have microsoft/wcf image on my server (which is base image for my locally built image), when pulling the locally built image why it goes to pull microsoft/wcf again. Is there any way I can use server's wcf image? </p>

<p><strong>docker verion</strong></p>

<p>Client:
 Version:       17.06.2-ee-13
 API version:   1.30
 Go version:    go1.8.7
 Git commit:    ac44d73
 Built: Mon Jun  4 16:46:59 2018
 OS/Arch:       windows/amd64</p>

<p>Server:
 Engine:
  Version:      17.06.2-ee-13
  API version:  1.30 (minimum version 1.24)
  Go version:   go1.8.7
  Git commit:   ac44d73
  Built:        Mon Jun  4 16:58:47 2018
  OS/Arch:      windows/amd64
  Experimental: true</p>
"
"51562179","Facing an issue with attaching EFS volume to Kubernetes pods","<amazon-web-services><docker><amazon-ec2><kubernetes><amazon-efs>","51569428","How to attach EFS volume to Kubernetes PODs","<amazon-web-services><docker><amazon-ec2><kubernetes>","<p>I am running my docker containers with the help of kubernetes cluster on AWS EKS. Two of my docker containers are using shared volume and both of these containers are running inside two different pods. So I want a common volume which can be used by both the pods on aws.</p>

<p>I created an EFS volume and mounted. I am following link to create <code>PersistentVolumeClaim</code>. But I am getting timeout error when <code>efs-provider</code> pod trying to attach mounted EFS volume space. <code>VolumeId</code>, region are correct only. </p>

<p>Detailed Error message for Pod describe: </p>

<blockquote>
  <p>timeout expired waiting for volumes to attach or mount for pod ""default""/""efs-provisioner-55dcf9f58d-r547q"". list of unmounted volumes=[pv-volume]. list of unattached volumes=[pv-volume default-token-lccdw] <br>
  MountVolume.SetUp failed for volume ""pv-volume"" : mount failed: exit status 32</p>
</blockquote>
","<p>I am using aws EFS volumes to share volume across pods, I am able to mount the volume but mounted volume is not attaching to the pod.</p>

<p>I am getting following error:
<strong>timeout expired waiting for volumes to attach or mount for pod</strong>
<strong>MountVolume.SetUp failed for volume ""pv-volume"" : mount failed: exit status 32</strong></p>

<p>manifest.yaml file link <a href=""https://github.com/kubernetes-incubator/external-storage/blob/master/aws/efs/deploy/manifest.yaml"" rel=""nofollow noreferrer"">https://github.com/kubernetes-incubator/external-storage/blob/master/aws/efs/deploy/manifest.yaml</a></p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","62630456","Where on my drive is my docker image? (It's not under /)","<linux><docker><filesystems>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I recently installed a docker image that is 26.3GB uncompressed (as reported by its listing under <code>docker images</code>)</p>
<p>I'm on Ubuntu 18.04, and Dolphin reports my free hard drive space as 3GB. However, when I run QDirStat (a disk usage analyzer) it tells me that the total space the <code>/</code> subtree is taking up is only 25.6GB. The partition Linux is on is 60GB, so I can only conclude that the docker image is not anywhere under <code>/</code>. So where is it?</p>
"
"19234831","Where are Docker images stored on the host machine?","<docker><docker-image>","62866360","Where is the location of images that i pulled from docker hub on my windows?","<docker>","<p>I managed to find the containers under directory <code>/var/lib/docker/containers</code>, but I can't find the images.</p>

<p>What are the directories and files under <code>/var/lib/docker</code>?</p>
","<p>I am a newbie in docker. I pulled some images and want to see the location of these images.</p>
<p>I already look at docker disk image location. This is the path where my images is located  --&gt;&gt;</p>
<p>C:\ProgramData\DockerDesktop\vm-data</p>
<p>i went through this specified path. There was DockerDesktop.vhdx file but</p>
<p>nothing more. How can i see the location of my images on windows that i pulled from docker hub ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","62849739","docker windows : how to access my localhost","<docker><ssl><ssl-certificate><docker-for-windows>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Under Windows, I have a software running into a docker image, this one need access to my localhost machine, on another API.</p>
<p>Let's say that I have a local CDN service locally running (not in a docker image)</p>
<pre><code>https://localhost:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg
</code></pre>
<p>So, the software in the container needs access to &quot;localhost&quot;, the localhost outside the docker image.</p>
<p>Noticed that my local server use SSL, then the container has to access the resource:</p>
<p><a href=""https://host.docker.internal:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg"" rel=""nofollow noreferrer"">https://host.docker.internal:44359/cdn/e_76372856-f7a0-49cc-d3d9-39f5ad58ad6d/653d147084637b2af68b39f5b0733359.jpg</a></p>
<p>And I got the following error message:</p>
<blockquote>
<p>SSL certificate problem: unable to get local issuer certificate</p>
</blockquote>
<p>PS: my localhost=192.168.65.1</p>
<p>I have tried several kinds of stuff, like:</p>
<p>1 - create a network</p>
<pre><code>docker network create -d bridge --subnet 192.168.0.0/24 --gateway 192.168.0.1 dockernet
docker run -d --network dockernet -p 8080:8080 jolibrain/deepdetect_cpu
</code></pre>
<p>2 - use loopback names, like:</p>
<pre><code>host.docker.internal
docker.for.win.localhost
host.docker.internal
10.0.0.2
</code></pre>
<p>3 - tried to use --network host, but then the container failed to start</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","62713079","dockefile-copy from absolute path","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>Dockerfile is in <code>/home/user/docker</code> directory</p>
<p>I want to copy all files on host from <code>/app/publish</code> to the same path on docker container</p>
<pre><code>WORKDIR /app
RUN cp /app/publish /app/
</code></pre>
<p>But getting</p>
<pre><code>cp: cannot stat '/app/publish/': No such file or directory
</code></pre>
<p>I need to copy files outside of build context</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","62894597","set up flask with docker","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I got an issue I'm able to launch flask container but when I go in my browser I got &quot;
This site is inaccessible&quot;, my api is not loaded</p>
<p>docker-compose :</p>
<pre><code>version: &quot;3&quot;

services:
  api:
    container_name: api
    build: ./api
    environment:
      - FLASK_APP=main.py
    command: flask run
    ports:
      - &quot;5000:5000&quot;
</code></pre>
<p>Dockerfile :</p>
<pre><code>FROM python:3

# Install and setup flask
WORKDIR /app
COPY requirements.txt requirements.txt
RUN pip3 install -r requirements.txt
COPY . .

EXPOSE 5000
</code></pre>
<p><strong>UPDATE</strong></p>
<p>basic app</p>
<pre><code>from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'
</code></pre>
<p>Result of <code>docker-compose up --build</code>
<a href=""https://i.stack.imgur.com/mVDwD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVDwD.png"" alt=""enter image description here"" /></a></p>
<p>When I go to localhost:5000 I got &quot;This site is inaccessible&quot;</p>
<p>I don't have access to my app in my browser but why ? I exposed port 5000 in dockerfile and map port 5000:5000</p>
<p>Thank's in advance</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","62932150","I want to start my spring boot application aftet Mysql is up and running","<docker><docker-compose><dockerfile>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I have one java spring boot application which uses mysql DB . I want to start my spring application only after mysql is up and running . ( mysql takes 40-60 sec to up ) . Please suggest how to achieve it .</p>
<p>here is the compose file :</p>
<pre><code>version: &quot;3.8&quot;
services:
    mysql:
        networks: 
            - my-network-1
         
        image: mysql:latest
            
        environment: 
             MYSQL_ROOT_PASSWORD: root
             MYSQL_ROOT_USER: root
             MYSQL_DATABASE: mydb
        expose: 
            - &quot;3306&quot;
        
 
    my-spring: 
 
        depends_on: 
            - mysql
        build: 
            context: .
            dockerfile: dockerfile.dockerfile
        networks: 
            - my-network-1
        expose: 
            - &quot;8080&quot;
 
networks: 
    my-network-1:
        driver: overlay    
</code></pre>
<p>Here is docker file :</p>
<pre><code>FROM  openjdk:8u252-jdk
 
 
ARG JAR_FILE=/somepath/jar.jar
COPY ${JAR_FILE} my.jar
ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;my.jar&quot;]
</code></pre>
<p>currently getting connection refused error.</p>
<p>Thanks</p>
<p>Adarsha</p>
"
"32353055","How to start a stopped Docker container with a different command?","<docker>","62637213","How to start a docker container without executing the specified command","<docker>","<p>I would like to start a stopped Docker container with a different command, as the default command crashes - meaning I can't start the container and then use 'docker exec'. </p>

<p>Basically I would like to start a shell so I can inspect the contents of the container.</p>

<p>Luckily I created the container with the -it option!</p>
","<p>I made a Docker container to act as a Jupyter server</p>
<pre><code>docker run -i -t -p 8888:8888 continuumio/anaconda3 /bin/bash -c &quot;/opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks &amp;&amp; /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser&quot;
</code></pre>
<p>Is it possible to start the stopped container without executing the specified command?
At startup it stops when trying to create an existing directory.</p>
<pre><code>kosh@LinuxPC:~$ docker ps -a --no-trunc
CONTAINER ID                                                       IMAGE               COMMAND                                                                                                                                                                                                    CREATED             STATUS                     PORTS               NAMES
bb9ff79baf4b2a18289e14338cecdd3cdfa3bbe2a84cba0a63430de1e624b769   condaim             &quot;/bin/bash -c '/opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks &amp;&amp; /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser --allow-root'&quot;   24 hours ago        Exited (1) 2 minutes ago                       jovial_clarke

kosh@LinuxPC:~$ docker start -i jovial_clarke
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... done

# All requested packages already installed.

mkdir: cannot create directory ‘/opt/notebooks’: File exists
</code></pre>
"
"33054369","How to change the default docker registry from docker.io to my private registry?","<docker><docker-registry>","62897750","is it possible to write a dockerfile without registry in FROM statements?","<docker><dockerfile><docker-registry>","<p>By default, if I issue command: </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from the docker.io offical site by default.</p>

<pre><code>Pulling repository docker.io/library/ruby
</code></pre>

<p>How do I change it to my private registry. That means if I issue </p>

<pre><code>sudo docker pull ruby:2.2.1
</code></pre>

<p>it will pull from my own private registry, the output is something like:</p>

<pre><code>Pulling repository my_private.registry:port/library/ruby
</code></pre>
","<p>For example, I have a base dockerfile:</p>
<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:3.1
...
</code></pre>
<p>which I build <code>docker build -t my-base</code>, then build a second dockerfile:</p>
<pre><code>FROM my-base
...
</code></pre>
<p>which works because <code>my-base</code> is available locally.  However, if I push these to a registry, it seems I am forced to replace the second docker file with:</p>
<pre><code>FROM my-dockerhub-accountname/my-base
...
</code></pre>
<p>or if not using dockerhub:</p>
<pre><code>FROM 1234.myregistryprovider/something/my-base
...
</code></pre>
<p>And likewise doing a <code>docker push -t my-dockerhub-accountname/my-base</code> or <code>1234.myregistryprovider/something/my-base</code> for the first image.</p>
<p>But docker is only ever connected to one registry (this still applies if it was not), why do these registry provder uris have to poison the docker files I write?  This makes doing CI considerably harder, binding whatever registry provider we choose to declarations in source code.</p>
<p>By comparison, when I reference a nuget/npm package, I do so by package name while registry is handled by the package source manager.  It does not and should not come into the question :/</p>
<p>Is there a way to avoid all this?</p>
"
"33443912","Commit to jenkins docker image does not save changes","<jenkins><docker>","62772216","using Jenkins in DockerToolbox in Windows, what's changed in image after docker commit a container?why installed plugins not commit to the image?","<windows><docker><jenkins><svn><offline>","<p>I pull the official Jenkins docker image from <a href=""https://hub.docker.com/_/jenkins/"">here</a>.
From Jenkins UI I create a new job , install the github plugin and set the repo urls in the job configuration.</p>

<p>Finally I save the changes from Jenkins.</p>

<p>I want to create a new image as it is. I stop the container, and commit it to a new image.</p>

<p>Then I start a new container from the new image...and Jenkins does not contain any of my changes.</p>

<p>I use <code>Docker version 1.6.2, build 7c8fca2</code></p>
","<p>using Jenkins in DockerToolbox in Windows, what's changed in image after docker commit a container?why installed plugins not commit to the image?
and what's the best way to include all the plugins(including pre-installed) in a new image which will be used in the other offline windows pc.
thank you very much!</p>
"
"33913020","Docker remove <none> TAG images","<docker>","63054339","Docker remove image and container before creating","<docker>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>I have the following script, which creates the docker image and container as expected.</p>
<pre><code>echo &quot;password&quot; | sudo -S docker stop /nexct-approval-service-container | xargs sudo docker rm
echo &quot;docker build ...&quot;
echo &quot;password&quot; | sudo -S docker build . -t nexct-approval-service-image:latest
echo &quot;docker run ...&quot;
echo &quot;password&quot; | sudo -S docker run -t --network=host -p 8081:8081 --name nexct-approval-service-container nexct-approval-service-image
</code></pre>
<p>As you can see I am creating a container and image with the following names:</p>
<pre><code>nexct-approval-service-container
nexct-approval-service-image
</code></pre>
<p>However, what I cannot get to work is the following line:</p>
<pre><code>echo &quot;password&quot; | sudo -S docker stop /nexct-approval-service-container | xargs sudo docker rm
</code></pre>
<p>My intention is to remove the container and image if it exists before I create a new one. What I am finding however, when I do <code>sudo docker images</code> is that my list of images continues to grow.</p>
<p><strong>Question</strong></p>
<p>How can I build my images with the specific name, but stop a new one from being built each time?</p>
<p>More info:</p>
<p>As you can see, a number of images called <code>&lt;none&gt;</code> are left below after each build.</p>
<pre><code>$ docker images
REPOSITORY                     TAG                 IMAGE ID            CREATED              SIZE
nexct-approval-service-image   latest              7359e1ff2138        21 seconds ago       557MB
&lt;none&gt;                         &lt;none&gt;              c4f98256b401        About a minute ago   557MB
&lt;none&gt;                         &lt;none&gt;              908eebf0227a        2 minutes ago        557MB
&lt;none&gt;                         &lt;none&gt;              8b6b0cb01773        3 minutes ago        557MB
openjdk                        14                  cdc43cc23d2d        6 days ago           511MB
</code></pre>
"
"38980547","Multiple Docker containers, same image, different config","<docker><containers>","62673792","How to run multiple containers of docker image with different configuration in it","<python><docker>","<p>I'm totally new to Docker so I appreciate your patience.</p>

<p>I'm looking for a way to deploy multiple containers with the same image, however I need to pass in a different config (file) to each? </p>

<p>Right now, my understanding is that once you build an image, that's what gets deployed, but the problem for me is that I don't see the point in building multiple images of the same application when it's only the config that is different between the containers.</p>

<p>If this is the norm, then I'll have to deal with it however if there's another way then please put me out of my misery! :)</p>

<p>Thanks!</p>
","<p>I have a <code>opencv python</code> project where I am reading <code>rtsp</code> url from <code>config.json</code> and based on that I am doing some logic. Now I have created a docker image for this and its container runs fine.</p>
<p>Now I need to run 2 containers of the same docker image but both of the containers will use different rtsp url which means I need to have 2 config files in the project. But the python docker image I have uses only 1 config.json</p>
<p>Is there any way during run time I can provide <code>config1.json</code> <code>config2.json</code> respectively to both the containers and for each container it acts like <code>config.json</code>.</p>
"
"41203970","Pull image Azure Container Registry - Kubernetes","<azure><docker><kubernetes><azure-container-service><azure-container-registry>","63012200","k8s deploymet with image from private registry","<azure><docker><kubernetes>","<p>Does anyone have any advice on how to pull from Azure container registry whilst running within Azure container service (kubernetes)</p>

<p>I've tried a sample deployment like the following but the image pull is failing:</p>

<pre><code>kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: jenkins-master
spec:
  replicas: 1
  template:
    metadata:
      name: jenkins-master
      labels:
        name: jenkins-master
    spec:
      containers:
      - name: jenkins-master
        image: myregistry.azurecr.io/infrastructure/jenkins-master:1.0.0
        imagePullPolicy: Always
        readinessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 20
          timeoutSeconds: 5
        ports:
        - name: jenkins-web
          containerPort: 8080
        - name: jenkins-agent
          containerPort: 50000
</code></pre>
","<p>I've k8s deployment yaml which I need to pull image
from private registry
where should I put the</p>
<pre><code>host 
user 
password 
</code></pre>
<p>deployment.yml</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: tra
  namespace: ba
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tra
  template:
    metadata:
      labels:
        app: tra
    spec:
      containers:
        - name: tra
          image: de/sec:0.0.10
          imagePullPolicy: Always
          ports:
            - containerPort: 5000
</code></pre>
<p>I found this but it doesnt really helps</p>
<p><a href=""https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</a></p>
"
"42280792","Reuse inherited image's CMD or ENTRYPOINT","<shell><docker><dockerfile>","62869559","How do I inject a command to be executed prior to entrypoint in docker-compose?","<bash><docker><docker-compose>","<p>How can I include my own shell script <code>CMD</code> on container start/restart/attach, without removing the <code>CMD</code> used by an inherited image?</p>

<p>I am using this, which does execute my script fine, but appears to overwrite the PHP <code>CMD</code>:</p>

<pre><code>FROM php

COPY start.sh /usr/local/bin

CMD [""/usr/local/bin/start.sh""]
</code></pre>

<p>What should I do differently?  I am avoiding the prospect of copy/pasting the ENTRYPOINT or CMD of the parent image, and maybe that's not a good approach.</p>
","<p>I'm certain that I at some point read about how to run a specific command as a part of docker-compose, and then afterward still run whatever that docker-compose would be running by default. But now when I need it, I cannot find it.</p>
<p>I've got a dockerfile. It specifies some command that should be run when the rest of the docker-compose.yml file has been carried out. As I recall it, I would need to specify a bash script at the entrypoint, replacing whatever would be run by default. And then in that script, I could add something, as the final command that would run whatever the default command was.</p>
<p>example.sh:</p>
<pre><code>echo Hello World!
[Something that runs the default command]
</code></pre>
<p>Putting example.sh in my entrypoint, would then run the echo first, and then default command.</p>
<p>I would like to be able to do that. Or some other solution that achieves the same thing.</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","63008434","How to login from one container to another","<docker><containers>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>I have 2 containers: <code>c1</code> and <code>c2</code>. How can I run a script in <code>c1</code> from <code>c2</code>? Thank you in advance!</p>
"
"49270109","How to open a Chrome Profile through Python","<python><selenium><google-chrome><selenium-chromedriver><chrome-profile>","62615331","selenium docker - load profile can't auto do test","<python><docker><selenium><selenium-webdriver>","<p>My script I have been writing has been working great. I just added the option so it would open a profile on chrome using this code.</p>

<pre><code>options = webdriver.ChromeOptions
browser = webdriver.Chrome(executable_path=r""C:\Users\princess\AppData\Local\Programs\Python\Python36-32\chromedriver.exe"", chrome_options=options)
options.add_argument(r'user-data-dir=C:\Users\princess\AppData\Local\Google\Chrome\User Data')
options.add_argument('--profile-directory=Profile 1')
</code></pre>

<p>When used, I get this error code. </p>

<pre><code>C:\Users\Princess\Desktop&gt;CHBO.py
Traceback (most recent call last):
  File ""C:\Users\Princess\Desktop\CHBO.py"", line 12, in &lt;module&gt;
    browser = webdriver.Chrome(executable_path=r""C:\Users\princess\AppData\Local\Programs\Python\Python36-32\chromedriver.exe"", chrome_options=options)
  File ""C:\Users\Princess\AppData\Local\Programs\Python\Python36-32\lib\site-packages\selenium\webdriver\chrome\webdriver.py"", line 59, in __init__
    desired_capabilities = options.to_capabilities()
TypeError: to_capabilities() missing 1 required positional argument: 'self'
</code></pre>

<p>How can I fix this?</p>
","<p>When use <a href=""https://registry.hub.docker.com/u/selenium/standalone-chrome-debug/"" rel=""nofollow noreferrer"">standalone-chrome-debug</a> After loading the profile, the automatic test cannot be performed. The auto test needs to be performed after manually loading a page. How to fix this problem?</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

option = webdriver.ChromeOptions()
option.add_argument('--user-data-dir=/home/seluser/.config/google-chrome')
option.add_argument('--disable-gpu')
driver = webdriver.Remote(
    command_executor=&quot;http://diskstation:4444/wd/hub&quot;,
    desired_capabilities = option.to_capabilities()
)
driver.get(&quot;https://xxxx.com&quot;)
driver.close()
</code></pre>
"
"49754286","Multiple images, one Dockerfile","<docker><dockerfile>","62794200","Docker image build with shared library files","<docker><docker-compose><dockerfile>","<p>How to create two images in one Dockerfile, they only copy different files.</p>

<p>Shouldn't this produce two images <strong>img1</strong> &amp; <strong>img2</strong>, instead it produces two unnamed images <strong>d00a6fc336b3</strong> &amp; <strong>a88fbba7eede</strong></p>

<p>Dockerfile:</p>

<pre><code>FROM alpine as img1
COPY file1.txt .

FROM alpine as img2
COPY file2.txt .
</code></pre>

<p>Instead this is the result of <em>docker build .</em></p>

<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
&lt;none&gt;              &lt;none&gt;              d00a6fc336b3        4 seconds ago       4.15 MB
&lt;none&gt;              &lt;none&gt;              a88fbba7eede        5 seconds ago       4.15 MB
alpine              latest              3fd9065eaf02        3 months ago        4.15 MB
</code></pre>
","<p>Here is my file structure:</p>
<pre><code>-- Dir1
---- Dockerfile
---- index.php
-- Dir2
---- Dockerfile
---- index.php
-- SharedLibrary
---- class.php
</code></pre>
<p>I want to build multiple images with <code>docker-compose</code>, and they both need the shared file in the other directory.</p>
<p>But <code>Dockerfile</code> can't add files with a relative path.</p>
<p>What does one usually do to resolve this?</p>
<p>By the way, it's not only run with local docker, so it should not handle with volume.</p>
<hr />
<p>Additional Information:</p>
<ul>
<li>I want to make a <code>shellscript</code> to copy the required files to each directory and run <code>docker-compose</code>.</li>
<li>I just want to know how to solve this issue gracefully.</li>
</ul>
"
"51349410","ConnectionResetError: [Errno 104] Connection reset by peer and ERR_NAME_NOT_RESOLVED on heroku with mobile testing through Selenium","<python><selenium><selenium-webdriver><heroku><selenium-chromedriver>","62900102","Selenium error: ConnectionResetError: [Errno 104] Connection reset by peer","<python><docker><selenium><selenium-chromedriver>","<p>I'd like to test multiple mobile user agents with selenium and chrome. I'm using python 3.6 and deploying to heroku. Based on <a href=""http://chromedriver.chromium.org/mobile-emulation"" rel=""nofollow noreferrer"">http://chromedriver.chromium.org/mobile-emulation</a> .</p>

<p>You can download my project for both windows and heroku use at:</p>

<pre><code>https://github.com/kc1/mobiletest
</code></pre>

<p>(keep in mind that if you deploy to heroku you have to set FLASK_CONFIG to production. Also please note that the code in the project is slightly different than in this question because I've been playing with the code over the past week.)</p>

<p>I have:</p>

<pre><code>def some_long_calculation():
    driver = create_chromedriver('kkk')
    # driver = create_chromedriver()

    driver.get(""https://www.yahoo.com/"")
    .....
</code></pre>

<p>and :</p>

<pre><code>def create_chromedriver(ua=False):
    options = webdriver.ChromeOptions()
    CHROMEDRIVER_PATH = os.getenv('$HOME') or basedir+'/chromedriver.exe'
    FLASK_CONFIG = os.getenv('FLASK_CONFIG')

    if ua:

        mobile_emulation = {""deviceName"": ""Nexus 5""}
        options.add_experimental_option(""mobileEmulation"", mobile_emulation)


    if FLASK_CONFIG and FLASK_CONFIG == ""production"":
        CHROMEDRIVER_PATH = '/app/.chromedriver/bin/chromedriver'
        GOOGLE_CHROME_SHIM = os.getenv('$GOOGLE_CHROME_SHIM') or 'no path found'
        options.binary_location = '/app/.apt/usr/bin/google-chrome-stable'

        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')

    return webdriver.Chrome(executable_path=CHROMEDRIVER_PATH, options=options)  
</code></pre>

<p>If I run it locally with the mobile browser enabled It works as expected:</p>

<p><a href=""https://i.stack.imgur.com/fSGZY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fSGZY.png"" alt=""enter image description here""></a></p>

<p>If I run it on heroku with the mobile browser enabled :</p>

<p><a href=""https://i.stack.imgur.com/eVP8Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eVP8Z.png"" alt=""enter image description here""></a></p>

<p>Then I tried it on heroku with the mobile user disabled I get:</p>

<p><a href=""https://i.stack.imgur.com/32ObW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/32ObW.png"" alt=""enter image description here""></a></p>

<p>So at least I know the setup is working as far as chrome and chromedriver. </p>

<p>heroku Logs:</p>

<pre><code>2018-07-15T17:37:53.967643+00:00 app[web.1]:     driver = create_chromedriver('kkk')
2018-07-15T17:37:53.967637+00:00 app[web.1]:     png = some_long_calculation()
2018-07-15T17:37:53.967645+00:00 app[web.1]:   File ""/app/app/main/cl.py"", line 120, in create_chromedriver
2018-07-15T17:37:53.967640+00:00 app[web.1]:   File ""/app/app/main/cl.py"", line 123, in some_long_calculation
2018-07-15T17:37:53.967648+00:00 app[web.1]:     return webdriver.Chrome(executable_path=CHROMEDRIVER_PATH, options=options)
2018-07-15T17:37:53.967651+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py"", line 75, in __init__
2018-07-15T17:37:53.967654+00:00 app[web.1]:     desired_capabilities=desired_capabilities)
2018-07-15T17:37:53.967656+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 156, in __init__
2018-07-15T17:37:53.967659+00:00 app[web.1]:     self.start_session(capabilities, browser_profile)
2018-07-15T17:37:53.967661+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 251, in start_session
2018-07-15T17:37:53.967669+00:00 app[web.1]:     response = self.command_executor.execute(driver_command, params)
2018-07-15T17:37:53.967664+00:00 app[web.1]:     response = self.execute(Command.NEW_SESSION, parameters)
2018-07-15T17:37:53.967667+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 318, in execute
2018-07-15T17:37:53.967672+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py"", line 472, in execute
2018-07-15T17:37:53.967674+00:00 app[web.1]:     return self._request(command_info[0], url, body=data)
2018-07-15T17:37:53.967677+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py"", line 496, in _request
2018-07-15T17:37:53.967679+00:00 app[web.1]:     resp = self._conn.getresponse()
2018-07-15T17:37:53.967682+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/http/client.py"", line 1331, in getresponse
2018-07-15T17:37:53.967685+00:00 app[web.1]:     response.begin()
2018-07-15T17:37:53.967687+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/http/client.py"", line 297, in begin
2018-07-15T17:37:53.967695+00:00 app[web.1]:     line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
2018-07-15T17:37:53.967690+00:00 app[web.1]:     version, status, reason = self._read_status()
2018-07-15T17:37:53.967698+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/socket.py"", line 586, in readinto
2018-07-15T17:37:53.967692+00:00 app[web.1]:   File ""/app/.heroku/python/lib/python3.6/http/client.py"", line 258, in _read_status
2018-07-15T17:37:53.967700+00:00 app[web.1]:     return self._sock.recv_into(b)
2018-07-15T17:37:53.967712+00:00 app[web.1]: ConnectionResetError: [Errno 104] Connection reset by peer
</code></pre>

<p>How can I fix this?</p>

<p>EDIT:</p>

<p>Thanks for your detailed answer. I've changed the code to incorporate the flags you mentioned. Chrome version is 67.0.3396.99. Chromedriver is 2.40 and selenium is 3.13. Unfortunately, there is no change in the result. I'm still getting the same error. As far as  your stage 2 and 3 advice. I'm currently deploying to heroku so I do not have total control of the environment variables. Is there a way to make of these changes using python?</p>

<p>EDIT 2:</p>

<p>As I think about it more on <a href=""https://sites.google.com/a/chromium.org/chromedriver/mobile-emulation"" rel=""nofollow noreferrer"">https://sites.google.com/a/chromium.org/chromedriver/mobile-emulation</a> the example uses</p>

<pre><code>from selenium import webdriver
mobile_emulation = { ""deviceName"": ""Nexus 5"" }
chrome_options = webdriver.ChromeOptions()
chrome_options.add_experimental_option(""mobileEmulation"", mobile_emulation)
driver = webdriver.Remote(command_executor='http://127.0.0.1:4444/wd/hub',
                  desired_capabilities = chrome_options.to_capabilities())
</code></pre>

<p>Are you suggesting the browser is at '<a href=""http://127.0.0.1:4444/wd/hub"" rel=""nofollow noreferrer"">http://127.0.0.1:4444/wd/hub</a>'</p>
","<p>I have a selenium robot that downloads two CSV files on a website. First, he does download a CSV, waits to finish and start the download process of the second CSV.</p>
<p>However, they are unable to complete the download of the second CSV and the following error appears:</p>
<p>During handling of the above exception, another exception occurred:</p>
<pre><code>Traceback (most recent call last):
  File &quot;site-packages/requests/adapters.py&quot;, line 449, in send
  File &quot;site-packages/urllib3/connectionpool.py&quot;, line 725, in urlopen
  File &quot;site-packages/urllib3/util/retry.py&quot;, line 403, in increment
  File &quot;site-packages/urllib3/packages/six.py&quot;, line 734, in reraise
  File &quot;site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
  File &quot;site-packages/urllib3/connectionpool.py&quot;, line 426, in _make_request
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
  File &quot;site-packages/urllib3/connectionpool.py&quot;, line 421, in _make_request
  File &quot;http/client.py&quot;, line 1344, in getresponse
  File &quot;http/client.py&quot;, line 306, in begin
  File &quot;http/client.py&quot;, line 267, in _read_status
  File &quot;socket.py&quot;, line 589, in readinto
  File &quot;ssl.py&quot;, line 1071, in recv_into
  File &quot;ssl.py&quot;, line 929, in read
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;threading.py&quot;, line 926, in _bootstrap_inner
  File &quot;threading.py&quot;, line 870, in run
  File &quot;compose/cli/log_printer.py&quot;, line 169, in tail_container_logs
  File &quot;compose/cli/log_printer.py&quot;, line 200, in wait_on_exit
  File &quot;compose/container.py&quot;, line 281, in wait
  File &quot;site-packages/docker/utils/decorators.py&quot;, line 19, in wrapped
  File &quot;site-packages/docker/api/container.py&quot;, line 1289, in wait
  File &quot;site-packages/docker/utils/decorators.py&quot;, line 46, in inner
  File &quot;site-packages/docker/api/client.py&quot;, line 226, in _post
  File &quot;site-packages/requests/sessions.py&quot;, line 581, in post
  File &quot;site-packages/requests/sessions.py&quot;, line 533, in request
  File &quot;site-packages/requests/sessions.py&quot;, line 646, in send
  File &quot;site-packages/requests/adapters.py&quot;, line 498, in send
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
</code></pre>
<p>Any idea what might be going on?</p>
<p>The project is in the docker and I am using versions:</p>
<pre><code>    Python 3.6

    chromedriver --version
    ChromeDriver 83.0.4103.39 
    
    
    google-chrome --version
    Google Chrome 83.0.4103.116
</code></pre>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","63005184","docker daemon is not running for windows 10","<docker>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>I am trying to install Docker for my Windows 10 PC by using virtualbox,docker is configured to use the default machine with IP but I am getting the following error</p>
<p>C:\Program Files\Docker Toolbox\docker.exe: error during connect: Post http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/containers/create: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.<a href=""https://i.stack.imgur.com/19Tpg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19Tpg.png"" alt=""enter image description here"" /></a></p>
"
"54218632","How to use local proxy settings in docker-compose","<proxy><docker-compose><redmine>","62792743","Is there a way to reroute traffic in docker?","<docker><networking><proxy><docker-compose>","<p>I am setting up a new server for our Redmine installation, since the old installation was done by hand, which makes it difficult to update everything properly. I decided to go with a Docker image but am having trouble starting the docker container due to an error message. The host is running behind a proxy server, which I think, is causing this problem, as everything else such as wget, curl, etc. is working fine.</p>

<p>Error message:</p>

<pre><code>Pulling redmine (redmine:)...
ERROR: Get https://registry-1.docker.io/v2/: dial tcp 34.206.236.31:443: connect: connection refused
</code></pre>

<p>I searched on Google about using Docker/Docker-Compose with a proxy server in the background and found a few websites where people had the same issue but none of these really helped me with my problem.</p>

<p>I checked with the Docker documentation and found a guide but this does not seem to work for me: <a href=""https://docs.docker.com/network/proxy/"" rel=""noreferrer"">https://docs.docker.com/network/proxy/</a></p>

<p>I also found an answered question here on StackOverflow: <a href=""https://stackoverflow.com/questions/53477114/using-proxy-on-docker-compose-in-server"">Using proxy on docker-compose in server</a> which might be the solution I am after but I am unsure where exactly I have to put the solution. I guess the person means the docker-compose.yml file but I could be wrong.</p>

<p>This is what my docker-compose.yml looks like:</p>

<pre><code>version: '3.1'

services:

redmine:
 image: redmine
 restart: always
 ports:
   - 80:3000
 environment:
   REDMINE_DB_MYSQL: db
   REDMINE_DB_PASSWORD: SECRET_PASSWORD

db:
image: mysql:5.7
restart: always
environment:
  MYSQL_ROOT_PASSWORD: SECRET_PASSWORD
  MYSQL_DATABASE: redmine
</code></pre>

<p>I expect to run the following command without the above error message</p>

<pre><code>docker-compose -f docker-compose.yml up -d
</code></pre>
","<p>Currently, I have a docker environment where every container can access the internet and every HTTP request that goes to the internet is unchecked. I want to measure and controll the in- and outgoing traffic of my applications in docker-compose. Is there a way to set some kind of global proxy that handles all traffic for every container?</p>
<p>The above picture shows my desired setup and the picture below shows my current setup. My current setup is super messy and I can't control or measure my traffic.</p>
<pre><code>+------------+            +--------------+            +--------------+
|            +&lt;-----------+              +&lt;-----------+              |
| Interne-App|            |  Proxy       |            |  Container   |
|            +-----------&gt;+              +-----------&gt;+              |
+------------+            +--------------+            +--------------+



                    +-------------------------+
                    |                         |
+------------+      |     +--------------+    |       +--------------+
|            +------+     |              |    +------^+              |
| Interne-App|            |  Proxy       +------------|  Container   |
|            +-----------&gt;+              |    +-------+              |
+----+-------+            +--------------+    |       +--------------+
     ^                                        |
     |                                        |
     +----------------------------------------+
</code></pre>
<p>Is there a way to solve this with docker-compose networking? Do i need to manually reroute all traffic from a container to the proxy first ?</p>
"
"54981900","Can we exec into container in a POD in K8S?","<docker><kubernetes>","62874800","Is it possible to exec in to a K8s pod the same way we exec in to a docker containers or containers running inside of a pod?","<docker><kubernetes><namespaces><kubernetes-pod><nsenter>","<p>I am putting docker image into POD.
We can exec into a Docker container using ""docker exec...""
Similarly is there a way to exec into container in a POD to check some data ?</p>
","<p>Is it possible to <code>exec</code> in to a K8s pod the same way we <code>exec</code> into docker containers or containers running inside of a pod?p</p>
<p><strong>Edit -</strong><br />
This question not about <code>exec</code>ing into a container in a pod. This is about the pod itself. Might be it's not possible but that is what the question is about. So stop marking it duplicate with - <a href=""https://stackoverflow.com/questions/54981900/can-we-exec-into-container-in-a-pod-in-k8s"">Can we exec into container in a POD in K8S?</a></p>
"
"55174274","Understanding docker layers and future changes","<docker><ubuntu><debian>","63057529","Do docker images share identical layers?","<docker><dockerfile><docker-image>","<p><a href=""https://medium.com/@nagarwal/docker-containers-filesystem-demystified-b6ed8112a04a"" rel=""nofollow noreferrer"">So</a>, </p>

<p><a href=""https://i.stack.imgur.com/fotPN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fotPN.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem. </p>
</blockquote>

<p>and, </p>

<p><a href=""https://i.stack.imgur.com/qEm9R.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qEm9R.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Because each container has its own thin writable container layer, and all changes are stored in this container layer, this means that multiple containers can <strong>share access</strong> to the <strong>same underlying image</strong> and yet have their own data state.</p>
</blockquote>

<p>and <a href=""https://medium.com/@jessgreb01/digging-into-docker-layers-c22f948ed612"" rel=""nofollow noreferrer"">also</a>, </p>

<blockquote>
  <p>Layers of a Docker image are essentially just files generated from running some command. You can view the contents of each layer on the Docker host at <code>/var/lib/docker/aufs/diff</code>. </p>
</blockquote>

<p>Now, questions, </p>

<ul>
<li>Say I build my docker images <strong>layer by layer</strong>. <code>A &lt; B &lt; C &lt; D</code>, etc. </li>
<li>Now if I <strong>update my docker image</strong> <code>A</code>, would the rest of docker images <code>B, C, D</code> see the changes as well, provided that the changes are not touched by them when building them? E.g., adding <code>/etc/apt/sources.list.d/somethingnew</code> that was never there before.</li>
<li>If I had built <strong>another set</strong> of docker images layer by layer. <code>A &lt; X &lt; Y &lt; Z</code>, then the above changes will be reflected in <code>X, Y, Z</code> as well, right? </li>
<li>Now if however, the future changes to <code>A</code>, is <strong>made to the same file</strong> that will be changed when building <code>B, C, D</code>, then what would happen? For e.g., let's make it simple that docker images <code>B, C, D</code> each only add pkgB, pkgC, and pkgD in its layer. If I add a pkgA to <code>A</code> <em>after</em> <code>B, C, D</code> are build, what would happen? -- I guess there should be one single version of truth as what packages are in, for a single system, so what would it be for this case? </li>
<li>What if I'm <strong>only upgrading</strong> the packages in <code>A</code>? This should be OK right? Would the rest of docker images see the changes as well? </li>
</ul>
","<p>I know that each container is an image with a readable/writeable layer on top of a bunch of read-only layers, and that multiple containers can share the read-only layers of the image. Do two images that were created from the same base image share their identical images?</p>
<p>Example:</p>
<ul>
<li>Image A has 5 layers and weighs 1GB.</li>
<li>Image B is created with A as its base image and adds another layer and weighs 1.1GB.</li>
<li>Image C is created with A as its base image and adds another layer and weighs 1.5GB</li>
</ul>
<p>Is the total disk space now 3.6GB or 1.6GB?</p>
"
"55495223","Push Docker Image task to ACR fails in Azure DevOps Pipeline with ""unauthorized: authentication required""","<azure-devops>","62895425","Getting unauthorized: authentication required in docker image deployment","<azure><docker><azure-devops><dockerfile><azure-pipelines>","<p>Push and image to Azure Container Registry task in Azure DevOps pipeline fails. Previous tasks are executed fine ie. docker image is create and login to ACR is successful. However push-task fails with the following result:</p>

<pre><code>unauthorized: authentication required
[error]unauthorized: authentication required
[error]/usr/bin/docker failed with return code: 1
[section]Finishing: Push Docker image
</code></pre>

<p>docker push to that given acr works fine from local command line.</p>

<pre><code># Docker image
# Build a Docker image to deploy, run, or push to a container registry.
# Add steps that use Docker Compose, tag images, push to a registry, run an image, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/docker

trigger:
- master

pool:
  vmImage: 'Ubuntu-16.04'

variables:
  imageName: 'renamed:$(build.buildId)'
  azureSubscriptionEndpoint: Renamed
  azureContainerRegistry: renamed.azurecr.io
steps: 
- task: Docker@1
  displayName: Build docker image
  inputs:
    command: Build an image
    dockerFile: Dockerfile
    imageName: $(imageName)
    containerregistrytype: Azure Container Registry
    azureSubscriptionEndpoint: $(azureSubscriptionEndpoint)
    azureContainerRegistry: $(azureContainerRegistry)
- task: Docker@1
  displayName: Login to container registry
  inputs:
    command: login
    containerregistrytype: Azure Container Registry
    azureSubscriptionEndpoint: $(azureSubscriptionEndpoint)
    azureContainerRegistry: $(azureContainerRegistry)
    dockerFile: Dockerfile
    imageName: $(imageName)
- task: Docker@1
  displayName: Push Docker image
  inputs:
    command: Push an image
    containerregistrytype: Azure Container Registry
    azureSubscriptionEndpoint: $(azureSubscriptionEndpoint)
    azureContainerRegistry: $(azureContainerRegistry)
    imageName: $(imageName)
</code></pre>
","<p>I have created an Azure Container registry and azure web app for containers and publish my code in it.
After that, I have created a CICD pipeline using Azure. Build and release pipelines are passed successfully. But on browsing the application, it is showing 'unauthorized: authentication required' error.</p>
<p>2020-07-14T12:24:42.737Z INFO  - Pulling image from Docker hub: api.azurecr.io/api
2020-07-14T12:24:43.270Z ERROR - DockerApiException: Docker API responded with status code=InternalServerError, response={&quot;message&quot;:&quot;Get <a href=""https://api.azurecr.io/v2/api/manifests/latest"" rel=""nofollow noreferrer"">https://api.azurecr.io/v2/api/manifests/latest</a>: unauthorized: authentication required, visit <a href=""https://aka.ms/acr/authorization"" rel=""nofollow noreferrer"">https://aka.ms/acr/authorization</a> for more information.&quot;}</p>
<p>Do I need to create a repository in the docker hub? What is the problem here?</p>
"
"57331050","How can I install package from private repository using docker","<docker><npm><npm-install>","62712426","npm WARN deprecated on node v14+","<node.js><linux><docker><npm><openshift>","<p>I am installing a package from my private repository. I am able to install it using:
<code>npm i -S git+https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git</code></p>

<p>I am using docker container but while installation process I am getting an error:</p>

<pre><code>npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent
</code></pre>

<p>How can I solve it?</p>

<p>My docker file:</p>

<pre><code>FROM node:alpine

COPY package.json package.json
COPY src src
COPY .babelrc .babelrc

RUN npm install  
RUN npm run gitlab-build

RUN ls
EXPOSE 8080
CMD [""npm"", ""run"", ""docker-start""]
</code></pre>
","<p>I'm facing this issue for past 2days. But then the docker image fails. I've update chokidar,
fsevents doesn't update.
for the request I'm not sure what to do&quot; request,request,urix,resolve-url@,left-pad, core-js</p>
<pre><code>                  npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
                    npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
                    npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
                    npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
                    npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
                    npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
                    npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
                    npm ERR! path git
                    npm ERR! code ENOENT
                    npm ERR! errno ENOENT
                    npm ERR! syscall spawn git
                    npm ERR! enoent Error while executing:
                    npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
                    npm ERR! enoent
                    npm ERR! enoent
                    npm ERR! enoent spawn git ENOENT
                    npm ERR! enoent This is related to npm not being able to find a file.
                    npm ERR! enoent


                    npm ERR! A complete log of this run can be found in:
                    npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
                    The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please lemme know where do i do the changes I'm new to it</p>
"
"58462473","Google Cloud VM Image to docker image","<docker><google-cloud-platform><google-kubernetes-engine><google-cloud-build>","63003082","How can I clone my Google Cloud Instance so I can download it and host it locally using Docker","<docker><ubuntu><hosting>","<p>I have a Google Cloud VM that installed with my application. The installation step is completed and I:</p>

<ol>
<li>Turned off the VM instance. </li>
<li>Exported the disk to disk image called <code>MY_CUSTOM_IMAGE_1</code></li>
</ol>

<p>My wish now is to use <code>MY_CUSTOM_IMAGE_1</code> as the starting image of my docker image build. For building the images I'm using Google Cloud Build. </p>

<p>My docker file should look like this:</p>

<pre><code>FROM MY_CUSTOM_IMAGE_1 AS BUILD_ENV
...
</code></pre>

<p>When I tried to use this image I got the build error:</p>

<pre><code>ERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: exit status 1
ERROR
pull access denied for MY_CUSTOM_IMAGE_1, repository does not exist or may require 'docker login'
Step 1/43 : FROM MY_CUSTOM_IMAGE_1 AS BUILD_ENV
</code></pre>

<p>The reason is that VM images are not the same as Docker images. </p>

<p>Is this possible to make this transform (GCP VM Image -> Docker image), without external tools (outside GCP, like ""docker private repositories"")?</p>

<p>Thanks!</p>
","<p>I have an ubuntu server in Google Cloud which I'd like move completely to host in Docker on my desktop machine. How can I just clone it and download it for local usage on ubuntu using Docker?</p>
<p>Is there a way to install docker on the VM and create an image of the machine which i can import into my local machine?</p>
<p>PS: Docker Noob</p>
"
"61793815","Cannot create deno docker image","<docker><deno>","62960813","Dockerfile error no such file or directory""","<docker><deno>","<p>I want to create <code>deno</code> docker image using Dockerfile</p>

<pre><code>FROM alpine:latest

WORKDIR /

RUN apk update &amp;&amp; \
    apk upgrade

RUN apk add curl

RUN curl -fsSL https://deno.land/x/install/install.sh | sh

ENV DENO_INSTALL=""/root/.deno""

ENV PATH=""${DENO_INSTALL}/bin:${PATH}""

RUN deno --help
</code></pre>

<p>But when run <code>docker build -t deno .</code> it shows at last <code>/bin/sh: deno: not found</code></p>

<p>full output:</p>

<pre><code>Sending build context to Docker daemon  54.78kB
Step 1/8 : FROM alpine:latest
 ---&gt; f70734b6a266
Step 2/8 : WORKDIR /
 ---&gt; Using cache
 ---&gt; b1bbfa810906
Step 3/8 : RUN apk update &amp;&amp;     apk upgrade
 ---&gt; Using cache
 ---&gt; a7761425faba
Step 4/8 : RUN apk add curl
 ---&gt; Using cache
 ---&gt; 9099d4f65cb1
Step 5/8 : RUN curl -fsSL https://deno.land/x/install/install.sh | sh
 ---&gt; Using cache
 ---&gt; b4ea95c69a73
Step 6/8 : ENV DENO_INSTALL=""/root/.deno""
 ---&gt; Using cache
 ---&gt; bdc7e1e85e9c
Step 7/8 : ENV PATH=""${DENO_INSTALL}/bin:${PATH}""
 ---&gt; Using cache
 ---&gt; d35db1caba71
Step 8/8 : RUN deno --help
 ---&gt; Running in d1ca4e1d0dc6
/bin/sh: deno: not found
The command '/bin/sh -c deno --help' returned a non-zero code: 127
</code></pre>
","<p>I try to use my Dockerfile at <a href=""https://labs.play-with-docker.com/"" rel=""nofollow noreferrer"">https://labs.play-with-docker.com</a>. Its EOL conversion is LF. My file:</p>
<pre><code>FROM alpine
RUN apk --no-cache add curl
RUN curl -fsSL https://deno.land/x/install/install.sh | sh
WORKDIR /root/.deno/bin
# To print deno version at container start.
CMD [&quot;/root/.deno/bin/deno&quot;, &quot;--version&quot;]
</code></pre>
<p>I start the commands:</p>
<pre><code>docker build -t deno .
docker run --rm --name deno deno
</code></pre>
<p>But I get the error:</p>
<pre><code>standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Why does it happen? How can I fix it?</p>
"
"61981156","Unable to locate package python-pip Ubuntu 20.04","<python><python-2.7><ubuntu><pip><ubuntu-20.04>","62897241","Unable to install python-pip module","<python><docker><ubuntu><pip>","<p>I am trying to install mininet-wifi. After downloading it, I have been using the following command to install it:</p>

<pre><code>    sudo util/install.sh -Wlnfv
</code></pre>

<p>However, I keep getting the error:</p>

<pre><code>    E: Unable to locate package python-pip
</code></pre>

<p>I have tried multiple times to download python-pip. I know mininet-wifi utilizes python 2 instead of python 3. I have tried to download python-pip using the command:</p>

<pre><code>    sudo apt-get install python-pip
</code></pre>

<p>But that leads to the same error:</p>

<pre><code>    E: Unable to locate package python-pip
</code></pre>
","<p><strong>System</strong>:</p>
<pre><code>lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 20.04 LTS
Release:        20.04
Codename:       focal
</code></pre>
<p><strong>Issue</strong>:</p>
<p>Hi. I am using latest Ubuntu image from a docker library. However, I am unable to  install pip module using</p>
<pre><code>apt install python-pip # python2.7
</code></pre>
<p>I installed python 2.7 after installing <code>software-properties-common</code>.</p>
<p><code>universe</code> repository is set for all sources but still no success.</p>
<pre><code>add-apt-repository universe
'universe' distribution component is already enabled for all sources.
</code></pre>
<p><code>apt update</code>:</p>
<pre><code>Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
All packages are up to date.
</code></pre>
<p>Please advise.</p>
"
"62011537","How can I run Selenium tests in a docker container with a visible browser?","<docker><selenium><selenium-webdriver><docker-compose><dockerfile>","62757187","is it possible to run selenium tests in docker container with selenium WebDriver?","<docker><selenium><grid>","<p>If I want to run Selenium tests inside a Docker container with a visible (not headless) browser, what are my options?</p>

<ul>
<li>Do I need to use a remote display viewer such as VNC?</li>
<li>Is it possible to use a browser on the host? (I.e. a browser that is not in the Docker container). How does this work?</li>
<li>Any other option?</li>
</ul>
","<p>I want to build a docker image from the sources of my Selenium project without selenium Grid is it possible?</p>
"
"62708135","How to create separate volumes for each container in docker compose","<docker><docker-compose><mounted-volumes>","62692255","Multiple containers of docker image using same volumes in docker compose?","<docker><docker-compose><docker-volume>","<p>I have created a <code>docker</code> image for which I want to run multiple containers. This docker image is dependent on few things which will remain same for all the containers but the only difference will be configuration of the containers.</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
</code></pre>
<p>In the above file, you can see that I am giving <code>s1config</code> and <code>s2config</code> for <code>s1</code> <code>s2</code> containers but rest of the things remains same and this is why <code>/home/andrew/Documents/CVAI</code> volume is same for both the containers. Due to this, docker is sharing the volume between both the containers and thus all the data is mixing between containers and not getting separated.</p>
<p><strong>Is there any way we can separate the volumes between multiple containers.?</strong> I do not want to create multiple dockers for this. Please help. Thanks</p>
<p><strong>EDIT</strong></p>
<p>Updated docker-compose file :</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/s1/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/s1:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/s2/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/s2:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
</code></pre>
","<p>I have a python project which I have converted to a docker image. I need to run 2 containers of this docker image with different configs and different volumes. So I have put everything in docker compose file. Below is the file content:</p>
<pre><code>version: '2.4'

services:
    s1:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s1config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g

    s2:
        image: testdockerimg:latest
        volumes:
            - /home/andrew/Documents/CVAI/configs/s2config.json:/home/andrew/Documents/CVAI/configs/config.json
            - /home/andrew/Documents/CVAI:/home/andrew/Documents/CVAI/
        restart: always
        mem_limit: 2g
         
            
        
</code></pre>
<p>As you can see that I am starting two containers i.e. <code>s1</code> <code>s2</code> of docker image <code>testdockerimg</code>. Both of them has same volume but difference is I am giving <code>s1config.json</code> as <code>config.json</code> to s1 container and <code>s2config.json</code> as <code>config.json</code> to s2 container.  This <code>config.json</code> has different configuration for different containers and is read by python file in docker to perform all the logic.</p>
<p>Now as per my understanding, both of these containers will have their own separate volumes, which means they are acting as two different machines having the volumes as mentioned above. But looks like both of the containers are using the 1 shared volume instead of having 2 separate volumes. I can confirm this as python code inside the docker pushes all the data to iot hub portal and I can see that <code>s1</code> has same data  as <code>s2</code>. I can also confirm this looking at the log file as their are 2 entries for each log in file.</p>
<p>Can anyone please tell me what is wrong. Does volumes across each containers remains same or its different. I do not want to create another docker image and would like to simply run another container having different config and volume so that all the data of the container is separate from other containers. Please help. Thanks</p>
"
"62712426","npm WARN deprecated on node v14+","<node.js><linux><docker><npm><openshift>","62715789","npm WARN deprecated on node v14+ Nodejs on openshift cluster","<node.js><linux><docker><npm><openshift>","<p>I'm facing this issue for past 2days. But then the docker image fails. I've update chokidar,
fsevents doesn't update.
for the request I'm not sure what to do&quot; request,request,urix,resolve-url@,left-pad, core-js</p>
<pre><code>                  npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
                    npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
                    npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
                    npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
                    npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
                    npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
                    npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
                    npm ERR! path git
                    npm ERR! code ENOENT
                    npm ERR! errno ENOENT
                    npm ERR! syscall spawn git
                    npm ERR! enoent Error while executing:
                    npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
                    npm ERR! enoent
                    npm ERR! enoent
                    npm ERR! enoent spawn git ENOENT
                    npm ERR! enoent This is related to npm not being able to find a file.
                    npm ERR! enoent


                    npm ERR! A complete log of this run can be found in:
                    npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
                    The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please lemme know where do i do the changes I'm new to it</p>
","<p>Please, any help would be appreciated.</p>
<p>I'm facing this issue for past 2 days. But then the docker image fails. I've update <code>chokidar</code>, <code>fsevents</code> doesn't update.</p>
<p>For the request I'm not sure what to do about <code>request</code>, <code>request</code>, <code>urix</code>, <code>resolve-url@</code>, <code>left-pad</code>, <code>core-js</code>.</p>
<pre class=""lang-none prettyprint-override""><code>npm WARN deprecated chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.
npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
npm WARN deprecated fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.
npm WARN deprecated urix@0.1.0: Please see https://github.com/lydell/urix#deprecated
npm WARN deprecated resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated
npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
npm WARN deprecated core-js@2.6.11: core-js@&lt;3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.
npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t ssh://git@github.com/eligrey/FileSaver.js.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent


npm ERR! A complete log of this run can be found in:
npm ERR!     /root/.npm/_logs/2020-06-30T11_34_14_726Z-debug.log
         The command '/bin/sh -c npm install' returned a non-zero code: 1
</code></pre>
<p>Please let me know where do I do the changes. I'm new to it.</p>
"
"62748938","Docker container to connect to Postgres not in docker","<postgresql><docker><rest>","62804279","Windows Docker container Networking to Postges on host (windows 10)","<docker><windows-10-desktop><postgresql-12>","<p>I have a .NET application running on a windows 10 computer using docker and postgres. When I run using the</p>
<pre><code> &quot;DockerTest&quot;: {
      &quot;commandName&quot;: &quot;Project&quot;,
      &quot;launchBrowser&quot;: true,
      &quot;launchUrl&quot;: &quot;asset&quot;,
      &quot;environmentVariables&quot;: {
        &quot;ASPNETCORE_ENVIRONMENT&quot;: &quot;Development&quot;
      },
      &quot;applicationUrl&quot;: &quot;https://localhost:5001;http://localhost:5000&quot;
    },
</code></pre>
<p>Connection string =  &quot;Server=localhost;Port=5433;Database=xxx;Uid=yyy;Pwd=zzz;&quot;,</p>
<p>option in VS2019 I can connect to my postgres database without problems.</p>
<p>When I try to run the application using the</p>
<pre><code> &quot;Docker&quot;: {
      &quot;commandName&quot;: &quot;Docker&quot;,
      &quot;launchBrowser&quot;: true,
      &quot;launchUrl&quot;: &quot;{Scheme}://{ServiceHost}:{ServicePort}/asset&quot;,
      &quot;publishAllPorts&quot;: true,
      &quot;useSSL&quot;: true
    },
</code></pre>
<p>option, I am getting errors connecting. Here is my connection string.</p>
<pre><code> &quot;Database1&quot;: &quot;Server=host.docker.internal;Port=5433;Database=xxx;Uid=yyy;Pwd=zzz;&quot;,
</code></pre>
<p>No matter what I try for a connection string I get CONNECTION WAS REFUSED or TIMEOUT errors.</p>
<p>Can anyone give me pointer as to what to try? I have gone over the message boards for two days and tries WINDOWS DEFENDER additions for com.docker.backend, I have tried different connection strings, I have tried many many other ways to solve this and have come up blank.</p>
<p>The end goal is to have docker run my REST API and have postgres installed on the same computer but not running in docker.</p>
<p><strong>NEW EDIT:</strong> July 6 1:45 AM MST</p>
<p>inside the container I ran the following:</p>
<p>C:\app&gt;ipconfig</p>
<p>Windows IP Configuration</p>
<pre><code>Ethernet adapter Ethernet:

   Connection-specific DNS Suffix  . : allworx.activeis.ca
   Link-local IPv6 Address . . . . . : fe80::c06e:18b5:f9e0:48bb%4
   IPv4 Address. . . . . . . . . . . : 172.30.81.112
   Subnet Mask . . . . . . . . . . . : 255.255.240.0
   Default Gateway . . . . . . . . . : 172.30.80.1
</code></pre>
<p>C:\app&gt;ping host.docker.internal</p>
<pre><code>Pinging host.docker.internal [10.0.0.47] with 32 bytes of data:
Reply from 10.0.0.47: bytes=32 time&lt;1ms TTL=127
</code></pre>
<p>NOTE : if I try to run my own app in the environment it actually works in with the 10.0.0.47 it works (even with host.docker.internal)... It is just from inside the container that it does not make the Postgres connection.</p>
<pre><code>**PG_HBA.CONF FILE :** 
host    replication     all             127.0.0.1/32            md5
host    replication     all             ::1/128                 md5
host    all     all     172.17.0.1/16       md5
</code></pre>
<p><strong>NEW EDIT:</strong> July 6 2:00 AM MST
If I use the IP address of my other network IPs on my host they all work as long as I put them into the PG_HBA.CONF FILE (in the dockertest option)</p>
<p>Nothing seems to make the transition from container to host when I run with the DOCKER option. Even when I add the 10.0.0.47 to the file and run with HOST.DOCKER.INTERNAL (which translates to 10.0.0.47) it still does not work</p>
<pre><code>**PG_HBA.CONF FILE :** 
host    replication     all             127.0.0.1/32            md5
host    replication     all             ::1/128                 md5
host    all     all     172.17.0.1/16       md5
host    all     all     10.0.0.47/16        md5
host    all     all     169.254.214.72/16   md5

C:\Users\Chris\source\repos\QuickTech.Com\QuickTechAPI&gt;docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
661118d67505        Default Switch      ics                 local
eae6a3536ef1        nat                 nat                 local
9d0b1f0209f6        none                null                local
</code></pre>
<p>I've seen a bridge or other network added in my hours of scouring the internet. Do I need to add a network to docker to make this transition?</p>
<p><strong>NEW EDIT:</strong> July 6 11:00 AM MST</p>
<pre><code>Host File in host computer
10.0.0.47 host.docker.internal
10.0.0.47 gateway.docker.internal
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
</code></pre>
<p>I cannot open the hosts file (c:\windows\system32\drivers\etc) in the container to see what it contains.</p>
","<p>OK.. Sorry to clog up this site with endless questions.
I have a .NET REST API that works in DOCKER. (Windows container)
But, the moment I try to connect to Postgres on my host I am unable to connect. I get unable to connect, request timed out, connection was actively refused... I have modified my connection string over a thousand times trying to get this to work.</p>
<p>when I look at docker networks is get:</p>
<pre><code>C:\Windows\SysWOW64&gt;docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
4c79ae3895aa        Default Switch      ics                 local
40dd0975349e        nat                 nat                 local
90a25f9de905        none                null                local
</code></pre>
<p>when I inspect my container, it says it is using NAT for network.</p>
<pre><code>C:\Windows\SysWOW64&gt;docker network inspect nat
[
    {
        &quot;Name&quot;: &quot;nat&quot;,
        &quot;Id&quot;: &quot;40dd0975349e1f4b334e5f7b93a3e8fb6aef864315ca884d8587c6fa7697dec5&quot;,
        &quot;Created&quot;: &quot;2020-07-08T15:02:17.5277779-06:00&quot;,
        &quot;Scope&quot;: &quot;local&quot;,
        &quot;Driver&quot;: &quot;nat&quot;,
        &quot;EnableIPv6&quot;: false,
        &quot;IPAM&quot;: {
            &quot;Driver&quot;: &quot;windows&quot;,
            &quot;Options&quot;: null,
            &quot;Config&quot;: [
                {
                    &quot;Subnet&quot;: &quot;172.22.96.0/20&quot;,
                    &quot;Gateway&quot;: &quot;172.22.96.1&quot;
                }
            ]
        },
        &quot;Internal&quot;: false,
        &quot;Attachable&quot;: false,
        &quot;Ingress&quot;: false,
        &quot;ConfigFrom&quot;: {
            &quot;Network&quot;: &quot;&quot;
        },
        &quot;ConfigOnly&quot;: false,
        &quot;Containers&quot;: {
            &quot;0d2dc2658a9948d84b01eaa9f5eb5a0e7815933f5af17e5abea17b82a796e1ec&quot;: {
                &quot;Name&quot;: &quot;***MyAPI***&quot;,
                &quot;EndpointID&quot;: &quot;3510dac9e5c5d49f8dce18986393e2855008980e311fb48ed8c4494c9328c353&quot;,
                &quot;MacAddress&quot;: &quot;00:15:5d:fc:4f:8e&quot;,
                &quot;IPv4Address&quot;: &quot;172.22.106.169/16&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            }
        },
        &quot;Options&quot;: {
            &quot;com.docker.network.windowsshim.hnsid&quot;: &quot;3007307C-49DC-4DB5-91C8-0E05DAC8E2B6&quot;,
            &quot;com.docker.network.windowsshim.networkname&quot;: &quot;nat&quot;
        },
        &quot;Labels&quot;: {}
    }
]
</code></pre>
<p>When I look at my network properties of my host I have :</p>
<pre><code>Name:   vEthernet (nat)
Description:    Hyper-V Virtual Ethernet Adapter #2
Physical address (MAC): 00:15:5d:fc:43:56
Status: Operational
Maximum transmission unit:  1500
IPv4 address:   172.22.96.1/20
IPv6 address:   fe80::d017:d598:692a:2e67%63/64
DNS servers:    fec0:0:0:ffff::1%1, fec0:0:0:ffff::2%1, fec0:0:0:ffff::3%1
Connectivity (IPv4/IPv6):   Disconnected
</code></pre>
<p>I am guessing that the NAT in the docker network ls linking to this network hyper v adapter.
both have 172.22.96.1 as the IPAddress</p>
<p>connection string:</p>
<p>Server=172.22.96.1;Port=5433;Database=QuickTechAssetManager;Uid=QuickTech;Pwd=password;</p>
<p>SO... when I try to connect from container to host to connect to postgres I get errors even though the I can ping the UP address.</p>
<p>when I look at my host file, host.docker.internal is set to 10.0.0.47 (my wifi connection).
Is this &quot;disconnect&quot; part of my network problems.</p>
<p>I have posted a few questions on this and I get one answer and then nothing further.
I am would absolutely love to have someone work with me for a bit to resolve this one - what should be minor - issue.</p>
<p>I have modified my pg_hba.conf file, I have done everything I can find...</p>
<p>I will give a phone number or email to anyone who wants to help me solve this. I have been killing myself for over a week and am getting nowhere. I am not even sure is this sort of request is allowed here but I am desperate. I am three months into a project and cant get paid until I get this one minor problem solved.</p>
<p>here is the other question I asked a few days ago:
<a href=""https://stackoverflow.com/questions/62748938/docker-container-to-connect-to-postgres-not-in-docker"">Docker container to connect to Postgres not in docker</a></p>
<p>rentedtrout@gmail.com for anyone who wants to work with me on this.</p>
<p>Please and thank you in advance.</p>
"
"7745578","""[notice] child pid XXXX exit signal Segmentation fault (11)"" in apache error.log","<php><apache><cakephp><error-logging>","64033858","How can I solve this ""Segmentation failure"" in PHP when I generate a large file?","<php><linux><apache><docker>","<p>I am using Apache/PHP/MySQL stack.<br>
Using as framework CakePHP. </p>

<p>Every now and then I get a blank white page. I can't debug it through Cake, so I peek in the apache error.log and here's what I get:</p>

<pre><code>[Wed Oct 12 15:27:23 2011] [notice] child pid 3580 exit signal Segmentation fault (11)
[Wed Oct 12 15:27:34 2011] [notice] child pid 3581 exit signal Segmentation fault (11)
[Wed Oct 12 15:30:52 2011] [notice] child pid 3549 exit signal Segmentation fault (11)
[Wed Oct 12 16:04:27 2011] [notice] child pid 3579 exit signal Segmentation fault (11)
zend_mm_heap corrupted
[Wed Oct 12 16:26:24 2011] [notice] child pid 3625 exit signal Segmentation fault (11)
[Wed Oct 12 17:57:24 2011] [notice] child pid 3577 exit signal Segmentation fault (11)
[Wed Oct 12 17:58:54 2011] [notice] child pid 3550 exit signal Segmentation fault (11)
[Wed Oct 12 17:59:52 2011] [notice] child pid 3578 exit signal Segmentation fault (11)
[Wed Oct 12 18:01:38 2011] [notice] child pid 3683 exit signal Segmentation fault (11)
[Wed Oct 12 22:20:53 2011] [notice] child pid 3778 exit signal Segmentation fault (11)
[Wed Oct 12 22:29:51 2011] [notice] child pid 3777 exit signal Segmentation fault (11)
[Wed Oct 12 22:33:42 2011] [notice] child pid 3774 exit signal Segmentation fault (11)
</code></pre>

<p>What is this segmentation fault, and how can I fix it?</p>

<p>UPDATE:</p>

<pre><code>PHP Version 5.3.4, OSX local development
Server version: Apache/2.2.17 (Unix)
CakePhp: 1.3.10
</code></pre>
","<p>I have read a lot about this error in different forums but still cannot find a solution</p>
<p>My productive server currently has:</p>
<p>Apache: 2.4.25 PHP: 5.6.40</p>
<p>Using the PHPSpreadsheet library in version 1.8.2, different Excel files are generated; For different reasons (Large amounts of information and styles applied to the cell sheet) an xlsx file of maximum 3MB is exported in quite long execution time (It can reach 30/40 minutes).</p>
<p>And all this worked until we moved and dockerized to our new server, whose OS is CentOS7 and has a docker for apache, php and another for mysql.</p>
<p>Now when generating these &quot;large&quot; files the error.log trace shows the following error: [core: notice] <code>[pid 8] AH00052: child pid 15829 exit signal Segmentation fault (11)</code></p>
<p>I tried to replicate the error using virtual machines in my local machine and in another one, I put OS centos7 and dockerized on them, I even copied the same configuration files from the productive server, however, on these machines the error does not appear and allows me to download correctly.</p>
<p>Currently I am desperate, I have tried every internet solution in reverse to replicate the error before trying any solution on the production line, but this error is by no means replicated.</p>
<p>I appreciate any information and help with a just cause ... I know that at this point php 5.6 is quite sent to collect, but I cannot commit the work time to an update if in the end it is not going to solve this incident.</p>
"
"16296753","Can you run GUI applications in a Docker container?","<x11><sandbox><docker><vnc>","64169954","How to run xterm in docker mininet","<docker><mininet><xterm>","<p>How can you run GUI applications in a <a href=""http://www.docker.io"" rel=""noreferrer"">Docker</a> container?</p>

<p>Are there any images that set up <code>vncserver</code> or something so that you can - for example - add an extra speedbump sandbox around say Firefox?</p>
","<p>I am new to docker. Inside of the docker, I opened mininet CLI and ran <code>xterm h1</code>, but got the error message <code>Error: Cannot connect to display</code>
<a href=""https://i.stack.imgur.com/OvNOZ.png"" rel=""nofollow noreferrer"">(msg picture)</a> Is there anyone know how to fix this problem?</p>
<p>p.s. I found a page to solving this problem (<a href=""https://hub.docker.com/r/iwaseyusuke/mininet/"" rel=""nofollow noreferrer"">link</a>), but could not understand what is the &quot;docker host&quot; and where should I run <code>xhost +si:localuser:root</code>.</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","64347101","How to can I edit Docker port and args on a container? (using Dockers Desktop for Windows)","<python><docker><pip><anaconda>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>I creasted a docker container using:</p>
<pre><code>    docker run -t -i continuumio/anaconda3 /bin/bash
</code></pre>
<p>I've installed all the software, however, I missed out the initial port setup.</p>
<p>When I run the container I can see that I have not setup the port and command line args:</p>
<pre><code>    docker container inspect 135c2d60901e
</code></pre>
<p>I can see what I need to modify to in the JSON it returns but am not able to modify in via Dockers Desktop for windows.  Are there Dockers commands that I should use or do I need to find the location of these files and modify them?</p>
"
"20635472","Using the RUN instruction in a Dockerfile with 'source' does not work","<bash><shell><docker>","64212302","Source command not working from entrypoint/dockerfile","<linux><docker><openssl>","<p>I have a Dockerfile that I am putting together to install a vanilla python environment (into which I will be installing an app, but at a later date).</p>

<pre><code>FROM ubuntu:12.04

# required to build certain python libraries
RUN apt-get install python-dev -y

# install pip - canonical installation instructions from pip-installer.org
# http://www.pip-installer.org/en/latest/installing.html
ADD https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py /tmp/ez_setup.py
ADD https://raw.github.com/pypa/pip/master/contrib/get-pip.py /tmp/get-pip.py
RUN python /tmp/ez_setup.py
RUN python /tmp/get-pip.py
RUN pip install --upgrade pip 

# install and configure virtualenv
RUN pip install virtualenv 
RUN pip install virtualenvwrapper
ENV WORKON_HOME ~/.virtualenvs
RUN mkdir -p $WORKON_HOME
RUN source /usr/local/bin/virtualenvwrapper.sh
</code></pre>

<p>The build runs ok until the last line, where I get the following exception:</p>

<pre><code>[previous steps 1-9 removed for clarity]
...
Successfully installed virtualenvwrapper virtualenv-clone stevedore
Cleaning up...
 ---&gt; 1fc253a8f860
Step 10 : ENV WORKON_HOME ~/.virtualenvs
 ---&gt; Running in 8b0145d2c80d
 ---&gt; 0f91a5d96013
Step 11 : RUN mkdir -p $WORKON_HOME
 ---&gt; Running in 9d2552712ddf
 ---&gt; 3a87364c7b45
Step 12 : RUN source /usr/local/bin/virtualenvwrapper.sh
 ---&gt; Running in c13a187261ec
/bin/sh: 1: source: not found
</code></pre>

<p>If I <code>ls</code> into that directory (just to test that the previous steps were committed) I can see that the files exist as expected:</p>

<pre><code>$ docker run 3a87 ls /usr/local/bin
easy_install
easy_install-2.7
pip
pip-2.7
virtualenv
virtualenv-2.7
virtualenv-clone
virtualenvwrapper.sh
virtualenvwrapper_lazy.sh
</code></pre>

<p>If I try just running the <code>source</code> command I get the same 'not found' error as above. If I RUN an interactive shell session however, source does work:</p>

<pre><code>$ docker run 3a87 bash
source
bash: line 1: source: filename argument required
source: usage: source filename [arguments]
</code></pre>

<p>I can run the script from here, and then happily access <code>workon</code>, <code>mkvirtualenv</code> etc.</p>

<p>I've done some digging, and initially it looked as if the problem might lie in the difference between <strong>bash</strong> as the Ubuntu <em>login shell</em>, and <strong>dash</strong> as the Ubuntu <em>system shell</em>, <strong>dash</strong> not supporting the <code>source</code> command.</p>

<p>However, the answer to this appears to be to use <strong>'.'</strong> instead of <code>source</code>, but this just causes the Docker runtime to blow up with a go panic exception.</p>

<p>What is the best way to run a shell script from a Dockerfile RUN instruction to get around this (am running off the default base image for Ubuntu 12.04 LTS).</p>
","<p>I'm trying to install <em>openssl</em> from a <em>Dockerfile</em> manually. It's all working fine, until the last command <code>source /etc/environment</code>.
I've tried <code>RUN source /etc/environment</code> inside my Dockerfile, and <code>source /etc/environment</code> in my entrypoint script but neither of them worked.
But if I  run the command manually inside a running container, it works fine.
Any ideas why is this happening?</p>
<p><strong>EDIT</strong></p>
<p>This is the content of my Dockerfile. I don't have any errors building the image but the <em>source</em> doesn't seem to have any effect, unless I access the container and run it manually.</p>
<pre><code>RUN wget https://www.openssl.org/source/openssl-1.1.1c.tar.gz
RUN tar -xf openssl-1.1.1c.tar.gz
RUN cd openssl-1.1.1c; ./config --prefix=/usr/local/ssl --openssldir=/usr/local/ssl shared zlib; make; make test;  make install
RUN echo &quot;/usr/local/ssl/lib&quot; &gt; /etc/ld.so.conf.d/openssl-1.1.1c.conf
RUN mv /usr/bin/c_rehash /usr/bin/c_rehash.backup
RUN mv /usr/bin/openssl /usr/bin/openssl.backup
RUN echo &quot;PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/ssl/bin&quot;&quot; &gt; /etc/environment
RUN source /etc/environment
</code></pre>
"
"25135897","How to automatically start a service when running a docker container?","<docker>","64043175","Can I have Postgres + Python in a single Docker container for testing?","<python><postgresql><docker><pytest>","<p>I have a <a href=""https://github.com/larazest/db/blob/master/Dockerfile"">Dockerfile</a> to install MySQL server in a container, which I then start like this:</p>

<pre><code>sudo docker run -t -i 09d18b9a12be /bin/bash
</code></pre>

<p>But the MySQL service does not start automatically, I have to manually run (from within the container):</p>

<pre><code>service mysql start
</code></pre>

<p>How do I automatically start the MySQL service when I run the docker container?</p>
","<p>I would like to run pytest on some routes of a Flask application which need a database. Sadly, sqlite is not working as the setup uses database schemas. It should by Postgres.</p>
<p>I want to integrate this into the CI pipeline and I don't think docker-compose works there. But Docker does. So I need pytest + postgres in a single Docker container. How do I do that?</p>
<h2>What I tried</h2>
<pre><code>FROM python:3.7.9-slim-buster
RUN apt-get update
RUN apt-get -y install postgresql
RUN service postgresql start
COPY setup_dev_user.sql ./
RUN chmod 777 setup_dev_user.sql

USER postgres
RUN psql --file=setup_dev_user.sql
RUN psql createdb dev_db
ENV SQLALCHEMY_DATABASE_URI postgresql://dev_user:dev_pw@db:5432/dev_db

USER root
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install --no-cache-dir psycopg2-binary
COPY app /app/app
WORKDIR /app
CMD pytest
</code></pre>
<p>where <code>setup_dev_user.sql</code> is</p>
<pre><code>create user dev_user password 'dev_pw'
</code></pre>
<p>I get</p>
<pre><code>psql: could not connect to server: No such file or directory
        Is the server running locally and accepting
        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
The command '/bin/sh -c psql --file=setup_dev_user.sql' returned a non-zero code: 2
</code></pre>
<p>for the step <code>RUN psql --file=setup_dev_user.sql</code>. How can I fix that?</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","64065824","Dockerfile Service failed to build: COPY failed: Forbidden path outside the build context: ../../../../*","<docker><jenkins><docker-compose><jenkins-pipeline><wildfly>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I want to specify files outside of the dockerfile location.  Below is my dockerfile and I get this error: &quot;Step 2/3 : COPY ../../../../wildfly/standalone /opt/jboss/wildfly/
Service 'wildfly' failed to build: COPY failed: Forbidden path outside the build context: ../../../../wildfly/standalone ()&quot;
How can I reference the root of the directory or reference a file outside of the dockerfile workspace?</p>
<p>`FROM jboss/wildfly:10.1.0.Final</p>
<p>COPY ../../../../wildfly/standalone /opt/jboss/wildfly/
`</p>
"
"28302178","How can I add a volume to an existing Docker container?","<docker>","64120664","How do I attach a volume to a running container starting from the Docker Run command?","<amazon-web-services><docker><docker-volume>","<p>I have a Docker container that I've created simply by installing Docker on Ubuntu and doing:</p>
<pre><code>sudo docker run -i -t ubuntu /bin/bash
</code></pre>
<p>I immediately started installing Java and some other tools, spent some time with it, and stopped the container by</p>
<pre><code>exit
</code></pre>
<p>Then I wanted to add a volume and realised that this is not as straightforward as I thought it would be. If I use <code>sudo docker -v /somedir run ...</code> then I end up with a fresh new container, so I'd have to install Java and do what I've already done before just to arrive at a container with a mounted volume.</p>
<p>All the documentation about mounting a folder from the host seems to imply that mounting a volume is something that can be done when creating a container. So the only option I have to avoid reconfiguring a new container from scratch is to commit the existing container to a repository and use that as the basis of a new one whilst mounting the volume.</p>
<p>Is this indeed the only way to add a volume to an existing container?</p>
","<p>I need to attach a volume to the running container which has started from the docker run command on my AWS EC2 server (Ubuntu 18.04 LTS) with the following command</p>
<pre><code>$ docker run --name example -p 8080:80 -dt example-image:latest
</code></pre>
<p>I'm using Docker version 19.03.12.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","64219058","docker-compose ports not mapping; logs shows app is running","<docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>When I run <code>flask run</code> the localhost port is exposed and the application loads with no errors, but when I run <code>docker-compose up -d --build</code> the image is built and running but the localhost is not exposed. inspecting the container shows that it built with no errors and is still running. I've reviewed <a href=""https://github.com/docker/compose/issues/4799"" rel=""nofollow noreferrer"">this github issue</a> and <a href=""https://stackoverflow.com/questions/35429837/docker-compose-port-mapping"">this stackoverflow</a> which suggested <code>network_host mode</code> was the problem but I am not using it. It also suggested manually using 0.0.0.0 instead of 127.0.0.1 in the ports mapping but that didn't help.</p>
<p>docker-compose.yml</p>
<pre><code>#~/Projects/mysite/docker-compose.yml
version: '3.7'
services:
  flask-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    image: &quot;site-development&quot;
    ports:
      - &quot;5000:5000&quot;
    volumes:
      - &quot;.:/personal_site&quot;
    tty: true
</code></pre>
<p>Dockerfile</p>
<pre><code>#~/Projects/mysite/Dockerfile
# multi build stages
#https://docs.docker.com/develop/develop-images/multistage-build/

# ==================================== BASE ====================================
FROM python:3.8-slim-buster AS base

WORKDIR /personal_site

ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV FLASK_APP mysite
ENV FLASK_ENV development
ENV FLASK_DEBUG 1


RUN apt-get update
RUN apt-get install -y \
    gcc

RUN pip install --upgrade pip
COPY . .

# ================================= DEVELOPMENT ================================
FROM base as development
COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
RUN pip install -r requirements-dev.txt

CMD [&quot;flask&quot;, &quot;run&quot;]

# ================================= PRODUCTION =================================
FROM base as production
COPY ./requirements-prod.txt /personal_site/requirements-prod.txt
RUN pip install -r requirements-prod.txt

# =================================== MANAGE ===================================
FROM base as manage
COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
RUN pip install -r requirements-dev.txt
</code></pre>
<p>Docker versions:</p>
<pre><code>docker-compose version 1.25.5, build 8a1c60f6
Docker version 19.03.13, build 4484c46d9d
</code></pre>
<p>Folder Structure:</p>
<pre><code>#~/Projects/mysite
- docker-compose.yml
- Dockerfile
- mysite/__init__.py
</code></pre>
<p>Running from cmd with <code>flask run</code></p>
<pre><code> * Serving Flask app &quot;mysite&quot; (lazy loading)
 * Environment: development
 * Debug mode: on
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 331-834-605
</code></pre>
<p>Docker-compose stout</p>
<pre><code>stephen@stephen:~/Projects/mysite$ docker-compose up -d --build
docker-compose up -d --build
Building flask-dev
Step 1/15 : FROM python:3.8-slim-buster AS base
 ---&gt; 6cf621cb1327
Step 2/15 : WORKDIR /personal_site
 ---&gt; Using cache
 ---&gt; 751c8aa117f3
Step 3/15 : ENV PYTHONDONTWRITEBYTECODE 1
 ---&gt; Using cache
 ---&gt; 8b29a6862b7a
Step 4/15 : ENV PYTHONUNBUFFERED 1
 ---&gt; Using cache
 ---&gt; a5f8751c9c12
Step 5/15 : ENV FLASK_APP mysite
 ---&gt; Using cache
 ---&gt; bc212892cc17
Step 6/15 : ENV FLASK_ENV development
 ---&gt; Using cache
 ---&gt; 9ffa29dcea47
Step 7/15 : ENV FLASK_DEBUG 1
 ---&gt; Using cache
 ---&gt; 9b2e876d307a
Step 8/15 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; bd323216e65a
Step 9/15 : RUN apt-get install -y  gcc
 ---&gt; Using cache
 ---&gt; 3105c1727b5d
Step 10/15 : RUN pip install --upgrade pip
 ---&gt; Using cache
 ---&gt; 6f8fb5bc0015
Step 11/15 : COPY . .
 ---&gt; 76ff5e26c0a9

Step 12/15 : FROM base as development
 ---&gt; 76ff5e26c0a9
Step 13/15 : COPY ./requirements-dev.txt /personal_site/requirements-dev.txt
 ---&gt; 06e9dac6dec6
Step 14/15 : RUN pip install -r requirements-dev.txt
 ---&gt; Running in f4743afd3a31
Collecting click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
Collecting Flask==1.1.2
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting itsdangerous==1.1.0
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2==2.11.2
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting MarkupSafe==1.1.1
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Collecting Werkzeug==1.0.1
  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Collecting pytest==6.0.2
  Downloading pytest-6.0.2-py3-none-any.whl (270 kB)
Collecting black==19.10b0
  Downloading black-19.10b0-py36-none-any.whl (97 kB)
Collecting flake8==3.8.3
  Downloading flake8-3.8.3-py2.py3-none-any.whl (72 kB)
Collecting flake8-blind-except==0.1.1
  Downloading flake8-blind-except-0.1.1.tar.gz (2.5 kB)
Collecting flake8-debugger==3.2.1
  Downloading flake8-debugger-3.2.1.tar.gz (4.2 kB)
Collecting flake8-docstrings==1.5.0
  Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl (5.5 kB)
Collecting flake8-isort==4.0.0
  Downloading flake8_isort-4.0.0-py2.py3-none-any.whl (14 kB)
Collecting isort==5.5.2
  Downloading isort-5.5.2-py3-none-any.whl (95 kB)
Collecting pep8-naming==0.11.1
  Downloading pep8_naming-0.11.1-py2.py3-none-any.whl (8.4 kB)
Collecting pluggy&lt;1.0,&gt;=0.12
  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)
Collecting toml
  Downloading toml-0.10.1-py2.py3-none-any.whl (19 kB)
Collecting more-itertools&gt;=4.0.0
  Downloading more_itertools-8.5.0-py3-none-any.whl (44 kB)
Collecting attrs&gt;=17.4.0
  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)
Collecting py&gt;=1.8.2
  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)
Collecting iniconfig
  Downloading iniconfig-1.0.1-py3-none-any.whl (4.2 kB)
Collecting packaging
  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)
Collecting appdirs
  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting regex
  Downloading regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675 kB)
Collecting pathspec&lt;1,&gt;=0.6
  Downloading pathspec-0.8.0-py2.py3-none-any.whl (28 kB)
Collecting typed-ast&gt;=1.4.0
  Downloading typed_ast-1.4.1-cp38-cp38-manylinux1_x86_64.whl (768 kB)
Collecting pyflakes&lt;2.3.0,&gt;=2.2.0
  Downloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)
Collecting mccabe&lt;0.7.0,&gt;=0.6.0
  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)
Collecting pycodestyle&lt;2.7.0,&gt;=2.6.0a1
  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from flake8-blind-except==0.1.1-&gt;-r requirements-dev.txt (line 10)) (50.3.0)
Collecting pydocstyle&gt;=2.1
  Downloading pydocstyle-5.1.1-py3-none-any.whl (35 kB)
Collecting testfixtures&lt;7,&gt;=6.8.0
  Downloading testfixtures-6.14.2-py2.py3-none-any.whl (89 kB)
Collecting flake8-polyfill&lt;2,&gt;=1.0.2
  Downloading flake8_polyfill-1.0.2-py2.py3-none-any.whl (7.3 kB)
Collecting six
  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting pyparsing&gt;=2.0.2
  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Collecting snowballstemmer
  Downloading snowballstemmer-2.0.0-py2.py3-none-any.whl (97 kB)
Building wheels for collected packages: flake8-blind-except, flake8-debugger
  Building wheel for flake8-blind-except (setup.py): started
  Building wheel for flake8-blind-except (setup.py): finished with status 'done'
  Created wheel for flake8-blind-except: filename=flake8_blind_except-0.1.1-py3-none-any.whl size=3045 sha256=ee8ec372f4ef9ba5aa472cc70cf2b34462c2e5742c4431a88d4811ba3c6941b9
  Stored in directory: /root/.cache/pip/wheels/7f/70/4f/6002af5c24a072eec72570bf0a265bf95f1968c4508563b6cd
  Building wheel for flake8-debugger (setup.py): started
  Building wheel for flake8-debugger (setup.py): finished with status 'done'
  Created wheel for flake8-debugger: filename=flake8_debugger-3.2.1-py3-none-any.whl size=4201 sha256=0399bc5b93a43d4f70912c25e9251000b1003a04215c0d3b0656da88f0fd828b
  Stored in directory: /root/.cache/pip/wheels/39/b8/cc/049c8f7f02a46a88305db6b9ed26799fded45f98e20728640c
Successfully built flake8-blind-except flake8-debugger
Installing collected packages: click, MarkupSafe, Jinja2, Werkzeug, itsdangerous, Flask, pluggy, toml, more-itertools, attrs, py, iniconfig, six, pyparsing, packaging, pytest, appdirs, regex, pathspec, typed-ast, black, pyflakes, mccabe, pycodestyle, flake8, flake8-blind-except, flake8-debugger, snowballstemmer, pydocstyle, flake8-docstrings, isort, testfixtures, flake8-isort, flake8-polyfill, pep8-naming
Successfully installed Flask-1.1.2 Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 appdirs-1.4.4 attrs-20.2.0 black-19.10b0 click-7.1.2 flake8-3.8.3 flake8-blind-except-0.1.1 flake8-debugger-3.2.1 flake8-docstrings-1.5.0 flake8-isort-4.0.0 flake8-polyfill-1.0.2 iniconfig-1.0.1 isort-5.5.2 itsdangerous-1.1.0 mccabe-0.6.1 more-itertools-8.5.0 packaging-20.4 pathspec-0.8.0 pep8-naming-0.11.1 pluggy-0.13.1 py-1.9.0 pycodestyle-2.6.0 pydocstyle-5.1.1 pyflakes-2.2.0 pyparsing-2.4.7 pytest-6.0.2 regex-2020.9.27 six-1.15.0 snowballstemmer-2.0.0 testfixtures-6.14.2 toml-0.10.1 typed-ast-1.4.1
Removing intermediate container f4743afd3a31
 ---&gt; c63db68e69d5
Step 15/15 : CMD [&quot;flask&quot;, &quot;run&quot;]
 ---&gt; Running in c0f90d383fb2
Removing intermediate container c0f90d383fb2
 ---&gt; 8552c5d064ca

Successfully built 8552c5d064ca
Successfully tagged site-development:latest
Recreating mysite_flask-dev_1 ... 
stephen@stephen:~/Projects/mysite$ docker-compose ps
docker-compose ps
       Name           Command    State           Ports         
---------------------------------------------------------------
mysite_flask-dev_1   flask run   Up      0.0.0.0:5000-&gt;5000/tcp
stephen@stephen:~/Projects/mysite$ docker-compose logs
docker-compose logs
Attaching to mysite_flask-dev_1
flask-dev_1  |  * Tip: There are .env or .flaskenv files present. Do &quot;pip install python-dotenv&quot; to use them.
flask-dev_1  |  * Serving Flask app &quot;mysite&quot; (lazy loading)
flask-dev_1  |  * Environment: development
flask-dev_1  |  * Debug mode: on
flask-dev_1  |  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
flask-dev_1  |  * Restarting with stat
flask-dev_1  |  * Tip: There are .env or .flaskenv files present. Do &quot;pip install python-dotenv&quot; to use them.
flask-dev_1  |  * Debugger is active!
flask-dev_1  |  * Debugger PIN: 124-299-806
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","64284575","docker container on windows does not give any response","<azure><docker><flask><docker-compose><flask-restful>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Nice time of the day for everybody and Hello,
I am new at docker and I have faced a lot of problems with deploying of a Flask app.
My Dockerfile:</p>
<pre><code># For more information, please refer to https://aka.ms/vscode-docker-python
FROM ubuntu:20.04

EXPOSE 5000

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE 1

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED 1

# Install pip requirements
RUN apt-get update &amp;&amp; apt-get install -y python3-pip unixodbc-dev python3-dev
ADD requirements.txt .
RUN pip3 install -r requirements.txt
RUN pip3 install flask-restful
RUN pip3 install werkzeug

WORKDIR /app
ADD . /app

# Switching to a non-root user, please refer to https://aka.ms/vscode-docker-python-user-rights
RUN useradd appuser &amp;&amp; chown -R appuser /app
USER appuser

# During debugging, this entry point will be overridden. For more information, please refer to https://aka.ms/vscode-docker-python-debug
CMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;127.0.0.1:5000&quot;, &quot;main:app&quot;]
</code></pre>
<p>Docker-compose:</p>
<pre><code>version: '3.4'

services:
  tutorial:
    image: tutorial
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 5000:5000
</code></pre>
<p>main.py:</p>
<pre><code>api.add_resource(Video,&quot;/videos/&lt;int:video_id&gt;&quot;) 
if __name__=='__main__':
    app.run(host=&quot;127.0.0.1&quot;, port=5000,debug=False)
</code></pre>
<p>One if the main problems is that I cannot get a respose from container when it is locally runned on the local host.
In my docker log I see that <code>gunicorn</code> listening to a <code>port 5000</code></p>
<pre><code>[2020-10-09 14:20:19 +0000] [1] [INFO] Starting gunicorn 20.0.4

[2020-10-09 14:20:19 +0000] [1] [INFO] Listening at: http://127.0.0.1:5000 (1)

[2020-10-09 14:20:19 +0000] [1] [INFO] Using worker: sync

[2020-10-09 14:20:19 +0000] [7] [INFO] Booting worker with pid: 7
</code></pre>
<p>But according infromation from terminal I  see my container port opened.But it is not binded as it is specified in docker-compose file <code>ports:- 5000:5000.</code></p>
<pre><code>C:\Users\fele&gt;docker ps
CONTAINER ID        IMAGE                                                                       COMMAND                  CREATED             STATUS              PORTS               NAMES
f3e0bfae76ec        containerregistr.azurecr.io/restvideoshop:1 &quot;gunicorn --bind 127…&quot;   5 hours ago         Up 33 seconds       5000/tcp            beautiful_chatterjee
</code></pre>
<p>Furthermore if I open my windows docker desktop and inspect my container I see one port opened in not binded state.
<a href=""https://i.stack.imgur.com/oG9h5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oG9h5.png"" alt=""enter image description here"" /></a></p>
<p>So when I run my test.py</p>
<pre><code>import requests

BASE=&quot;http://127.0.0.1:5000/&quot;
data=[{&quot;name&quot;:&quot;Duke Nuken&quot;,&quot;views&quot;:1000,&quot;likes&quot;:10},
    {&quot;name&quot;:&quot;Never Again&quot;,&quot;views&quot;:20,&quot;likes&quot;:80},
    {&quot;name&quot;:&quot;Belarus&quot;,&quot;views&quot;:500,&quot;likes&quot;:70}]

for i in range(len(data)):
    response=requests.put(BASE+&quot;videos/&quot;+str(i),data[i])
    print(response.json())
</code></pre>
<p>I see folowing log in the terminal:</p>
<pre><code>requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /videos/0 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x02FB8C30&gt;: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer 
die Verbindung verweigerte'))
</code></pre>
<p>Can anybody give my advice and tell me why I cannot get a response from container? Should I do any other configuration in docker-compose file or set some additional code for gunicorn?One more question is not directly related to docker, I am planning to deploy container with Azure container instances so is it requiered to use inside of container a Nginx or Azure takes itself things to be done with Nginx?
Thanks in advance</p>
"
"31381322","Docker in Docker cannot mount volume","<linux><jenkins><docker><containers>","64358000","Jenkins in docker-compose run another docker-compose","<docker><jenkins><docker-compose>","<p>I am running a Jenkins cluster where in the Master and Slave, both are running as a Docker containers. </p>

<p>The Host is latest boot2docker VM running on MacOS.</p>

<p>To allow Jenkins to be able to perform deployment using Docker, I have mounted the docker.sock and docker client from the host to the Jenkins container like this :-</p>

<pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker -v $HOST_JENKINS_DATA_DIRECTORY/jenkins_data:/var/jenkins_home -v $HOST_SSH_KEYS_DIRECTORY/.ssh/:/var/jenkins_home/.ssh/ -p 8080:8080 jenkins
</code></pre>

<p>I am facing issues while mounting a volume to Docker containers that are run inside the Jenkins container. For example, if I need to run another Container inside the Jenkins container, I do the following :-</p>

<pre><code>sudo docker run -v $JENKINS_CONTAINER/deploy.json:/root/deploy.json $CONTAINER_REPO/$CONTAINER_IMAGE 
</code></pre>

<p>The above runs the container, but the file ""deploy.json"" is NOT mounted as a file, but instead as a ""Directory"". Even if I mount a Directory as a Volume, I am unable to view the files in the resulting container. </p>

<p>Is this a problem, because of file permissions due to Docker in Docker case?</p>
","<p>I am try to start a docker-compose project from Jenkins launched in another docker-compose.
This is the docker-compose of jenkins:</p>
<pre><code>version: '3.7'
services:
  jenkins:
    image: jenkins/jenkins:lts
    privileged: true
    user: root
    ports:
      - 8081:8080
      - 50000:50000
    container_name: jenkins
    volumes:
      - ./jenkins:/var/jenkins_home
      - ./test_deploy:/test_deploy
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker
      - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose
</code></pre>
<p>I have a pipeline like this:</p>
<pre><code>pipeline {
    agent any

    stages {
        ... clone project etc ...
        stage('Run APP') {
            steps {
                dir('/test_deploy/app') {
                    sh 'docker-compose up -d'
                }
            }
        }
    }
}

</code></pre>
<p>the docker-compose that I launch via pipeline not load correctly the volumes, for example this is a fragment of the docker-file I launch</p>
<pre><code>rabbitmq:
    image: rabbitmq:3-management
    container_name: app_rabbitmq
    restart: unless-stopped
    user: root
    hostname: &quot;rabbit&quot;
    environment:
      RABBITMQ_DEFAULT_USER: &quot;rabbitmq&quot;
      RABBITMQ_DEFAULT_PASS: &quot;rabbitmq&quot;
      RABBITMQ_DEFAULT_VHOST: &quot;/&quot;
    ports: 
      - &quot;5672:5672&quot;
      - &quot;15672:15672&quot;
    volumes:
      - ./rabbitdata:/var/lib/rabbitmq
    networks:
      - APP
</code></pre>
<p>the <code>rabbitdata</code> folder is created on root of host machine on <code>/test_deploy/rabbitdata</code> but I need that is created in the <code>./test_deploy</code> of volume mounted on jenkis.</p>
<p>I have other services that I starts in my docker-file and the attached volume with file and code necessary for run. The error I receive is there are not found. How I can fix that?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","64029351","connecting two docker containers nodejs and mongodb","<node.js><mongodb><docker>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I'm trying to connect two conainers using networks docker, but for some reason I'm getting a error:</p>
<p>MongoNetworkError: failed to connect to server [db:27017] on first connect [Error: connect ECONNREFUSED 172.22.0.3:27017</p>
<p>Any suggestion?</p>
<p>Thanks!</p>
<p>docker-compose.yml</p>
<pre><code>version: '3'

services:
  nodejs:
    build:
      context: .
      dockerfile: Dockerfile.dev
    image: nodejs
    container_name: nodejs
    restart: unless-stopped
    env_file: 
      - .env
    environment:
      - MONGO_USERNAME=$MONGO_USERNAME
      - MONGO_PASSWORD=$MONGO_PASSWORD
      - MONGO_HOSTNAME=db
      - MONGO_PORT=$MONGO_PORT
      - MONGO_DB=$MONGO_DB 
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - .:/home/node/app
      - node_modules:/home/node/app/node_modules
    networks:
      - app-network
    command: /home/node/app/node_modules/.bin/nodemon app.js
  db:
    image: mongo:4.2.9-bionic
    container_name: db
    restart: unless-stopped
    env_file: 
      - .env
    environment: 
      - MONGO_INITDB_ROOT_USERNAME=$MONGO_USERNAME
      - MONGO_INITDB_ROOT_PASSWORD=$MONGO_PASSWORD
    volumes:
      - dbdata:/data/db
    networks:
      - app-network
networks:
  app-network:
    driver: bridge
volumes:
  dbdata:
  node_modules:
</code></pre>
"
"33913020","Docker remove <none> TAG images","<docker>","64330661","How To Remove Anonymous Image In Docker?","<docker><dockerfile><docker-cli>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>So, I want to create an image for my app using</p>
<pre><code>docker build .
</code></pre>
<p>But I forgot to give it a name and the name became <code>&lt;none&gt;</code>. When I tried to remove this with <code>docker image rm &lt;none&gt;</code>
The output is</p>
<pre><code>bash: syntax error near unexpected token `newline'
</code></pre>
<p>How to remove this anonymous image?</p>
"
"35290901","How to populate a Mysql docker container during build of another container?","<docker><dockerfile><docker-compose>","64247215","Automatically create the database with all tables during the Docker build","<mysql><database><docker>","<p>I am trying to build a docker-compose file that automatically builds a container for my application and a mysql-container that stores all the data.</p>
<p>Within my dockerfile for my application I have a script, that sets up all the database tables and preset values, that are necessary for the application to run. Is it possible to somehow pass the reference of the mysql container to myapplication that during build of myapplication it can access the mysql container?</p>
<p>If so, how can I do that, and if not, how should it be done then?</p>
<p>docker-compose.yml</p>
<pre class=""lang-yaml prettyprint-override""><code>mysql:
  image: mysql/mysql-server
  ports:
    - &quot;3306:3306&quot;
  environment:
    MYSQL_USER: root
    MYSQL_ROOT_PASSWORD: mysql-root-password

myapplication:
  command: /home/myapplication/startup.sh
  build: ./..
  dockerfile: ./dockerfiles/myapplication
  ports:
    - &quot;443:443&quot;
    - &quot;8090:8090&quot;
  links:
    - mysql:docker-mysql
  environment:
    MYSQL_SERVER: docker-mysql
</code></pre>
<p>myapplication-dockerfile</p>
<pre><code>FROM ubuntu:latest

EXPOSE 443

ENV applicationPath=&quot;/home/application“

COPY . ${applicationPath}

#Execute installationScript
RUN /bin/bash -c &quot;./${applicationPath}/Tools/install_all.sh&quot;
</code></pre>
","<p>I'm currently starting to work with docker. For my application I need to provide a MySQL database, which is already there and the mysql server works also. My problem is that I want Docker to automatically import the database (e.g. from an sql file) during the build process, so that it's available from the beginning so I dont have to import it manually afterwards.</p>
<p>Thanks for your answers!</p>
"
"36362150","Docker with a new nameserver","<docker><dockerfile>","64370210","How to Add nameserver as echo nameserver 8.8.8.8 >> /etc/resolv.conf using docker file","<python><docker>","<p>How I can add new nameserver in <code>/etc/resolv.conf</code> (dockerfile)?</p>

<p>On my dockerfile I use:</p>

<pre><code>FROM ubuntu:14.04

RUN echo ""nameserver 10.111.122.1"" &gt;&gt; /etc/resolv.conf
</code></pre>

<p>On my test I use:</p>

<pre><code>docker run --rm 746cb98d6c9b echo cat /etc/resolv.conf
</code></pre>

<p>I didn't get my change (the new nameserver)... So I try adding mannualy with </p>

<pre><code>docker run --rm 746cb98d6c9b echo ""nameserver 10.111.122.1"" &gt;&gt; /etc/resolv.conf 
</code></pre>

<p>and I get </p>

<pre><code>zsh: permission denied: /etc/resolv.conf
</code></pre>

<p>How I can change permission of this file OR use a root user OR use a chmod in docker files ? My real task is to add and dns server for my build of this dockerfile.</p>

<p>I'm using a linux mint.</p>

<p>I'm get a correct result with a ping test on docker run command (with <code>--dns</code>)</p>
","<p>I need to add nameserver as 8.8.8.8 in /etc/resolv.conf using docker file</p>
<p>I used below command
echo nameserver 8.8.8.8 &gt;&gt; /etc/resolv.conf in docker file but not working.
When I tried same command inside docker conatainer it is working.</p>
<p>Please help me on this issue to resolve using docker file</p>
"
"40465979","Change Docker native images location on Windows 10 Pro","<docker><windows-10>","64318985","How to save Docker files (images, container) in specific driver?","<docker><docker-for-windows>","<p><em>This is not a duplicate of</em> <a href=""https://stackoverflow.com/questions/33933107/change-docker-machine-location-windows"">Change Docker machine location - Windows</a></p>

<p>I'm using docker native, version 1.12.1-stable (build: 7135) on Windows 10 Pro with Hyper-V enabled.
So docker is <strong>not</strong> running with VirtualBox nor do I have the folder <em>C:\Users\username\.docker</em></p>

<p>I'd like to move docker's images, caches, ... to my secondary drive <em>D:\</em></p>

<p>I guess I should edit the Docker Daemon configuration.</p>

<p><img src=""https://i.imgur.com/nC9XaRu.jpg"" alt=""Docker Daemon Configuration""></p>

<p>I tried to add <code>""graph"": ""/D/docker""</code>. Docker started correctly but I couldn't pull any image because of an error</p>

<blockquote>
  <p>open /D/docker/tmp/GetImageBlob135686954: no such file or directory</p>
</blockquote>

<p>How to tell docker to use another path to store its images, etc ?</p>
","<p>I am using Docker for Windows 19.03.13 on Windows 10 x64 version 2004. (I seen this type of question 3 years ago, and at this time, many thing changed.)</p>
<p>How to save Docker files (images, container) in specific driver? My <code>C:\</code> is nearly full now.</p>
<p>This is my system information</p>
<pre><code>C:\Users\donhuvy&gt;docker version
Client: Docker Engine - Community
 Cloud integration  0.1.18
 Version:           19.03.13
 API version:       1.40
 Go version:        go1.13.15
 Git commit:        4484c46d9d
 Built:             Wed Sep 16 17:00:27 2020
 OS/Arch:           windows/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.13
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       4484c46d9d
  Built:            Wed Sep 16 17:07:04 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.3.7
  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175
 runc:
  Version:          1.0.0-rc10
  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683

C:\Users\donhuvy&gt;docker system df
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              9                   5                   2.95GB              1.245GB (42%)
Containers          14                  0                   39.01kB             39.01kB (100%)
Local Volumes       2                   2                   48.77MB             0B (0%)
Build Cache         12                  0                   32.56kB             32.56kB

C:\Users\donhuvy&gt;

</code></pre>
<p><a href=""https://i.stack.imgur.com/JwGmE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwGmE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/EbC3R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbC3R.png"" alt=""enter image description here"" /></a></p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","64098685","Can't mount docker container's working directory to host","<python><docker><dockerfile><docker-volume><docker-build>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I'm trying to build a docker container for my python application that allow edits to a config file in the application's working directory so I don't need to rebuild the image every time a change is made.</p>
<p>Dockerfile:</p>
<pre><code>FROM python:3

VOLUME /app
COPY . /app
WORKDIR /app

RUN pip install requirements.txt

CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p>Once the image is built I deploy the container with</p>
<pre><code>docker run -d --name app -v /path/on/host/app:/app app:latest
</code></pre>
<p>I get an error in the container, <code>python: can't open file 'main.py': [Errno 2] No such file or directory</code> and <code>/path/on/host/app</code> stays empty.</p>
<p>I can't figure out what I need in my Dockerfile to make this work.</p>
<h3>Update - Found a solution</h3>
<p>Since I only need access to the config file here are the changes I made to get this to work</p>
<p>Dockerfile:</p>
<pre><code>FROM python:3
COPY . /app
WORKDIR /app
RUN pip install requirements.txt
VOlUME /app/config
CMD [&quot;python&quot;,&quot;main.py&quot;]
</code></pre>
<p>Now when I run the container I copy the config to <code>/path/on/host/app</code></p>
<pre><code>docker run -d --name app --mount type=bind,source=/path/on/host/app,target=/app/config
</code></pre>
<p>This solved my issue and now I can update the config and restart the container to get the changes to apply.</p>
"
"45282608","How to directly mount NFS share/volume in container using docker compose v3","<docker><docker-compose><docker-swarm><docker-swarm-mode>","64138567","How to mount a folder in a machine to a docker container in another machine?","<docker><nfs><mounted-volumes>","<p>I have a compose file with v3 where there are 3 services sharing/using the same volume. While using swarm mode we need to create extra containers &amp; volumes to manage our services across the cluster. </p>

<p>I am planning to use NFS server so that single NFS share will get mounted directly on all the hosts within the cluster. </p>

<p>I have found below two ways of doing it but it needs extra steps to be performed on the docker host -</p>

<ul>
<li><p>Mount the NFS share using ""fstab"" or ""mount"" command on the host &amp; then use it as a host volume for docker services.</p></li>
<li><p>Use Netshare plugin - <a href=""https://github.com/ContainX/docker-volume-netshare"" rel=""noreferrer"">https://github.com/ContainX/docker-volume-netshare</a></p></li>
</ul>

<p>Is there a standard way where i can directly use/mount NFS share using docker compose v3 by performing only few/no steps(I understand that ""nfs-common"" package is required anyhow) on the docker host?</p>
","<p>I have a running docker container that uses volume mounting and NFS to store data in another machine.</p>
<p>Now I want to map this volume to a new folder in the filesystem of a new machine.</p>
<p>shall I do bind mounting in the container to the new VM location like this &lt;VM_IP&gt;:/path_to_folder is that possible?</p>
<p>or is there a way to make the existing volume always uses that new folder as a reference instead of the NFS?</p>
<p>Thank you!</p>
"
"46080290","ReactJS browser app cannot see things in the Docker Compose network","<reactjs><docker><axios>","64098159","HTTP requests between two docker services works fine but I don't know why","<node.js><reactjs><docker><docker-compose>","<p>I have a ReactJS project with its own Dockerfile, exposing port 3000:3000.</p>

<p>I also have a PHP project with its own Dockerfile, exposing port 80:80.  The PHP app also has containers for MySQL, Redis and Nginx</p>

<p>For the PHP app, I have a docker-compose file that creates a network (<code>my-net</code>) for PHP, Nginx, MySQL and Redis to communicate on.  However, I now want the ReactJS (which is in a separate project) to be able to communicate with the PHP app.</p>

<p>I added a docker-compose file to the React project, and added it to the network from the PHP project <code>my-net</code> and declared it as <code>external</code> so that it doesn't try to create it.</p>

<p>This seems to work: From the ReactJS container, I can ping <code>app</code> (the name of my backend service) and it works properly.  However, from the ReactJS code, if I use something like <code>axios</code> to try and hit the backend API, it can't resolve <code>app</code> or <code>http://app</code> or any variation.  It can however access the underlying IP address if I substitute that into in <code>axios</code>.</p>

<p>So there seems to be some issue with the hostname resolution, and presumably this is on the <code>axios</code> / JavaScript end.  is there something I'm missing or a reason this isn't working?</p>
","<p>I have two nodejs services:</p>
<ul>
<li><code>/server</code> =&gt; NodeJS API built using NestJS. Exposes one <code>GET</code> route: <code>/</code> that return &quot;Hello World&quot;</li>
<li><code>/client</code> =&gt; ReactJS application that fetches data(&quot;Hello World&quot;) from the <code>server</code> api.</li>
</ul>
<hr />
<p>This is my project structure:</p>
<pre><code>.
├── client
│   ├── Dockerfile
│   ├── package.json
│   ├── package-lock.json
│   ├── public
│   │   ├── favicon.ico
│   │   ├── index.html
│   │   ├── logo192.png
│   │   ├── logo512.png
│   │   ├── manifest.json
│   │   └── robots.txt
│   ├── README.md
│   ├── src
│   │   ├── App.css
│   │   ├── App.test.tsx
│   │   ├── App.tsx
│   │   ├── index.css
│   │   ├── index.tsx
│   │   ├── logo.svg
│   │   ├── react-app-env.d.ts
│   │   ├── serviceWorker.ts
│   │   └── setupTests.ts
│   ├── tsconfig.json
│   └── yarn.lock
├── docker-compose.yml
└── server
    ├── dist
    │   ├── app.controller.d.ts
    │   ├── app.controller.js
    │   ├── app.controller.js.map
    │   ├── app.module.d.ts
    │   ├── app.module.js
    │   ├── app.module.js.map
    │   ├── app.service.d.ts
    │   ├── app.service.js
    │   ├── app.service.js.map
    │   ├── main.d.ts
    │   ├── main.js
    │   ├── main.js.map
    │   └── tsconfig.build.tsbuildinfo
    ├── Dockerfile
    ├── nest-cli.json
    ├── package.json
    ├── package-lock.json
    ├── README.md
    ├── src
    │   ├── app.controller.spec.ts
    │   ├── app.controller.ts
    │   ├── app.module.ts
    │   ├── app.service.ts
    │   └── main.ts
    ├── test
    │   ├── app.e2e-spec.ts
    │   └── jest-e2e.json
    ├── tsconfig.build.json
    └── tsconfig.json
</code></pre>
<hr />
<p><code>server/Dockerfile</code></p>
<pre><code>FROM node:latest

WORKDIR /usr/src/app

COPY package.json .
COPY package-lock.json .

RUN npm install

COPY . .

CMD [&quot;npm&quot;, &quot;run&quot;, &quot;start:dev&quot;]
</code></pre>
<hr />
<p><code>client/Dockerfile</code></p>
<pre><code># pull official base image
FROM node:13.12.0-alpine

# set working directory
WORKDIR /usr/src/app

# add `/app/node_modules/.bin` to $PATH
ENV PATH /app/node_modules/.bin:$PATH

# install app dependencies
COPY package*.json ./

RUN npm install --silent
RUN npm install react-scripts@3.4.1 -g --silent

# add app
COPY . ./

# start app
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<hr />
<p><code>docker-compose.yml</code></p>
<pre class=""lang-yml prettyprint-override""><code>version: &quot;3.8&quot;
services:
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ''
      POSTGRES_DB: the-notebook-dev-db
  server:
    build: ./server
    volumes:
      - ./server:/usr/src/app
    ports:
      - '4000:4000'
    depends_on:
      - postgres
  client:
    tty: true
    build: ./client
    volumes:
      - ./client:/usr/src/app
    ports:
      - '3000:3000'
    depends_on:
      - server
</code></pre>
<p>Everything works as expected. Client makes a GET request to <code>http://localhost:4000</code>, receives the &quot;Hello World&quot; message. However, shouldn't <code>localhost</code>, when used by the client container, point to the client container itself? How is this setup working and why am I not able to use <code>http://service:4000</code> instead of <code>http://localhost:4000</code>?</p>
"
"46166304","Docker Compose: volumes without colon (:)","<docker><docker-compose>","64316357","What is the differences between /aaa:/bbb /bbb ccc:/bbb in volumes of docker-compose.yml","<docker><docker-compose>","<p>I have a <code>docker-compose.yml</code> file with the following:</p>

<pre><code>volumes:
  - .:/usr/app/
  - /usr/app/node_modules
</code></pre>

<p>First option maps current host directory to <code>/usr/app</code>, but what does the second option do?</p>
","<p>I'd like to understand <code>volumes</code> in <code>docker-compose.yml</code></p>
<p>What is the difference between them.</p>
<p><code>/aaa:/bbb</code> <code>/bbb</code> <code>ccc:/bbb</code></p>
<p>for example here is sample of  <code>docker-compose.yml</code></p>
<pre><code>version: '3'

services:
  api-server:
    build: ./api
    links:
      - 'db'
    ports:
      - '3000:3000'
    volumes:
      - ./api:/src
      - ./src/node_modules
    tty: true
    container_name: api-server
</code></pre>
<p><code>Dockerfile</code> is following.</p>
<pre><code>FROM node:alpine

WORKDIR /src
COPY . .

RUN rm -rf /src/node_modules
RUN rm -rf /src/package-lock.json
RUN apk --no-cache add curl



RUN yarn install

CMD yarn start:dev
</code></pre>
<p>What the following mean ?</p>
<pre><code>    volumes:
      - ./api:/src
      - ./src/node_modules
</code></pre>
<p>Following is directory structure</p>
<pre><code>base) [api]
 $ ls
Dockerfile          dist                node_modules        package-lock.json   src                 tsconfig.build.json yarn.lock
README.md           nest-cli.json       ormconfig.js        package.json        test                tsconfig.json
</code></pre>
<p>If someone has opinion, please let me know.</p>
<p>Thanks</p>
"
"48561981","Activate python virtualenv in Dockerfile","<python><docker><virtualenv><dockerfile>","64132470","How to install pip packages in container","<python><docker><pip><virtualenv>","<p>I have a Dockerfile where I try to activate python virtualenv after what, it should install all dependencies within this env. However, everything still gets installed globally. I used different approaches and non of them worked. I also do not get any errors. Where is a problem?</p>

<p>1.
<code>ENV PATH $PATH:env/bin</code></p>

<p>2.
<code>ENV PATH $PATH:env/bin/activate</code></p>

<p>3.
<code>RUN . env/bin/activate</code></p>

<p>I also followed <a href=""https://github.com/GoogleCloudPlatform/python-runtime#kubernetes-engine--other-docker-hosts"" rel=""noreferrer"">an example of a Dockerfile config for the python-runtime image on Google Cloud</a>, which is basically the same stuff as above.</p>

<blockquote>
  <p>Setting these environment variables are the same as running source /env/bin/activate.</p>
</blockquote>

<p><code>ENV VIRTUAL_ENV /env</code></p>

<p><code>ENV PATH /env/bin:$PATH</code></p>

<p>Additionally, what does <code>ENV VIRTUAL_ENV /env</code> mean and how it is used?</p>
","<p>I am trying to setup a python virtual environment on a docker image running a <strong>docker build</strong></p>
<p>The terminal output is ok when I run <strong>docker build ..</strong> but when I login into my container, no packages are installer in my virtual environment.</p>
<pre class=""lang-sh prettyprint-override""><code>#Dockerfile

RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
RUN python3.8 get-pip.py
RUN pip install virtualenv
RUN virtualenv venv
RUN /home/ubuntu/venv/bin/pip install -r auto/requirements.txt
...
</code></pre>
<pre class=""lang-sh prettyprint-override""><code># Docker build command
docker build --no-cache -t auto-server:1.0 .
</code></pre>
<pre class=""lang-sh prettyprint-override""><code># Terminal output

Step 27/27 : RUN /home/ubuntu/venv/bin/pip install -r auto/requirements.txt
 ---&gt; Running in d27dbb9a4c97
Collecting asgiref==3.2.10
  Downloading asgiref-3.2.10-py3-none-any.whl (19 kB)
Collecting beautifulsoup4==4.9.1
  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)
Collecting Django==3.1.1
  Downloading Django-3.1.1-py3-none-any.whl (7.8 MB)

...

Successfully installed Django-3.1.1 asgiref-3.2.10 beautifulsoup4-4.9.1 fake-useragent-0.1.11 joblib-0.16.0 numpy-1.19.2 pandas-1.1.2 python-dateutil-2.8.1 pytz-2020.1 scikit-learn-0.23.2 scipy-1.5.2 six-1.15.0 sklearn-0.0 soupsieve-2.0.1 sqlparse-0.3.1 threadpoolctl-2.1.0
</code></pre>
<p>Here is what I get when I list pakages in my virtual environment:</p>
<pre class=""lang-sh prettyprint-override""><code>$ docker exec -ti auto-server bash
root@9c1f914d1b7b:/home/ubuntu# source venv/bin/activate
(venv) root@9c1f914d1b7b:/home/ubuntu# pip list
Package    Version
---------- -------
pip        20.2.2
setuptools 49.6.0
wheel      0.35.1
WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.
You should consider upgrading via the '/home/ubuntu/venv/bin/python -m pip install --upgrade pip' command.

</code></pre>
<p>Is it the right way to do it? How to make sure packages will be installed?</p>
"
"48633244","Access Angular app running inside Docker container","<angular><docker><dockerfile><redhat><redhat-containers>","64248411","run an application live for dev purpose","<angular><docker>","<p>I've created a docker image to test an Angular app but I cannot connect from host to running app inside docker container.</p>

<p>The image was created using a Dockerfile with:
EXPOSE 4200 8080 80</p>

<p>I run the docker container with command:
docker run -ti -p 4200:4200 angulardev /bin/bash</p>

<p>Inside container I create the Angular application and start it using:
ng serve</p>

<p>From container, if I open localhost:4200 I see the application but I cannot access it from host OS (RHEL7)</p>

<p>What is wrong? The Angular app starts on port 4200 which is exposed and mapped to host 4200.</p>

<p>Thanks.</p>
","<p>I'am a beginner in Docker and I'am trying to run my source code in a Container with making my source-code folder as Volume to be able to modify source code in realtime.</p>
<p>My source code is a starter angular app</p>
<p>So my current repo contains :</p>
<pre><code>DockerFile
angular-app/
</code></pre>
<p>#DockerFile:</p>
<pre><code>FROM node:latest
LABEL author=&quot;Karim&quot;

RUN npm install -g @angular/cli

WORKDIR /var/www/angular-app

ENTRYPOINT [&quot;npm&quot;,&quot;start&quot;]
</code></pre>
<p>in this repo I run :</p>
<pre><code>&gt; docker build -t angular-image .
&gt; docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
angular-image       latest              fcbcf2d10c65        58 minutes ago      989MB

&gt; docker run -p 1000:4200 -v &quot;C:\Users\k.garali\Desktop\Docker\angular-app-image/angular-app&quot;:/var/www/angular-app angular-image


&gt; angular-app@0.0.0 start /var/www/angular-app
&gt; ng serve


chunk {main} main.js, main.js.map (main) 56.9 kB [initial] [rendered]
chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 141 kB [initial] [rendered]
chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
chunk {styles} styles.js, styles.js.map (styles) 12.5 kB [initial] [rendered]
chunk {vendor} vendor.js, vendor.js.map (vendor) 2.38 MB [initial] [rendered]
Date: 2020-10-07T16:27:10.277Z - Hash: 17160092220a366bdbcb - Time: 18247ms
** Angular Live Development Server is listening on localhost:4200, open your browser on http://localhost:4200/ **
: Compiled successfully.

&gt; docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS
    PORTS                    NAMES
7f9a2185636d        angular-image       &quot;npm start&quot;         2 seconds ago       Up 2 seconds
    0.0.0.0:1000-&gt;4200/tcp   fervent_morse
</code></pre>
<p>But Finally when I go in browser to localhost:1000, I get no reponse ERR_EMPTY_RESPONSE</p>
<p>any clarification ?</p>
"
"48700453","Docker image build with PHP zip extension shows ""bundled libzip is deprecated"" warning","<php><docker><php-extension><libzip>","64076482","how to install ZipArchive in the PHP Docker Container","<php><docker><docker-compose><dockerfile><php-7>","<p>I have a <code>Dockerfile</code> with a build command like this:</p>

<pre><code>#install some base extensions
RUN apt-get install -y \
        zlib1g-dev \
        zip \
  &amp;&amp; docker-php-ext-install zip
</code></pre>

<p>I get this warning from build output:</p>

<blockquote>
  <p>WARNING: Use of bundled libzip is deprecated and will be removed.<br>
  configure: WARNING: Some features such as encryption and bzip2 are not available.<br>
  configure: WARNING: Use system library and --with-libzip is
  recommended.</p>
</blockquote>

<p>What is the correct way to install the zip extension without these warnings?</p>

<p>My complete Dockerfile looks like:</p>

<pre><code>FROM php:7.2-apache

RUN apt-get clean
RUN apt-get update

#install some basic tools
RUN apt-get install -y \
        git \
        tree \
        vim \
        wget \
        subversion

#install some base extensions
RUN apt-get install -y \
        zlib1g-dev \
        zip \
  &amp;&amp; docker-php-ext-install zip

#setup composer
RUN curl -sS https://getcomposer.org/installer | php \
        &amp;&amp; mv composer.phar /usr/local/bin/ \
        &amp;&amp; ln -s /usr/local/bin/composer.phar /usr/local/bin/composer


WORKDIR /var/www/
</code></pre>
","<p>in my laravel project I have this error</p>
<pre><code>Class 'ZipArchive' not found
</code></pre>
<p>I need to install php-zip into my php container</p>
<p>I've changed the Dockerfile as I found some answers on the internet and added <code>extension=zip.so</code> to php.ini but it doesn't work.</p>
"
"49293967","How to pass environment variable to docker-compose up","<docker><docker-compose><dockerfile>","64389243","Docker compose with environment variables in command","<docker><docker-compose>","<p>I am trying to run a container. I already have the image uploaded to private Docker registry. I want to write a compose file to download and deploy the image. But I want to pass the TAG name as a variable from the docker-compose run command.My compose file looks like below. How can I pass the value for KB_DB_TAG_VERSION as part of docker-compose up command?</p>

<pre><code>version: '3'
services:
   db:
    #build: k-db
    user: ""1000:50""
    volumes:
      - /data/mysql:/var/lib/mysql
    container_name: k-db
    environment:
      - MYSQL_ALLOW_EMPTY_PASSWORD=yes
    image:  XX:$KB_DB_TAG_VERSION
    image: k-db
    ports:
      - ""3307:3306""
</code></pre>
","<p>Is it possible to pass environment variables in command to Docker compose like:</p>
<pre><code>docker-compose up --build -d --env VAR1=123 VAR2=456
</code></pre>
<p>I know that it is possible in <code>docker</code> but for Docker compose it seems to work for me.</p>
"
"49991978","Kubernetes not copying data into mounted Volume","<mysql><docker><kubernetes>","64026599","Volume mount folder being erased by kubernetes","<docker><kubernetes>","<p>According to the documentation here:
<a href=""https://docs.docker.com/storage/volumes/"" rel=""nofollow noreferrer"">https://docs.docker.com/storage/volumes/</a></p>

<blockquote>
  <p>If you start a container which creates a new volume, as above, and the container has files or directories in the directory to be mounted (such as /app/ above), the directory’s contents are copied into the volume. The container then mounts and uses the volume, and other containers which use the volume also have access to the pre-populated content.</p>
</blockquote>

<p>In other words, the expectation is that, if i have files like so in /var/lib/mysql</p>

<pre><code>root@d8fa9a8b305a:/var/lib/mysql# ls
auto.cnf         xtz           ib_logfile0  ibdata1  mysql               sys
debian-5.7.flag  ib_buffer_pool  ib_logfile1  ibtmp1   performance_schema
</code></pre>

<p>Then, when I mount a volume into <code>/var/lib/mysql</code> then all the files from the container should be copied into my volume.</p>

<p>But I find this is not happening:</p>

<pre><code>/var/lib/mysql/mysql # ls
auto.cnf        ib_buffer_pool  ib_logfile0     ib_logfile1     ibdata1
</code></pre>

<p>This is content of the volume that I mounted into /var/lib/mysql, and as you can see the data is not the same as the one present in /var/lib/mysql of the docker image itself. So, as a result, there's a failure on startup.</p>

<p><strong>Note</strong>: The Volume in question is actually mounted by kubernetes. So, I'm making a major assumption here that </p>

<pre><code>volumeMounts:
        - name: xtz-persistent-storage
          mountPath: ""/var/lib/mysql/""
</code></pre>

<p>is the equivalent of doing this : <code>docker run -p 443:443 --rm -v mysql:/var/lib/mysql &lt;image&gt;</code></p>
","<p>I have a pod (whose deployment config I have included below whichever I guessed as relevant to this question. Please tell me if more is needed) which uses a persistent volume. And the container is a docker whose docker file creates this folder <code>/mounted/folder</code>  and also some hierarchy of subfolders. After this pod is deployed, I can see that this <code>/mounted/folder</code> is wiped clean of all the subfolders. I saw this <a href=""https://stackoverflow.com/questions/39751421/mountpath-overrides-the-rest-of-the-files-in-that-same-folder"">question</a> where it was mentioned that adding <code>subPath</code> field will ensure that the folder contents are not deleted. But it didn't work. I may have to mention here that I have another pod using the same pvc which this pod is using (for having a common shared storage). When this subPath didn't work, I tried (unsuccessfully) to add subPath in that pod deployment also. Please help! Thanks!</p>
<pre><code>spec:
  containers:
    - name: nginx
      image: docker-registry-address/imagename:version
      volumeMounts:
        - mountPath: /mounted/folder
          subPath: folder
          name: name-of-volume

  volumes:
    - name: name-of-volume
      persistentVolumeClaim:
        claimName: pv-claim
</code></pre>
"
"50057998","How can I setup an official maven docker image with my own global settings.xml?","<maven><docker>","64392367","Building docker image with maven and dependency in corporate nexus repo with authentication","<java><docker><maven><build><nexus>","<p>I use docker for the first time in connection with GitLab CI. I am happy that GitLab does most of the work for me. 
I am using the official maven docker image <code>maven:3-jdk-8</code> (<a href=""https://hub.docker.com/_/maven/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/maven/</a>)</p>

<p>Now I would like to set the global <code>settings.xml</code> on that docker image, which contains data to my nexus server for the deploy phase.</p>

<p>I tried to follow this guide: <a href=""https://www.thegeekdiary.com/how-to-update-add-a-file-in-the-docker-image/"" rel=""nofollow noreferrer"">Adding a directory and image in the docker image</a> , but unfortunally I cannot connect to the bash of the docker image.</p>

<pre><code>root@build:~# docker run maven:3-jdk-8 /bin/bash -it
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
root@7d62e8b066f7:/# exit
</code></pre>

<p>How can I add my own global <code>settings.xml</code> to my maven docker image?</p>
","<p>I'm trying to use a docker image with maven to build my sources.
The project needs a dependency which is stored on our corporate authenticated nexus.
I've defined the repository in pom.xml but since the Nexus repo is authenticated I don't know where to provide the credentials.
If the build was local i would use settings.xml but given I wanna do this in the image build I can find a way to do this.</p>
<p>The dockerfile is like this:</p>
<pre><code>FROM diamol/maven AS builder


WORKDIR D:/wsEmy/myproject/src/
COPY pom.xml .
RUN mvn -B dependency:go-offline

COPY . .
RUN mvn package

# app
FROM diamol/openjdk

WORKDIR /app
COPY --from=builder D:/wsEmy/myproject/target/myproject.jar .
COPY --from=builder D:/wsEmy/myproject/confExt/application.properties .

EXPOSE 80
ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app/myproject.jar --com.led.appconf=&quot;/app&quot;]
</code></pre>
"
"52314900","How to connect local Mongo database to docker","<mongodb><docker><go>","64307867","Docker container of flask app is possible connect to localhost MongoDB","<mongodb><docker><flask><connection>","<p>I am working on golang project, recently I read about docker and try to use docker with my app. I am using mongoDB for database.
Now problem is that, I am creating Dockerfile to install all packages and compile and run the go project. 
I am running mongo data as locally, if I am running go program without docker it gives me output, but if I am using docker for same project (just installing dependencies with this and running project), it compile successfully but not gives any output, having error::</p>

<pre><code>CreateSession: no reachable servers 
</code></pre>

<p>my Dockerfile::</p>

<pre><code># Start from a Debian image with the latest version of Go installed
# and a workspace (GOPATH) configured at /go.
FROM golang
WORKDIR $GOPATH/src/myapp

# Copy the local package files to the container's workspace.
ADD . /go/src/myapp

#Install dependencies
RUN go get ./...

# Build the installation command inside the container.
RUN go install myapp

# Run the outyet command by default when the container starts.
ENTRYPOINT /go/bin/myapp

# Document that the service listens on port 8080.
EXPOSE 8080
EXPOSE 27017
</code></pre>
","<p>I’m new to docker.
I have the one flask app was running on my docker container and the application needs to connect to MongoDB for the CRUD action.</p>
<p>but I have some connection problems between the docker and the localhost. The container cannot connect to my localhost MongoDB.</p>
<p>So is a possible flask app from docker container connect to the localhost MongoDB?</p>
<p>My Flask app MongoDB config setup:</p>
<pre><code>cilent = pymongo.MongoClient('127.0.0.1',27017)
</code></pre>
<p>My Dockerfile config:</p>
<pre><code>FROM ubuntu:latest

MAINTAINER Michael Levan

CMD tail -f /dev/null

RUN apt-get update -y &amp;&amp; apt-get install -y python3-pip python-dev

EXPOSE 8080
EXPOSE 5000

COPY ./requirements.txt /app/requirements.txt

WORKDIR /app

RUN pip3 install -r requirements.txt

COPY . /app

ENTRYPOINT [ &quot;python3&quot; ]
CMD [ &quot;app.py&quot; ]
</code></pre>
"
"52848004","How to increase docker disk image size in Ubuntu","<docker><ubuntu-16.04>","64232127","Extending Docker Container Size on Amazon Linux 2","<amazon-web-services><docker><amazon-ec2>","<p>I am trying to increase the docker image size on ubuntu. When I do docker info I get following info</p>

<pre><code>Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 17.09.0-ce
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host macvlan null overlay
 Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0
runc version: 3f2f8b84a77f73d38244dd690525642a72156c64
init version: 949e6fa
Security Options:
 apparmor
 seccomp
  Profile: default
Kernel Version: 4.4.0-87-generic
Operating System: Ubuntu 16.04.3 LTS
OSType: linux
Architecture: x86_64
CPUs: 8
Total Memory: 15.67GiB
Name: no1010042033112.corp.adobe.com
ID: PYZE:KYTG:DXED:QI37:43ZM:56BB:TLM6:X2OJ:WDPA:35UP:Z4CU:DSNC
Docker Root Dir: /var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Experimental: false
Insecure Registries:
 127.0.0.0/8
Live Restore Enabled: false
</code></pre>

<p>As you can see that total memory is Total Memory: 15.67GiB. I couldn't find a way to do it on Ubuntu. I tried following ways</p>

<p>1) sudo dockerd --storage-opt dm.basesize=100G
2) Changing DOCKER_OPTS =""--storage-opt dm.basesize=50G"" in /etc/default/docker.</p>

<p>But none of these helped. This option is easily available in Docker config in Windows. But how to do it from a ubuntu terminal</p>
","<p>I have Docker installed on my Amazon Linux Instance. Below are the details of the docker install. I am trying to increase the default container size and am having errors in doing so. I have added on &quot;--storage-opt dm.basesize=100G&quot; to my /etc/sysconfig/docker-storage file, but am unable to get docker running after that statement. In my research I think it may have something to do with the storage driver that it is using: overlay2. Any advice on how I can increase would be helpful.</p>
<p><strong>Docker Info</strong>
<a href=""https://i.stack.imgur.com/DKJxP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DKJxP.png"" alt=""enter image description here"" /></a></p>
<p><strong>Docker Storage Options</strong></p>
<pre><code>DOCKER_STORAGE_OPTIONS=&quot;--storage-opt dm.basesize=100G&quot;
</code></pre>
<p><strong>Error when restarting Docker</strong></p>
<pre><code>    [root@ip-172-31-35-212 sysconfig]# systemctl status docker.service
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: failed (Result: start-limit) since Tue 2020-10-06 18:52:25 UTC; 2s ago
     Docs: https://docs.docker.com
  Process: 21254 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_ADD_RUNTIMES (code=exited, status=1/FAILURE)
</code></pre>
"
"53375613","Why is the Java 11 base Docker image so large? (openjdk:11-jre-slim)","<java><docker><alpine><java-11>","64009638","Why image base of spring boot microservices changed from alpine to ubuntu","<spring-boot><docker><ubuntu><jhipster><alpine>","<p>Java 11 is announced to be the most recent LTS version. So, we're trying to start new services based on this Java version.</p>

<p>However, the base Docker image for Java 11 is much larger than the equivalent  for Java 8:</p>

<ul>
<li><p><a href=""https://github.com/docker-library/openjdk/blob/master/8/jre/alpine/Dockerfile"" rel=""noreferrer""><code>openjdk:8-jre-alpine</code></a>: 84 MB</p></li>
<li><p><a href=""https://github.com/docker-library/openjdk/blob/master/11/jre/slim/Dockerfile"" rel=""noreferrer""><code>openjdk:11-jre-slim</code></a>: <strong>283</strong> MB</p></li>
</ul>

<p>(I'm considering only the <a href=""https://hub.docker.com/_/openjdk/"" rel=""noreferrer""><strong><em>official OpenJDK</em></strong></a> and <strong><em>the most lightweight</em></strong> images for each Java version.)</p>

<p>Deeper digging uncovered the following ""things"":</p>

<ul>
<li><p>the <a href=""https://github.com/docker-library/openjdk/blob/master/11/jre/slim/Dockerfile"" rel=""noreferrer""><code>openjdk:11-jre-slim</code></a> image uses the base image <code>debian:sid-slim</code>. This brings 2 issues: </p>

<ul>
<li><p>this is 60 MB larger than <code>alpine:3.8</code></p></li>
<li><p>the <a href=""https://www.debian.org/releases/sid/"" rel=""noreferrer"">Debian <code>sid</code></a> versions are unstable</p></li>
</ul></li>
<li><p>the <code>openjdk-11-jre-headless</code> package installed in the image is <strong>3 times larger</strong> than <code>openjdk8-jre</code> (inside running Docker container):</p>

<ul>
<li><p><code>openjdk:8-jre-alpine</code>: </p>

<blockquote>
<pre><code>/ # du -hs /usr/lib/jvm/java-1.8-openjdk/jre/lib/
57.5M   /usr/lib/jvm/java-1.8-openjdk/jre/lib/
</code></pre>
</blockquote></li>
<li><p><code>openjdk:11-jre-slim</code>:</p>

<blockquote>
<pre><code># du -sh /usr/lib/jvm/java-11-openjdk-amd64/lib/
179M    /usr/lib/jvm/java-11-openjdk-amd64/lib/
</code></pre>
</blockquote>

<p>Going deeper I discovered the ""root"" of this heaviness - it's the <code>modules</code> file of the JDK:</p>

<blockquote>
<pre><code># ls -lhG /usr/lib/jvm/java-11-openjdk-amd64/lib/modules
135M    /usr/lib/jvm/java-11-openjdk-amd64/lib/modules
</code></pre>
</blockquote></li>
</ul></li>
</ul>

<p>So, now the questions which came:</p>

<ul>
<li><p>Why is <code>alpine</code> not used any more as a base image for Java 11 slim images?</p></li>
<li><p>Why is the unstable <em>sid</em> version used for LTS Java images?</p></li>
<li><p>Why is the slim/headless/JRE package for OpenJDK 11 so large compared to the similar OpenJDK 8 package? </p>

<ul>
<li>What is this <em>modules</em> file which brings 135 MB in OpenJDK 11?</li>
</ul></li>
</ul>

<p><strong>UPD</strong>: as a solutions for these challenges one could use this answer: <a href=""https://stackoverflow.com/questions/53669151/java-11-application-as-docker-image/53669152#53669152"">Java 11 application as docker image</a></p>
","<p>Why image base in containers of spring boot microservices changed from alpine to ubuntu</p>
<ul>
<li>In Jhipster 5.3.4: openjdk:8-jre-alpine</li>
<li>In Jhipster 6.10.1: ubuntu...</li>
</ul>
<p>It seems, the image ubuntu have more MB of older image base...</p>
<p>Thanks</p>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","64310899","unknown shorthand flag: 'a' in -aq","<docker>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I follow tutorial <a href=""https://phoenixnap.com/kb/remove-docker-images-containers-networks-volumes"" rel=""nofollow noreferrer"">https://phoenixnap.com/kb/remove-docker-images-containers-networks-volumes</a> . My error</p>
<p><a href=""https://i.stack.imgur.com/g6jam.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g6jam.png"" alt=""enter image description here"" /></a></p>
<pre><code>Microsoft Windows [Version 10.0.17134.1304]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Users\MyPC&gt;docker container stop $(docker container ls -aq)
unknown shorthand flag: 'a' in -aq)
See 'docker container stop --help'.
</code></pre>
<p>How to fix it?</p>
"
"54121031","Multiple commands on docker ENTRYPOINT","<docker><dockerfile>","64006872","Docker: How can I execute 2 scripts one after the another using DockerFile","<docker><dockerfile>","<p>I'm trying to build a custom tcserver docker image. But I'm having some problems starting the webserver and the tomcat.<br/>
As far as I understand I should use ENTRYPOINT to run the commands I want.<br/>
The question is, is it possible to run multiple commands with ENTRYPOINT?<br/>
Or should I create a small bash script to start all?<br/><br/></p>

<p>Basically what I would like to do is:<br/></p>

<pre><code>ENTRYPOINT /opt/pivotal/webserver/instance1/bin/httpdctl start &amp;&amp; /opt/pivotal/webserver/instance2/bin/httpdctl start &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance1 -i /opt/pivotal/pivotal-tc-server-standard &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance2 -i /opt/pivotal/pivotal-tc-server-standard
</code></pre>

<p>But I don't know if that is a good practice or if that would even work.</p>
","<p>In my Dockerfile, I want to run 2 scripts.</p>
<p>I want once the first script ran successfully then only it will run another script because another script is consuming the output of the first script.</p>
<p>What changes do I need to make in Dockerfile to complete this task?</p>
<pre><code>FROM ubuntu

WORKDIR /usr/bin/

COPY ./example.py .

RUN chmod +x example.py

ENTRYPOINT [&quot;python3&quot;, &quot;example.py&quot;]

</code></pre>
<p>I want to include one move ENTRYPOINT to execute another script.</p>
"
"62169568","Docker Alpine Linux python (missing)","<docker><alpine>","64301417","I want to build a docker environment, but I got ERROR: unsatisfiable constraints: python (missing):","<python><docker>","<p>I have a pipeline which deploys my container from GitLab. Last deployment was 5 days ago and went without any problems. Today I deploy it and get the following error:</p>
<pre><code>$ apk add --no-cache curl python py-pip
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
 ERROR: unsatisfiable constraints:
   python (missing):
     required by: world[python]
</code></pre>
<p>My job definition is:</p>
<pre><code>my-deploy:
  type: my-deploy
  image: docker:stable
  script:
    - apk update
    - apk add --no-cache curl python py-pip &lt;-- Here the erorr happens
    ...
</code></pre>
","<p>I am trying to run a model and it asks me to build a docker environment. And I think I get an error message about my python? I do not understand it very much.</p>
<p>I have tried the top answer in <a href=""https://stackoverflow.com/questions/62169568/docker-alpine-linux-python-missing"">Docker Alpine Linux python (missing)</a>, but I still receive the same error message. I wonder if it is the specific python package requirement the Dockerfile asks for? Do I need to install these packages one-by-one on the directory that the model is in?</p>
<pre><code>RUN apk update &amp;&amp; apk add gfortran \
    musl-dev bash python py-pip doxygen git graphviz
</code></pre>
<p>I have both Python and Gfortran installed.</p>
<pre><code>$ which python
/opt/anaconda3/bin/python

$ which gfortran
/usr/local/bin/gfortran

$ python
Python 3.7.6 (default, Jan  8 2020, 13:42:34) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
</code></pre>
<p><strong>This is what I have in the terminal</strong></p>
<pre><code>$ sudo docker build -t simstrat:alpine .
Sending build context to Docker daemon  13.31kB
Step 1/6 : FROM alpine:latest
 ---&gt; a24bb4013296
Step 2/6 : MAINTAINER SURF Team &quot;davide.vanzo@eawag.ch&quot;
 ---&gt; Using cache
 ---&gt; e84c1b2e9e15
Step 3/6 : RUN apk update &amp;&amp; apk add gfortran   musl-dev bash python py-pip doxygen git graphviz
 ---&gt; Running in 7f7ed9219d01
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
v3.12.0-376-gb3fc85f1ac [http://dl-cdn.alpinelinux.org/alpine/v3.12/main]
v3.12.0-381-g200ba6e281 [http://dl-cdn.alpinelinux.org/alpine/v3.12/community]
OK: 12750 distinct packages available
ERROR:   python (missing):
unsatisfiable constraints:
    required by: world[python]
The command '/bin/sh -c apk update &amp;&amp; apk add gfortran  musl-dev bash python py-pip doxygen git graphviz' returned a non-zero code: 1
</code></pre>
<p><strong>This is the Docker file I have:</strong></p>
<pre><code>FROM alpine:latest
MAINTAINER SURF Team &quot;davide.vanzo@eawag.ch&quot;

# get the tools we need
RUN apk update &amp;&amp; apk add gfortran \
    musl-dev bash python py-pip doxygen git graphviz

RUN pip install FoBiS.py ford pygooglechart

# root dir
RUN mkdir /home/Simstrat
WORKDIR /home/Simstrat

# calls that are needed to build and start the container with the build environment:
# docker build -t simstrat:alpine .
# docker create --name simstrat -it -v &lt;pathToLocalGitRepoDirectory&gt;:/home/Simstrat simstrat:alpine
# docker start simstrat
# docker exec -it simstrat bash
</code></pre>
"
"62649255","How to increase/check default memory Docker has on linux?","<docker>","64073019","Increasing the memory allocation to docker daemon (dockerd) on Linux","<docker><docker-engine>","<p>I've seen that on Windows and Mac it's very easy to change the RAM containers are given - you just go into the GUI. But how do you do this on Linux, where it's a CLI instead of a GUI?</p>
<p>The Docker docs it mention an -m flag, but this flag doesn't give any response (just prints the entirety of the help output again) so I don't know whether it worked. It also seems specific to containers, whereas I'd like to change the global default.</p>
<p>Lastly, is there a way to check the current default RAM, so I can make sure whatever I do in the end actually worked?</p>
","<p>When using the Docker for macOS or Windows we can set the amount of memory that is allocated to the Docker daemon (by default it is 2GB). My question is there such a setting on Linux (Ubuntu 20.04.1) as well or is the docker daemon free to use as much memory as it wants on the Linux environment.</p>
<p>I was reading that we can control some configuration to dockerd using the /etc/docker/daemon.json file but got no example of memory allocation.</p>
"
"2342826","How can I pipe stderr, and not stdout?","<bash><grep><stdout><pipe><stderr>","59676625","Command `docker` can not work with pipeline. Do not work with `|`, `>`, `>>`","<linux><docker><ubuntu><pipeline>","<p>I have a program that writes information to <code>stdout</code> and <code>stderr</code>, and I need to process the <code>stderr</code> with <code>grep</code>, leaving <code>stdout</code> aside.</p>
<p>Using a temporary file, one could do it in two steps:</p>
<pre><code>command &gt; /dev/null 2&gt; temp.file
grep 'something' temp.file
</code></pre>
<p>But how can this be achieved without temp files, using one command and pipes?</p>
","<p>I want to search some text output by command <code>docker</code>, but it seems not work with pipeline.</p>

<p>My expected result of <code>docker &gt; a</code> should write all output in the terminal to file a. And expected result of <code>docker | grep info</code> should print something that contains <code>info        Display system-wide information</code>.</p>

<p>I added myself to the group docker.
The following is test cases.</p>

<pre>
lala@ubu:~/projects/docker 14:19:42
$ grep docker /etc/group
docker:x:999:lala
lala@ubu:~/projects/docker 14:19:47
$ ls -al 
总用量 8
drwxr-xr-x  2 lala lala 4096 1月  10 14:15 .
drwxr-xr-x 10 lala lala 4096 1月  10 10:21 ..
lala@ubu:~/projects/docker 14:19:57
$ ls -al | grep x
drwxr-xr-x  2 lala lala 4096 1月  10 14:15 .
drwxr-xr-x 10 lala lala 4096 1月  10 10:21 ..
lala@ubu:~/projects/docker 14:20:01
$ docker

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Options:

...... unnecessary output

Run 'docker COMMAND --help' for more information on a command.
lala@ubu:~/projects/docker 14:20:03
$ docker | grep x

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Options:

...... the same as above. unnecessary output

Run 'docker COMMAND --help' for more information on a command.
lala@ubu:~/projects/docker 14:20:15
$ docker > a

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Options:

...... the same as above again. unnecessary output

Run 'docker COMMAND --help' for more information on a command.
lala@ubu:~/projects/docker 14:21:26
$ cat a
lala@ubu:~/projects/docker 14:21:28
$ file a
a: empty
lala@ubu:~/projects/docker 14:21:30
$ 
</pre>

<p><code>docker &gt;&gt; a</code> did not work, either.</p>
"
"5178292","pip install mysql-python fails with EnvironmentError: mysql_config not found","<python><pip><mysql-python>","60020341","Getting ""OSError: mysql_config not found"" when trying to build a Docker image for my Python 3.7/Django project","<mysql><django><python-3.x><docker><dockerfile>","<p><strong>This is the error I get</strong></p>

<pre><code>(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install mysql-python
Downloading/unpacking mysql-python
  Downloading MySQL-python-1.2.3.tar.gz (70Kb): 70Kb downloaded
  Running setup.py egg_info for package mysql-python
    sh: mysql_config: not found
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 14, in &lt;module&gt;
      File ""/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py"", line 15, in &lt;module&gt;
        metadata, options = get_config()
      File ""setup_posix.py"", line 43, in get_config
        libs = mysql_config(""libs_r"")
      File ""setup_posix.py"", line 24, in mysql_config
        raise EnvironmentError(""%s not found"" % (mysql_config.path,))
    EnvironmentError: mysql_config not found
    Complete output from command python setup.py egg_info:
    sh: mysql_config: not found

Traceback (most recent call last):

  File ""&lt;string&gt;"", line 14, in &lt;module&gt;

  File ""/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py"", line 15, in &lt;module&gt;

    metadata, options = get_config()

  File ""setup_posix.py"", line 43, in get_config

    libs = mysql_config(""libs_r"")

  File ""setup_posix.py"", line 24, in mysql_config

    raise EnvironmentError(""%s not found"" % (mysql_config.path,))

EnvironmentError: mysql_config not found

----------------------------------------
Command python setup.py egg_info failed with error code 1
Storing complete log in /home/zjm1126/.pip/pip.log
(mysite)zjm1126@zjm1126-G41MT-S2:~/zjm_test/mysite$ pip install mysql-python
Downloading/unpacking mysql-python
  Running setup.py egg_info for package mysql-python
    sh: mysql_config: not found
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 14, in &lt;module&gt;
      File ""/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py"", line 15, in &lt;module&gt;
        metadata, options = get_config()
      File ""setup_posix.py"", line 43, in get_config
        libs = mysql_config(""libs_r"")
      File ""setup_posix.py"", line 24, in mysql_config
        raise EnvironmentError(""%s not found"" % (mysql_config.path,))
    EnvironmentError: mysql_config not found
    Complete output from command python setup.py egg_info:
    sh: mysql_config: not found

Traceback (most recent call last):

  File ""&lt;string&gt;"", line 14, in &lt;module&gt;

  File ""/home/zjm1126/zjm_test/mysite/build/mysql-python/setup.py"", line 15, in &lt;module&gt;

    metadata, options = get_config()

  File ""setup_posix.py"", line 43, in get_config

    libs = mysql_config(""libs_r"")

  File ""setup_posix.py"", line 24, in mysql_config

    raise EnvironmentError(""%s not found"" % (mysql_config.path,))

EnvironmentError: mysql_config not found

----------------------------------------
Command python setup.py egg_info failed with error code 1
Storing complete log in /home/zjm1126/.pip/pip.log
</code></pre>

<p>What can I do to resolve this?</p>
","<p>I'm using Python 3.7 and Django 2.  I have the following requirements.txt file, exported from my current project ...</p>

<pre><code>asgiref==3.2.3
Babel==2.8.0
Django==2.0
django-address==0.2.1
django-mysql==3.3.0
django-phonenumber-field==4.0.0
mysqlclient==1.4.6
phonenumbers==8.11.2
pycountry==19.8.18
pytz==2019.3
PyYAML==5.3
ruamel.yaml==0.16.6
ruamel.yaml.clib==0.2.0
sqlparse==0.3.0
</code></pre>

<p>I would like to create a docker image to run my Django virutal environment.  I have this Dockerfile ...</p>

<pre><code>FROM python:3.7-slim

RUN python -m pip install --upgrade pip

COPY requirements.txt requirements.txt
RUN python -m pip install -r requirements.txt

COPY . .
</code></pre>

<p>However I'm getting this error when I attempt to build the image.  I'm not certain what it means.  I can run the server (python manage.py runserver 8000) without problems so not sure why my virtual environment is dying ...</p>

<pre><code>(venv) localhost:web davea$ docker build .
Sending build context to Docker daemon  132.5MB
Step 1/5 : FROM python:3.7-slim
 ---&gt; f2d2e1dc8c72
Step 2/5 : RUN python -m pip install --upgrade pip
 ---&gt; Using cache
 ---&gt; cf901765d254
Step 3/5 : COPY requirements.txt requirements.txt
 ---&gt; Using cache
 ---&gt; e4770d5b0c4a
Step 4/5 : RUN python -m pip install -r requirements.txt
 ---&gt; Running in d4d937c92f06
Collecting asgiref==3.2.3
  Downloading asgiref-3.2.3-py2.py3-none-any.whl (18 kB)
Collecting Babel==2.8.0
  Downloading Babel-2.8.0-py2.py3-none-any.whl (8.6 MB)
Collecting Django==2.0
  Downloading Django-2.0-py3-none-any.whl (7.1 MB)
Collecting django-address==0.2.1
  Downloading django-address-0.2.1.tar.gz (16 kB)
Collecting django-mysql==3.3.0
  Downloading django_mysql-3.3.0-py3-none-any.whl (63 kB)
Collecting django-phonenumber-field==4.0.0
  Downloading django_phonenumber_field-4.0.0-py3-none-any.whl (51 kB)
Collecting mysqlclient==1.4.6
  Downloading mysqlclient-1.4.6.tar.gz (85 kB)
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-2p1gjaea/mysqlclient/setup.py'""'""'; __file__='""'""'/tmp/pip-install-2p1gjaea/mysqlclient/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-install-2p1gjaea/mysqlclient/pip-egg-info
         cwd: /tmp/pip-install-2p1gjaea/mysqlclient/
    Complete output (12 lines):
    /bin/sh: 1: mysql_config: not found
    /bin/sh: 1: mariadb_config: not found
    /bin/sh: 1: mysql_config: not found
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-2p1gjaea/mysqlclient/setup.py"", line 16, in &lt;module&gt;
        metadata, options = get_config()
      File ""/tmp/pip-install-2p1gjaea/mysqlclient/setup_posix.py"", line 61, in get_config
        libs = mysql_config(""libs"")
      File ""/tmp/pip-install-2p1gjaea/mysqlclient/setup_posix.py"", line 29, in mysql_config
        raise EnvironmentError(""%s not found"" % (_mysql_config_path,))
    OSError: mysql_config not found
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>
"
"15930782","Call ""java -jar MyFile.jar"" with additional classpath option","<java><jar><classpath>","59958629","Java class not found despite Jar being included","<java><linux><docker><docker-compose><gurobi>","<p>I created a jar file containing all my compiled stuff. Additionally my ant build script copies the required libs into a subfolder ""libs"". The structure looks like this:</p>

<pre><code>MyProgram.jar
libs/
</code></pre>

<p>So when I try to run my program now I get the following error:</p>

<pre><code>java -cp "".:/home/user/java/MyProgram/jar/libs"" -jar MyProgram.jar
java.lang.ClassNotFoundException: org.postgresql.Driver
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:186)
    at database.PostgresQL.getConnection(PostgresQL.java:38)
    at recommender.dao.Creative2IdxDAO.createCreatives2Idx(Creative2IdxDAO.java:19)
    at main.Main.calculateCorrelationMatrix(Main.java:51)
    at main.Main.main(Main.java:28)
java.lang.NullPointerException
    at recommender.dao.Creative2IdxDAO.createCreatives2Idx(Creative2IdxDAO.java:25)
    at main.Main.calculateCorrelationMatrix(Main.java:51)
    at main.Main.main(Main.java:28)
</code></pre>

<p>Why does this happen?</p>
","<p>I'm using Docker with the <a href=""https://hub.docker.com/r/bitnami/java"" rel=""nofollow noreferrer"">https://hub.docker.com/r/bitnami/java</a> image. I've got my jar file aa.jar and it uses Gurobi.</p>

<p>I'm setting the environment variables so that Gurobi can be found.</p>

<pre><code>GUROBI_HOME=/code/gurobi811/linux64
LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${GUROBI_HOME}/lib
GRB_LICENSE_FILE=/code/gurobi.lic
PATH=${PATH}:/opt/bitnami/java/bin:/opt/bitnami/java/lib:/opt/bitnami/java:/bin:/usr/bin/:/code/gurobi811/linux64/bin:/code/bin
JAVA_HOME=/opt/bitnami/java
</code></pre>

<p>When I run the code I also include gurobi.jar in the classpath</p>

<pre><code>java -cp gurobi.jar -jar aa.jar evaluate
</code></pre>

<p>This is working to an extent. When I don't include the classpath I get the error <code>Exception in thread ""main"" java.lang.NoClassDefFoundError: gurobi/GRBException</code>. The exception is the first mention of Gurobi in the code. When I include it I get </p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: gurobi/GRBExpr
java_1    |     at Alg.evaluate.main(evaluate.java:44)
java_1    | Caused by: java.lang.ClassNotFoundException: gurobi.GRBExpr
</code></pre>

<p>I'm not sure why this isn't working as all the files should be included in the jar being included by the -cp parameter.</p>

<p>Also I'm only on the Gurobi academic license, so I can't go to the official Gurobi support.</p>
"
"17633422","psql: FATAL: database ""<user>"" does not exist","<postgresql><psql>","60018004","Problem with Docker Compose - FATAL: database ""root"" does not exist (PostgresSQL)","<postgresql><docker><docker-compose>","<p>I'm using the PostgreSql app for mac (<a href=""http://postgresapp.com/"" rel=""noreferrer"">http://postgresapp.com/</a>). I've used it in the past on other machines but it's giving me some trouble when installing on my macbook. I've installed the application and I ran:</p>

<pre><code>psql -h localhost
</code></pre>

<p>It returns:</p>

<pre><code>psql: FATAL:  database ""&lt;user&gt;"" does not exist
</code></pre>

<p>It seems I can't even run the console to create the database that it's attempting to find. The same thing happens when I just run:</p>

<pre><code>psql 
</code></pre>

<p>or if I launch psql from the application drop down menu:</p>

<p>Machine stats:</p>

<ul>
<li><p>OSX 10.8.4</p></li>
<li><p>psql (PostgreSQL) 9.2.4</p></li>
</ul>

<p>Any help is appreciated. </p>

<p>I've also attempted to install PostgreSql via homebrew and I'm getting the same issue. I've also read the applications documentation page that states: </p>

<blockquote>
  <p>When Postgres.app first starts up, it creates the $USER database,
  which is the default database for psql when none is specified. The
  default user is $USER, with no password.</p>
</blockquote>

<p>So it would seem the application is not creating $USER however I've installed->uninstalled-reinstalled several times now so it must be something with my machine.</p>

<p>I found the answer but I'm not sure exactly how it works as the user who answered on this thread -> <a href=""https://stackoverflow.com/questions/13515834/getting-postgresql-running-in-mac-database-postgres-does-not-exist"">Getting Postgresql Running In Mac: Database &quot;postgres&quot; does not exist</a> didn't follow up. I used the following command to get psql to open: </p>

<pre><code>psql -d template1
</code></pre>

<p><em>I'll leave this one unanswered until someone can provide an explanation for why this works.</em></p>
","<p>I trying to create a Postgres Database with Docker Compose, but i'm not understanding why the PSQL client is telling me that a database called root doesn't exists. </p>

<p>That's the docker-compose.yml </p>

<pre><code>version: '3.3'

services:
  postgres:
    container_name: postgres
    image: ""library/postgres:latest""
    env_file:
      - .env # configure postgres
    volumes:
      - database-data:/var/lib/postgresql/data/ # persist data even if container shuts down
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_DB: ${DB_DATABASE}
    ports:
      - ""5432:5432""
    restart: unless-stopped

volumes:
  database-data:
</code></pre>

<p>When i run <code>docker-compose exec postgres env</code> it gives the right variables </p>

<pre><code>POSTGRES_PASSWORD=4544
POSTGRES_USER=root
POSTGRES_DB=xpack
</code></pre>

<p>And when i run <code>docker exec -it postgres /bin/bash</code> and <code>psql</code> it returns the following error: </p>

<pre><code>root@0719bff80472:/# psql -U root
psql: error: could not connect to server: FATAL:  database ""root"" does not exist
</code></pre>

<p>I don't if my database was created or not</p>
"
"18846174","Django: Detect database backend","<database><django><backend><detect>","59894507","Django 2.2 - Testing which DB my application is connected to","<django><database><docker><celery>","<p>I'm doing some ""extra"" queries in Django that need to work on both sqlite and postgres. The syntax of these queries varies between backend but I have no way of figuring out if I'm sending my queries to either postgres or sqlite.</p>

<p>Is there a way to get the current database adapter so I can branch my code and send the right query for the active database server?</p>
","<p>I'm hoping this will be similar to a few questions which have been previously answered. </p>

<p>I was wondering what the full processes are for testing the DB connection string (not the DB name) for the current connected DB in Django is? I see a few methods for printing the name, but across local development and staging, these names will be the same. I need to know more, such as the db engine (e.g., is it <code>PostgreSQL</code>, or is it <code>db.sqlite3</code>. etc etc), and various other DB parameters.</p>

<p>I'm currently trying to debug why a Celery beat task is failing to create database entries for the application DB <em>I think</em> I am connected to.</p>

<p>I'm performing a simple <code>obj, created = Object.objects.get_or_create()</code> DB method, and on logging <code>created</code>, some are <code>True</code> and some are <code>False</code>. All good so far.</p>

<p>However, the admin section of the Django CMS is not showing any more entries to the DB.</p>

<p>I believe testing the location and connection strings of the DB the Django application is using might be useful in debugging this ghost object creation...unless someone can advise on similar issues they have had with the Celery daemon not actually persisting a DB create/update etc...</p>

<p>Perhaps, something such as:</p>

<pre><code>from django.db import connection
print(connection.settings_dict)
</code></pre>

<p>Would be sufficient?</p>
"
"19104847","How to generate a Dockerfile from an image?","<image><repository><docker>","59730797","How to generate the Dockerfile from an official image?","<docker><dockerfile>","<p>Is it possible to generate a Dockerfile from an image?  I want to know for two reasons:</p>

<ol>
<li><p>I can download images from the repository but would like to see the recipe that generated them.</p></li>
<li><p>I like the idea of saving snapshots, but once I am done it would be nice to have a structured format to review what was done.</p></li>
</ol>
","<p>I want to generate the <code>Dockerfile</code> from a official Dockerhub image namely the <a href=""https://hub.docker.com/_/influxdb"" rel=""nofollow noreferrer"">influxDB</a>  </p>

<p>After trying the suggested method on </p>

<pre><code>$ alias dfimage=""docker run -v /var/run/docker.sock:/var/run/docker.sock --rm centurylink/dockerfile-from-image""
$ dfimage --help
$ docker pull influxdb
$ dfimage influxdb  
</code></pre>

<p>I get the error message </p>

<pre><code>/usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:42:in `rescue in request': 400 Bad Request: malformed Host header (Docker::Error::ClientError)
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:38:in `request'
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/connection.rb:65:in `block (2 levels) in &lt;class:Connection&gt;'
    from /usr/lib/ruby/gems/2.2.0/gems/docker-api-1.24.1/lib/docker/image.rb:172:in `all'
    from /usr/src/app/dockerfile-from-image.rb:32:in `&lt;main&gt;'
</code></pre>

<p>Therefore how can I generate the Dockerfile from the aforementioned image.</p>
"
"19425857","env: python\r: No such file or directory","<python><macos><osx-mountain-lion><shebang><env>","60165687","/usr/bin/env: ‘python\r’: No such file or directory","<python><docker><git-bash><docker-for-windows>","<p>My Python script <code>beak</code> contains the following shebang:</p>

<pre><code>#!/usr/bin/env python
</code></pre>

<p>When I run the script <code>$ ./beak</code>, I get</p>

<pre><code>env: python\r: No such file or directory
</code></pre>

<p>I previously pulled this script from a repository. What could be the reason for this?</p>
","<p>I'm new to Docker and i decided to take the beginners guide on <a href=""https://training.play-with-docker.com/microservice-orchestration/"" rel=""nofollow noreferrer"">Play with docker classroom (Application Containerization and Microservice Orchestration)</a>. I tried this tutorial on my PC and installing docker for windows. In <a href=""https://training.play-with-docker.com/microservice-orchestration/#step-1-containerized-link-extractor-script"" rel=""nofollow noreferrer"">Step 1: Containerized Link Extractor Script</a> on running the script <code>docker container run -it --rm linkextractor:step1 http://example.com/</code> I'm getting an error </p>

<p><code>IT-KEVIN@IT-KEVIN MINGW64 ~/linkextractor (step1)
$ docker container run -it --rm linkextractor:step1 http://example.com/
/usr/bin/env: ‘python\r’: No such file or directory</code></p>

<p>How can i fix the error? </p>
"
"19537645","Get environment variable value in Dockerfile","<docker>","60154995","Why can't I load environment variables in dockerfile?","<ruby-on-rails><ruby><docker>","<p>I'm building a container for a ruby app. My app's configuration is contained within environment variables (loaded inside the app with <a href=""http://github.com/bkeepers/dotenv"">dotenv</a>).</p>

<p>One of those configuration variables is the public ip of the app, which is used internally to make links.
I need to add a dnsmasq entry pointing this ip to 127.0.0.1 inside the container, so it can fetch the app's links as if it were not containerized.</p>

<p>I'm therefore trying to set an <code>ENV</code> in my Dockerfile which would pass an environment variable to the container.</p>

<p>I tried a few things.</p>

<pre><code>ENV REQUEST_DOMAIN $REQUEST_DOMAIN
ENV REQUEST_DOMAIN `REQUEST_DOMAIN`
</code></pre>

<p>Everything passes the ""REQUEST_DOMAIN"" string instead of the value of the environment variable though.
Is there a way to pass environment variables values from the host machine to the container?</p>
","<p>this is my dockerfile. environment variables was not recognized.<br>
However, hardcoded environment variables were successfully recognized. like 'DOMAIN' or 'NOTICE_FILE_PATH' were successfully recognized.</p>

<pre><code>MAINTAINER JeongWooYeong(wjd030811@gmail.com)

ENV SECRET_KEY_BASE $SECRET_KEY_BASE
ENV SCARFS_PASSWORD $SCARFS_PASSWORD
ENV API_KEY $API_KEY
ENV DOMAIN dsm-scarfs.hs.kr
ENV NOTICE_FILE_PATH /home/ubuntu/scarfs/storage/notice_file
ENV EXCEL_FILE_PATH /home/ubuntu/scarfs/storage/excel_file
ENV SINGLE_FILE_PATH /home/ubuntu/scarfs/storage/single_file
ENV MULTI_FILE_PATH /home/ubuntu/scarfs/storage/multi_file
ENV SCARFS_DB $SCARFS_DB

RUN apt-get update &amp;&amp; \
    apt-get install -y \
    default-libmysqlclient-dev \
    nodejs

RUN gem install bundler

COPY . .
WORKDIR .

RUN bundle install

EXPOSE 3000
VOLUME ["".storage"", ""/home/ubuntu/T-bone""]
RUN  [""bundle"", ""exec"", ""unicorn"", ""-E"", ""production"", ""-c"", ""config/unicorn.rb""]
</code></pre>

<p>and it caused the following error:</p>

<pre><code>ArgumentError: `secret_key_base` for production environment must be a type of String`
</code></pre>

<p>what can i do?</p>
"
"20845056","How can I expose more than 1 port with Docker?","<docker><docker-networking>","60131882","Publish several ports","<docker>","<p>So I have 3 ports that should be exposed to the machine's interface. Is it possible to do this with a Docker container?</p>
","<p>Docker command <code>docker container run --publish 8000:8080 --detach --name bb bulletinboard:1.0</code> publishes dockers <code>8080</code> port to <code>8000</code>. But how to deal when I need to publish two ports <code>8000:8080</code> and <code>1001:1010</code>?</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","59672952","How can i make docker to pull image from another host machine's local repository?","<docker><docker-registry><docker-pull>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>Please refer this link:</p>

<p><a href=""https://dev.to/mayeu/saving-time-and-bandwidth-by-caching-docker-images-with-a-local-registry-98b"" rel=""nofollow noreferrer"">Caching Docker Images</a></p>

<p>I have installed a local registry on Machine A, and it caches all the images I pull on Machine A.</p>

<p>Now, I am trying to pull some images on Machine B using Machine B's local repo. I cannot achieve it.</p>

<p>Machine A and B are on same network.</p>

<p>The ways I have tried.</p>

<ol>
<li>Set up proxy configuration on Machine B and tried to route it. </li>
<li>Added the registry-mirrors set to remote machines local registry.</li>
</ol>

<p>Nothing was fruitful. I am looking for help to achieve it.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","59315094","How can a container connect to a service on the docker host?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>There were two <a href=""https://stackoverflow.com/questions/29808920/how-to-get-the-hostname-of-the-docker-host-from-inside-a-docker-container-on-tha"">related</a> <a href=""https://stackoverflow.com/questions/29808920/how-to-get-the-hostname-of-the-docker-host-from-inside-a-docker-container-on-tha"">questions</a> but they didn't quite answer the question.  But if mods think this is a duplicate, please let me know.</p>

<p>I have a docker-compose.yml file that deploys phpmyadmin.  There is a mysql server hosted locally on the host proper (NOT as a container).</p>

<p>I have a config file for phpmyadmin to connect to my database.  I can't seem to find a domain name for docker host so I've been taking the subnet that the containers deploy on and using the <code>.1</code> of the subnet.  For example, initially containers deployed to 172.16.0.0/24 and so I declared in phpmyadmin's configuration to connect to 172.16.0.1</p>

<p>This question was born out of the fact that every time I re-deployed, i.e. issued <code>docker-compose down &amp;&amp; docker-compose up -d</code> the network address kept changing.  My work around is to explicitly declare the default ipam network subnet, which is a fine workaround and actually preferred because I can then pin mysql user logins to the ip address range.</p>

<p>But given that docker knows to resolve the container service ""phpmyadmin"" to the container's IP address, I figured there MUST be something for the host so I wouldn't have to re-declare the IP address each time.</p>

<p>So, is there a ""hostname"" that a container can use to talk to the host or am I stuck using IP addresses?</p>

<p>Edit: I'm using docker on Linux and would very much prefer to not run the container in host mode.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60148788","How can I retrieve my host machines IP address from inside a docker compose file?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I've looked at many stackoverflow answers but couldn't find a solution to my problem, so I apologise if it seems similar to other questions.<br>
I have a docker compose file which needs to connect to a local database sitting on port 5555. I need to be able to set an env var in the docker compose file that will translate to my host machines IP address.<br>
Something like</p>

<pre><code>environment:
  - db_uri=postgres://(HOST_IP):5555/db_name
</code></pre>

<p>How can I do this?</p>
"
"26239116","Run docker inside a docker container?","<centos><docker><fedora>","59710910","is there any way to run a docker image on host from other docker image?","<bash><shell><docker><ubuntu><docker-swarm>","<p>I am using a docker container to build and deploy my software to a collection of ec2's. In the deployment script I build my software and then package it in a docker image. The image is pushed to my private registry, pulled by my production ec2's and then run. So essentially I will need to run docker within a docker container.</p>

<p>The problem is that I can't actually start docker on my container. If I try</p>

<pre><code>service docker start
</code></pre>

<p>I get </p>

<pre><code>bash: service: command not found
</code></pre>

<p>And if I try </p>

<pre><code>docker -d
</code></pre>

<p>I get </p>

<pre><code>2014/10/07 15:54:35 docker daemon: 0.11.1-dev 02d20af/0.11.1; execdriver: native; graphdriver:
[e2feb6f9] +job serveapi(unix:///var/run/docker.sock)
[e2feb6f9] +job initserver()
[e2feb6f9.initserver()] Creating server
2014/10/07 15:54:35 Listening for HTTP on unix (/var/run/docker.sock)
[error] attach_loopback.go:42 There are no more loopback device available.
loopback mounting failed
[e2feb6f9] -job initserver() = ERR (1)
2014/10/07 15:54:35 loopback mounting failed
</code></pre>

<p>The service command doesn't exist on the docker container so I can't start docker. I'm not sure what I should be doing now to start docker so I'm a bit stuck here, any help is appreciated.</p>

<p>A bit more information</p>

<p>Host machine is running fedora 20 (will eventually be running amazon linux on an ec2)</p>

<p>Docker container is running centos 7.0</p>

<p>Host is running Docker version 1.2.0, build fa7b24f/1.2.0</p>

<p>Container is running docker-0.11.1-22.el7.centos.x86_64</p>
","<p>First i'm beginner, bad in explaining thing but i real need help.</p>

<p>i'm trying to create a cluster for MapReduce(my own MapReduce platform)using docker,
 so here is how:</p>

<p><br>1- Run Ubuntu container (Represent the Resource manager: to  controller the number of node to be created).[this container contain listener code every time need to create  an new node,mapperNode,reduceNode,...].
<br>2- Resource manager node or Ubuntu image  should be able to create a new ubuntu container on host.
<br>Note there is no need to understand MapRedce things 
<br> <strong>the question is how can i run other Ubuntu container on host From Ubuntu container?</strong> </p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","59678512","Copy file with absolute path to Docker Container using a Dockerfile","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have a Dockerfile located at path:
<code>/Users/userx/Library/Documents/dockerProject</code></p>

<p>I want to copy a file into the container from:
<code>/Users/userx/Library/temp/otherfiles/myFile.txt</code></p>

<p>I would like to do this from with in the Dockerfile. I tried using <code>COPY</code> but it only works in the context of the project.</p>

<p>some people implied I can do it with the <code>RUN</code> command chaining bash commands but when I use:</p>

<p><code>RUN 'cd /Users/userx/Library/temp/otherfiles/;pwd'</code></p>

<p>I get the error <code>returned a non-zero code</code></p>

<p>How can I copy a file using a Dockerfile from a system absolute path.</p>

<p>thanks</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","59334833","problem with Dockerfile copy folder error with absolute or relative path","<docker><path><dockerfile>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>Error when trying to copy folder with absolute path <code>COPY /src/www/ /usr/share/nginx/html/</code> in Dockerfile</p>

<blockquote>
  <p>Step 3/4 : COPY /src/www/ /usr/share/nginx/html/
  COPY failed: stat /var/lib/docker/tmp/docker-builder685852786/src/www: no such file or directory</p>
</blockquote>

<p>Error when trying to copy folder with relative path <code>COPY ../../../src/www/ /usr/share/nginx/html/</code> in Dockerfile</p>

<blockquote>
  <p>Step 3/4 : COPY ../../../src/www/ /usr/share/nginx/html/
  COPY failed: Forbidden path outside the build context: ../../../src/www/ ()
  ERROR: Job failed: exit code 1</p>
</blockquote>

<p>My folders are like this:</p>

<pre><code>------docker-compose.yml
|_____.gitlab-ci.yml  
L_______infrastructure
|   L_____docker
|      L______nginx
|         L____Dockerfile
L______src
   L____www
</code></pre>

<p><a href=""https://i.stack.imgur.com/VRMKq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VRMKq.png"" alt=""enter image description here""></a></p>

<p>I'm running Dockerfile directly from .gitlab-ci.yml with <code>docker build --pull -t ""$CI_REGISTRY_IMAGE"" ./infrastructure/docker/nginx/.</code></p>

<p>Dockerfile is:</p>

<pre><code>FROM nginx:stable
MAINTAINER Salle MPWAR Maintainers
COPY /src/www/ /usr/share/nginx/html/
// or COPY ../../../src/www/ /usr/share/nginx/html/ I tried both
</code></pre>

<p>I'm missing something with the routes can't make it work with absolute or relative paths.</p>

<p>So as it's explained in <a href=""https://stackoverflow.com/questions/27068596/how-to-include-files-outside-of-dockers-build-context"">this question</a>, I changed the .gitlab-ci.yml command with <code>docker build --pull -t ""$CI_REGISTRY_IMAGE"" -f ./infrastructure/docker/nginx/Dockerfile .</code></p>

<p>And the Dockerfile COPY command with <code>COPY ./src/www/ /usr/share/nginx/html/</code> </p>

<p>But it didn't solve my question, as now I have the error:</p>

<blockquote>
  <p>Step 3/3 : COPY ../../../src/www /usr/share/nginx/html COPY failed:
  Forbidden path outside the build context: ../../../src/www () ERROR:
  Job failed: exit code 1</p>
</blockquote>
"
"27937185","Assign static IP to Docker container","<docker>","59606281","Docker how to specify an ip address with docker-compose","<docker><docker-compose>","<p>I'm now trying to assign a static IP 172.17.0.1 when a Docker container be started up. </p>

<p>I use port 2122 as the ssh port of this container so that I let this container listen port 2122.</p>

<pre><code>sudo docker run -i -t -p 2122:2122 ubuntu
</code></pre>

<p>This command will run a Docker container with a random IP like 172.17.0.5, but I need to assign a specific IP to the container.</p>

<p>The following shell script is what I reference Docker documentation in advanced network settings.</p>

<pre><code>pid=$(sudo docker inspect -f '{{.State.Pid}}' &lt;container_name&gt; 2&gt;/dev/null)
sudo rm -rf /var/run/netns/*
sudo ln -s /proc/$pid/ns/net /var/run/netns/$pid
sudo ip link add A type veth peer name B
sudo brctl addif docker0 A
sudo ip link set A up
sudo ip link set B netns $pid
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link delete eth0
sudo ip netns exec $pid ip link set dev B name eth0
sudo ip netns exec $pid ip link set eth0 address 12:34:56:78:9a:bc
sudo ip netns exec $pid ip link set eth0 down
sudo ip netns exec $pid ip link set eth0 up
sudo ip netns exec $pid ip addr add 172.17.0.1/16 dev eth0
sudo ip netns exec $pid ip route add default via 172.17.42.1
</code></pre>

<p>This shell script will assign a static IP 172.17.0.1 and link to the world fine. But whenever I try to ssh to this container from my local, it didn't work. What's the problem possibly I met?</p>
","<p>I use docker-compose to charge my container, I need to connect between container and host, so I need an ip that I always could access, I know docker's default ip on linux is <code>172.17.0.x</code>, but something strange is that every time I start up the container the ip will change, from <code>172.18.0.1</code> to <code>172.19.0.1</code> to <code>172.20.0.1</code> and etc ..., step by step</p>
"
"28212380","Why docker container exits immediately","<docker>","59736415","Why Docker container always exited after start?","<docker><dockerfile>","<p>I run a container in the background using</p>

<pre><code> docker run -d --name hadoop h_Service
</code></pre>

<p>it exits quickly. But if I run in the foreground, it works fine. I checked logs using</p>

<pre><code>docker logs hadoop
</code></pre>

<p>there was no error. Any ideas?</p>

<p><strong>DOCKERFILE</strong></p>

<pre><code> FROM java_ubuntu_new
 RUN wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
 RUN dpkg -i cdh4-repository_1.0_all.deb
 RUN curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | apt-key add -
 RUN  apt-get update
 RUN apt-get install -y hadoop-0.20-conf-pseudo
 RUN dpkg -L hadoop-0.20-conf-pseudo
 USER hdfs
 RUN hdfs namenode -format
 USER root
 RUN apt-get install -y sudo
 ADD . /usr/local/
 RUN chmod 777 /usr/local/start-all.sh
 CMD [""/usr/local/start-all.sh""]
</code></pre>

<p>start-all.sh</p>

<pre><code> #!/usr/bin/env bash
 /etc/init.d/hadoop-hdfs-namenode start
 /etc/init.d/hadoop-hdfs-datanode start
 /etc/init.d/hadoop-hdfs-secondarynamenode start
 /etc/init.d/hadoop-0.20-mapreduce-tasktracker start
 sudo -u hdfs hadoop fs -chmod 777 /
 /etc/init.d/hadoop-0.20-mapreduce-jobtracker start
 /bin/bash
</code></pre>
","<p>I'm a new to Docker. Tried to start docker container with simple java code.</p>

<p>I saw the printing <code>Hello-World</code>, that means that my code is executed, but the container always exits.</p>

<p>This is my Dockerfile:</p>

<pre><code>FROM nicb/alpine-openjdk8-jre
COPY HelloDockerTest.jar /
CMD java -jar HelloDockerTest.jar
</code></pre>

<p><code>FROM nicb/alpine-openjdk8-jre</code> - this is image from remote repository</p>

<p>The steps that I do are:</p>

<pre><code>1. docker build -t testtt .
2. docker run -i testtt
3. docker start 2520e85b333a
</code></pre>

<p>In the last step I see the printing ""Hello World"" but when I do <code>docker ps</code> I see nothing.</p>

<p>What did I do wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","59759034","Flask + Nginx reverse proxy refuses to connect in docker containers","<docker><nginx><flask><docker-compose>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a React app that uses Flask as a back-end. I am trying to move the react app to be served from an Nginx server. Both the Nginx and the Flask server are inside docker containers. The static content works fine but when the site tries to proxy a call to the flask server the connection is refused. </p>

<p>I don't see any logs on the Flask server so I don't think the requests are getting into the Flask container. I've tried changing the proxy to hit a different port and got the same issue. Shouldn't the containers be allowed to communicate with each other or do I have to explicitly allow certain ports to be open? Do I need a WSGI server between them?</p>

<p>Nginx default.conf</p>

<pre><code>server {

   listen        80;
   server_name   localhost;

   location / {
      index      index.html;
      root       /usr/share/nginx/html;
   }

   location /api/ {
      proxy_pass http://flask:5000;
      proxy_http_version 1.1;
      proxy_set_header Connection """";
      proxy_set_header Host ""localhost"";
   }

}
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: ""3""
services:
    nginx:
        image: nginx
        volumes:
            - /home/cookery/Cookery/dining/build:/usr/share/nginx/html
            - /home/cookery/Cookery/docker/docker.conf:/etc/nginx/conf.d/default.conf
        ports:
            - ""8080:80""
    flask:
        build: ../kitchen
        ports:
            - ""5000:5000""
</code></pre>

<p>This is the error I get on the request to the flask server</p>

<pre><code>2020/01/15 21:18:08 [error] 6#6: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 172.26.0.1, server: localhost, request: ""GET /api/recipes/all HTTP/1.1"", upstream: ""http://172.26.0.2:5000/api/recipes/all"", host: ""localhost:8080"", referrer: ""http://localhost:8080/""
</code></pre>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","59837965","waiting service database running before others services running in Docker","<postgresql><docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I am trying to run my app which <code>depends_on</code> my Postgresql in Docker
let say my database PostgreSQL not running now</p>

<p>and in my <code>docker-compose.yml</code>:</p>

<pre><code>version: ""3""
services:
  myapp:
    depends_on:
      - db
    container_name: myapp
    build:
      context: .
      dockerfile: Dockerfile
    restart: on-failure
    ports:
      - ""8100:8100""

  db:
    container_name: postgres
    restart: on-failure
    image: postgres:10-alpine
    ports:
      - ""5555:5432""
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: 12345678
      POSTGRES_DB: dev
</code></pre>

<p>when I try <code>docker-compose up -d</code> yes it created the <code>postgres</code>  and then create that <code>myapp</code> service
but it seems my Postgresql  is not running yet, after finish install and running <code>myapp</code>,
it said: </p>

<p><code>my database server not running yet</code></p>

<p>how to make <code>myapp</code> running until that <code>db</code> service know that my <code>db</code> running ??</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","59844977","Use Linux grep to check if docker-compose service is healthy","<docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I have 2 services: <strong>service-background</strong> &amp; <strong>service-webapi</strong>. Those are registered in <strong>docker-compose</strong>. Service-background needs to be started up first. Only then service-webapi may be booted. This is a fixed order. I have following configuration in docker-compose.yml:</p>

<pre><code>networks:
   my-fantastic-network

services:
   background-service:
      build: backgroundproject
      networks:
         my-fantastic-network:
            aliases:
               -background-service

webapi-service:
   build: webapi
   depends_on: 
     -background-service
   networks:
      my-fantastic-network:
         aliases:
            -webapi-service
</code></pre>

<p>I thought this would be enough to let the webapi-service wait for the background-service but this one starts up too soon and messes up totally. </p>

<p>My background-service has a log file in following file structure: server/database/logs/startup.log
If the log file contains the text ""Database initialized successfully"", the background-service is ready and the webapi-service can startup safely. I could use the Linux <strong>grep</strong> function to check if my logfile contains this specific text. But I don't know how to do this with docker-compose.</p>

<p>I did some research and there is a <strong>healthcheck</strong> attribute available to use in docker-compose, but I'm not too familiar with it. </p>

<p>Can someone help me?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","60079114","docker-compose: depends-on container runs before depended MySQL container fully initialized","<mysql><docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I have a docker-compose file as shown below, that has 2 containers. One is a MySQL DB and the other is a Python crawler app that reads/writes to the DB. When I do <code>docker-compose up</code>, I see:</p>

<ol>
<li>the DB container is built</li>
<li>then the app container is built</li>
<li>Then my CMD on the app container is run (eg my crawler is started)</li>
<li>Then my database is initialized in the DB container based on the environment variables in the docker-compose file.</li>
</ol>

<p>My question is why is my crawler script running before my database is created in the DB container? How can I ensure that the database is already created before my crawler script is run?</p>

<pre><code>version: '3.7' 
services: 
    db:
        image: mysql:8
        restart: always
        environment: 
            MYSQL_DATABASE: my-database-name
            MYSQL_USER: root
            MYSQL_ROOT_PASSWORD: password
            MYSQL_PASSWORD: password
        ports:
            - ""3308:3306""
        command: --default-authentication-plugin=mysql_native_password
    app:
        build:
            context: ./
            dockerfile: Dockerfile-crawler-dev
        depends_on: 
            - db
        environment: 
            MYSQL_DATABASE: my-database-name
            MYSQL_USER: root
            MYSQL_ROOT_PASSWORD: password
            MYSQL_PASSWORD: password
            MYSQL_HOST: db
            MYSQL_PORT: 3306
        volumes:
            - ./:/crawler/
</code></pre>
"
"32486779","apt-add-repository: command not found error in Dockerfile","<docker>","59614258","Installing python using Dockerfile not working","<docker><ubuntu><dockerfile>","<p>I just made a very simple Docker file in my terminal, basically I did the following:</p>

<pre><code>mkdir pgrouted
cd pgrouted
touch Dockerfile
</code></pre>

<p>Now I open the Docker file in the <a href=""https://en.wikipedia.org/wiki/GNU_nano"" rel=""noreferrer"">nano</a> editor, and I add the following commands to the Docker file:</p>

<pre><code>FROM ubuntu

MAINTAINER Gautam &lt;gautamx07@yahoo.com&gt;

LABEL Description=""pgrouting excercise"" Vendor=""skanatek"" Version=""1.0""

ENV BBOX=""-122.8,45.4,-122.5,45.6""

# Add pgRouting launchpad repository
RUN sudo apt-add-repository -y ppa:ubuntugis/ppa
RUN sudo apt-add-repository -y ppa:georepublic/pgrouting
RUN sudo apt-get update

# Install pgRouting package (for Ubuntu 14.04)
RUN sudo apt-get install postgresql-9.3-pgrouting

# Install osm2pgrouting package
RUN sudo apt-get install osm2pgrouting

# Install workshop material (optional, but maybe slightly outdated)
RUN sudo apt-get install pgrouting-workshop

# For workshops at conferences and events:
# Download and install from http://trac.osgeo.org/osgeo/wiki/Live_GIS_Workshop_Install
RUN wget --no-check-certificate https://launchpad.net/~georepublic/+archive/pgrouting/+files/pgrouting-workshop_2.0.6-ppa1_all.deb

RUN sudo dpkg -i pgrouting-workshop_2.0.6-ppa1_all.deb

# Review: Not sure weather this should be in the dockerfile
RUN cp -R /usr/share/pgrouting/workshop ~/Desktop/pgrouting-workshop

# Log in as user ""user""
RUN psql -U postgres

# Create routing database
RUN CREATE DATABASE routing;

# Add PostGIS functions
RUN CREATE EXTENSION postgis;

# Add pgRouting core functions
CREATE EXTENSION pgrouting;

# Download using Overpass XAPI (larger extracts possible than with default OSM API)
wget --progress=dot:mega -O ""sampledata.osm"" ""http://www.overpass-api.de/api/xapi?*[bbox=${BBOX}][@meta]""
</code></pre>

<p>The entire Dockerfile can be see <strong><a href=""http://chopapp.com/#yd9pqtlj"" rel=""noreferrer"">HERE</a></strong> at a glance.</p>

<p>Now when I try to build the Dockerfile, like so:</p>

<pre><code>docker build -t gautam/pgrouted:v1 .
</code></pre>

<p>The Dockerfile runs and then I get the below error:</p>

<pre><code>Step 4 : RUN sudo apt-add-repository -y ppa:ubuntugis/ppa
 ---&gt; Running in c93c3c5fd5e8
sudo: apt-add-repository: command not found
The command '/bin/sh -c sudo apt-add-repository -y ppa:ubuntugis/ppa' returned a non-zero code: 1
</code></pre>

<p>Why am I getting this error?</p>
","<p>I'm trying to run the below Dockerfile contents on ubuntu image.</p>

<pre><code>    FROM ubuntu
    RUN apt-get update
    RUN apt-get install -y python
    RUN apt-get install -y python-pip
    RUN pip install flask
    COPY app.py /opt/app.py
    ENTRYPOINT FLASK_APP=/opt/app.py flask run --host=0.0.0.0
</code></pre>

<p>But im getting the below error at layer 3</p>

<pre><code>tep 1/7 : FROM ubuntu
 ---&gt; 549b9b86cb8d
Step 2/7 : RUN apt-get update
 ---&gt; Using cache
 ---&gt; 78d87d6d9188
Step 3/7 : RUN apt-get install -y python
 ---&gt; Running in a256128fde51
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package python

</code></pre>

<p>Athough while i run the below command individually</p>

<p><strong>sudo apt-get install -y python</strong></p>

<p>it's successfully getting installed. </p>

<p>Can anyone please help me out. </p>

<p>Thanks,
Sourav</p>
"
"34398632","Docker how to run pip requirements.txt only if there was a change?","<python><docker><dockerfile>","59373148","There is a docker file and i want to try to rebuild the dockerfile every time","<python><linux><docker>","<p>In a Dockerfile I have a layer which installs <code>requirements.txt</code>:</p>

<pre><code>FROM python:2.7
RUN pip install -r requirements.txt
</code></pre>

<p>When I build the docker image it runs the whole process <strong>regardless</strong> of any changes made to this file.</p>

<p>How do I make sure Docker only runs <code>pip install -r requirements.txt</code> if there has been a change to the file?</p>

<pre><code>Removing intermediate container f98c845d0f05
Step 3 : RUN pip install -r requirements.txt
 ---&gt; Running in 8ceb63abaef6
Collecting https://github.com/tomchristie/django-rest-framework/archive/master.zip (from -r requirements.txt (line 30))
  Downloading https://github.com/tomchristie/django-rest-framework/archive/master.zip
Collecting Django==1.8.7 (from -r requirements.txt (line 1))
</code></pre>
","<p>Every time I build the image and run it and then delete it. Then after I want to run this image again after changing some code in our application, then all the requirements.txt file and the base image python:3 will be downloaded again. Then what I have to stop again downloading the requirements.txt file every time. Can you give me any solution?</p>

<p>Just have a look of my Dockerfile</p>

<pre><code>FROM  python:3
ADD . /usr/src/app
WORKDIR /usr/src/app
RUN pip3 install -r requirements.txt
EXPOSE 5050
CMD [ ""python"", ""app.py"" ]
</code></pre>
"
"35540885","Display the contents of a log file as it is updated","<javascript><python><flask><stream>","60001305","How to continuously display live Kubernetes pods logs on a flask UI?","<docker><flask><logging><kubernetes>","<p>I have external programs such as ffmpeg and gstreamer running in the background and writing to a log file.  I want to display the contents of this log with my Flask application, so that the user can watch the log update, like <code>tail -f job.log</code> would do in the terminal.</p>

<p>I tried to use <code>&lt;object data=""/out.log"" type=""text/plain""&gt;</code> to point at the log file, but that failed to show the data, or the browser told me I needed a plugin.</p>

<p>How can I embed and update the log file in an HTML page?</p>
","<p>I want to display live logs of a Kubernetes pod on a web page in flask. I have a process running and want to display its current state on a webpage for a user. For that I want the live logs to be displayed on the UI.</p>
"
"36283908","Re-using environment variables in docker-compose.yml","<docker><docker-compose><environment-variables>","59894230","Sharing environment variables across multiple Docker services","<docker><docker-compose>","<p>Is it possible to re-use environment variables that are shared among multiple containers?</p>
<p>The idea is to avoid duplication, as illustrated in this example:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:

  db:
    image: example/db
    ports:
      - &quot;8443:8443&quot; 
    container_name: db
    hostname: db
    environment:
      - USER_NAME = admin 
      - USER_PASSWORD = admin 

svc:
  image: example/svc
  depends_on:
    - db
  ports:
    - &quot;9443:9443&quot;
  container_name: svc
  hostname: svc
  environment:
    - DB_URL = https://db:8443
    - DB_USER_NAME = admin
    - DB_USER_PASSWORD = admin 
</code></pre>
","<p>I was wondering, let's say I had the following .env file:</p>

<pre><code>STAGING_ALLOWED_DOMAINS=example.com
STAGING_SITES=example.com=web:8000
</code></pre>

<p>Is there a way to condense the following services to utilise the same environment settings, or do they always need to be explicitly listed for each service?</p>

<pre><code>services:
    web:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    nginx:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    celery:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
    celery-beat:
        environment:
            ALLOWED_DOMAINS: ""${STAGING_ALLOWED_DOMAINS}""
            SITES: ""${STAGING_SITES}""
</code></pre>

<p>Is there a cleaner way to share environment configurations?</p>
"
"37057468","Conditional ENV in Dockerfile","<docker><dockerfile>","59710802","conditional set of a variable based on another ENV variable","<docker><dockerfile>","<p>Is it possible to conditionally set an <code>ENV</code> variable in a Dockerfile based on the value of a build <code>ARG</code>?</p>

<p>Ex: something like</p>

<pre><code>ARG BUILDVAR=sad
ENV SOMEVAR=if $BUILDVAR -eq ""SO""; then echo ""hello""; else echo ""world""; fi
</code></pre>

<p>Update: current usage based on Mario's answer:</p>

<pre><code>ARG BUILD_ENV=prod
ENV NODE_ENV=production
RUN if [ ""${BUILD_ENV}"" = ""test"" ]; then export NODE_ENV=development; fi
</code></pre>

<p>However, running with <code>--build-arg BUILD_ENV=test</code> and then going onto the host, I still get </p>

<pre><code>docker run -it mycontainer bin/bash
[root@brbqw1231 /]# echo $NODE_ENV
production
</code></pre>
","<p>In a given Dockerfile, I want to set a variable based on content of another ENV variable (which is injected into the container beforehand, or defined within the Dockerfile)</p>

<p>I'm looking at something like this</p>

<pre><code>FROM centos:7
ENV ENABLE_REMOTE_DEBUG ""true""

ENV DEBUG_FLAG=""""
RUN if [ ""$ENABLE_REMOTE_DEBUG"" = ""true"" ] ; then echo ""set debug flag"" ;export DEBUG_FLAG=""some_flags""; else echo ""remote debug not set"" ; fi

RUN echo debug flags: ${DEBUG_FLAG}

## Use the debug flag in the Entrypoint : java jar ${DEBUG_FLAG} ...
</code></pre>

<p>the problem with this Dockerfile is <code>$DEBUG_FLAG</code> is not properly set (or is not being used in the next line? ) ... since output is empty:</p>

<pre><code>debug flags:

</code></pre>

<p>What am I missing here? (I prefer not to call external bash script)</p>
"
"37242217","Access docker container from host using containers name","<docker><docker-compose>","59267479","How to connect to Docker containers using hostnames from Docker host?","<docker>","<p>I am developing a service and using there docker compose to spin services like postgres, redis, elasticsearch. I have a web application that is based on RubyOnRails and writes and reads from all those services.</p>

<p>Here is my <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      - frontapp

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      - frontapp

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      - frontapp

networks:
  frontapp:
    driver: bridge
</code></pre>

<p>And i can ping containers within this network</p>

<pre><code>$ docker-compose run redis /bin/bash
root@777501e06c03:/data# ping postgres
PING postgres (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.346 ms
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.047 ms
...
</code></pre>

<p>So far so good. Now I want to run ruby on rails application on my host machine but be able to access postgres instance with url like <code>postgresql://username:password@postgres/database</code> currently that is not possible</p>

<pre><code>$ ping postgres
ping: unknown host postgres
</code></pre>

<p>I can see my network in docker</p>

<pre><code>$ docker network ls
NETWORK ID          NAME                DRIVER
ac394b85ce09        bridge              bridge              
0189d7e86b33        elephant_default    bridge              
7e00c70bde3b        elephant_frontapp   bridge              
a648554a72fa        host                host                
4ad9f0f41b36        none                null 
</code></pre>

<p>And I can see an interface to it</p>

<pre><code>$ ifconfig 
br-0189d7e86b33 Link encap:Ethernet  HWaddr 02:42:76:72:bb:c2  
          inet addr:172.18.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:76ff:fe72:bbc2/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:36 errors:0 dropped:0 overruns:0 frame:0
          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2000 (2.0 KB)  TX bytes:8792 (8.7 KB)

br-7e00c70bde3b Link encap:Ethernet  HWaddr 02:42:e7:d1:fe:29  
          inet addr:172.20.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e7ff:fed1:fe29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1584 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1597 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:407137 (407.1 KB)  TX bytes:292299 (292.2 KB)
...
</code></pre>

<p>But i am not sure what should I do next. I tried to play a bit with <code>/etc/resolv.conf</code>, mainly with <code>nameserver</code> directive, but that had no effect.</p>

<p>I would appreciate any help of suggestions how to configure this setup correctly.</p>

<p><strong>UPDATE</strong></p>

<p>After browsing through Internet resources I managed to assign static IP addresses to boxes. For now it is enough for me to continue development. Here is my current <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      frontapp:
        ipv4_address: 172.25.0.11

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      frontapp:
        ipv4_address: 172.25.0.12

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      frontapp:
        ipv4_address: 172.25.0.10

networks:
  frontapp:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1
</code></pre>
","<p>I want to connect to my Docker containers from my Docker host using hostnames.</p>

<p>I already know how to connect to containers by mapping their ports using <code>docker run -p &lt;host-port&gt;:&lt;container-port&gt; ...</code> and then access them through <code>localhost</code>.</p>

<p>Also, I can connect to containers using their IP-addresses given by <code>docker inspect &lt;container&gt;</code>. But these IP-adresses are not static.</p>

<p>How can I give containers hostnames, so that I can connect to them through exposed ports without having to think about non-static IPs?</p>
"
"37586811","Pass commands as input to another command (su, ssh, sh, etc)","<bash><shell><unix><ssh><sh>","59665386","Why does a sed command work interactively but not in my script? [Updated]","<bash><image><docker><sed><dockerfile>","<p>I have a script where I need to start a command, then pass some additional commands <em>as commands</em> to that command.  I tried</p>

<pre><code>su
echo I should be root now:
who am I
exit
echo done.
</code></pre>

<p>... but it doesn't work: The <code>su</code> succeeds, but then the command prompt is just staring at me.  If I type <code>exit</code> at the prompt, the <code>echo</code> and <code>who am i</code> etc start executing!  And the <code>echo done.</code> doesn't get executed at all.</p>

<p>Similarly, I need for this to work over <code>ssh</code>:</p>

<pre><code>ssh remotehost
# this should run under my account on remotehost
su
## this should run as root on remotehost
whoami
exit
## back
exit
# back
</code></pre>

<p>How do I solve this?</p>

<blockquote>
  <p>I am looking for answers which solve this in a general fashion, and which are not specific to <code>su</code> or <code>ssh</code> in particular.  The intent is for this question to become a <a href=""https://meta.stackoverflow.com/questions/291992/what-is-a-canonical-question-answer-and-what-is-their-purpose"">canonical</a> for this particular pattern.</p>
</blockquote>
","<p>I'm creating a Docker image where I use <code>sed</code> to modify two parameters, but when i create the images and check the file I want to modify it remains the same.</p>

<p>If i run the very sed command interactively, it works. Why? Could somebody help me make my image work without having to modify every container? </p>

<p>Before I was pointed a mistake in this point because i was using exit in before another command in the RUN command, now sed is an independent command and still doesn't work.</p>

<p>Dockerfile</p>

<pre><code>FROM python:slim-buster

WORKDIR /home/scr_dca

COPY . . 

ENV FLASK_APP Screenly.py

RUN su &amp;&amp; \
apt-get update &amp;&amp; \
apt install curl gnupg -y &amp;&amp; \
curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp; \
curl https://packages.microsoft.com/config/debian/10/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp; \
apt-get update &amp;&amp; ACCEPT_EULA=Y apt-get install msodbcsql17 unixodbc-dev -y &amp;&amp; \
apt-get install libgssapi-krb5-2 &amp;&amp; \
exit &amp;&amp; \
pip3 install -r requirements.txt --trusted-host pypi.python.org &amp;&amp; \
sed -i ""s/\(MinProtocol *= *\).*/\1TLSv1.0 /"" ""/etc/ssl/openssl.cnf"" &amp;&amp; \
sed -i ""s/\(CipherString *= *\).*/\1DEFAULT@SECLEVEL=1 /"" ""/etc/ssl/openssl.cnf""

CMD [""gunicorn"", ""-b"", "":8000"", ""scr_dca:app""]
</code></pre>

<p>I'm doing:</p>

<ul>
<li><code>docker run --name screenly_dca3 -d -p 5050:8000 src_dca_v1.0</code></li>
<li><code>docker container exec -it screenly_dca3 bash</code></li>
<li>then in bash: <code>cat /etc/ssl/openssl.cnf</code></li>
</ul>

<p>I checked sed has not worked yet during the image creation and I ran the following commands:</p>

<pre><code>sed -i ""s/\(MinProtocol *= *\).*/\1TLSv1.0 /"" ""/etc/ssl/openssl.cnf""
sed -i ""s/\(CipherString *= *\).*/\1DEFAULT@SECLEVEL=1 /"" ""/etc/ssl/openssl.cnf""
</code></pre>

<p>original part of the file I want to modify:</p>

<pre><code>[system_default_sect]
MinProtocol = TLSv1.2
CipherString = @SECLEVEL=1
</code></pre>

<p>sed expected result </p>

<pre><code>[system_default_sect]
MinProtocol = TLSv1.0
CipherString = DEFAULT@SECLEVEL=1
</code></pre>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","59669845","Connecting to Postgres docker container from docker-compose file","<postgresql><docker><docker-compose>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have 2 projects. The first contains this <code>docker-compose.yml</code>:</p>

<pre><code># pg/docker-compose.yml
version: ""3""
services:
  postgres:
    image: postgres
    environment:
      POSTGRES_USER: pguser
      POSTGRES_DB: pg
    ports:
      - ""5432:5432""
    volumes:
      - ""postgres-data:/var/lib/postgresql/data""
    networks:
      - some_net

volumes:
  postgres-data:

networks:
  some_net:
    driver: bridge
</code></pre>

<p>The second project has this docker compose file:</p>

<pre><code># pg_client/docker-compose.yml
version: ""3""
services:
  postgres_client:
    image: tmaier/postgresql-client
    command: postgres://pguser@localhost:5432/pg
    networks:
      - pg_some_net

networks:
  pg_some_net:
    external: true
</code></pre>

<p>The <a href=""https://hub.docker.com/r/tmaier/postgresql-client"" rel=""nofollow noreferrer""><code>tmaier/postgresql-client</code></a> image is extremely simple, it has PostgresSQL installed and runs the command <code>psql DOCKER_COMMAND</code> at build time. In my case this is <code>psql postgres://pguser@localhost:5432/pg</code>.</p>

<p>I can connect to my Postgres db just fine from the command line:</p>

<pre><code>$ psql postgres://pgmuser@localhost:5432/pg
psql (12.1)
Type ""help"" for help.

pg=#
</code></pre>

<p>But when the <code>postgres_client</code> container attempts to connect (after running <code>docker-compose up</code>), it fails saying </p>

<pre><code>postgres_client_1  | psql: could not connect to server: Connection refused
postgres_client_1  |        Is the server running on host ""localhost"" (127.0.0.1) and accepting
postgres_client_1  |        TCP/IP connections on port 5432?
postgres_client_1  | could not connect to server: Address not available
postgres_client_1  |        Is the server running on host ""localhost"" (::1) and accepting
postgres_client_1  |        TCP/IP connections on port 5432?
</code></pre>

<p>Is there something special about docker-compose that I need to account for when writing my Postgres connection URI?</p>

<p>Any help is much appreciated.</p>
"
"38755516","How to change the port number for Asp.Net core app?","<c#><asp.net-core><.net-core>","59955540",".NET CORE app to start on port 80 instead on port 5000","<docker><asp.net-core><.net-core>","<p>I added the following section in <code>project.json</code>.</p>

<pre><code>  ""commands"": {
    ""run"": ""run server.urls=http://localhost:8082"",
    ""web"": ""Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.Kestrel --server.urls http://localhost:8082"",
    ""weblistener"": ""Microsoft.AspNet.Hosting --server WebListener --server.urls http://localhost:8082""
  },
</code></pre>

<p>However, it still shows ""Now listening on: <a href=""http://localhost:5000"" rel=""noreferrer"">http://localhost:5000</a>"" when run it using <code>dotnet myapp.dll</code>?</p>

<p>BTW, will clients from other machine be able to access the service?</p>
","<p>I'm trying to run a .net core app inside of the docker container. The docker image that I'm using to run a container is <code>mcr.microsoft.com/dotnet/core/sdk:3.1</code>. </p>

<p>I need to SDK image because I'm trying to build and run the app inside of the same container. </p>

<p>So, I need help with starting the app on port 80 inside of the container. I have tried setting <code>ASPNETCORE_URLS=http://+80</code> but the app still runs on port 5000. </p>
"
"39445047","Python Tkinter in Docker .TclError: couldn't connect to display","<python><ubuntu><docker><tkinter><xfce>","59576282","Running a Tkinter application inside a Docker container","<python><python-3.x><docker><tkinter>","<p>I am new to python and am trying to build a small app. It needs to be a GUI app and I was wanting to containerise it with docker. I am getting the following error and can not find a solution</p>

<pre><code>No protocol specified
No protocol specified
Traceback (most recent call last):
  File ""tkinker.py"", line 7, in &lt;module&gt;
    tinker = Tk()
  File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 1818, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display "":0.0""
</code></pre>

<p>It starts locally but wont start in docker. My OS is Xubuntu.</p>

<p>I have created a sample app (below), see run-test.sh <a href=""https://github.com/jeremysells/test/tree/master/docker-tkinter"" rel=""nofollow"">https://github.com/jeremysells/test/tree/master/docker-tkinter</a></p>
","<p>I wrote a small application in Python3 using <a href=""https://docs.python.org/3/library/tkinter.html"" rel=""nofollow noreferrer"">Tkinter</a>. I would like to containerize this application in order to reduce any installation and dependency hassle. </p>

<p>I wrote the following Dockerfile:</p>

<pre><code>FROM python:3.6-buster

WORKDIR /home/python/app

COPY . .

RUN pip3 install -r requirements.txt

# Install my package
RUN python3 -m pip install --user --upgrade setuptools wheel
RUN python3 setup.py install

CMD [ ""charts-app"" ]
</code></pre>

<p>If I run the same commands outside Docker, everything works. </p>

<p>Inside Docker, I am able to build the image, but when I run it, the following message is displayed:</p>

<pre><code>File ""/usr/local/lib/python3.6/tkinter/__init__.py"", line 2023, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable
</code></pre>

<p>I understand that as the application is running inside the container, it has not directly access to a Graphical Interface. But, is there any way that I can ""force"" the container to use the <code>host</code> as its Graphical Interface? </p>
"
"40366192","Kubernetes how to make Deployment to update image","<docker><kubernetes>","59546980","Updating kubernetes deployment with same image name","<java><spring-boot><docker><kubernetes><rolling-updates>","<p>I do have deployment with single pod, with my custom docker image like:</p>

<pre><code>containers:
  - name: mycontainer
    image: myimage:latest
</code></pre>

<p>During development I want to push new latest version and make Deployment updated.
Can't find how to do that, without explicitly defining tag/version and increment it for each build, and do</p>

<pre><code>kubectl set image deployment/my-deployment mycontainer=myimage:1.9.1
</code></pre>
","<p>We have a micro-services java applications, as and when we have changes in code we have to perform the kubernetes deployment.</p>

<p>How will I apply the latest changes to the deployment with the same Image name</p>

<p>we have a single replica and when I execute <code>kubectl apply -f deployment.yaml</code> it says unchanged.</p>

<p>We have kubelet version of v1.13.12</p>

<p>Please help.</p>
"
"40801772","What is the difference between docker-compose ports vs expose","<docker><docker-compose>","59183027","docker-compose: does EXPOSE function on the host port or the container port?","<docker>","<p>What is the difference between <code>ports</code> and <code>expose</code> options in <code>docker-compose.yml</code></p>
","<p>Below is example code: </p>

<pre><code>services:
  db:
    image: ""mysql:8""
    restart: always
    environment:
      MYSQL_DATABASE: 'test'
      MYSQL_USER: 'root'
      MYSQL_PASSWORD: 'test'
      MYSQL_ROOT_PASSWORD: 'test'
    ports:
      - ""3309:3306""
    expose:
      - ""3309""
</code></pre>

<p>By definition, in docker-compose file, does <code>expose:</code> function on the host port or the container port? </p>

<p>Does <code>ports:</code> follow the  <code>[host_port]:[container_port]</code> convention or  <code>[container_port]:[host_port]</code>? </p>

<p>what exactly is the example code above doing with ports? </p>
"
"43467847","Setup git via windows docker file","<dockerfile><docker-for-windows><docker-desktop><docker>","60163945","Docker file with git installation for windows","<python><git><powershell><docker><dockerfile>","<p>I write <code>Dockerfile</code> which is based on <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>. I need to add to this image git. In order to achieve it I did the following:</p>

<pre><code>RUN Invoke-WebRequest 'https://github.com/git-for-windows/git/releases/download/v2.12.2.windows.2/Git-2.12.2.2-64-bit.exe'
RUN Invoke-Expression ""c:\Git-2.12.2.2-64-bit.exe""
</code></pre>

<p>But when I execute this lines via docker build, I receive following error message:</p>

<blockquote>
  <p>Invoke-Expression : The term 'c:\Git-2.12.2.2-64-bit.exe' is not
  recognized as the name of a cmdlet, function, script file, or operable
  program. Check the spelling of the name, or if a path was included,
  verify that the path is correct and try again.</p>
</blockquote>

<p>I realize that this error message indicates that due to console nature of windows docker images I'll not be able to execute GUI installers. Unfortunately git doesn't have console installer. <a href=""https://chocolatey.org/"" rel=""nofollow noreferrer"">Chocolatey</a> works fine under <a href=""https://hub.docker.com/r/microsoft/windowsservercore/"" rel=""nofollow noreferrer"">windowsservercore</a> image but doesn't work at <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>. In order to install git for <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a> I have idea to repeat in <code>Dockerfile</code> commands from <a href=""https://chocolatey.org/packages/git.install"" rel=""nofollow noreferrer"">chocolatey git installer</a> which is fine for me, but still I'd like to know is there any simpler way to install git on <a href=""https://hub.docker.com/r/microsoft/nanoserver/"" rel=""nofollow noreferrer"">windowsnanoserver</a>? </p>
","<p>I need to create a docker file that does the following:</p>

<p>Install python. </p>

<p>Install git. </p>

<p>Clone git repo.</p>

<p>Run with latest commit.</p>

<p>I have the following for linux:</p>

<pre><code>FROM ubuntu:16.04

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends python-pip
RUN apt-get install -y --no-install-recommends python-setuptools
RUN apt-get install -y --no-install-recommends python-wheel
RUN apt-get install -y --no-install-recommends git

# Clone project github repository
WORKDIR /repo
RUN git clone https://github.com/&lt;repo&gt;.git
# Checkout version 
WORKDIR /repo/project
RUN git checkout &lt;latest commit&gt;
</code></pre>

<p>In the windows docker file creation I'll skip python installation by using <code>python:2.7-windowsservercore-1809</code> as the base image.</p>

<p>I tried to search online for ways to install git like the above in linux, but couldn't find any way that worked for me (like using chocolaty)</p>

<p>Any ideas how to create an equivalent docker file for windows container? </p>
"
"43654656","Dockerfile if else condition with external arguments","<docker><dockerfile>","59834620","How to compare a variable and set the value in alpine linux","<shell><docker><alpine>","<p>I have dockerfile</p>

<pre><code>FROM centos:7
ENV foo=42
</code></pre>

<p>then I build it </p>

<pre><code>docker build -t my_docker .
</code></pre>

<p>and run it.</p>

<pre><code>docker run -it -d  my_docker
</code></pre>

<p>Is it possible to pass arguments from command line and use it with if else in Dockerfile? I mean something like</p>

<pre><code>FROM centos:7
if (my_arg==42)
     {ENV=TRUE}
else:
     {ENV=FALSE}
</code></pre>

<p>and build with this argument.</p>

<pre><code> docker build -t my_docker . --my_arg=42
</code></pre>
","<p>Never wrote any shell script before but after extensively googling it I came up with the following code for my docker file. But don't understand why is it doesn't work.</p>

<pre><code>###stage 2####################
FROM nginx:alpine

##########Calculate the environment type #########
ARG BUILD_TYPE

####echo of build build_type does gives me output of Development when passed argument is Development.
RUN if [ ""$BUILD_TYPE"" = ""Development"" ]; then BUILD_TYPE='dev'; fi
RUN if [ ""$BUILD_TYPE"" = ""Production"" ]; then BUILD_TYPE='prod'; fi
RUN echo ""UI BUILD_TYPE=$BUILD_TYPE---------""
##########Calculate the environment type #########
</code></pre>

<p>The above echo always comes as Development.</p>

<p>UPDATE</p>

<p>Now I built a sample in a separate docker file to isolate the issue. After this I realised that the assignment is not happening though the condition matched.</p>

<p>Here is the new sample docker file code.</p>

<pre><code>FROM nginx:alpine

ARG BUILD_TYPE
ARG ENV_TYPE

RUN if [ ""$BUILD_TYPE"" = ""Development"" ]; then ENV_TYPE='dev'; echo ""matched dev""; fi
RUN if [ ""$BUILD_TYPE"" = ""Production"" ]; then ENV_TYPE=""prod""; echo ""matched prod""; fi
RUN echo ""UI BUILD_TYPE=$BUILD_TYPE ENV_TYPE = $ENV_TYPE---------""
</code></pre>

<p>The output is</p>

<ol>
<li>matched dev </li>
<li>UI BUILD_TYPE=Development ENV_TYPE = ---------</li>
</ol>

<p>I see ENV_TYPE is empty.</p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","59632583","Jenkins inside a container do not find docker command","<docker><jenkins><jenkins-pipeline>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I'm trying to run a container inside another. Where the first it's based in a jenkins image and the second in a gcc image (only for test).  </p>

<p>That's the steps I maked:  </p>

<ol>
<li>I upped a container jenkins based in the image <code>jenkins/jenkins</code> with the docker command:  

<ul>
<li><code>docker run -it -p 1234:8080 -v jenkins_home:/var/jenkins_home -u 0 jenkins/jenkins</code></li>
<li>where <code>jenkins_home</code> is a volume created with the command: <code>docker volume create jenkins_home</code></li>
</ul></li>
<li>I installed the plugins Docker, Docker Slaves and Docker Pipeline.</li>
<li>I created a pipeline job:</li>
</ol>

<pre><code>pipeline {
    stages {        
        stage('Container') {
            agent {
                docker {
                    image 'gcc'
                    args '-v ${nomedocurso}:/curso/'
                }
            }
            steps {
                sh 'ls -l'
            }
        }
    }
}
</code></pre>

<ul>
<li>In this case, the error is jenkins not find docker:  

<blockquote>
  <p>+ docker inspect -f . gcc /var/jenkins_home/workspace/curso-c@2@tmp/durable-ee333f1e/script.sh: 1: /var/jenkins_home/workspace/curso-c@2@tmp/durable-ee333f1e/script.sh: docker: not found</p>
</blockquote></li>
<li>I tried to include the <code>label</code> parameter:</li>
</ul>

<pre><code>pipeline {
    agent {
        label 'docker' 
    }
    stages {        
        stage('Container') {
            agent {
                docker {
                    label 'docker'
                    image 'gcc'
                    args '-v ${nomedocurso}:/curso/'
                }
            }
            steps {
                sh 'ls -l'
            }
        }
    }
}
</code></pre>

<ul>
<li>But jenkins still continued complaining:</li>
</ul>

<blockquote>
  <p>Still waiting to schedule task<br>
  ‘Jenkins’ doesn’t have label ‘docker’</p>
</blockquote>

<hr>

<ul>
<li>Why docker do not be find by the jenkins?</li>
<li>How do I set up and run a pipeline with docker in one of the stages?</li>
</ul>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","59645589","Unable to bind docker.sock to Docker Service","<docker><terraform><bind><docker-swarm>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I am trying to setup a docker service (via terraform) which will run jenkins as containers. I want to bind the host VM's docker docket to the docker service. I tried two options.</p>

<ol>
<li>Docker Service as command</li>
<li>Spawn docker service using terraform</li>
</ol>

<p>While the service gets instantiated successfully, the container doesn't recognize the docker socket. I can see the <code>/var/run/docker.sock</code> file inside the container but docker command wont work.</p>

<p>The code block for your reference.</p>

<p>DOCKER SERVICE COMMAND</p>

<pre><code>docker service  create --name aws --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock jenkins/jenkins:lts
</code></pre>

<p>TERRAFORM MAIN TF FILE</p>

<pre><code>resource ""docker_service"" ""jenkins_service"" {
  name = var.project_name
  task_spec {
    container_spec {
      image = docker_image.jenkins_image.name
      mounts {
        source = var.jenkins_volume
        target = ""/var/jenkins_home""
        type   = ""bind""
      }
      mounts {
        source = ""/var/run/docker.sock""
        target = ""/var/run/docker.sock""
        type   = ""bind""
      }
    }
    networks = [""${docker_network.jenkins_network.name}""]
  }

  endpoint_spec {
    ports {
      target_port    = ""8080""
      published_port = var.web_interface_port
      publish_mode   = ""ingress""
      name           = ""WEB_INTERFACE""
    }
    ports {
      target_port    = ""50000""
      published_port = var.api_interface_port
      publish_mode   = ""ingress""
      name           = ""API_INTERFACE""
    }
  }
}
</code></pre>

<p><strong>Error Message</strong></p>

<blockquote>
  <p>bash-4.2$ docker exec  -u 0 -it 9bc2a468174a bash<br>
  root@9bc2a468174a:/# docker<br>
  bash: docker: command not found<br>
  root@9bc2a468174a:/# ls -ltr /var/run/docker.sock<br>
  srwxrwxrwx. 1 root 167 0 Sep 24 11:02 /var/run/docker.sock  </p>
</blockquote>

<p><strong>Docker Version</strong></p>

<blockquote>
  <p>-bash-4.2$ docker version<br>
  Client:<br>
   Version:           18.09.8<br>
   API version:       1.39<br>
   Go version:        go1.10.8<br>
   Git commit:        0dd43dd87f<br>
   Built:             Wed Jul 17 17:40:31 2019<br>
   OS/Arch:           linux/amd64<br>
   Experimental:      false  </p>
  
  <p>Server: Docker Engine - Community<br>
   Engine:<br>
    Version:          18.09.8
    API version:      1.39 (minimum version 1.12)<br>
    Go version:       go1.10.8<br>
    Git commit:       0dd43dd<br>
    Built:            Wed Jul 17 17:10:42 2019<br>
    OS/Arch:          linux/amd64<br>
    Experimental:     false  </p>
</blockquote>

<p>Any inputs will be much appreciated.</p>

<p>Regards
Senthil Nathan M</p>
"
"45356494","5xx or 4xx error with “No 'Access-Control-Allow-Origin' header is present”","<javascript><node.js><cors><fetch-api>","59513096","How to set CORS for two applications running as docker","<javascript><docker><express>","<p>My browser is logging the following message in the devtools console:</p>

<blockquote>
  <p>No 'Access-Control-Allow-Origin' header is present on the requested resource.… The response had HTTP status code 503. </p>
</blockquote>

<p><strong>Background</strong>: I have two apps. One that is an Express Node application connected to a Mongo database. The other is a basic web application that makes <code>POST</code> requests to the Node application via the Fetch API to get data from Mongo.</p>

<p><strong>Issue</strong>: Though I receive no <code>CORS</code> errors on my local machine, I am given the error below as soon as I deploy my basic web application to production. The web application that makes a <code>POST</code> request to the Node app and gives me this:</p>

<p>The <code>POST</code> request does seem to work and the data is saved into Mongo but this error is being marked as a ""Critical Error"" in Heroku and is quite annoying. </p>

<p>I realize that I could set the <code>no-cors</code> option in Fetch but I believe that it is required since I am making a request to a url that is different than the origin. Right?</p>

<p><strong>Express Node App Code</strong></p>

<p>In my <code>app.js</code> file I have set the correct headers to ensure that other applications can make requests from different origins</p>

<p>app.js</p>

<pre><code>// Add headers so we can make API requests
app.use(function (req, res, next) {
    res.setHeader('Access-Control-Allow-Origin', '*');
    res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS, PUT, PATCH, DELETE');
    res.setHeader('Access-Control-Allow-Headers', 'X-Requested-With,content-type');
    res.setHeader('Access-Control-Allow-Credentials', true);
    next();
});
</code></pre>

<p>routes/api/api.js</p>

<pre><code>router.post('/users/:url/upload-csv/:csv_name', (req, res) =&gt; {
  let csv_name = req.params.csv_name;
  let csv_string = csv_name+req.body.csv_string;

  User.findOne({url: req.params.url})
    .then((user) =&gt; {
      if (user.csv_files.length === 0) {
        user.csv_files.push(csv_string);
      } else {
        let foundExistingCSV = false;
        for (var i = 0; i &lt; user.csv_files.length; i++) {
          if (user.csv_files[i].includes(csv_name)) {
            foundExistingCSV  = true;
            user.csv_files[i] = csv_string;
            break;
          }
        }
        if (!foundExistingCSV) user.csv_files.push(csv_string);
      }
      user.markModified('csv_files');
      user.save();

      res.status(204);
    })
    .catch((err) =&gt; {
      console.log(err);
      res.status(400);
    });
});
</code></pre>

<p><strong>Basic Web App Code</strong></p>

<p><code>POST</code> request I am making</p>

<p>utils.js</p>

<pre><code>utils.exportToMongo = functions(table, name) {
  var exportPlugin = table.getPlugin('exportFile');
  var csv_string   = exportPlugin.exportAsString('csv');

  // Upload the CSV string and its name to Users DB
  fetch(`${utils.fetchUserURL()}/upload-csv/${name}`, {
    method: 'POST',
    body: JSON.stringify({csv_string: csv_string}),
    headers: new Headers({
      'Content-Type': 'application/json',
      Accept: 'application/json',
    })
  }).then((res) =&gt; {
    return {};
  }).catch((error) =&gt; {
    console.log(error);
    return {};
  });
}
</code></pre>

<p>How can I remove the <code>503</code> error? Any insight would be greatly appreciated!</p>
","<p>I'm running my frontend application (nextJS) at <a href=""https://editor.website.com"" rel=""nofollow noreferrer"">https://editor.website.com</a> and my backend application (expressJS) at <a href=""https://api.editor.website.com"" rel=""nofollow noreferrer"">https://api.editor.website.com</a> - both as docker container.
I'm trying to upload some image files using graphQL and mostly it works, but sometimes it fails with these two errors:</p>

<pre><code>POST https://api.editor.website.com/graphql 413

Access to fetch at 'https://api.editor.website.com/graphql' from origin 'https://editor.website.com' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.
</code></pre>

<p>I tried to set universal cors to prevent this problem, but this doesn't work. So what am I missing?</p>

<p><strong>Server</strong></p>

<pre><code>import express from 'express'
import cors from 'cors'

const app = express()
app.options('*', cors())
// app.use(cors())

server.applyMiddleware({ app, path: '/graphql' })
</code></pre>
"
"46210934","ImportError: Couldn't import Django","<django>","59246934","Docker : python3 manage.py runserver error","<python><django><docker>","<p>I've already configured virtualenv in pycharm, when using the python manage.py command, this is error shown:</p>

<pre><code>E:\video course\Python\code\web_worker\MxOnline&gt;python manage.py runserver
Traceback (most recent call last):
  File ""manage.py"", line 17, in &lt;module&gt;
    ""Couldn't import Django. Are you sure it's installed and ""
ImportError: Couldn't import Django. Are you sure it's installed and available on your PYTHONPATH environment variable? Did you forget to activate a virtual environment?
</code></pre>

<p>How should I fix it, I've installed django.</p>
","<p>I am using Docker and I have a problem.</p>

<p>error : </p>

<pre><code>WARNING: Image for service django was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
Creating develserver_db_1     ... done
Creating develserver_django_1 ... done
Attaching to develserver_django_1, develserver_db_1
django_1  | Traceback (most recent call last):
django_1  |   File ""manage.py"", line 10, in main
django_1  |     from django.core.management import execute_from_command_line
django_1  | ModuleNotFoundError: No module named 'django'
django_1  | 
django_1  | The above exception was the direct cause of the following exception:
django_1  | 
django_1  | Traceback (most recent call last):
django_1  |   File ""manage.py"", line 21, in &lt;module&gt;
django_1  |     main()
django_1  |   File ""manage.py"", line 12, in main
django_1  |     raise ImportError(
django_1  | ImportError: Couldn't import Django. Are you sure it's installed and available on your PYTHONPATH environment variable? Did you forget to activate a virtual environment?
develserver_django_1 exited with code 1
db_1      | 
db_1      | PostgreSQL Database directory appears to contain a database; Skipping initialization
db_1      | 
db_1      | 2019-12-09 10:01:20.171 UTC [1] LOG:  starting PostgreSQL 12.1 (Debian 12.1-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
db_1      | 2019-12-09 10:01:20.172 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
db_1      | 2019-12-09 10:01:20.172 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
db_1      | 2019-12-09 10:01:20.175 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
db_1      | 2019-12-09 10:01:20.234 UTC [27] LOG:  database system was interrupted; last known up at 2019-12-07 09:01:08 UTC
db_1      | 2019-12-09 10:01:23.250 UTC [27] LOG:  database system was not properly shut down; automatic recovery in progress
db_1      | 2019-12-09 10:01:23.295 UTC [27] LOG:  redo starts at 0/1646188
db_1      | 2019-12-09 10:01:23.295 UTC [27] LOG:  invalid record length at 0/16461C0: wanted 24, got 0
db_1      | 2019-12-09 10:01:23.295 UTC [27] LOG:  redo done at 0/1646188
db_1      | 2019-12-09 10:01:23.407 UTC [1] LOG:  database system is ready to accept connections
</code></pre>

<p>Dockerfile :</p>

<pre><code>FROM python:3
RUN apt update
RUN apt-get install  postgresql-contrib -y
RUN apt-get install musl-dev -y
RUN apt-get install libpq-dev gcc
RUN apt-get install python3-dev -y
RUN export PATH=/usr/lib/postgresql/X.Y/bin/:$PATH
RUN pip3 install psycopg2-binary
RUN pip3 install psycopg2


# Delete build dependencies 
# RUN apk del .build-deps
ENV PYTHONUNBUFFERED 0
RUN mkdir /code
WORKDIR /code
COPY requirements.txt /code/
RUN pip3 install -r requirements.txt
COPY . /code/
CMD source develvenv/bin/activate
</code></pre>

<p>at first , I thought the virtual environment was the problem </p>

<p>So , i tried add <code>CMD source develvenv/bin/activate</code> But error still doesn't go away..</p>

<p>As far as i know, that kind of error is not a problem of Django itself.. I think virtual environment causes problems. </p>

<p>How is your idea ?</p>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","59596564","Unable to install Psycopg2 in Docker alpine image","<python><postgresql><docker><docker-compose><dockerfile>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>I am trying to run docker for an alpine image but unable to install psycopg2 from requirements.txt, tried all the solutions for similar problems but none worked.</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM frolvlad/alpine-miniconda3:python3.7 as base

FROM base as builder
# RUN apt-get update \
#     &amp;&amp; apt-get install -y build-essential \
#     &amp;&amp; apt-get install -y nginx 
WORKDIR /usr/src/app
COPY ./requirements.txt /usr/src/app/requirements.txt
RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=""/opt/venv/bin:$PATH""
RUN pip install gunicorn


RUN pip install -r requirements.txt


FROM base
RUN apk add nginx bash
# RUN conda install python==3.7.1
COPY --from=builder /usr/src/app /usr/src/app
COPY --from=builder /opt/venv /opt/venv
ENV PATH=""/opt/venv/bin:$PATH""

COPY . /usr/src/app/rapid-nrn-backend
WORKDIR /usr/src/app/rapid-nrn-backend

CMD [""/bin/bash"", ""entrypoint.sh""]
EXPOSE 80
EXPOSE 8000
</code></pre>

<p><strong>requirements.txt</strong>
<a href=""https://drive.google.com/open?id=1XTOG9PxnzfwqI5sCb6UE5RdlLor590hn"" rel=""nofollow noreferrer"">requirements.txt</a></p>

<p>Any help is highly appreciated.</p>
"
"46898193","docker exec or docker container exec","<docker>","60097132","Difference between docker exec and docker container exec","<docker><docker-container><docker-exec>","<p>I noticed in the latest Docker CLI documentation that Docker CLI command list has expanded. 
If I used <strong>docker exec</strong> earlier to start executable inside container now I can also use <strong>docker container exec</strong> command.</p>

<p><strong>docker container run</strong> command is similar to <strong>docker run</strong>, etc.</p>

<p>So which commands are preferrable now? Old syntax or new docker container syntax? Unfortunately I couldn't find any explanation in the docs.</p>

<p>Also what is the difference between docker container run and docker container create commands? And between docker container stop and docker container kill? The description and syntax are very similar.</p>

<p>Thanks.</p>
","<p>What is the difference between ""docker exec"" and ""docker container exec""?
The same for the difference between ""docker commit"" and ""docker container commit""?</p>

<p>I researched a lot but couldn't find any.
Your help is appreciated.</p>
"
"47900844","Reached error page: about:neterror when trying to navigate to other tabs if there is a form submit under that tab","<selenium><firefox><selenium-webdriver><webdriver><geckodriver>","59630869","selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?","<python><django><selenium><docker><selenium-webdriver>","<p>when I use Selenium to do automation testing, I hit an issue, here are all scenarios:</p>

<p>There are several tabs on top of the page, now that I want to click those tabs and fill up all forms under those tabs, but if I submit <code>formA</code> which under <code>tabA</code>, then I can not navigate to other tabs automatically. If I didn't submit the form data, the issue will not be happened. Here is the log:</p>

<pre><code>1513753361368 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361388 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""pagehide"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""unload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361427 Marionette DEBUG Received DOM event ""DOMContentLoaded"" for ""about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82""
org.openqa.selenium.WebDriverException: Reached error page: about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82
Build info: version: '3.8.1', revision: '6e95a6684b', time: '2017-12-01T18:33:54.468Z'
System info: host: 'PC-20161127KZEG', ip: '192.168.131.1', os.name: 'Windows 7', os.arch: 'amd64', os.version: '6.1', java.version: '1.8.0_111'
Driver info: org.openqa.selenium.firefox.FirefoxDriver
Capabilities {acceptInsecureCerts: true, browserName: firefox, browserVersion: 57.0.2, javascriptEnabled: true, moz:accessibilityChecks: false, moz:headless: false, moz:processID: 42248, moz:profile: C:\Users\Administrator\AppD..., moz:webdriverClick: false, pageLoadStrategy: normal, platform: XP, platformName: XP, platformVersion: 6.1, rotatable: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}}
</code></pre>
","<p>I'm using <a href=""https://bok-choy.readthedocs.io/en/latest/&gt;"" rel=""nofollow noreferrer"">bok_choy</a> which uses Selenium in order to write acceptance tests. The tests are working fine with chromedriver/geckodriver on a local machine but it doesn't work with Selenium Images inside docker container. Gives the following error:</p>

<pre><code>======================================================================
ERROR: test_result_page (__main__.TestPolls)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/bok_choy/page_object.py"", line 329, in visit
    self.browser.get(self.url)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 333, in get
    self.execute(Command.GET, {'url': url})
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=nssFailure2&amp;u=https%3A//demo.app%3A8000/polls/&amp;c=UTF-8&amp;f=regular&amp;d=%20


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""app/tests/test_polls.py"", line 35, in test_result_page
    self.polls_index_page.visit().click_question('question1')
  File ""/usr/local/lib/python3.8/site-packages/bok_choy/page_object.py"", line 332, in visit
    raise PageLoadError(u""Could not load page '{!r}' at URL '{}'"".format(
bok_choy.page_object.PageLoadError: Message: Could not load page '&lt;pages.PollsIndexPage object at 0x7f19190eed00&gt;' at URL 'http://demo.app:8000/polls/'


======================================================================
ERROR: test_vote_again (__main__.TestPolls)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/bok_choy/page_object.py"", line 329, in visit
    self.browser.get(self.url)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 333, in get
    self.execute(Command.GET, {'url': url})
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=nssFailure2&amp;u=https%3A//demo.app%3A8000/polls/&amp;c=UTF-8&amp;f=regular&amp;d=%20


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""app/tests/test_polls.py"", line 42, in test_vote_again
    self.polls_index_page.visit().click_question('question1')
  File ""/usr/local/lib/python3.8/site-packages/bok_choy/page_object.py"", line 332, in visit
    raise PageLoadError(u""Could not load page '{!r}' at URL '{}'"".format(
bok_choy.page_object.PageLoadError: Message: Could not load page '&lt;pages.PollsIndexPage object at 0x7f1918f24100&gt;' at URL 'http://demo.app:8000/polls/'



</code></pre>

<p>I used <code>python app/tests/test_polls.py</code> to run the tests inside <code>demo.app</code> container.</p>

<p>my dockerfile:</p>

<pre><code>FROM python:latest

COPY ./requirements.txt /requirements.txt
RUN pip install -r /requirements.txt

RUN mkdir /app
COPY ./app /app

ENV SELENIUM_BROWSER=firefox
ENV SELENIUM_HOST=selenium-hub
ENV SELENIUM_PORT=4444

</code></pre>

<p>My docker-compose.yml file:</p>

<pre><code>version: ""3""

services:
  app:
    container_name: demo.app
    build:
      context: .
    ports:
      - ""8000:8000""
    volumes:
      - ./app:/app
      - /etc/hosts:/etc/hosts
    command: &gt;
      sh -c ""python app/manage.py runserver 0.0.0.0:8000
      &amp;&amp; python test_polls.py""
  selenium-hub:
    image: selenium/hub:3.141.59-yttrium
    container_name: selenium-hub
    ports:
      - ""4444:4444""
  chrome:
    container_name: demo.chrome
    image: selenium/node-chrome:3.141.59-yttrium
    volumes:
      - /dev/shm:/dev/shm
    depends_on:
      - selenium-hub
    environment:
      - HUB_HOST=selenium-hub
      - HUB_PORT=4444
  firefox:
    container_name: demo.firefox
    image: selenium/node-firefox:3.141.59-yttrium
    volumes:
      - /dev/shm:/dev/shm
    depends_on:
      - selenium-hub
    environment:
      - HUB_HOST=selenium-hub
      - HUB_PORT=4444

</code></pre>

<p>Pages.py:</p>

<pre><code>from bok_choy.page_object import PageObject


class PollsIndexPage(PageObject):
    url = 'http://demo.app:8000/polls/'

    def is_browser_on_page(self):
        return self.q(css='.questions').is_present()

    def click_question(self, question):
        self.q(css='#{} a'.format(question)).click()
        PollsDetailPage(self.browser).wait_for_page()


class PollsDetailPage(PageObject):
    url = None

    def is_browser_on_page(self):
        return self.q(css='input#vote').is_present()

    def select_choice(self, choice):
        self.q(css='#{}'.format(choice)).click()

    def vote(self):
        self.q(css='input#vote').click()
        PollsResultPage(self.browser).wait_for_page()

    def vote_a_question(self, choice):
        self.select_choice(choice)
        self.vote()

    @property
    def title(self):
        return self.q(css='h1').text[0]

    @property
    def vote_error(self):
        return self.q(css='#error strong').text[0]


class PollsResultPage(PageObject):
    url = None

    def is_browser_on_page(self):
        return self.q(css='#vote_again').is_present()

    def vote_again(self):
        self.q(css='#vote_again').click()
        PollsDetailPage(self.browser).wait_for_page()

</code></pre>

<p>test_polls.py:</p>

<pre><code>import unittest

from bok_choy.web_app_test import WebAppTest

from pages import PollsIndexPage, PollsDetailPage, PollsResultPage


class TestPolls(WebAppTest):
    def setUp(self):
        """"""
        Instantiate the page objects.
        """"""
        super(TestPolls, self).setUp()
        self.polls_index_page = PollsIndexPage(self.browser)
        self.polls_detail_page = PollsDetailPage(self.browser)
        self.polls_result_page = PollsResultPage(self.browser)

    def test_page_existence(self):
        """"""
        Make sure that the page is accessible.
        """"""
        self.polls_index_page.visit()

    def test_detail_page(self):
        """"""
        Check if the detail page opens after clicking a question
        """"""
        self.polls_index_page.visit().click_question('question1')
        assert ""What's up?"" == self.polls_detail_page.title

    def test_result_page(self):
        """"""
        Check if the result page opens after voting a question
        """"""
        self.polls_index_page.visit().click_question('question1')
        self.polls_detail_page.vote_a_question('choice1')

    def test_vote_again(self):
        """"""
        Check vote_again option in results page
        """"""
        self.polls_index_page.visit().click_question('question1')
        self.polls_detail_page.vote_a_question('choice1')
        self.polls_result_page.vote_again()
        self.polls_detail_page.vote_a_question('choice2')

    def test_invalid_vote(self):
        """"""
        make sure that an error message is shown when no choice is selecteddadad
        """"""
        self.polls_index_page.visit().click_question('question1')
        self.polls_detail_page.q(css='input#vote').click()
        assert ""You didn't select a choice."" == self.polls_detail_page.vote_error


if __name__ == '__main__':
    unittest.main()

</code></pre>
"
"48066994","Docker: ""no matching manifest for windows/amd64 in the manifest list entries""","<docker>","59291295","How to start a nginx Docker container?","<docker><nginx><containers>","<p>I use Docker on Windows, and when I tried to pull a PHP image with this command</p>

<pre><code>$ docker pull php
</code></pre>

<p>I got this message:</p>

<pre><code>Using default tag: latest
latest: Pulling from library/php no matching manifest for windows/amd64 
        in the manifest list entries
</code></pre>

<p>How can I fix this problem?</p>
","<p>I found some tips online but I couldn't really follow.<br>
I tried:  </p>

<pre><code>docker run --name nginx1 -p 80:80 -d nginx  
</code></pre>

<p>and got  </p>

<pre><code>Unable to find image 'nginx:latest' locally
latest: Pulling from library/nginx  
C:\Program Files\Docker\Docker\Resources\bin\docker.exe: no matching manifest for windows/amd64 10.0.18362 in the manifest list entries.  
See 'C:\Program Files\Docker\Docker\Resources\bin\docker.exe run --help'.
</code></pre>

<p>Edit: solved by running Docker in experimental mode (right click Docker Desktop instance > Settings > check Experimental features)</p>
"
"48561981","Activate python virtualenv in Dockerfile","<python><docker><virtualenv><dockerfile>","59777926","Dockerized flask app builds and runs locally but wont work when deployed on Azure","<python><azure><docker><flask><dockerfile>","<p>I have a Dockerfile where I try to activate python virtualenv after what, it should install all dependencies within this env. However, everything still gets installed globally. I used different approaches and non of them worked. I also do not get any errors. Where is a problem?</p>

<p>1.
<code>ENV PATH $PATH:env/bin</code></p>

<p>2.
<code>ENV PATH $PATH:env/bin/activate</code></p>

<p>3.
<code>RUN . env/bin/activate</code></p>

<p>I also followed <a href=""https://github.com/GoogleCloudPlatform/python-runtime#kubernetes-engine--other-docker-hosts"" rel=""noreferrer"">an example of a Dockerfile config for the python-runtime image on Google Cloud</a>, which is basically the same stuff as above.</p>

<blockquote>
  <p>Setting these environment variables are the same as running source /env/bin/activate.</p>
</blockquote>

<p><code>ENV VIRTUAL_ENV /env</code></p>

<p><code>ENV PATH /env/bin:$PATH</code></p>

<p>Additionally, what does <code>ENV VIRTUAL_ENV /env</code> mean and how it is used?</p>
","<p>I'm attempting to deploy a flask web app using a docker container. I'm dockerizing the app because I have a package (RDKit) which cannot be pip installed and must be done using conda.</p>

<p>When using 
<code>docker run my_image</code></p>

<p>I get an import error for flask - the first library that my python script imports. However, I'm able to deploy the app locally when I specify the port using the command: <code>docker run -p 80:80 my_image</code>. I have a feeling that my Dockerfile might be the problem:</p>

<pre><code>FROM cameroncruz/flask-nginx-uwsgi-miniconda:python3.6

# Create conda env (base image is built on python 3.6 and will have to migrate to 3.7 later)
RUN conda create --name myenv python=3.6

# Install the required dependencies
RUN /bin/bash -c "". activate myenv &amp;&amp; \
    conda config --add channels conda-forge &amp;&amp; \
    conda install -y \
        scikit-learn \
        numpy \
        scipy \
    rdkit \
    wtforms \
    ipython \
        flask \
        uwsgi""

# Copy custom supervisor config
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# Copy application files

# The application to be served
COPY server/main.py /app/main.py

# Folder containing the styling
COPY server/static/ /app/static

# Folder containing the html
COPY server/templates/ /app/templates

# The trained machine learning model
copy server/model.pkl /app/model.pkl

# EXPOSE PORT 80
EXPOSE 80

# Run app.py when the container launches
CMD [""python"", ""main.py""]
</code></pre>

<p>The beginning of my python code:</p>

<pre><code>from flask import Flask, jsonify, request, json
</code></pre>

<p>The end of my python code:</p>

<pre><code>    app.run(debug=False, host='0.0.0.0', port=80)
</code></pre>

<p>Here is the supervisord.conf file:</p>

<pre><code>nodaemon=true

[program:uwsgi]
environment=PATH='/opt/conda/envs/myenv/bin'
command=/opt/conda/envs/myenv/bin/uwsgi --ini /etc/uwsgi/uwsgi.ini --die-on-term --need-app

stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0

[program:nginx]
command=/usr/sbin/nginx
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0
# Graceful stop, see http://nginx.org/en/docs/control.html
stopsignal=QUIT
</code></pre>

<p>I have deployed apps on Azure before but this is my first time with a container. I'm also using Ubuntu 18.04, if that matters. Any and all help is appreciated, thank you!</p>
"
"48568172","docker.sock permission denied","<linux><docker>","59244849","Docker build not working- permission denied","<docker><fedora>","<p>When I try to run simple docker commands like:</p>

<pre><code>$ docker ps -a
</code></pre>

<p>I get an error message:</p>

<blockquote>
  <p>Got permission denied ... /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<p>When I check permissions with</p>

<pre><code>$ ls -al /var/run/
</code></pre>

<p>I see this line:</p>

<pre><code>srw-rw---- root docker docker.sock
</code></pre>

<p>So, I follow an advice from many forums and add local user to docker group:</p>

<pre><code>$ sudo usermod -aG docker $USER
</code></pre>

<p>But it does not help. I still get the very same error message. How can I fix it?</p>
","<p>I am trying to build my docker images but failing with permission denied. 
Docker service is up an running.
Do I need to be root/have sudo access to build images?</p>

<p><code>[jack@localhost work]$ docker build -t my-app .</code> </p>

<blockquote>
  <p>Got permission denied while trying to connect to the Docker daemon
  socket at unix:///var/run/docker.sock: Post
  <a href=""http://%2Fvar%2Frun%2Fdocker.sock/v1.26/build?buildargs=%7B%7D&amp;buildbinds=null&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=my-app&amp;ulimits=null"" rel=""nofollow noreferrer"">http://%2Fvar%2Frun%2Fdocker.sock/v1.26/build?buildargs=%7B%7D&amp;buildbinds=null&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=my-app&amp;ulimits=null</a>: dial unix /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<pre><code>[jack@localhost work]$ service docker status
</code></pre>

<p><a href=""https://i.stack.imgur.com/A1l86.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A1l86.png"" alt=""enter image description here""></a></p>
"
"50294165","What is the best practice to use kubernetes for local development?","<docker><kubernetes><google-cloud-platform>","59783602","Kubernetes best way 4 locally","<docker><kubernetes><docker-compose><containers><google-kubernetes-engine>","<p>I am using docker containers and have docker-compose files for both local development and production environment. I want to try Google Cloud Platform for my new app and specifically Google Kubernetes Engine. My tools is Docker for Mac with Kubernetes on local machine.</p>

<p>It is super important for developers to be able to change code and to see changes live for local development.</p>

<p>Use cases:</p>

<ol>
<li><p>Backend developer make changes to basic Flask API (or whatever you use) and should see changes on reloaded app immediately.</p></li>
<li><p>Frontend developer make changes to HTML layout and should see changes on web page immediately.</p></li>
</ol>

<p>At the moment i am using docker-compose files to mount source code to local containers. But Kubernetes does not support relative paths to mount the source code.</p>

<p>Ideally i should be able to set the variable </p>

<blockquote>
  <p>Deployment.spec.templates.spec.containers.volumes.hostPath</p>
</blockquote>

<p>as relative path to my repo. For example, in our team developers clone repo to this folders:</p>

<blockquote>
  <p>/User/BACKEND_developer/code/project_repo</p>
  
  <p>/User/FRONTEND_developer/code/project_repo</p>
</blockquote>

<p>Obviously you can't commit and build the image after every little change to the source code.</p>

<p>So what is the best practice for local development with Kubernetes? Do i need some additional tools to modify .yaml files for every developer?</p>
","<p>Im new at kubernetes and would like to manage docker on my local servers.
Which ist he best way to use kubernetes locally?
Minikube just support 1 node. There are better ways?</p>
"
"50790733","unknown error: DevToolsActivePort file doesn't exist error while executing Selenium UI test cases on ubuntu","<linux><google-chrome><selenium><selenium-webdriver><selenium-chromedriver>","59638845","Capybara Docker Headless Chrome - unknown error: DevToolsActivePort file doesn't exist","<ruby-on-rails><selenium><docker><selenium-chromedriver><capybara>","<p>I have an ubuntu server having the UI as well. U can execute the test cases by firing mvn test command. But the problem is when I do ssh of the machine through the terminal from another machine I get the following error-</p>

<pre><code>unknown error: DevToolsActivePort file doesn't exist
  (Driver info: chromedriver=2.40.565383 (76257d1ab79276b2d53ee976b2c3e3b9f335cde7),platform=Linux 4.4.0-121-generic x86_64) (WARNING: The server did not provide any stacktrace information)
Command duration or timeout: 60.04 seconds
Build info: version: '3.8.1', revision: '6e95a6684b', time: '2017-12-01T18:33:54.468Z'
System info: host: 'ubuntu-test', ip: 'X.X.X.X', os.name: 'Linux', os.arch: 'amd64', os.version: '4.4.0-121-generic', java.version: '1.8.0_171'
Driver info: driver.version: ChromeDriver
</code></pre>

<p>but the same command starts chrome successfully if I take remote of the machine through remmina and then execute the same command of this machines terminal.</p>
","<p>Running tests within Docker using capybara and headless chrome.  I can see a bunch of defunct processes which is from the chromedriver trying to start chrome.</p>

<pre><code>sh-4.2# google-chrome --version
Google Chrome 79.0.3945.117
sh-4.2# chromedriver --version
ChromeDriver 79.0.3945.36 (3582db32b33893869b8c1339e8f4d9ed1816f143-refs/branch-heads/3945@{#614})
</code></pre>

<p>Capybara driver config:</p>

<pre><code>  Capybara.register_driver(:headless_chrome) do |app|
    capabilities = Selenium::WebDriver::Remote::Capabilities.chrome(
      chromeOptions: { args: %w[no-sandbox headless disable-gpu disable-dev-shm-usage window-size=1920,1080] }
    )

    Capybara::Selenium::Driver.new(
      app,
      browser: :chrome,
      desired_capabilities: capabilities
    )
  end
  Capybara.default_driver = :selenium
  Capybara.javascript_driver = :headless_chrome
</code></pre>

<p>Gem versions installed:</p>

<ul>
<li>gem 'selenium-webdriver', '3.6.0'</li>
<li>gem 'capybara', '2.6.2'</li>
</ul>

<p>Within the container I can run commands against google-chrome using the arguments in the capybara config and I get results which leads me to believe it's something with the driver in capybara, but I'm at a loss debugging this app that I didn't write.  Any ideas would be greatly appreciated.  Thanks!</p>
"
"51140055","Cannot access web server using docker","<docker>","60147804","Nuxt and Docker: specifying server in nuxt.config.js - even to default - prevents access on localhost?","<node.js><docker><nuxt.js>","<p>I used the following command to run the container :</p>

<pre><code>docker run -p 3333:3333 -d maill/node-web-app
</code></pre>

<p>Here is the result of docker ps :</p>

<pre><code>CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS                    NAMES
f26270107bfa        maill/node-web-app   ""npm run dev""       49 seconds ago      Up 46 seconds       0.0.0.0:3000-&gt;3000/tcp   musing_fermi
</code></pre>

<p>However when I try to access webserver on host using localhost:3333  it doesn't work.</p>

<p>I am using windows 10 pro.</p>

<p><code>docker logs musing_fermi</code> shows:</p>

<pre><code>DONE Compiled successfully in 3541ms16:04:50 | OPEN localhost:3000
</code></pre>

<p>Dockerfile :</p>

<pre><code>FROM node:8
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm i
COPY . .
EXPOSE 3333
CMD [ ""npm"", ""run"", ""dev"" ]
</code></pre>

<p>package.json :</p>

<pre><code>{
  ""name"": ""webapp-pst-horizon"",
  ""version"": ""1.0.0"",
  ""description"": ""Webapp pour les formations enedis"",
  ""author"": ""Léo Coletta"",
  ""private"": true,
  ""scripts"": {
    ""dev"": ""cross-env HOST=0.0.0.0 PORT=3333 nuxt"",
    ""build"": ""nuxt build"",
    ""start"": ""nuxt start"",
    ""generate"": ""nuxt generate"",
    ""lint"": ""eslint --ext .js,.vue --ignore-path .gitignore ."",
    ""precommit"": ""npm run lint""
  },
  ""dependencies"": {
    ""@nuxtjs/axios"": ""^5.3.1"",
    ""@nuxtjs/proxy"": ""^1.2.4"",
    ""axios"": ""^0.18.0"",
    ""babel-polyfill"": ""^6.26.0"",
    ""cookie"": ""^0.3.1"",
    ""js-cookie"": ""^2.2.0"",
    ""nuxt"": ""^1.4.1"",
    ""vuetify"": ""^1.0.19"",
    ""webpack"": ""^3.1.0""
  },
  ""devDependencies"": {
    ""babel-eslint"": ""^8.2.3"",
    ""cross-env"": ""^5.2.0"",
    ""eslint"": ""^4.9.0"",
    ""eslint-config-airbnb-base"": ""^12.1.0"",
    ""eslint-loader"": ""^2.0.0"",
    ""eslint-plugin-import"": ""^2.7.0"",
    ""eslint-plugin-vue"": ""^4.5.0"",
    ""stylus"": ""^0.54.5"",
    ""stylus-loader"": ""^3.0.2""
  }
}
</code></pre>
","<p>In essence the project was setup as follows:</p>

<pre><code>mkdir project
cd project
npx create-nuxt-app frontend
cd frontend
touch Dockerfile.development
cd ..
touch docker-compose.web.development.yml
</code></pre>

<p>In <code>frontend/nuxt.config.js</code> the following was updated:</p>

<pre class=""lang-js prettyprint-override""><code>const isDevelopment = process.env.NODE_ENV !== 'PRODUCTION'

export default {
  // ... 
  server: {
    host: isDevelopment ? 'localhost' : '0.0.0.0', // default: localhost
    port: isDevelopment ? 3000 : 80, // default: 3000
  },
  // ...
}
</code></pre>

<p>In <code>frontend/Dockerfile.development</code>:</p>

<pre><code>FROM node:10.15
ENV APP_ROOT /src

RUN mkdir ${APP_ROOT}
WORKDIR ${APP_ROOT}
COPY ./frontend/package.json .
RUN npm install
</code></pre>

<p>and in <code>./docker-compose.web.development.yml</code></p>

<pre><code>version: '3'

services:
  nuxt:
    image: frontend
    container_name: nuxt_fe
    build:
      context: .
      dockerfile: ./frontend/Dockerfile.development
    restart: always
    environment:
      - HOST
    ports:
      - ""3000:3000""
    command: ""npm run dev""
    volumes:
      - ./frontend/assets:/src/assests
      # and then the rest of the nuxt directories
      # if I just map ./frontend:/src it will overwrite node_modules installed during build
</code></pre>

<p>If I in <code>nuxt.config.js</code> comment out the <code>server</code> configuration (even though it is set to default values), when I go to <code>localhost:3000</code> I see the site. If I uncomment <code>server</code> in <code>nuxt.config.js</code>, I see nothing.</p>

<p>Thought?</p>
"
"51462200","Docker Compose connect ECONNREFUSED 172.18.0.4:3306","<mysql><docker><docker-compose><sequelize.js>","59765806","Docker: NodeJs crashing due to Mysql connections","<mysql><docker><docker-compose>","<p>When I build the container of the project with this command:</p>

<pre><code>sudo docker build -t PROJECT_NAME .
</code></pre>

<p>And then I download the mysql's image through this Docker-Compose config:</p>

<pre><code>  db:
    image: mysql
    restart: always
    ports:
      - ""8999:3306""
    networks:
      - webnet
    environment:
      MYSQL_DATABASE: slack
      MYSQL_ROOT_PASSWORD: admin
</code></pre>

<p>And then, the project will connect with MySQL through the <a href=""http://docs.sequelizejs.com/"" rel=""nofollow noreferrer"">Sequelize ORM</a></p>

<p>I have this error:</p>

<pre><code>Unhandled rejection SequelizeConnectionRefusedError: connect ECONNREFUSED 172.18.0.4:3306
</code></pre>

<p><a href=""https://i.stack.imgur.com/s4LGb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s4LGb.png"" alt=""enter image description here""></a></p>

<p>How can I resolve this?</p>

<p>The Sequelize config is this:</p>

<pre><code>const sequelize = new Sequelize(process.env.TEST_DB || 'slack', 'root', 'admin', {
  host: process.env.DB_HOST || 'localhost',
  operatorsAliases: Sequelize.Op,
  dialect: 'mysql',
  define: {
    underscored: true
  }
});
</code></pre>

<p>The config of the web is this:</p>

<pre><code>web:
    image: slack-clone-server
    ports:
      - ""8080:8080""
    networks:
      - webnet
    environment:
      DB_HOST: db
      REDIS_HOST: redis
    command: [""./wait-for-it.sh"", ""db:3306"", ""--"", ""npm"", ""run"", ""prod""]
</code></pre>

<p>The script called ""wait for it"" is <a href=""https://github.com/vishnubob/wait-for-it/blob/master/wait-for-it.sh"" rel=""nofollow noreferrer"">this</a>.</p>

<p>If someone needs the complete code, here you go:</p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/src/models/index.js"" rel=""nofollow noreferrer"">Sequelize config</a></p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/docker-compose.yml"" rel=""nofollow noreferrer"">Docker Compose config</a></p>

<p><a href=""https://github.com/MontoyaAndres/slack-clone/blob/master/server/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile config</a></p>
","<p>Using <code>Docker-compose version 3</code>,  Both container is running fine at 3306(mysql), 5002(app), but nodejs not connecting to <code>mysql</code> service</p>

<p><strong>Docker containers list</strong>:</p>

<pre><code>8e475b1fd110        app           ""npm start""              15 seconds ago      Up 10 seconds       0.0.0.0:5002-&gt;5002/tcp             
d5fe68309cd4        mysql:5.7     ""docker-entrypoint.s…""   19 seconds ago      Up 15 seconds       0.0.0.0:3306-&gt;3306/tcp, 33060/tcp
</code></pre>

<p>docker logs for both <code>app</code> and <code>mysql</code>.</p>

<p><strong>App Docker Logs:</strong></p>

<pre><code> docker logs --details 8e475b1fd110

 &gt; rest@1.0.0 start /home/node/src/server
 &gt; nodemon --exec babel-node src/server.js

 [nodemon] 2.0.2
 [nodemon] to restart at any time, enter `rs`
 [nodemon] watching dir(s): *.*
 [nodemon] watching extensions: js,mjs,json
 [nodemon] starting `babel-node src/server.js`
 Server started at port 5002
 /home/node/src/server/src/config/db.js:26
   if (err) throw err;
            ^

 Error: connect ECONNREFUSED 172.22.0.2:3306
     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1097:14)
     --------------------
     at Protocol._enqueue (/home/node/src/server/node_modules/mysql/lib/protocol/Protocol.js:144:48)
     at Protocol.handshake (/home/node/src/server/node_modules/mysql/lib/protocol/Protocol.js:51:23)
     at Connection.connect (/home/node/src/server/node_modules/mysql/lib/Connection.js:119:18)
     at Object.&lt;anonymous&gt; (/home/node/src/server/src/config/db.js:12:12)
 [nodemon] app crashed - waiting for file changes before starting...
</code></pre>

<p><strong>Mysql Docker Logs:</strong></p>

<pre><code> 2020-01-16T08:20:29.252232Z 0 [Note] mysqld (mysqld 5.7.28) starting as process 1 ...
 2020-01-16T08:20:29.273098Z 0 [Note] InnoDB: PUNCH HOLE support available
 2020-01-16T08:20:29.273139Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
......
 2020-01-16T08:20:29.652300Z 0 [Note] IPv6 is available.
 2020-01-16T08:20:29.652315Z 0 [Note]   - '::' resolves to '::';
 2020-01-16T08:20:29.652337Z 0 [Note] Server socket created on IP: '::'.
 2020-01-16T08:20:29.689338Z 0 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
 2020-01-16T08:20:29.708091Z 0 [Note] Event Scheduler: Loaded 0 events
 2020-01-16T08:20:29.708416Z 0 [Note] mysqld: ready for connections.
 Version: '5.7.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
</code></pre>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM node:10.15.3-jessie
ENV APP_HOME=/home/node/src/server
WORKDIR ${APP_HOME}
COPY package.json .
RUN npm install -g npm
COPY . .
USER node
EXPOSE 5002
CMD [""npm"", ""start""]
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3'
services:
  db:
    container_name: db_mysql
    image: mysql:5.7
    ports:
      - '3306:3306'
    environment:
      - MYSQL_ROOT_USER=root
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=sampledb
    networks:
      - dbnet
  app:
    container_name: app_server
    restart: always
    build: .
    ports:
      - '5002:5002'
    depends_on: 
      - db
    environment:
      - PORT=5002
      - NODE_ENV=development
      - DB_HOST=db_mysql
      - DB_PORT=3306
      - DB_PASSWORD=root
      - DB_USER=root
      - DB_DATABASE=sampledb
    networks:
      - dbnet
networks:
  dbnet:
    external: true
</code></pre>

<p>Created network by run <code>docker network create dbnet</code></p>

<p><strong>database.js</strong></p>

<pre><code>import mysql from ""mysql"";

const connection = mysql.createConnection({
  host: ""db_mysql"",            // using DB container name
  user: ""root"",
  password: ""root"",
  database: ""sampledb""
});

// connect to database
connection.connect(function(err) {
  if (err) throw err
  console.log('You are now connected with mysql database...')
});


export default connection
</code></pre>
"
"53045112","Explore Docker's image files if container exits immediately?","<docker><dockerfile>","59968631","Inspect docker container at container startup","<docker><containers><inspect>","<p>I have a Docker image that I pulled from Docker hub.</p>

<p>When I run <code>docker run image_name</code>, container exits immediately.</p>

<p>I don't have access to the source code of the Docker image, including Dockerfile. All I have is an image I pulled from <code>hub.docker.com</code>.</p>

<p>I need to debug/see what is inside the image (e.g. see and explore image's filesystem), without running it as a container.</p>

<p>Is it possible?</p>
","<p>I downloaded an image which immediately stops. How can I inspect it (or any container spawned from it)?</p>

<p>I cannot use anything like <code>docker exec -it CONTAINER_ID bash</code> since I don't have time to get the <code>CONTAINER_ID</code>.</p>

<p>(<code>docker run -it 5413e661e579 bash</code> does not help, it starts the container and stops immediatly.)</p>

<p>I don't know how the image was built, I don't have the Dockerfile ; the only thing I know is the entrypoint: <code>[""python"" ""app.py""]</code> but it does not output anything useful.</p>

<p><strong>Answer <a href=""https://stackoverflow.com/a/53045181/719276"">from duplicate question</a>:</strong></p>

<p><code>docker run -it --entrypoint ""/bin/bash"" image_name</code></p>
"
"53429062","Docker: Springboot container can not connect to PostgreSql Container Connection error","<postgresql><spring-boot><docker><docker-compose>","59611011","docker-compose spring boot can't connect to Postgres","<postgresql><spring-boot><docker><docker-compose>","<p>I am building my first Springboot 2.0 application. I am trying to put my Springboot application into one docker container and my PostgresDB into another container.  </p>

<p><strong>My Dockerfile</strong></p>

<pre><code>    FROM frolvlad/alpine-oraclejdk8:slim
    VOLUME /tmp
    ADD springboot-api-demo-0.1*.jar app.jar
    RUN sh -c 'touch /app.jar'
    EXPOSE 9443
    ENTRYPOINT [ ""sh"", ""-c"", ""java $JAVA_OPTS -Djava.security.egd=file:/dev/urandom -jar /app.jar"" ]
</code></pre>

<p><strong>My docker-compose.yml file</strong></p>

<pre><code>version: ""2.1""

services:
  springboot-api-demo:
    image: ""fw/springboot-api-demo""
    mem_limit: 1024m
    ports:
      - ""8080:8080""
    environment:
      - SPRING_PROFILES_ACTIVE=local
      - AWS_REGION=local
      - ENVIRONMENT=local
      - AUTH_ENABLED=false
  postgres:
    container_name: pgdb
    image: postgres:9.6-alpine
    environment:
    - 'POSTGRES_ROOT_PASSWORD=postgres'
    - 'POSTGRES_USER=postgres'
    - 'POSTGRES_PASSWORD=postgres'
    ports:
    - ""54321:5432""
</code></pre>

<p>I am using Springboot JPA Data 2.0 with below config data in my <strong>application.properties</strong></p>

<pre><code>spring.datasource.url= jdbc:postgresql://localhost:54321/java_learning
spring.datasource.username=postgres
spring.datasource.password=postgres
</code></pre>

<p>I can test that Both of the Images are up. Also from docker log and docker events, I see that postgres  Container is running fine, even I can access it and also created a DB too.
But springboot container started but i died because it could not connect to postgress and throwing error below. </p>

<blockquote>
  <p>Unable to obtain connection from database: The connection attempt
  failed</p>
</blockquote>

<p>Note that my host machine already has Postgres on port 5432 thats why I did a port mapping ofr 54321:5432 on my postgres container. Here is Proof :) -</p>

<pre><code>➜  springboot-api-demo git:(master) ✗ lsof -i:54321              
COMMAND     PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 44345 shailendra.singh   18u  IPv4 0xf62897fbdd69e31d      0t0  TCP *:54321 (LISTEN)
com.docke 44345 shailendra.singh   21u  IPv6 0xf62897fbdd119975      0t0  TCP localhost:54321 (LISTEN)

➜  springboot-api-demo git:(master) ✗ lsof -i:5432 
COMMAND  PID             USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 715 shailendra.singh    5u  IPv6 0xf62897fbb43e03b5      0t0  TCP localhost:postgresql (LISTEN)
postgres 715 shailendra.singh    6u  IPv4 0xf62897fbbaeea9bd      0t0  TCP localhost:postgresql (LISTEN)
</code></pre>

<p>I am not sure what is the problem. But my Springboot application is not able to connect my postgres container which is running fine with proper creadentials. </p>
","<p>I am facing the weirdest issue ever, I have an spring boot application with postgres,</p>

<p>I deployed Postgres on docker and I have connect my spring application to it and everything went fine.</p>

<p>but when I try to docker-compose my application with Postgres it refuses to connect with this stack error</p>

<pre><code>springbootapp_1  | 2020-01-06 10:53:54.419  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
springbootapp_1  | 2020-01-06 10:53:54.484 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
springbootapp_1  | 
springbootapp_1  | org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.makeConnection(Driver.java:458) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.connect(Driver.java:260) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:175) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:118) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1245) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
springbootapp_1  |      at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
springbootapp_1  |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
springbootapp_1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
springbootapp_1  |      at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
springbootapp_1  |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
springbootapp_1  |      at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
springbootapp_1  |      at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:75) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      ... 58 common frames omitted
springbootapp_1  | 
springbootapp_1  | 2020-01-06 10:53:54.485  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  | 2020-01-06 10:53:54.502  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQLDialect
springbootapp_1  | 2020-01-06 10:53:55.307  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
springbootapp_1  | 2020-01-06 10:53:55.309 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
springbootapp_1  | 
springbootapp_1  | org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.makeConnection(Driver.java:458) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.Driver.connect(Driver.java:260) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
springbootapp_1  |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:60) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.SequenceInformationExtractorLegacyImpl.extractMetadata(SequenceInformationExtractorLegacyImpl.java:40) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.initializeSequences(DatabaseInformationImpl.java:65) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.&lt;init&gt;(DatabaseInformationImpl.java:59) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.Helper.buildDatabaseInformation(Helper.java:155) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:96) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:184) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:73) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:320) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:462) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1249) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
springbootapp_1  |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
springbootapp_1  |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
springbootapp_1  |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
springbootapp_1  |      at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
springbootapp_1  |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
springbootapp_1  |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
springbootapp_1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
springbootapp_1  |      at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
springbootapp_1  |      at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
springbootapp_1  |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
springbootapp_1  |      at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
springbootapp_1  |      at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:75) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192) ~[postgresql-42.2.8.jar!/:42.2.8]
springbootapp_1  |      ... 56 common frames omitted
springbootapp_1  | 
springbootapp_1  | 2020-01-06 10:53:55.310  WARN 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 08001
springbootapp_1  | 2020-01-06 10:53:55.310 ERROR 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
springbootapp_1  | 2020-01-06 10:53:55.319  WARN 1 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
springbootapp_1  | 2020-01-06 10:53:55.322  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
springbootapp_1  | 2020-01-06 10:53:55.336  INFO 1 --- [           main] ConditionEvaluationReportLoggingListener : 
springbootapp_1  | 
springbootapp_1  | Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
springbootapp_1  | 2020-01-06 10:53:55.339 ERROR 1 --- [           main] o.s.boot.SpringApplication               : Application run failed
springbootapp_1  | 
springbootapp_1  | org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution

appointmentssystem_springbootapp_1 exited with code 1

</code></pre>

<p>I will share my Dockerfile </p>

<pre><code>FROM adoptopenjdk/openjdk11
ADD ./build/libs/AppointmentsSystem-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar
</code></pre>

<p>and this is my build.gradle</p>

<pre><code>plugins {
    id 'org.springframework.boot' version '2.2.1.RELEASE'
    id 'io.spring.dependency-management' version '1.0.8.RELEASE'
    id 'java'
}

group = 'com.sulimanLab'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '11'

configurations {

    compileOnly {
        extendsFrom annotationProcessor
    }
}

repositories {
    mavenCentral()
}

wrapper {
    gradleVersion = ""5.0""
}

dependencies {

    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa'
    implementation group: 'org.postgresql', name: 'postgresql'


    compileOnly 'org.projectlombok:lombok'
    annotationProcessor 'org.projectlombok:lombok'
    testImplementation('org.springframework.boot:spring-boot-starter-test') {
        exclude group: 'org.junit.vintage', module: 'junit-vintage-engine'
    }
    testImplementation 'io.projectreactor:reactor-test'
}

test {
    useJUnitPlatform()
}

</code></pre>

<p>and this is my docker-compose file</p>

<pre><code>version: ""3.7""
services:
  postgres:
    image: postgres:latest
    container_name: postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - 5432:5432
    environment:
      - 'POSTGRES_ROOT_PASSWORD=admin'
      - 'POSTGRES_USER=postgres'
      - 'POSTGRES_PASSWORD=admin'
      - 'POSTGRES_DB=appointmentSystem'

  springbootapp:
    build: .
    environment:
      - 'spring.datasource.url=jdbc:postgresql://postgres:5432/appointmentSystem'
    ports:
      - 8080:8080
    depends_on:
      - postgres
volumes:
  postgres-data:

</code></pre>

<p>lastly this is my application.yml</p>

<pre><code>spring:
  datasource:
    driver-class-name: org.postgresql.Driver
    url: jdbc:postgresql://localhost:5432/appointmentSystem
    username: postgres
    password: admin
  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    database: postgresql
    hibernate:
      ddl-auto: update
    properties:
      dialect: org.hibernate.dialect.PostgreSQLDialect

</code></pre>

<p><strong>------------------</strong></p>

<p><strong>NOTE</strong></p>

<p>when I build the jar file and run it alone it works, so I started to think that the problem came from dockerfile but I don't know, Please help.</p>
"
"53988649","Docker - SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306","<node.js><docker><express><sequelize.js>","59889986","Cant connect Postgres with NodeJS (Docker Compose)","<node.js><postgresql><docker><docker-compose><sequelize.js>","<p>I'm trying to get my nodejs application up and running using a docker container. I have no clue what might be wrong. The credentials seems to be passed correctly when I debug the credentials with the console. Also firing up sequel pro and connecting directly with the same username and password seems to work. When node starts in the container I get the error message: </p>

<blockquote>
  <p>SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306</p>
</blockquote>

<p>The application itself is loading correctly on port 3000, however no data is retrieved from the database. If have also tried adding the environment variables directly to the docker compose file, but this also doesn't seem to work.</p>

<p>My project code is hosted over here: <a href=""https://github.com/pietheinstrengholt/rssmonster"" rel=""noreferrer"">https://github.com/pietheinstrengholt/rssmonster</a></p>

<p>The following database.js configuration is used. When I add console.log(config) the correct credentials from the .env file are displayed.</p>

<pre><code>require('dotenv').load();

const Sequelize = require('sequelize');
const fs = require('fs');
const path = require('path');
const env = process.env.NODE_ENV || 'development';
const config = require(path.join(__dirname + '/../config/config.js'))[env];

if (config.use_env_variable) {
  var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
  var sequelize = new Sequelize(config.database, config.username, config.password, config);
}

module.exports = sequelize;
</code></pre>

<p>When I do a console.log(config) inside the database.js I get the following output:</p>

<pre><code>{
username: 'rssmonster',
password: 'password',
database: 'rssmonster',
host: 'localhost',
dialect: 'mysql'
}
</code></pre>

<p>Following .env:</p>

<pre><code>DB_HOSTNAME=localhost
DB_PORT=3306
DB_DATABASE=rssmonster
DB_USERNAME=rssmonster
DB_PASSWORD=password
</code></pre>

<p>And the following docker-compose.yml:</p>

<pre><code>version: '2.3'
services:

  app:
    depends_on:
      mysql:
        condition: service_healthy
    build:
      context: ./
      dockerfile: app.dockerfile
    image: rssmonster/app
    ports:
      - 3000:3000
    environment:
      NODE_ENV: development
      PORT: 3000
      DB_USERNAME: rssmonster
      DB_PASSWORD: password
      DB_DATABASE: rssmonster
      DB_HOSTNAME: localhost
    working_dir: /usr/local/rssmonster/server
    env_file:
      - ./server/.env
    links:
      - mysql:mysql

  mysql:
    container_name: mysqldb
    image: mysql:5.7
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_RANDOM_ROOT_PASSWORD: ""yes""
      MYSQL_DATABASE: ""rssmonster""
      MYSQL_USER: ""rssmonster""
      MYSQL_PASSWORD: ""password""
    ports:
      - ""3307:3306""
    volumes:
      - /var/lib/mysql
    restart: unless-stopped
    healthcheck:
      test: [""CMD"", ""mysqladmin"" ,""ping"", ""-h"", ""localhost""]
      timeout: 5s
      retries: 10

volumes:
  dbdata:
</code></pre>

<p>Error output:</p>

<pre><code>{ SequelizeConnectionRefusedError: connect ECONNREFUSED 127.0.0.1:3306
app_1    |     at Promise.tap.then.catch.err (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:128:19)
app_1    | From previous event:
app_1    |     at ConnectionManager.connect (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/mysql/connection-manager.js:125:13)
app_1    |     at sequelize.runHooks.then (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:306:50)
app_1    | From previous event:
app_1    |     at ConnectionManager._connect (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:306:8)
app_1    |     at ConnectionManager.getConnection (/usr/local/rssmonster/server/node_modules/sequelize/lib/dialects/abstract/connection-manager.js:247:46)
app_1    |     at Promise.try (/usr/local/rssmonster/server/node_modules/sequelize/lib/sequelize.js:564:34)
app_1    | From previous event:
app_1    |     at Promise.resolve.retryParameters (/usr/local/rssmonster/server/node_modules/sequelize/lib/sequelize.js:464:64)
app_1    |     at /usr/local/rssmonster/server/node_modules/retry-as-promised/index.js:60:21
app_1    |     at new Promise (&lt;anonymous&gt;)
</code></pre>
","<p>I have the next docker-compose file:</p>

<pre><code>version: ""3""

services:
  proxy:
    build:
      context: ./client
      dockerfile: proxy.Dockerfile
    ports:
      - ""80:80""
  server:
    build:
      context: .
      dockerfile: server.Dockerfile
    ports:
      - ""3000:3000""
    depends_on: [db]
  db:
    image: postgres
    environment:
      POSTGRES_USER: lautidamo
      POSTGRES_PASSWORD: la12papa
      POSTGRES_DB: product-manager
    ports:
      - ""5432:5432""
</code></pre>

<p>And i have this database.ts:</p>

<pre><code>import { Sequelize } from ""sequelize"";

export const sequelize = new Sequelize(
  ""product-manager"",
  ""lautidamo"",
  ""la12papa"",
  {
    dialect: ""postgres"",
    host: ""localhost"",
    port: 5432
  }
);

</code></pre>

<p>The proxy and the server services works fine. But when I make a request to a route where have to connect to the database, i have this error connect ECONNREFUSED 127.0.0.1:5432.</p>
"
"54525301","Browsing context has been discarded using GeckoDriver Firefox through Selenium","<python><selenium><firefox><geckodriver><selenium-firefoxdriver>","59638517","Selenium crashing in Docker due to Browsing context has been discarded","<selenium><docker><selenium-webdriver><docker-compose>","<p>I didn't make any changes to my python selenium program and it worked fine 3 days ago.  Now when i try to use it i get:</p>

<p>Browsing context has been discarded
Failed to decode response from marionette</p>

<p>Any idea what could have caused this outside the code? (since no changes were made)</p>

<p>I'm using firefox and geckodriver. After i got these errors i updated firefox, geckodriver, and selenium, but it didn't help.</p>
","<p>How do you run Selenium based tests inside Docker?</p>

<p>I'm trying to get some Python+Selenium tests, which use Firefox and Geckodriver, to run under an Ubuntu 18 Docker image.</p>

<p>My docker-compose.yml file is simply:</p>

<pre><code>version: ""3.5""
services:
  app_test:
    build:
      context: .
      shm_size: '4gb'
      mem_limit: 4096MB
      dockerfile: Dockerfile.test
</code></pre>

<p>Unfortunately, most tests are failing with errors like:</p>

<pre><code>selenium.common.exceptions.NoSuchWindowException: Message: Browsing context has been discarded
</code></pre>

<p>The few search results I can find mentioning this error suggest it's because of low memory. The server I'm running the tests on has 8GB of total memory, although I also tested on a machine with 32GB and received the same error.</p>

<p>I also added a call to print the output of <code>top</code> before each test, and it's showing virtually no memory usage, so I'm not sure what would be causing the test to crash due to insufficient memory.</p>

<p>Some articles suggested adding the shm_size and mem_limit lines, but those had no effect.</p>

<p>I've also tried different versions of Firefox, from the most recent 71 version to the older ESR releases, to rule out it's not a bug due to incompatible versions of Firefox+Selenium+Geckodriver. I'm otherwise following <a href=""https://firefox-source-docs.mozilla.org/testing/geckodriver/Support.html"" rel=""nofollow noreferrer"">this compatibility table</a>.</p>

<p>What is causing this error and how do I fix it?</p>
"
"55241474","Why docker-compose creates directories/files with user:group 999:999?","<docker><docker-compose><docker-volume><file-ownership>","59747054","What does chown 999 do?","<docker><chown>","<p>I used <code>docker-compose up</code> with the following <code>docker-compose.yml</code></p>

<pre><code>version: '3.5'
services:
 mysql-server:
  image: mysql:5.7
  environment:
    - MYSQL_ROOT_PASSWORD=root_pwd
  volumes:
   - ./var/lib/mysql:/var/lib/mysql:rw
</code></pre>

<p>The directory <code>./var/lib/mysql</code> does not exist initially. </p>

<p>After running <code>docker-compose up</code> .. </p>

<p><code>ls -al ./var/lib/mysql</code> command shows all the files with <code>user:group</code> <code>999:999</code>. </p>

<p>I cannot find user or group called <code>999</code> in my system. </p>

<p><strong>Why</strong> <code>docker-compose</code> chooses to <strong>create files</strong> with a <strong>non-existing uid:gid</strong> ?</p>

<p>In my case, I cannot commit the specific directory unless I change ownership. But even when I do, on a next run, <code>docker-compose up</code> updates the ownership again to <code>999:999</code>.</p>

<p>What is the solution to the above problem? e.g. Is there a way to instruct docker-compose to map files in host machine with a specific uid:gid pair?</p>

<p>Host: ubuntu-18.04<br>
docker-compose: 1.22.0</p>
","<p>I know what <code>chmod</code> (change mode) and what <code>chown</code> (change owner) do.</p>

<p>I also understand what the numbers mean after <code>chmod</code> do.</p>

<p>I was going through a <a href=""https://youtu.be/-XzMfd4XQak?t=410"" rel=""nofollow noreferrer"">docker tutorial</a> and I saw <code>chown 999</code> being used.</p>

<p>What does it mean and do?</p>
"
"56059921","How can I have a host and container read/write the same files with Docker?","<javascript><python><linux><docker>","59539233","Set permission for volume data that mount to a container?","<apache><docker>","<p>I would like to volume mount a directory from a Docker container to my work station, so when I edit the content in the volume mount from my work station it updated in the container as well. It would be very useful for testing and develop web applications in general.</p>

<p>However I get a permission denied in the container, because the UID's in the container and host isn't the same. Isn't the original purpose of Docker that it should make development faster and easier?</p>

<p><a href=""https://stackoverflow.com/a/48536224/256439"">This answer</a> works around the issue I am facing when volume mounting a Docker container to my work station. But by doing this, I make changes to the container that I won't want in production, and that defeats the purpose of using Docker during development.</p>

<p>The container is <a href=""https://en.wikipedia.org/wiki/Alpine_Linux"" rel=""nofollow noreferrer"">Alpine Linux</a>, work station <a href=""https://en.wikipedia.org/wiki/Fedora_%28operating_system%29"" rel=""nofollow noreferrer"">Fedora</a> 29, and editor <a href=""https://en.wikipedia.org/wiki/Atom_(text_editor)"" rel=""nofollow noreferrer"">Atom</a>.</p>

<p><strong>Question</strong></p>

<p>Is there another way, so both my work station and container can read/write the same files?</p>
","<p>I want to run a apache container to run code from bind mount vloume (<code>./src:/var/www/html</code>) in host.
I got a permission denied, then I attach to container and change owner to <code>apache:apache</code>, it works.
But I want to change the owner or permission without attach to container when build stack.</p>
"
"56066235","How to pull from multiple private registries with docker-compose?","<docker><docker-compose><gitlab-ci>","59666283","How to pull docker image from private docker repository in docker-compose.yml?","<docker><docker-compose><dockerfile><docker-registry><docker-repository>","<p>I've attempted to pull two images from two different projects registries (gitlab container registry). All that in a <code>docker-compose.yml</code> file. </p>

<p>How can I configure my <code>gitlab-ci.yml</code> or configure variables (whatever works) in order to pull my images properly without any access problems ?</p>

<p>I have found a solution using <code>docker login</code> with a <em>deploy token</em> to have read-only access to my project registry. The problem is that works if I had only one image to pull : <a href=""https://stackoverflow.com/questions/50683869/how-to-build-push-and-pull-multiple-docker-containers-with-gitlab-ci/50684269?noredirect=1#comment88403258_50684269"">How to build, push and pull multiple docker containers with gitlab ci?</a></p>
","<p>How can I add the docker hub credentials in <code>docker-compose.yml</code> and pull the private image?</p>

<p>I want to pull 2 images from 2 different private repositories?</p>
"
"56206819","Run docker command on host","<docker><jenkins><docker-container>","59455954","Execute script located inside linked volume on host environment","<node.js><docker><docker-compose><docker-volume>","<p>Can I run docker command on host? I installed <code>aws</code> inside my docker container, now can I somehow use <code>aws</code> command on host (that under the hood will use docker container's aws)?</p>

<p>My situation is like that: I have database backups on production host. now I have Jenkins cron job that will take sql file from db container and take it into server folder. Now I also want jenkins to upload this backup file on AWS storage, but on host I have no aws installed, also I don't want to install anything except docker on my host, so I think aws should be installed inside container.</p>
","<p>I have some scripts (script1.sh, script2.sh) located inside my node based container (linked by volume).</p>

<p>Those scripts are located at /var/www/myproject/cmd (inside container).</p>

<p><strong>IMPORTANT</strong>: the scripts are located inside a linked folder</p>

<p>I triggered them with nodejs spawn function:</p>

<pre><code>spawn( './cmd/script1.sh', [ '.' ] )
</code></pre>

<p>Actually, they run inside container environment, but I want them to be executed inside the host environment. How can I achieve that?</p>

<p>my docker-compose.yml looks like this:</p>

<pre><code>version: '3'
    services:
    nodejs:
         image: my-node-image
         volumes:
            - ./cmd:/var/www/myproject/cmd
</code></pre>
"
"56340537","Docker, Springboot and Mysql : com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure","<mysql><spring><spring-boot><docker><jdbc>","59602209","docker-compose spring boot cannot connect to mysql","<java><mysql><spring-boot><docker><docker-compose>","<p>I try to run a Springboot app with mysql connection on Docker. Without docker, it works well. But when I try to deploy on a container, I have <code>com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</code>.
I use this docker-compose:</p>

<pre><code>version: '3'
services:
  nginx:
   container_name: nginx
   image: nginx:latest
   restart: always
   ports:
   - 80:80
   - 443:443
   volumes:
   - ./nginx/conf.d:/etc/nginx/conf.d
   depends_on:
   - spring-boot-app


  mysql:
   container_name: mysql
   image: mysql:latest
   environment:
    MYSQL_ROOT_PASSWORD: ""rootpass""
    MYSQL_DATABASE: ""db""
    MYSQL_USER: ""user""
    MYSQL_PASSWORD: ""pass""
   ports:
   - ""1306:3306""
   volumes:
   - ./data/:/var/lib/mysql
   networks: 
   - mysql-db
   restart: always

  spring-boot-app:
    container_name: spring-boot-app
    image: spring-boot-app
    build:
      context: ./spring-boot-app
      dockerfile: Dockerfile
    ports:
      - ""8080:8080""
    depends_on:
      - mysql
    networks: 
      - mysql-db
    restart: always

networks: 
  mysql-db:
      driver: bridge
</code></pre>

<p>Here is the Dockerfile for springboot app:</p>

<pre><code>FROM openjdk:11
COPY target/spring-boot-app.jar /spring-boot-app/spring-boot-app.jar
ENTRYPOINT [ ""java"", ""-jar"", ""-Dspring.profiles.active=prod"",""/spring-boot-app/spring-boot-app.jar"" ]
</code></pre>

<p>And finally, here is application.properties:</p>

<pre><code>spring.datasource.url = jdbc:mysql://mysql:1306/db
spring.datasource.username = user
spring.datasource.password = pass

# Show or not log for each sql query
spring.jpa.show-sql = true

# Hibernate ddl auto (create, create-drop, update)
spring.jpa.hibernate.ddl-auto = update

# Use spring.jpa.properties.* for Hibernate native properties (the prefix is
# stripped before adding them to the entity manager)

# The SQL dialect makes Hibernate generate better SQL for the chosen database
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5Dialect

server.port=8080
</code></pre>

<p>I don't think this is a port problem because i tried to set mysql to default port (3306), and it still doesn't work.</p>

<p>If it can help, here is <code>docker ps</code> :</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                  PORTS                                      NAMES
08ea16361c61        nginx:latest        ""nginx -g 'daemon of…""   1 second ago        Up Less than a second   0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp   nginx
52ef8d5c2127        spring-boot-app     ""java -jar -Dspring.…""   3 seconds ago       Up 1 second             0.0.0.0:8080-&gt;8080/tcp                     spring-boot-app
7bbcdc1fae3c        mysql:latest        ""docker-entrypoint.s…""   4 seconds ago       Up 3 seconds            33060/tcp, 0.0.0.0:1306-&gt;3306/tcp          mysql
</code></pre>

<p>Edit : full stacktrace :</p>

<pre><code>ubuntu@ubuntu:~/Documents/SpringBoot$ docker-compose up
Creating network ""springboot_mysql-db"" with driver ""bridge""
Creating mysql ... done
Creating spring-boot-app ... done
Attaching to mysql, spring-boot-app
mysql              | 2019-05-27T21:50:19.645920Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
mysql              | 2019-05-27T21:50:19.645986Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.16) starting as process 1
mysql              | 2019-05-27T21:50:20.451523Z 0 [System] [MY-010229] [Server] Starting crash recovery...
mysql              | 2019-05-27T21:50:20.466040Z 0 [System] [MY-010232] [Server] Crash recovery finished.
mysql              | 2019-05-27T21:50:20.544487Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
mysql              | 2019-05-27T21:50:20.551894Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
mysql              | 2019-05-27T21:50:20.584210Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.16'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
mysql              | 2019-05-27T21:50:20.682338Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: '/var/run/mysqld/mysqlx.sock' bind-address: '::' port: 33060
spring-boot-app    | 
spring-boot-app    |   .   ____          _            __ _ _
spring-boot-app    |  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
spring-boot-app    | ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
spring-boot-app    |  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
spring-boot-app    |   '  |____| .__|_| |_|_| |_\__, | / / / /
spring-boot-app    |  =========|_|==============|___/=/_/_/_/
spring-boot-app    |  :: Spring Boot ::        (v2.1.5.RELEASE)
spring-boot-app    | 
spring-boot-app    | 2019-05-27 21:50:22.884  INFO 1 --- [           main] com.jv.demo.DemoApplication              : Starting DemoApplication v0.0.1-SNAPSHOT on c69469fd7461 with PID 1 (/spring-boot-app/spring-boot-app.jar started by root in /)
spring-boot-app    | 2019-05-27 21:50:22.903  INFO 1 --- [           main] com.jv.demo.DemoApplication              : The following profiles are active: prod
spring-boot-app    | 2019-05-27 21:50:25.152  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data repositories in DEFAULT mode.
spring-boot-app    | 2019-05-27 21:50:25.378  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 205ms. Found 1 repository interfaces.
spring-boot-app    | 2019-05-27 21:50:26.313  INFO 1 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$e43b97d] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
spring-boot-app    | 2019-05-27 21:50:27.043  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
spring-boot-app    | 2019-05-27 21:50:27.195  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
spring-boot-app    | 2019-05-27 21:50:27.196  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.19]
spring-boot-app    | 2019-05-27 21:50:27.472  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
spring-boot-app    | 2019-05-27 21:50:27.472  INFO 1 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 4316 ms
spring-boot-app    | 2019-05-27 21:50:27.928  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
spring-boot-app    | 2019-05-27 21:50:29.226 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
spring-boot-app    | 
spring-boot-app    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
spring-boot-app    | 
spring-boot-app    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
spring-boot-app    |    at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:835) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:455) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:240) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.2.0.jar!/:na]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.fetchConnection(DataSourceUtils.java:157) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:115) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:78) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:319) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:356) ~[spring-jdbc-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.DatabaseLookup.getDatabase(DatabaseLookup.java:73) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.JpaProperties.determineDatabase(JpaProperties.java:142) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.JpaBaseConfiguration.jpaVendorAdapter(JpaBaseConfiguration.java:113) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec.CGLIB$jpaVendorAdapter$5(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec$$FastClassBySpringCGLIB$$cb57d9da.invoke(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:244) ~[spring-core-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:363) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration$$EnhancerBySpringCGLIB$$2d1a7bec.jpaVendorAdapter(&lt;generated&gt;) ~[spring-boot-autoconfigure-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
spring-boot-app    |    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:622) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:456) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:277) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1248) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:857) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:760) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:277) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1248) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:857) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:760) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1321) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1160) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.7.RELEASE.jar!/:5.1.7.RELEASE]
spring-boot-app    |    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) ~[spring-boot-2.1.5.RELEASE.jar!/:2.1.5.RELEASE]
spring-boot-app    |    at com.jv.demo.DemoApplication.main(DemoApplication.java:10) ~[classes!/:0.0.1-SNAPSHOT]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
spring-boot-app    |    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    |    at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[spring-boot-app.jar:0.0.1-SNAPSHOT]
spring-boot-app    | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
spring-boot-app    | 
spring-boot-app    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:na]
spring-boot-app    |    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:na]
spring-boot-app    |    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[na:na]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.NativeSession.connect(NativeSession.java:152) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:955) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:825) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    ... 84 common frames omitted
spring-boot-app    | Caused by: java.net.ConnectException: Connection refused (Connection refused)
spring-boot-app    |    at java.base/java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242) ~[na:na]
spring-boot-app    |    at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224) ~[na:na]
spring-boot-app    |    at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403) ~[na:na]
spring-boot-app    |    at java.base/java.net.Socket.connect(Socket.java:591) ~[na:na]
spring-boot-app    |    at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
spring-boot-app    |    ... 87 common frames omitted
spring-boot-app    | 
spring-boot-app    | 2019-05-27 21:50:29.234  WARN 1 --- [           main] o.s.b.a.orm.jpa.DatabaseLookup           : Unable to determine jdbc url from datasource
spring-boot-app    | 
spring-boot-app    | org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta-data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Failed to obtain JDBC Connection; nested exception is com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
</code></pre>
","<p>When I make the docker-compose file only contains MySQL  and I run the application from Intellij or from the .jar file it works fine, but when i'm trying to get it running using docker compose I get this error
<code>com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure</code></p>

<p>My dockerfile</p>

<pre><code>FROM adoptopenjdk/openjdk13
ADD ./build/libs/AppointmentsSystem-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar
</code></pre>

<p>my docker-compose.yml file</p>

<pre><code>version: ""3""
services:
  app:
    build:
      dockerfile: Dockerfile
      context: .
    ports:
      - 8081:8081
    depends_on:
      - mysql

  mysql:
    image: mysql:5.7
    volumes:
      - ./mysql_data:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: appointmentSystem
    ports:
      - 3308:3306
</code></pre>

<p>build.gradle</p>

<pre><code>plugins {
    id 'org.springframework.boot' version '2.2.1.RELEASE'
    id 'io.spring.dependency-management' version '1.0.8.RELEASE'
    id 'java'
}

group = 'com.sulimanLab'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '13'

configurations {

    compileOnly {
        extendsFrom annotationProcessor
    }
}

repositories {
    mavenCentral()
}

wrapper {
    gradleVersion = ""6.0""
}

dependencies {
    compile group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa'
    compile group: 'mysql', name: 'mysql-connector-java'
    compileOnly 'org.projectlombok:lombok'
    annotationProcessor 'org.projectlombok:lombok'
    testImplementation('org.springframework.boot:spring-boot-starter-test') {
        exclude group: 'org.junit.vintage', module: 'junit-vintage-engine'
    }
    testImplementation 'io.projectreactor:reactor-test'
}

test {
    useJUnitPlatform()
}

</code></pre>

<p>And application.yml</p>

<pre><code>spring:
  datasource:
    url: jdbc:mysql://localhost:3308/appointmentSystem
    username: root
    password: rootpassword
  jpa:
    database-platform: org.hibernate.dialect.MySQL5Dialect
    database: MYSQL
    hibernate:
      ddl-auto: update
server:
  port: 8081

</code></pre>

<p>The error </p>

<pre><code>PS C:\Users\user\Desktop\Github\AppointmentsSystem - Copy&gt; docker-compose.exe up
Creating network ""appointmentssystem-copy_default"" with the default driver
Creating appointmentssystem-copy_mysql_1 ... done                                                                                                                                                                                                                              Creating appointmentssystem-copy_app_1   ... done                                                                                                                                                                                                                              Attaching to appointmentssystem-copy_mysql_1, appointmentssystem-copy_app_1
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.28-1debian9 started.
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
mysql_1  | 2020-01-05 17:05:10+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.28-1debian9 started.
mysql_1  | 2020-01-05T17:05:11.307944Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
mysql_1  | 2020-01-05T17:05:11.313597Z 0 [Note] mysqld (mysqld 5.7.28) starting as process 1 ...
mysql_1  | 2020-01-05T17:05:11.321895Z 0 [Note] InnoDB: PUNCH HOLE support available
mysql_1  | 2020-01-05T17:05:11.321911Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mysql_1  | 2020-01-05T17:05:11.321914Z 0 [Note] InnoDB: Uses event mutexes
mysql_1  | 2020-01-05T17:05:11.321916Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
mysql_1  | 2020-01-05T17:05:11.321919Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
mysql_1  | 2020-01-05T17:05:11.321921Z 0 [Note] InnoDB: Using Linux native AIO
mysql_1  | 2020-01-05T17:05:11.322089Z 0 [Note] InnoDB: Number of pools: 1
mysql_1  | 2020-01-05T17:05:11.322160Z 0 [Note] InnoDB: Using CPU crc32 instructions
mysql_1  | 2020-01-05T17:05:11.323705Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
mysql_1  | 2020-01-05T17:05:11.349344Z 0 [Note] InnoDB: Completed initialization of buffer pool
mysql_1  | 2020-01-05T17:05:11.351768Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
mysql_1  | 2020-01-05T17:05:11.420065Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
mysql_1  | 2020-01-05T17:05:11.525183Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
mysql_1  | 2020-01-05T17:05:11.527734Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
mysql_1  | 2020-01-05T17:05:11.735100Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
mysql_1  | 2020-01-05T17:05:11.744572Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
mysql_1  | 2020-01-05T17:05:11.744744Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
mysql_1  | 2020-01-05T17:05:11.745330Z 0 [Note] InnoDB: Waiting for purge to start
mysql_1  | 2020-01-05T17:05:11.796296Z 0 [Note] InnoDB: 5.7.28 started; log sequence number 12442260
mysql_1  | 2020-01-05T17:05:11.796835Z 0 [Note] Plugin 'FEDERATED' is disabled.
mysql_1  | 2020-01-05T17:05:11.797225Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
mysql_1  | 2020-01-05T17:05:11.878505Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
mysql_1  | 2020-01-05T17:05:11.878712Z 0 [Note] Skipping generation of SSL certificates as certificate files are present in data directory.
mysql_1  | 2020-01-05T17:05:11.898316Z 0 [Warning] CA certificate ca.pem is self signed.
mysql_1  | 2020-01-05T17:05:11.900205Z 0 [Note] Skipping generation of RSA key pair as key files are present in data directory.
mysql_1  | 2020-01-05T17:05:11.910728Z 0 [Note] Server hostname (bind-address): '*'; port: 3306
mysql_1  | 2020-01-05T17:05:11.911146Z 0 [Note] IPv6 is available.
mysql_1  | 2020-01-05T17:05:11.911674Z 0 [Note]   - '::' resolves to '::';
mysql_1  | 2020-01-05T17:05:11.912095Z 0 [Note] Server socket created on IP: '::'.
mysql_1  | 2020-01-05T17:05:11.917607Z 0 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
mysql_1  | 2020-01-05T17:05:11.947376Z 0 [Note] InnoDB: Buffer pool(s) load completed at 200105 17:05:11
mysql_1  | 2020-01-05T17:05:12.631614Z 0 [Note] Event Scheduler: Loaded 0 events
mysql_1  | 2020-01-05T17:05:12.632095Z 0 [Note] mysqld: ready for connections.
mysql_1  | Version: '5.7.28'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)
app_1    |
app_1    |   .   ____          _            __ _ _
app_1    |  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
app_1    | ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
app_1    |  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
app_1    |   '  |____| .__|_| |_|_| |_\__, | / / / /
app_1    |  =========|_|==============|___/=/_/_/_/
app_1    |  :: Spring Boot ::        (v2.2.1.RELEASE)
app_1    |
app_1    | 2020-01-05 17:05:13.641  INFO 1 --- [           main] c.s.A.AppointmentsSystemApplication      : Starting AppointmentsSystemApplication on 185f7c998f26 with PID 1 (/app.jar started by root in /)
app_1    | 2020-01-05 17:05:13.660  INFO 1 --- [           main] c.s.A.AppointmentsSystemApplication      : No active profile set, falling back to default profiles: default
app_1    | 2020-01-05 17:05:14.985  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data repositories in DEFAULT mode.
app_1    | 2020-01-05 17:05:15.132  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 131ms. Found 5 repository interfaces.
app_1    | 2020-01-05 17:05:15.844  INFO 1 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
app_1    | 2020-01-05 17:05:16.390  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8081 (http)
app_1    | 2020-01-05 17:05:16.416  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
app_1    | 2020-01-05 17:05:16.417  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.27]
app_1    | 2020-01-05 17:05:16.533  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
app_1    | 2020-01-05 17:05:16.533  INFO 1 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2786 ms
app_1    | 2020-01-05 17:05:17.203  INFO 1 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
app_1    | 2020-01-05 17:05:17.317  INFO 1 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.4.8.Final}
app_1    | 2020-01-05 17:05:17.565  INFO 1 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.1.0.Final}
app_1    | 2020-01-05 17:05:17.807  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
app_1    | 2020-01-05 17:05:18.033 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
app_1    |
app_1    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    |      at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:456) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:353) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:201) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:473) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:562) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.4.1.jar!/:na]
app_1    |      at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:175) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:118) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1214) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1245) ~[hibernate-core-5.4.8.Final.jar!/:5.4.8.Final]
app_1    |      at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:378) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1108) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.1.RELEASE.jar!/:5.2.1.RELEASE]
app_1    |      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) ~[spring-boot-2.2.1.RELEASE.jar!/:2.2.1.RELEASE]
app_1    |      at com.sulimanLab.AppointmentsSystem.AppointmentsSystemApplication.main(AppointmentsSystemApplication.java:10) ~[classes!/:na]
app_1    |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Method.invoke(Method.java:567) ~[na:na]
app_1    |      at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) ~[app.jar:na]
app_1    |      at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) ~[app.jar:na]
app_1    | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    |      at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:na]
app_1    |      at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[na:na]
app_1    |      at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[na:na]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.NativeSession.connect(NativeSession.java:144) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      ... 57 common frames omitted
app_1    | Caused by: java.net.ConnectException: Connection refused
app_1    |      at java.base/sun.nio.ch.Net.pollConnect(Native Method) ~[na:na]
app_1    |      at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:579) ~[na:na]
app_1    |      at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) ~[na:na]
app_1    |      at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
app_1    |      at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:339) ~[na:na]
app_1    |      at java.base/java.net.Socket.connect(Socket.java:585) ~[na:na]
app_1    |      at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.18.jar!/:8.0.18]
app_1    |      ... 60 common frames omitted
app_1    |
app_1    | 2020-01-05 17:05:18.035  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : Communications link failure
app_1    |
app_1    | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app_1    | 2020-01-05 17:05:18.068  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
app_1    | 2020-01-05 17:05:19.454  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
app_1    | 2020-01-05 17:05:19.462 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
app_1    |
app_1    | com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure
</code></pre>
"
"56870478","Cannot start docker container In docker CE on oracle linux","<docker>","59891824","Docker build error OCI runtime create failed","<docker><docker-compose>","<p>Recently I installed Docker CE on my Oracle Linux.<br>
Unfortunately, when I want to start my first container with:</p>

<pre><code>docker run hello-world
</code></pre>

<p>I get this error message:</p>

<blockquote>
  <p>docker: Error response from daemon: OCI runtime create failed:
  container_linux.go:345: starting container process caused
  ""process_linux.go:430: container init caused \""write
  /proc/self/attr/keycreate: permission denied\"""": unknown. ERRO[0000]
  error waiting for container: context canceled</p>
</blockquote>
","<p>I am trying to build an agent on the linux machine using docker. Everything was working fine and i was able to create the agent. But suddenly it was showing offline. When i tried to recreate the agent it was throwing below error:-</p>

<pre><code>ERROR: for build-agent_dl-build-agent_1  Cannot start service dl-build-agent: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown

ERROR: for dl-build-agent  Cannot start service dl-build-agent: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown
ERROR: Encountered errors while bringing up the project.
</code></pre>

<p>even if i am trying to run a simple docker command  'docker run hello-world' i am getting below error,</p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""write /proc/self/attr/keycreate: permission denied\"""": unknown.
ERRO[0000] error waiting for container: context cancelled
</code></pre>

<p>Any suggestions?</p>
"
"58880998","Communications link failure , Spring Boot + MySql +Docker + Hibernate","<mysql><spring><spring-boot><docker><docker-compose>","60109913","Docker compose error on mysql db connect for spring boot app Caused by: java.net.UnknownHostException:","<spring-boot><docker><docker-compose>","<p>I'm working with spring boot, hibernate &amp; MySql. While running the application it is running well as per expectation . But while making the docker-compose file and running the app docker image with mysql docker image it gives this error.</p>

<blockquote>
  <p><strong>Error</strong>
  com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
  java.net.ConnectException: Connection refused.</p>
</blockquote>

<pre><code>private Connection createConnection() throws SQLException 
{
        DriverManager.registerDriver(new com.mysql.jdbc.Driver());
        String mysqlUrl = ""jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false"";
        Connection connection = DriverManager.getConnection(mysqlUrl, ""root"", ""root"");
        return connection;
}
</code></pre>

<blockquote>
  <p><strong>Application.properties</strong></p>
  
  <p>spring.datasource.url=jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false
  spring.datasource.username=root </p>
  
  <p>spring.datasource.password=root</p>
</blockquote>

<p>Please guide me how to tackle this.</p>

<pre><code>**docker-compose.yml**

version: '3'

services:
  docker-mysql:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=database
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
    ports:
      - 3307:3306

  app:
    image: app:latest
    ports:
       - 8091:8091
    depends_on:
       - docker-mysql
</code></pre>
","<p>I have spring boot app</p>

<p>My Dockerfile is</p>

<pre><code>FROM openjdk:8-jdk-alpine
EXPOSE 8080
ARG JAR_FILE=target/demo-0.0.1-SNAPSHOT.jar
ADD ${JAR_FILE} demo.jar
ENTRYPOINT [""java"",""-jar"",""/demo.jar""]


My docker compose file
# Docker Compose file Reference (https://docs.docker.com/compose/compose-file/)

version: '3.7'

# Define services
services:
  # App backend service
  app-server:
    # Configuration for building the docker image for the backend service
    build:
      context: . # Use an image built from the specified dockerfile in the `polling-app-server` directory.
      dockerfile: ./Dockerfile
    container_name: empserver
    ports:
      - ""3000:3000"" # Forward the exposed port 8080 on the container to port 8080 on the host machine
    restart: always
    depends_on: 
      - db # This service depends on mysql. Start that first.
    environment: # Pass environment variables to the service
      SPRING_DATASOURCE_URL: jdbc:mysql://db:3306/employee_entries?useSSL=false&amp;serverTimezone=UTC&amp;useLegacyDatetimeCode=false
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_PASSWORD: root     

  # Database Service (Mysql)
  db:
    image: mysql:5.7
    ports:
      - ""3306:3306""
    restart: always
    environment:
      MYSQL_DATABASE: employee_entries
      MYSQL_USER: root
      MYSQL_PASSWORD: root
      MYSQL_ROOT_PASSWORD: root
</code></pre>

<p>My docker net works</p>

<pre><code>NETWORK ID          NAME                DRIVER              SCOPE
b95e3d99b266        Default Switch      ics                 local
7fff4f9713f8        demo_default        nat                 local
fe8883b77d1d        emp-mysql           ics                 local
f464aab9064a        nat                 nat                 local
a5bd5e8efe61        none                null                local
</code></pre>

<p>App is successfully running using java -jar target\demo-0.0.1-SNAPSHOT.jar</p>

<p>but when I am doing docker-compose up</p>

<p>I got below error</p>

<pre><code>app-server_1  | Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
app-server_1  |
app-server_1  | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
app-server_1  |         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_212]
app-server_1  |         at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_212]
app-server_1  |         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_212]
app-server_1  |         at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_212]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.NativeSession.connect(NativeSession.java:144) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         ... 56 common frames omitted
app-server_1  | Caused by: java.net.UnknownHostException: db
app-server_1  |         at java.net.InetAddress.getAllByName0(InetAddress.java:1281) ~[na:1.8.0_212]
app-server_1  |         at java.net.InetAddress.getAllByName(InetAddress.java:1193) ~[na:1.8.0_212]
app-server_1  |         at java.net.InetAddress.getAllByName(InetAddress.java:1127) ~[na:1.8.0_212]
app-server_1  |         at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:132) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.19.jar!/:8.0.19]
app-server_1  |         ... 59 common frames omitted
</code></pre>

<p>I am able to access mysql database and tables but from docker compose it was not</p>

<p>any suggestion would really be helpful</p>
"
"58962674","Docker bound mount - can not see changes on browser","<docker><jekyll>","59405715","What is wrong with my Gulp Watch task with Docker?","<django><docker><sass><gulp>","<p>I'm using docker-toolbox on windows home.
I was able to run a jekyll-serve web server image to see the default page on browser, but when I try to edit file on VS Code, I can not see the changes after refreshing the browser.</p>

<p><strong>Any idea why I can not see the changes after refresh?</strong></p>

<hr>

<h2>Step to reproduce:</h2>

<p>First I've git cloned this <a href=""https://github.com/BretFisher/udemy-docker-mastery/tree/master/bindmount-sample-1"" rel=""nofollow noreferrer"">repository</a> into my <code>c:/Users/shaharshokrani/udemy-docker-mastery/bind-mount-sample1</code> (I'm able to see the files with <code>ls</code> on 'cmder' console).</p>

<p>Then I was able to run this <a href=""https://hub.docker.com/r/bretfisher/jekyll-serve"" rel=""nofollow noreferrer"">image</a> with:</p>

<p><code>docker run -v //c/users/shaharshokrani/udemy-docker-mastery/bindmount-sample-1:/site bretfisher/jekyll new .</code></p>

<p><code>docker container run -p 80:4000 --name myjekyll -v //c/users/shaharshokrani/udemy-docker-mastery/bindmount-sample-1:/site bretfisher/jekyll-serve</code></p>

<p>And I'm able to see the default welcome page on <code>http://192.168.99.100/</code>.</p>

<p>I've tried to edit and save using VS Code this <code>2017-03-05-welcome-to-jekyll.markdown</code> but I can not see the changes after refreshing the browser.</p>

<hr>

<p>I also checked the VM for shared network - it shows <code>c:/users/</code>.</p>

<p>Even the <code>Mounts</code> on inspect looks good:</p>

<pre><code>""Mounts"": [
    {
        ""Type"": ""bind"",
        ""Source"": ""/c/users/shaharshokrani/udemy-docker-mastery/bindmount-sample-1"",
        ""Destination"": ""/site"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": ""rprivate""
    }
],
</code></pre>

<p>And the image <a href=""https://hub.docker.com/r/bretfisher/jekyll-serve/dockerfile"" rel=""nofollow noreferrer"">dockerfile</a>'s <code>CMD</code> has the <code>--force_polling</code> flag.</p>

<p>Both the images (<code>bretfisher/jekyll-serve</code>, <code>bretfisher/jekyll</code>) are latest.</p>

<p>The <code>docker container logs -f myjekyll</code> looks good:</p>

<pre><code>Bundle complete! 4 Gemfile dependencies, 28 gems now installed.
Bundled gems are installed into `/usr/local/bundle`
Configuration file: /site/_config.yml
            Source: /site
       Destination: /site/_site
 Incremental build: disabled. Enable with --incremental
      Generating...
       Jekyll Feed: Generating feed for posts
                    done in 1.031 seconds.
 Auto-regeneration: enabled for '/site'
    Server address: http://0.0.0.0:4000/
  Server running... press ctrl-c to stop.
</code></pre>

<hr>

<p>Docker version:</p>

<pre><code>Client:
 Version:       18.03.0-ce
 API version:   1.37
 Go version:    go1.9.4
 Git commit:    0520e24302
 Built: Fri Mar 23 08:31:36 2018
 OS/Arch:       windows/amd64
 Experimental:  false
 Orchestrator:  swarm

Server: Docker Engine - Community
 Engine:
  Version:      19.03.3
  API version:  1.40 (minimum version 1.12)
  Go version:   go1.12.10
  Git commit:   a872fc2f86
  Built:        Tue Oct  8 01:01:20 2019
  OS/Arch:      linux/amd64
  Experimental: false
 containerd:
  Version:      v1.2.10
  GitCommit:            b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:      1.0.0-rc8+dev
  GitCommit:            3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:      0.18.0
  GitCommit:            fec3683
</code></pre>
","<p>EDIT: Gulp ""Watch"" doesn't work on windows with a <a href=""https://forums.docker.com/t/file-system-watch-does-not-work-with-mounted-volumes/12038/3"" rel=""nofollow noreferrer"">mounted volumes</a> because no ""file change"" event is sent. My current solution is to run <a href=""https://github.com/merofeev/docker-windows-volume-watcher"" rel=""nofollow noreferrer"">Docker Windows Volume Watcher</a> on my local machine while I see if I can integrate <a href=""https://stackoverflow.com/questions/58962674/docker-bound-mount-can-not-see-changes-on-browser/58969398#58969398"">this solution</a> into my code.</p>

<p>I'm trying to run a gulp watch task in my dockerfile and gulp isn't catching when my files are getting changed.</p>

<p>Quick Notes:</p>

<ul>
<li>This set up works when I use it for my locally hosted wordpress installs</li>
<li>The file changes reflect in my docker container according to pycharm's docker service</li>
<li>Running the ""styles"" gulp task works, it's just the file watching that does not</li>
</ul>

<p>It's clear to me that there's some sort of disconnect between how gulp watches for changes, and how Docker is letting that happen.</p>

<p><a href=""https://github.com/austincollinpena/dockerize-blog-two/tree/master/django_project"" rel=""nofollow noreferrer"">Github link</a></p>

<p>**Edit: It looks possible to do what I want, <a href=""https://github.com/divio/djangocms-boilerplate-sass/blob/master/gulpfile.js"" rel=""nofollow noreferrer"">here's a link to someone doing it slightly differently.</a></p>

<p><a href=""https://github.com/austincollinpena/dockerize-blog-two/blob/master/django_project/gulpfile.babel.js"" rel=""nofollow noreferrer"">gulpfile excerpt</a>:</p>

<pre><code>export const watchForChanges = () =&gt; {

  watch('scss-js/scss/**/*.scss', gulp.series('styles'));
  watch('scss-js/js/**/*.js', scripts);
  watch('scss-js/scss/*.scss', gulp.series('styles'));
  watch('scss-js/js/*.js', scripts);
  // Try absolute path to see if it works
  watch('scss-js/scss/bundle.scss', gulp.series('styles'));
}
...
// Compile SCSS through styles command
export const styles = () =&gt; {
    // Want more than one SCSS file? Just turn the below string into an array
  return src('scss-js/scss/bundle.scss')
      // If we're in dev, init sourcemaps. Any plugins below need to be compatible with sourcemaps.
    .pipe(gulpif(!PRODUCTION, sourcemaps.init()))
      // Throw errors
    .pipe(sass().on('error', sass.logError))
      // In production use auto-prefixer, fix general grid and flex issues.
    .pipe(
      gulpif(
        PRODUCTION,
        postcss([
          autoprefixer({
            grid: true
          }),
          require(""postcss-flexbugs-fixes""),
          require(""postcss-preset-env"")
        ])
      )
    )
    .pipe(gulpif(PRODUCTION, cleanCss({compatibility:'ie8'})))
      // In dev write source maps
    .pipe(gulpif(!PRODUCTION, sourcemaps.write()))
      // TODO: Update this source folder
    .pipe(dest('blog/static/blog/'))
    .pipe(server.stream());
}
...
export const dev = series(parallel(styles, scripts), watchForChanges);
</code></pre>

<p>Docker-Compose:</p>

<pre><code>version: ""3.7""

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - ""8002:8000""
      - ""3001:3001""
      - ""3000:3000""
    volumes:
      - ./django_project:/django_project
    command: &gt;
      sh -c ""python manage.py runserver 0.0.0.0:8000""
    restart: always
    depends_on:
      - db
  db:
    image: postgres
    environment:
      POSTGRES_PASSWORD: example1
    ports:
      - ""5432:5432""
    restart: always
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM python:3.8-buster
MAINTAINER Austin

ENV PYTHONUNBUFFERED 1

# Install node
RUN apt-get update &amp;&amp; apt-get -y install nodejs
RUN apt-get install npm -y

# Replace shell with bash so we can source files
RUN rm /bin/sh &amp;&amp; ln -s /bin/bash /bin/sh

# Update Node
# Install base dependencies
RUN apt-get update &amp;&amp; apt-get install -y -q --no-install-recommends \
        apt-transport-https \
        build-essential \
        ca-certificates \
        curl \
        git \
        libssl-dev \
        wget

ENV NVM_DIR /usr/local/nvm
ENV NODE_VERSION 12.14.0

WORKDIR $NVM_DIR

RUN curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash \
    &amp;&amp; . $NVM_DIR/nvm.sh \
    &amp;&amp; nvm install $NODE_VERSION \
    &amp;&amp; nvm alias default $NODE_VERSION \
    &amp;&amp; nvm use default

ENV NODE_PATH $NVM_DIR/versions/node/v$NODE_VERSION/lib/node_modules
ENV PATH      $NVM_DIR/versions/node/v$NODE_VERSION/bin:$PATH


# What PIP installs need to get done?
COPY django_project/requirements.txt /requirements.txt
RUN pip install -r /requirements.txt



# Copy local directory to target new docker directory
RUN mkdir -p /django_project
WORKDIR /django_project
COPY ./django_project /django_project


# Make Postgres Work
EXPOSE 5432/tcp


WORKDIR /django_project

RUN npm install gulp-cli -g
</code></pre>

<p>What do you think could be going on?</p>
"
"59384708","ansible returns with ""Failed to import the required Python library (Docker SDK for Python: docker (Python >= 2.7) or docker-py (Python 2.6))","<ansible><ansible-2.x><ansible-inventory><ansible-facts>","59382861","Cannot install python in a specific path and get No module named docker","<python><docker><ansible>","<p>I am running myserver in ubuntu:</p>

<pre><code>+ sudo cat /etc/os-release
NAME=""Ubuntu""
VERSION=""16.04.6 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.6 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
</code></pre>

<p>I use ansible and when I run it I get the following error:</p>

<pre><code>fatal: [localhost]: FAILED! =&gt; {""changed"": false, ""msg"": ""Failed to import the required Python library (Docker SDK for Python: docker (Python &gt;= 2.7) or docker-py (Python 2.6)) on dd63315fad06's Python /usr/bin/python. Please read module documentation and install in the appropriate location, for example via `pip install docker` or `pip install docker-py` (Python 2.6). The error was: No module named docker""}
</code></pre>

<p>when I run </p>

<pre><code>python -c ""import sys; print(sys.path)""
</code></pre>

<p>I see: </p>

<pre><code>['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/local/lib/python2.7/dist-packages/pip-19.2.2-py2.7.egg', '/usr/local/lib/python2.7/dist-packages/fasteners-0.15-py2.7.egg', '/usr/local/lib/python2.7/dist-packages/monotonic-1.5-py2.7.egg', '/usr/lib/python2.7/dist-packages']
</code></pre>

<p>and python versions are as follows:</p>

<pre><code>+ python --version
Python 2.7.12
+ python3 --version
Python 3.5.2
</code></pre>

<p>Then as I see everything is fine and I am not sure why I get </p>

<pre><code>""Failed to import the required Python library (Docker SDK for Python: docker (Python &gt;= 2.7) or docker-py (Python 2.6)) on dd63315fad06's Python /usr/bin/python. Please read module documentation and install in the appropriate location, for example via `pip install docker` or `pip install docker-py` (Python 2.6). The error was: No module named docker""
</code></pre>

<p>in ansible?</p>
","<p>I use ansible to build docker image and I get the following error:</p>

<pre><code>fatal: [localhost]: FAILED! =&gt; {""changed"": false, ""msg"": ""Failed to import the required Python library (Docker SDK for Python: docker (Python &gt;= 2.7) or docker-py (Python 2.6)) on dd63315fad06's Python /usr/bin/python. Please read module documentation and install in the appropriate location, for example via `pip install docker` or `pip install docker-py` (Python 2.6). The error was: No module named docker""}
</code></pre>

<p>when I do the <code>sudo python -c ""import sys; print(sys.path)""</code></p>

<p>I see this:</p>

<pre><code>['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/local/lib/python2.7/dist-packages/pip-19.2.2-py2.7.egg', '/usr/local/lib/python2.7/dist-packages/fasteners-0.15-py2.7.egg', '/usr/local/lib/python2.7/dist-packages/monotonic-1.5-py2.7.egg', '/usr/lib/python2.7/dist-packages']
</code></pre>

<p>So I do not see any path like this <strong>/usr/bin/python</strong> and I assume that python is not installed in the right folder.
So I decided to install python in <strong>/usr/bin/python</strong> and make it a default one:
Here is I have:</p>

<pre><code>sudo apt-get install -y python3-pip \
  &amp;&amp; cd /usr/bin \
  &amp;&amp; sudo ln -s /usr/bin/python3 python #\
</code></pre>

<p>but still I see that the python is not installed /usr/bin/python: </p>

<p>Here is what I see in for python:</p>

<pre><code>['', '/usr/lib/python35.zip', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-x86_64-linux-gnu', '/usr/lib/python3.5/lib-dynload', '/usr/local/lib/python3.5/dist-packages', '/usr/lib/python3/dist-packages']
</code></pre>

<p>The operating system I am using is:</p>

<p>which os:</p>

<pre><code>+ sudo cat /etc/os-release
NAME=""Ubuntu""
VERSION=""16.04.6 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.6 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
</code></pre>

<p>Any idea?</p>
"
"59472638","Docker-compose depends on not waiting until depended on service isn't fully started","<postgresql><docker><docker-compose>","59648655","dependency is not respected in docker","<docker><docker-compose>","<pre><code>version: '3'
services: 
server:
  container_name: hotel-server
  build: 
    dockerfile: Dockerfiles/server/Dockerfile
    context: .
  environment:
    dbhost: db
  links: 
    - db
  depends_on:
    - db
  restart: always
  ports: 
    - ""3456:3456""

db:
  image: ""custom-postgis:latest""
  container_name: hotelsdb
  environment:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: alliswell
    # POSTGRES_DB: hotels

  ports: 
    - ""5437:5432""
  volumes: 
    - hoteldata:/var/lib/postgresql/data #persistence
volumes: 
hoteldata: {}
</code></pre>

<p>The way I've set up the custom-postgis, its a custom postgres database container that has some initialization scripts which I have to wait until it starts. 
The problem is the server service starts before the db service fully starts.</p>

<p>Is there any workaround to that?</p>
","<p>I am dealing with a docker composer project. Here is the compose file :</p>

<pre><code>version: '3.3'
services:
  tomcatserver:
    build: ./mavenServer
    depends_on:
      - db
    ports:
      - ""8010:8080""
  db:
    image: mariadb
    environment:
      MYSQL_ALLOW_EMPTY_PASSWORD: ""true""
      MYSQL_ROOT_PASSWORD: ""root""
      MYSQL_DATABASE: ""tpformation""
      MYSQL_USER: ""tomcat""
      MYSQL_PASSWORD: ""tomcat"" 
    expose:
      - ""3306""
</code></pre>

<p>when I start my stack with docker-composer up, it is always the maven containers that starts first. However, it should be the db one. Could you help please.</p>
"
"60126842","Docker: The command returned a non-zero code: 137","<python><django><docker><pip><dockerfile>","60117479","How to run a python script based on pip installation in docker?","<python><django><docker><dockerfile>","<p>My docker file is as follows:</p>
<pre><code>#Use python 3.6 image
FROM python:3.6
ENV PYTHONUNBUFFERED 1

#install required packages
RUN apt-get update
RUN apt-get install libsasl2-dev libldap2-dev libssl-dev python3-dev psmisc -y

#install a pip package
#Note: This pip package has a completely configured django project in it
RUN pip install &lt;pip-package&gt;

#Run a script
#Note: Here appmanage.py is a file inside the pip installed location(site-packages), but it will be accessible directly without cd to the folder
RUN appmanage.py appconfig appadd.json

#The &lt;pip-packge&gt; installed comes with a built in django package, so running it with following CMD
#Note: Here manage.py is present inside the pip package folder but it is accesible directly
CMD [&quot;manage.py&quot;,&quot;runserver&quot;,&quot;0.0.0.0:8000&quot;]
</code></pre>
<p>When i run :</p>
<pre><code>sudo docker build -t test-app .
</code></pre>
<p>The steps in dockerfile till: <code>RUN appmanage.py appconfig </code> runs sucessfully as expected but after that i get the error:</p>
<pre><code>The command '/bin/sh -c appmanage.py appconfig ' returned a non-zero code: 137
</code></pre>
<p>When i google for the error i get suggestions like memory is not sufficient. But i have verified, the system(centos) is having enough memory.</p>
<h1>Additional info</h1>
<p>The commandline output during the execution of <code>RUN appmanage.py appconfig</code> is :</p>
<pre><code>Step 7/8 : RUN appmanage.py appconfig
 ---&gt; Running in 23cffaacc81f

======================================================================================
configuring katana apps...
 Please do not quit (or) kill the server manually, wait until the server closes itself...!
======================================================================================
Performing system checks...

System check identified no issues (0 silenced).
February 08, 2020 - 12:01:45
Django version 2.1.2, using settings 'katana.wui.settings'
Starting development server at http://127.0.0.1:9999/
Quit the server with CONTROL-C.
9999/tcp:
    20Killed
</code></pre>
","<p>My docker file design is as follows:</p>
<pre><code>#Use python 3.6 image
FROM python:3.6
ENV PYTHONUNBUFFERED 1

#install required packages
RUN apt-get update
RUN apt-get install libsasl2-dev libldap2-dev libssl-dev python3-dev psmisc -y

#install a pip package
#Note: This pip package has a completely configured django project in it
RUN pip install &lt;pip-packge&gt;

#add a configuration file required for setup
ADD appAdd.json /

#Run a script
#Note: Here appmanage.py is a file inside the pip installed location, but it will be accesible directly without cd to the folder
RUN appmanage.py appAdd.json

#The &lt;pip-packge&gt; installed comes with a built in django package, so running it with following CMD
#Note: Here manage.py is present inside the pip package folder but it is accesible directly
CMD [&quot;manage.py&quot;,&quot;runserver&quot;,&quot;0.0.0.0:8000&quot;]
</code></pre>
<p>When i run :</p>
<pre><code>sudo docker build -t test-app .
</code></pre>
<p>The python script running part will be successful in terms of functionality but the image is not getting created because at this point it gets exited with following error:</p>
<pre><code>The command '/bin/sh -c appmanage.py appAdd.json' returned a non-zero code: 137
</code></pre>
<p>Is it treating it as shell script rather than python script. How can i overcome this and run the django project sucessfully?</p>
<p>Note: In local environment I could execute the steps in my machine and successfully setup.So no issues with the code of the django project which comes with pip package</p>
<h1>Update</h1>
<p>The script appmanage.py runs the django project in a port 9999 and performs some tests and kills the port 9999. Is the kill operation in the script causing the error(137) as mentioned above?</p>
"
"670191","Getting a 'source: not found' error when using source in a bash script","<bash><virtualenv>","48870477","Alpine not loading /etc/profile","<docker><alpine>","<p>I'm trying to write (what I thought would be) a simple bash script that will:</p>

<ol>
<li>run virtualenv to create a new environment at $1</li>
<li>activate the virtual environment</li>
<li>do some more stuff (install django, add django-admin.py to the virtualenv's path, etc.)</li>
</ol>

<p>Step 1 works quite well, but I can't seem to activate the virtualenv. For those not familiar with virtualenv, it creates an <code>activate</code> file that activates the virtual environment. From the CLI, you run it using <code>source</code></p>

<pre><code>source $env_name/bin/activate
</code></pre>

<p>Where $env_name, obviously, is the name of the dir that the virtual env is installed in.</p>

<p>In my script, after creating the virtual environment, I store the path to the activate script like this:</p>

<pre><code>activate=""`pwd`/$ENV_NAME/bin/activate""
</code></pre>

<p>But when I call <code>source ""$activate""</code>, I get this:</p>

<pre><code>/home/clawlor/bin/scripts/djangoenv: 20: source: not found
</code></pre>

<p>I know that <code>$activate</code> contains the correct path to the activate script, in fact I even test that a file is there before I call <code>source</code>. But <code>source</code> itself can't seem to find it. I've also tried running all of the steps manually in the CLI, where everything works fine.</p>

<p>In my research I found <a href=""http://www.doughellmann.com/articles/CompletelyDifferent-2008-05-virtualenvwrapper/Listing1.html"" rel=""noreferrer"">this script</a>, which is similar to what I want but is also doing a lot of other things that I don't need, like storing all of the virtual environments in a ~/.virtualenv directory (or whatever is in $WORKON_HOME). But it seems to me that he is creating the path to <code>activate</code>, and calling <code>source ""$activate""</code> in basically the same way I am.</p>

<p>Here is the script in it's entirety:</p>

<pre><code>#!/bin/sh

PYTHON_PATH=~/bin/python-2.6.1/bin/python

if [ $# = 1 ]
then
    ENV_NAME=""$1""
    virtualenv -p $PYTHON_PATH --no-site-packages $ENV_NAME
    activate=""`pwd`/$ENV_NAME/bin/activate""

    if [ ! -f ""$activate"" ]
    then
        echo ""ERROR: activate not found at $activate""
        return 1
    fi

    source ""$activate""
else
    echo 'Usage: djangoenv ENV_NAME'
fi
</code></pre>

<p>DISCLAIMER: My bash script-fu is pretty weak. I'm fairly comfortable at the CLI, but there may well be some extremely stupid reason this isn't working.</p>
","<p>The latest alpine docker image doesn't load the /etc/profile when starting up. See example of when it starts without /etc/profile loaded and when I load it.</p>

<pre><code>[13:12]~$docker run --rm -it alpine
/ # . /etc/profile
ed2318ad9ac7:/#
</code></pre>

<p>How can I make it load the /etc/profile automatically upon start-up?</p>
"
"920413","Make error: missing separator","<makefile>","48535295","Makefile error No rule to make target when building docker image","<bash><docker><makefile>","<p>I am getting the following error running <code>make</code>:</p>

<pre><code>Makefile:168: *** missing separator.  Stop.
</code></pre>

<p>What is causing this?</p>
","<p>I'm trying to learn to write my own Makefiles for docker automation.
I've read a bit about this subject and than tried to copy some example code I found online but it gives me an error. Any ideas?</p>

<pre><code>$ make build
make: *** No rule to make target '-f', needed by 'build'.  Stop.

NS ?= test
VERSION ?= latest
IMAGE_NAME ?= test
CONTAINER_NAME ?= test
build:
    docker build -t $(NS)/$(IMAGE_NAME):$(VERSION) -f Dockerfile .

run:
    docker run -d --rm --name $(CONTAINER_NAME) $(NS)/$(IMAGE_NAME):$(latest)

default: build
</code></pre>
"
"7023052","Configure Flask dev server to be visible across the network","<python><flask><werkzeug>","48591664","I can't access docker from outside the container","<python><docker><networking><flask>","<p>I'm not sure if this is Flask specific, but when I run an app in dev mode (<code>http://localhost:5000</code>), I cannot access it from other machines on the network (with <code>http://[dev-host-ip]:5000</code>). With Rails in dev mode, for example, it works fine. I couldn't find any docs regarding the Flask dev server configuration. Any idea what should be configured to enable this?</p>
","<p>I have to ""dockerize"" a service I'm writing (a small Flask app). This is what I wrote:</p>

<pre><code>FROM python:3

RUN apt-get update &amp;&amp; apt-get install -y build-essential

WORKDIR /app

COPY requirements.txt /app
RUN pip install --no-cache-dir -r requirements.txt

COPY . /app

EXPOSE 5000

ENV FLASK_APP=app.py
ENV FLASK_DEBUG=1

CMD flask run
</code></pre>

<p>I can build the image and run the container with</p>

<p><code>docker run -p 5000:5000 &lt;container-name&gt;</code></p>

<p>and flask tells me it's listening on 127.0.0.1:5000. But I can't access the application from my host machine. I've also tried using the <code>localhost</code> and <code>0.0.0.0</code> as the address. However, if I <code>exec</code> into the container I can <code>curl</code> the address to receive the response I expect.</p>

<p>Does anyone know what's going on here?</p>
"
"10382929","How to fix java.lang.UnsupportedClassVersionError: Unsupported major.minor version","<java><jvm><incompatibility><unsupported-class-version>","65736400","class file version 54.0,Java Runtime only 52.0, while trying to run docker","<java><spring-boot><docker><maven><jbi>","<p>I am trying to use <a href=""http://en.wikipedia.org/wiki/Notepad%2B%2B"" rel=""noreferrer"">Notepad++</a> as my all-in-one tool edit, run, compile, etc.</p>

<p>I have <a href=""http://en.wikipedia.org/wiki/Java_Virtual_Machine#Execution_environment"" rel=""noreferrer"">JRE</a> installed, and I have setup my path variable to the <code>.../bin</code> directory.</p>

<p>When I run my ""Hello world"" in Notepad++, I get this message:</p>

<pre class=""lang-none prettyprint-override""><code>java.lang.UnsupportedClassVersionError: test_hello_world :
 Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(Unknown Source)
       .........................................
</code></pre>

<p>I think the problem here is about versions; some versions of Java may be old or too new.</p>

<ol>
<li>How do I fix it?</li>
<li>Should I install the JDK, and setup my path variable to the JDK instead of JRE?</li>
<li>What is the difference between the <code>PATH</code> variable in JRE or JDK?</li>
</ol>
","<p>Im trying to dockerize my application and when I try to run the docker run command, I receive this error. I tried a lot of things where people say that java and javac are different versions, but they are the same; What is the problem in the situation and how can I resolve it? I will apply picture of the error, so you can see it. Also do I need to go back to previous version of java and how to do this? Thanks in advance! :)</p>
<p><a href=""https://i.stack.imgur.com/4zDSV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4zDSV.png"" alt=""Error"" /></a></p>
"
"20708685","Handle keyboard interrupt, execute end block","<powershell>","48008260","How to automatically stop and remove Docker container on Ctrl+C in terminal/gitbash/powershell/etc.?","<docker>","<p>I have a powershell function that processes a list of files. I use the <code>begin</code>, <code>process</code> and <code>end</code> blocks for this:</p>

<pre><code>begin {
    # prepate some stuff
}
process {
    # process each file
}
end {
    # clean up
}
</code></pre>

<p>Now, when I hit <kbd>Ctrl+C</kbd>, the whole script just terminates right at the spot where it was. That’s not really a problem for the process part as that will only do permanent changes on the very last command.</p>

<p>I do however still want to execute what’s in the <code>end</code> block to clean it up a bit, and to print some statistics about the files that did manage to get processed.</p>

<p>Is there a clean way to catch keyboard interrupts, while keeping the begin/process/end structure?</p>
","<p>For example I run</p>

<pre><code>docker run --rm --name mycontainer -p 8080:8080 myrepo/myimage
</code></pre>

<p>then I see an output of my application, everything is OK. Then I press the <kbd>Ctrl</kbd>+<kbd>C</kbd> but the container is still running and I'm forced to explicitly stop and remove it: </p>

<pre><code>docker rm -f &lt;container_id&gt;
</code></pre>

<p>Or even worse:</p>

<pre><code>docker stop &lt;container_id&gt;
docker rm &lt;container_id&gt;
</code></pre>

<p>Is there any way to do it automatically? If not it's OK.</p>

<p>PS: What is the purpose of all that stopped containers still kept on the harddrive?!</p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","48716302","Docker Tomcat container error: Caused by: java.lang.NoClassDefFoundError-- when deployed in Kubernetes","<java><tomcat><docker><elasticsearch><kubernetes>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>I'm stumped on this one, i've spent the past ~3 days on it and done plenty of debugging/research attempts to no avail.</p>

<p>I have a docker container of Tomcat with a WAR deployed.  This container works fine when started (independently or in docker-compose).  However, when this container is deployed in a kubernetes cluster, Tomcat fails to deploy the WAR app.  Due to the following exception:</p>

<pre><code>Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'searchIndexServiceProxyEs' defined in ServletContext resource [/WEB-INF/index-clients_es_portal.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.LogManager
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)
        ... 100 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.LogManager
        at org.apache.log4j.Logger.getLogger(Logger.java:104)
        at org.elasticsearch.common.logging.log4j.Log4jESLoggerFactory.newInstance(Log4jESLoggerFactory.java:38)
        at org.elasticsearch.common.logging.ESLoggerFactory.newInstance(ESLoggerFactory.java:82)
        at org.elasticsearch.common.logging.ESLoggerFactory.getLogger(ESLoggerFactory.java:66)
        at org.elasticsearch.common.logging.Loggers.getLogger(Loggers.java:121)
        at org.elasticsearch.common.settings.Settings.&lt;clinit&gt;(Settings.java:63)
        at com.esri.gw.index.elasticsearch.portal.ElasticSearchClient.init(ElasticSearchClient.java:100)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeCustomInitMethod(AbstractAutowireCapableBeanFactory.java:1702)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1641)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570)
        ... 107 more
</code></pre>

<p>I confirmed that log4j jar exists in <code>WEB-INF/lib</code> and is loaded by adding <code>-verbose:class</code> to tomcat JAVA_OPTS, this is outputted in catalina startup just before the above error:</p>

<pre><code>[Loaded org.elasticsearch.common.logging.DeprecationLogger from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                           
[Loaded org.elasticsearch.common.logging.Loggers from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                                     
[Loaded org.elasticsearch.common.Classes from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                                             
[Loaded org.elasticsearch.common.logging.ESLoggerFactory from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar][Loaded org.elasticsearch.com
mon.logging.jdk.JdkESLoggerFactory from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                                                   
[Loaded org.elasticsearch.common.logging.log4j.Log4jESLoggerFactory from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                  
[Loaded org.elasticsearch.common.logging.slf4j.Slf4jESLoggerFactory from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                  
[Loaded org.elasticsearch.common.logging.ESLogger from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/elasticsearch-2.3.2.jar]                                    
[Loaded org.apache.log4j.spi.LoggingEvent from file:/home/root/apache-tomcat-7.0.72/lib/log4j-1.2.16.jar]                                                                                    
[Loaded org.springframework.core.NestedExceptionUtils from file:/home/root/apache-tomcat-7.0.72/webapps/app%23sharing/WEB-INF/lib/spring-core-4.1.6.RELEASE.jar]                          
[Loaded java.lang.Throwable$PrintStreamOrWriter from /home/root/java/jdk1.8.0_111/jre/lib/rt.jar]                                                                                            
[Loaded java.lang.Throwable$WrappedPrintWriter from /home/root/java/jdk1.8.0_111/jre/lib/rt.jar]                                                                                             
[Loaded java.util.IdentityHashMap$KeySet from /home/root/java/jdk1.8.0_111/jre/lib/rt.jar]        
</code></pre>

<p>How to resolve this? And why does it occur only when container is deployed to k8s v1.9 which is running docker v1.2?   </p>

<p>EDIT: this is not a duplicate of  <a href=""https://stackoverflow.com/questions/22907231/copying-files-from-host-to-docker-container"">Copying files from host to Docker container</a>                                                                                        </p>
"
"22907231","How to copy files from host to Docker container?","<docker><docker-container>","65792745","How to copy file from local to a directory in namenode Docker?","<docker>","<p>I am trying to build a backup and restore solution for the Docker containers that we work with.</p>

<p>I have Docker base image that I have created, <code>ubuntu:base</code>, and do not want have to rebuild it each time with a Docker file to add files to it.</p>

<p>I want to create a script that runs from the host machine and creates a new container using the <code>ubuntu:base</code> Docker image and then copies files into that container.</p>

<p>How can I copy files from the host to the container?</p>
","<p>I'm trying to copy a file from local to a directory in the namenode in <code>docker</code>. I tried this command : <code>docker cp ./ventes.txt ID:/~/direc1</code> but the file doesn't exists when i access the directory in the namenode bash</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","48908712","Linking Docker to local MySQL database","<mysql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Rather than having the MySQL database in Docker, or in a separate volume, I'd like Docker to connect to my pre-existing MySQL database I have running locally.</p>

<p><em>I'm using Laravel, hence the environment variables.</em></p>

<p>Can anyone help?</p>

<p>This is my docker-compose.yml file:</p>

<pre><code>version: '2'

services:
    web:
        image: nginx:latest
        ports:
            - ""8080:80""
        volumes:
            - .:/code
            - .docker/nginx.conf:/etc/nginx/conf.d/default.conf
    php:
        build:
            context: .
            dockerfile: .docker/Dockerfile
        image: php:7-fpm
        volumes:
            - .:/code
        environment:
            DB_CONNECTION: mysql
            DB_PORT: 3306
            DB_HOST: mysql
            DB_DATABASE: website_development
            DB_USERNAME: website_username
            DB_PASSWORD: website_pA$£w0r6
            REDIS_HOST: redis
            SESSION_DRIVER: redis
            CACHE_DRIVER: redis
    mysql:
        image: mysql:5.7
        ports:
            - 13306:3306
        environment:
            MYSQL_DATABASE: website_development
            MYSQL_USER: website_username
            MYSQL_PASSWORD: website_pA$£w0r6
            MYSQL_ROOT_PASSWORD: root_pA$£w0r6
    redis:
        image: redis:4.0-alpine
        ports:
            - 16379:6379
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65804473","How connect to localhost mysql base from docker in spring application","<java><mysql><spring-boot><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I try connecting spring boot application in docker with MySQL base on localhost</p>
<p>It's my application.properties, but the application cannot find the base</p>
<pre><code>server.port = 8080


spring.jpa.hibernate.ddl-auto=update
spring.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/TestDB?useUnicode=true&amp;serverTimezone=UTC&amp;useSSL=true&amp;verifyServerCertificate=false
spring.datasource.username=test
spring.datasource.password=test
</code></pre>
<p>I think the IP for MySQL in the container is different from the IP on the localhost</p>
<p>How I can connect to MySQL on localhost?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65822002","How to connect External mysql DB to Docker Web Container","<mysql><spring-boot><docker><containers>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am trying to access the MySQL database in the Docker container but is not allowing me to access it.</p>
<p>The Mysql database is running on my local machine.</p>
<p>I tried with <strong>docker run --network=&quot;127.0.0.1&quot;  -it -p 8080:8080</strong> but no luck</p>
<p>my docker file</p>
<pre><code>FROM tomcat:latest
ADD target/sample.war /usr/local/tomcat/webapps/
EXPOSE 8080
CMD [&quot;catalina.sh&quot;, &quot;run&quot;]
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65861021","connect ECONNREFUSED 127.0.0.1:3000 error using Docker container","<javascript><node.js><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a javascript file (client.js) that runs node-fetch to make GET requests to my server. My server is listening on port 3000. Whenever I create a Docker container to run the client file, I get the following error:</p>
<pre><code>(node:9) UnhandledPromiseRejectionWarning: FetchError: request to http://localhost:3000/selectItem/ failed, reason: connect ECONNREFUSED 127.0.0.1:3000
</code></pre>
<p>When I run the client file without any container, it works fine. I just want to have multiple container instances that run the client.js file.</p>
<p>I tried solutions like <a href=""https://stackoverflow.com/questions/14168433/node-js-error-connect-econnrefused"">Node.js Error: connect ECONNREFUSED</a>, but they didn't work.</p>
<p>My Dockerfile:</p>
<pre><code>FROM node:10

WORKDIR /client

COPY package*.json ./

RUN npm install

COPY . .

CMD node client
EXPOSE 3000
</code></pre>
<p>Thank you :)</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","48692293","Unable to connect to mysql host from docker container","<java><mysql><docker><intellij-idea>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I'm running application from docker container, that attempts to connect with mysql host</p>

<p>mysql host is localhost
and my docker container running on localhost:8080</p>

<p>and here's my config.yml file </p>

<pre><code>database:
  driverClass: com.mysql.cj.jdbc.Driver
  # TODO: if running locally without k8s, change the service url to your localhost
  url: jdbc:mysql://localhost:3306/locations?useSSL=false
  user: root
  password: root
  maxWaitForConnection: 1s
  validationQuery: ""SELECT now()""
  validationQueryTimeout: 3s
  minSize: 8
  maxSize: 32
  checkConnectionWhileIdle: false
  evictionInterval: 10s
  minIdleTime: 1 minute
  checkConnectionOnBorrow: true
</code></pre>

<p>When i tried to access my app it always gives me the exception </p>

<pre><code>ERROR [2018-02-08 18:16:56,508] org.apache.tomcat.jdbc.pool.ConnectionPool: Unable to create initial connections of pool.
! java.net.ConnectException: Connection refused (Connection refused)
</code></pre>

<p>I've tried many solution over google :D, but nothing works for me!</p>

<p>my java connector version:</p>

<p>compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.9-rc'</p>

<p>Can anybody help!
Also i'm running on ubuntu,</p>

<p>When i tried to run my app from the IDE, it works well, but from docker container gives the exception!</p>
"
"28182047","Is there a way to lint the Dockerfile?","<docker><dockerfile>","48498846","Check Syntax errors in Dockerfile","<docker><dockerfile>","<p>If a Dockerfile is written with mistakes for example:</p>

<p><code>CMD [""service"", ""--config"", ""/etc/service.conf]</code> (missing quote)</p>

<p>Is there a way to lint it to detect such mistake before building?</p>
","<p>As title says, is there any way we can check for syntax errors in <code>Dockerfile</code> ?</p>

<p>Thank you.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","48766096","Start docker with flask application using ssl","<python><docker><ssl><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I've got a flask application with SSL authorization. 
Here is my run.py:</p>

<pre><code>#!flask/bin/python
from app import app
import ssl
ctx = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
ctx.load_cert_chain('sertnew/se-emulator.crt', 'sertnew/se-emulator.key')
app.run(debug=True, host='127.0.0.1', port=5000, ssl_context=ctx)
</code></pre>

<p>On my machine, I run it simply with 
<code>python run.py</code>
Then I open <a href=""https://localhost:5000"" rel=""noreferrer"">https://localhost:5000</a> in chrome and it works (there is a message of non-secure connection, but it's ok for me) </p>

<p>Now I'm trying to make it work in Docker container.
I've got a Dockerfile like this:</p>

<pre><code>FROM python:3.5-slim
RUN apt-get update &amp;&amp; apt-get install -y python3-pip
COPY . /storage-emulator
WORKDIR /storage-emulator
RUN pip3 install -r requirements.txt
EXPOSE 5000
ENTRYPOINT [""python""]
CMD [""run.py""]
</code></pre>

<p>and try to run it in different ways. 
I can see ""Running on <a href=""https://127.0.0.1:5000/"" rel=""noreferrer"">https://127.0.0.1:5000/</a> (Press CTRL+C to quit)"" message, but can't open the page in the browser. What am I doing wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65698602","Can't connect to docker container on localhost","<docker>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I am trying to put a very simple Python API that returns some text and logs the timestamps of sent requests and what not. And if ran on the machine directly, it works fine, but when I try to use it in a Docker, I get no connection of any kind while trying to connect to 127.0.0.1 or localhost.</p>
<p>Here's the Dockerfile</p>
<pre><code>FROM alpine
FROM python
FROM tiangolo/uwsgi-nginx-flask:python3.8
LABEL MAINTAINER=&quot;Asd &lt;asd@asd.com&quot;
COPY ./app /app
EXPOSE 5000


ENTRYPOINT [&quot;python3&quot;, &quot;/app/main.py&quot;]
</code></pre>
<p>I run the container with</p>
<pre><code>docker run --name unicorns -p 127.0.0.1:5000:5000 unicorn-image:latest
</code></pre>
<p>Thanks in Advance</p>
<p>EDIT1: Python code</p>
<pre><code># coding=utf8
import flask,json,sys,socket

import logging
log = logging.getLogger('werkzeug')
log.setLevel(logging.INFO)
logging.basicConfig(filename='./demo.log', level=logging.INFO)

from datetime import datetime


app = flask.Flask(__name__)
app.config[&quot;DEBUG&quot;] = True

@app.route('/', methods=['GET'])


def home():
    test_date = datetime.now()
    app.logger.info('%s', json.dumps(test_date, indent=4, sort_keys=True, default=str))
    return &quot;&lt;h1&gt;You are a unicorn!&lt;/h1&gt;&quot;

if __name__ == '__main__':
    app.run()
</code></pre>
"
"31112579","Why does bash -c ""false; echo $?"" print 0?","<bash><unix>","48289127","Unable to capture the exit code of `/bin/sh`","<bash><shell><docker><sh><busybox>","<p>I'm building a script that tries to run some commands on a server (over SSH), and writes on the screen whether they were successful.</p>

<p>I noticed a strange behaviour for <code>$?</code>, namely not being <code>0</code> when the previous command failed.</p>

<p>Initially, I had:</p>

<pre><code>ssh &lt;user&gt;@&lt;server&gt; &lt;&lt;EOF
    false
    if [ $? -ne 0 ]; then
        echo ""It failed""
    else
        echo ""It worked""
    fi
EOF
</code></pre>

<p>If I copy and paste the script inside <code>&lt;&lt;EOF</code> and <code>EOF</code>, it prints <code>It failed</code>. If I run it with the <code>ssh</code> part, it prints <code>It worked</code>. To simplify, I then tried:</p>

<pre><code>ssh &lt;user&gt;@&lt;server&gt; &lt;&lt;EOF
    false
    echo $?
EOF
</code></pre>

<p>Same thing happened. If I copy-paste or type the commands inside, it prints <code>1</code>, but if I run all of it (including the <code>ssh</code>), it prints <code>0</code>.</p>

<p>The same error happens if I directly use bash this way</p>

<pre><code>bash &lt;&lt;EOF
    false
    echo $?
EOF
</code></pre>

<p>or</p>

<pre><code>bash -c ""false; echo $?""
</code></pre>

<p>Why does this happen? How can I check if the previous command failed in this context?</p>
","<p>I am using <code>/bin/bash</code> as the outermost shell, and I'm using the default <code>/bin/sh</code> from <code>busybox</code>.</p>

<p>Below, I am able to capture the exit code as I would expect:</p>

<pre><code>$ docker run -it --rm busybox /bin/sh -c ""bin/sh -c \""cat /foo/bar\"" &amp;&amp; echo Shell Result: $?"" || echo Docker Result: $?
cat: can't open '/foo/bar': No such file or directory
Docker Result: 1
</code></pre>

<p>Clearly <code>cat /foo/bar</code> has failed, which is how we enter the statement on the other side of <code>||</code>. However, when I attempted to echo the result the value of <code>$?</code> is <code>0</code> (shown below):</p>

<pre><code>$ docker run -it --rm busybox /bin/sh -c ""bin/sh -c \""cat /foo/bar\"" || echo Shell Result: $?"" || echo Docker Result: $?
cat: can't open '/foo/bar': No such file or directory
Shell Result: 0
</code></pre>

<p>Where is the result stored, or how can I capture it?</p>
"
"34178272","how to pass a configuration file thought yaml on kubernetes to create new replication controller","<kubernetes>","65769299","Passing file reference into yaml file for k8","<docker><kubernetes><yaml>","<p>i am trying to pass a configuration file(which is located on master) on nginx container at the time of replication controller creation through kubernetes..  ex. as we are using ADD command in Dockerfile...</p>
","<p>I'm writing a yaml file to create a pod.
And i have to pass multiple json files into the docker container.</p>
<p>How can i pass these files into the yaml file dynamically?</p>
"
"40454470","How can I use a variable inside a Dockerfile CMD?","<docker><dockerfile>","65796030","CMD doesn't run in Dockerfile","<docker><dockerfile>","<p>Inside my Dockerfile:</p>

<pre><code>ENV PROJECTNAME mytestwebsite
CMD [""django-admin"", ""startproject"", ""$PROJECTNAME""]
</code></pre>

<p>Error:</p>

<pre><code>CommandError: '$PROJECTNAME' is not a valid project name
</code></pre>

<p>What is the quickest workaround here?  Does Docker have any plan to ""fix"" or introduce this functionality in later versions of Docker?</p>

<p>NOTE:  If I remove the CMD line from the Docker file and then run the Docker container, I am able to manually run Django-admin startproject $PROJECTNAME from inside the container and it will create the project...</p>
","<p>My Dockerfile contains the following..</p>
<pre><code>ENV APP_HOME=/app/foo-service
CMD [$APP_HOME/start-foo-service.sh]
</code></pre>
<p>What is wrong in the above syntax, since the following runs fine</p>
<pre><code>CMD [&quot;/app/foo-service/start-foo-service.sh&quot;]
</code></pre>
"
"40513545","How to prevent docker from starting a container automatically on system startup?","<docker><startup>","65746709","How do I disable all docker-compose automatic restarts on boot","<docker><docker-compose>","<p>Docker starts a container on every system startup (debian) but I didn't create a service to do so. How can I prevent docker from doing that?</p>
","<p>I have an old laptop that has a few docker-compose projects with restart=unless-stopped. One of them is possibly causing the laptop to crash on start. The laptop then crashes before I have a chance to stop that container.</p>
<p>I can mount the underlying linux system by booting from a USB and so can access the docker-compose.yml files.</p>
<p>How can I disable the restart policy (possibly once off) so when I boot the laptop, it doesn't start those docker-compose containers?</p>
<p>Can this be done without editting the docker-compose.yml files?</p>
"
"40746505","How to fix the ""Found Netty's native epoll transport in the classpath, but epoll is not available. Using NIO instead"" warning?","<java><cassandra><netty>","48337573","Netty so loading exception when run webflux in a docker container","<docker><spring-webflux><reactor-netty>","<p>I am using Cassandra and, during startup, Netty prints a warning with a stack trace:</p>

<blockquote>
  <p>Found Netty's native epoll transport in the classpath, but epoll is not available. Using NIO instead.""</p>
</blockquote>

<p>The application works normally, but is there a way to fix the warning? </p>

<p>Here is the full stack trace:</p>

<pre><code>16:29:46 WARN  com.datastax.driver.core.NettyUtil - Found Netty's native epoll transport in the classpath, but epoll is not available. Using NIO instead.
java.lang.UnsatisfiedLinkError: no netty-transport-native-epoll in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:168)
    at io.netty.channel.epoll.Native.&lt;clinit&gt;(Native.java:49)
    at io.netty.channel.epoll.Epoll.&lt;clinit&gt;(Epoll.java:30)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at com.datastax.driver.core.NettyUtil.&lt;clinit&gt;(NettyUtil.java:68)
    at com.datastax.driver.core.NettyOptions.eventLoopGroup(NettyOptions.java:101)
    at com.datastax.driver.core.Connection$Factory.&lt;init&gt;(Connection.java:709)
    at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1386)
    at com.datastax.driver.core.Cluster.init(Cluster.java:162)
    at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:341)
    at com.datastax.driver.core.Cluster.connect(Cluster.java:286)
    at org.springframework.cassandra.config.CassandraCqlSessionFactoryBean.connect(CassandraCqlSessionFactoryBean.java:100)
    at org.springframework.cassandra.config.CassandraCqlSessionFactoryBean.afterPropertiesSet(CassandraCqlSessionFactoryBean.java:94)
    at org.springframework.data.cassandra.config.CassandraSessionFactoryBean.afterPropertiesSet(CassandraSessionFactoryBean.java:60)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1642)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1579)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:207)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1128)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1056)
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:835)
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:741)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:467)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1128)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1023)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:510)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)
    at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:108)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1486)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1231)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:543)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:207)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1128)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1056)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:566)
    at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:88)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:349)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1219)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:543)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:751)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:861)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:541)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:761)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:371)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1186)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1175)
    ...
</code></pre>
","<p>The application works well in local system, but I when ran it in docker, failed.</p>

<p>The dockerfile is <a href=""https://github.com/hantsy/spring-reactive-microservice-sample/blob/master/auth-service/Dockerfile"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The following error in console when starts this service in Docker.</p>

<pre><code>auth-service        |           ... 185 common frames omitted
auth-service        |   Caused by: java.io.FileNotFoundException: META-INF/native/libnetty_transport_native_epoll.so
auth-service        |           at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:161)
auth-service        |           ... 186 common frames omitted
auth-service        |           Suppressed: java.lang.UnsatisfiedLinkError: no netty_transport_native_epoll in java.library.path
auth-service        |                   at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
auth-service        |                   at java.lang.Runtime.loadLibrary0(Runtime.java:870)
auth-service        |                   at java.lang.System.loadLibrary(System.java:1122)
auth-service        |                   at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
auth-service        |                   at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:243)
auth-service        |                   at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:124)
auth-service        |                   ... 186 common frames omitted
auth-service        |                   Suppressed: java.lang.UnsatisfiedLinkError: no netty_transport_native_epoll in java.library.path
auth-service        |                           at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
auth-service        |                           at java.lang.Runtime.loadLibrary0(Runtime.java:870)
auth-service        |                           at java.lang.System.loadLibrary(System.java:1122)
auth-service        |                           at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
auth-service        |                           at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
auth-service        |                           at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
auth-service        |                           at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</code></pre>
"
"41593135","Does putting ARG at top of Dockerfile prevent layer re-use?","<docker><dockerfile>","65813457","Use env with arg in Dockerfile cause cache layer invalidate","<docker><dockerfile>","<p>If an ARG that is declared at the top of a Dockerfile gets changed, but its value is only used for a RUN command near the end of the Dockerfile, does Docker rebuild the whole image from scratch or is it able to re-use the intermediate image from right before the relevant RUN command?</p>

<p>To better utilize layering, should I place my ARG declarations at the top of the Dockerfile, or just before the section that uses them?</p>

<p>I guess part of my question is whether or not an ARG directive generates an intermediate layer.</p>
","<p><strong>Why the cache layer of the <code>RUN</code> command is invalidate?</strong></p>
<pre><code>FROM python:3.7-slim
ARG VERSION
RUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt
ENV VERSION_2 2
ENV VERSION $VERSION
CMD [&quot;env&quot;]
</code></pre>
<p>As shown in the dockerfile above, using build arg to dynamically set environment variables will cause the cache layer of the <code>RUN</code> instruction to become invalid, because build-arg will change every time it is built, and the history of the image is as follows</p>
<pre><code>IMAGE          CREATED         CREATED BY                                      SIZE      COMMENT
13d8c1c91aed   2 minutes ago   CMD [&quot;env&quot;]                                     0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ENV VERSION=1                                   0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ENV VERSION_2=2                                 0B        buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   RUN |1 VERSION=1 /bin/sh -c pip install --up…   5.28MB    buildkit.dockerfile.v0
&lt;missing&gt;      2 minutes ago   ARG VERSION                                     0B        buildkit.dockerfile.v0
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  CMD [&quot;python3&quot;]              0B        
&lt;missing&gt;      7 days ago      /bin/sh -c set -ex;   savedAptMark=&quot;$(apt-ma…   8.49MB    
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_SHA256…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_URL=ht…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_PIP_VERSION=20…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c cd /usr/local/bin  &amp;&amp; ln -s idle3…   32B       
&lt;missing&gt;      7 days ago      /bin/sh -c set -ex   &amp;&amp; savedAptMark=&quot;$(apt-…   27.1MB    
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV PYTHON_VERSION=3.7.9     0B        
&lt;missing&gt;      7 days ago      /bin/sh -c #(nop)  ENV GPG_KEY=0D96DF4D4110E…   0B        
&lt;missing&gt;      7 days ago      /bin/sh -c set -eux;  apt-get update;  apt-g…   7.03MB    
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  ENV PATH=/usr/local/bin:/…   0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop)  CMD [&quot;bash&quot;]                 0B        
&lt;missing&gt;      8 days ago      /bin/sh -c #(nop) ADD file:422aca8901ae3d869…   69.2MB
</code></pre>
"
"42345235","How to specify Memory & CPU limit in docker compose version 3","<docker><docker-compose>","65850918","Add cpu limit via docker-compose","<docker><docker-compose>","<p>I am unable to specify CPU &amp; memory for services specified in version 3 .</p>

<p>With version 2 it works fine with ""mem_limit"" &amp; ""cpu_shares"" parameters under the services . But it fails while using version 3 , putting them under deploy section doesn't seem worthy unless i am using swarm mode .</p>

<p>Can somebody help ?</p>

<pre><code>version: ""3""
services:
  node:
    build:
     context: .
      dockerfile: ./docker-build/Dockerfile.node
    restart: always
    environment:
      - VIRTUAL_HOST=localhost
    volumes:
      - logs:/app/out/
    expose:
      - 8083
    command: [""npm"",""start""]
    cap_drop:
      - NET_ADMIN
      - SYS_ADMIN
</code></pre>
","<p>I found several posts and blogs on this but cannot get it to work as expected in my case.</p>
<p>I have a docker compose and I tried to add cpu limits per <a href=""https://www.baeldung.com/ops/docker-memory-limit"" rel=""nofollow noreferrer"">this</a> blog. Tried:</p>
<pre><code>version: &quot;3&quot;
services:
  shell:
    image: someaddress/executor-blah/main:latest
    cpus: 4
    entrypoint:
      - /bin/bash
    working_dir: /workspace
    volumes:
      - .:/workspace
</code></pre>
<p>However when I tried to run this container I got a compose error:</p>
<blockquote>
<p>The Compose file './docker-compose.yaml' is invalid because:
Unsupported config option for services.shell: 'cpus'</p>
</blockquote>
<p>Tried also:</p>
<pre><code>version: &quot;3&quot;
services:
  shell:
    image: someaddress/executor-blah/main:latest
    cpus: 4
    entrypoint:
      - /bin/bash
    working_dir: /workspace
    volumes:
      - .:/workspace
    deploy:
      resources:
        limits:
          cpus: '4'
</code></pre>
<p>This time the container composed but when I check on a async loop in my hosts terminal using <code>top &gt; 1</code>, all 8 of my laptops cores are used up by this process.</p>
<p>How can I limit a container to, in this case, 4 cores?</p>
"
"43099116","Error ""The input device is not a TTY""","<docker><jenkins><jenkins-pipeline>","48623005","docker error : the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'","<windows><git><docker><github><tty>","<p>I am running the following command from my <code>Jenkinsfile</code>. However, I get the error <em>""The input device is not a TTY""</em>.</p>

<pre><code>docker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh
</code></pre>

<p>Is there a way to run the script from the <code>Jenkinsfile</code> without doing interactive mode?</p>

<p>I basically have a file called <code>script.sh</code> that I would like to run inside the Docker container.</p>
","<p>After I run this<br>
 <code>$ docker run --rm -v ""/c/users/vipul rao/documents/github/wappalyzer:/opt/wappalyzer"" -it wappalyzer/dev</code></p>

<p>I am getting the following error </p>

<blockquote>
  <p>the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'</p>
</blockquote>

<p>What should I use here? I am running on Windows 8 in MINGW64.</p>
"
"44077815","dotnet docker /bin/sh: 1: [dotnet,: not found","<docker><.net-core><dockerfile>","48192444","/bin/sh: 1: [“npm”,: not found on docker-compose up","<node.js><docker><docker-compose><dockerfile>","<p>I build successfully with dockerfile, but when I try to run a new container from the build image, I get the following error:</p>

<p>What do i have to do for the solution?</p>

<blockquote>
  <p>Error : /bin/sh: 1: [dotnet,: not found</p>
</blockquote>

<h3>docker run command:</h3>

<pre><code>docker run --rm -it -p 8080:80 furkaandogan/myapp:0.1
</code></pre>

<h3>Dockerfile:</h3>

<pre><code>FROM microsoft/aspnetcore:1.1
ARG source
WORKDIR /app
ENV ASPNETCORE_URLS http://*:80
EXPOSE 80
COPY ./publish/myapp .
ENTRYPOINT [""dotnet"", ""myapp.dll""]
</code></pre>
","<p>I'm new to docker and trying to create a container for node apps.</p>

<p>I followed these <a href=""https://medium.com/statuscode/dockerising-a-node-js-and-mongodb-app-d22047e2806f"" rel=""noreferrer"">tutorial</a>, but on <code>docker-compose up</code> I'm getting always these error:</p>

<pre><code>Creating app ... done
Attaching to app
app    | /bin/sh: 1: [“npm”,: not found
app exited with code 127
</code></pre>

<p>Here is my Dockerfile:</p>

<pre><code>FROM node:latest
RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app
COPY package.json /usr/src/app/
COPY package-lock.json /usr/src/app/
RUN npm install
COPY . /usr/src/app
EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>and my docker-compose.yml:</p>

<pre><code>version: ""2""
services:
  app:
    container_name: app
    restart: always
    build: .
    ports:
      - ""3000:3000""
</code></pre>

<p>Has anyone an idea how to fix this error? </p>
"
"45223031","Powershell Call MSI with Arguments","<powershell>","48599026","Pass PowerShell variables to Docker commands","<powershell><docker><containers>","<p>I'm working on a powershell script to install Autodesk products and I am having some trouble. </p>

<p>I've tried this many different ways and keep running into errors.</p>

<p>Using double quotes</p>

<pre><code>(Start-Process ""msiexec.exe"" -ArgumentList """"/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR=""C:\Program Files\Autodesk\"" ADSK_SETUP_EXE=1 /qb!"""" -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>Error Cannot validate argument on parameter 'ArgumentList' Argument is null or empty.</p>

<p>Using a variable to define InstallDir</p>

<pre><code>$RevitInstallDir = ""C:\Program Files\Autodesk\""
    (Start-Process ""msiexec.exe"" -ArgumentList ""/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR=$RevitInstallDir ADSK_SETUP_EXE=1 /qb!"" -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>Doing this I get the msiexec /option Required Parameter error.</p>

<p>Tried this also, Single Quotes with Quotes on Path</p>

<pre><code>(Start-Process ""msiexec.exe"" -ArgumentList ""/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR=""C:\Program Files\Autodesk\"" ADSK_SETUP_EXE=1 /qb!"" -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>I receive A positional parameter cannot be found that accepts argument C:\Program</p>

<p>Using single quotes on InstallDir</p>

<pre><code>(Start-Process ""msiexec.exe"" -ArgumentList ""/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR='C:\Program Files\Autodesk\' ADSK_SETUP_EXE=1 /qb!"" -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>Doing this I get the msiexec /option Required Parameter error.</p>

<p>Using single quotes on the outside</p>

<pre><code>(Start-Process ""msiexec.exe"" -ArgumentList '/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR=""C:\Program Files\Autodesk\"" ADSK_SETUP_EXE=1 /qb!' -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>If I do this, it prevents the $dirFiles variable from working. </p>

<p>The reason I'm using Start-Process is because I have many installers one after the other and I want one installer to wait till the one before it finishes. Any help would be appreciated! Thanks</p>

<p>EDIT: Nevermind, I figured it out. </p>

<pre><code>(Start-Process ""msiexec.exe"" -ArgumentList ""/i $dirFiles\ABDS2017\Img\x64\RVT\RVT.msi INSTALLDIR=""""C:\Program Files\Autodesk\"""" ADSK_SETUP_EXE=1 /qb!"" -NoNewWindow -Wait -PassThru).ExitCode
</code></pre>

<p>Double Quotes around the Install Dir</p>

<p>Got the idea from here. <a href=""https://blogs.technet.microsoft.com/heyscriptingguy/2015/06/20/weekend-scripter-understanding-quotation-marks-in-powershell/"" rel=""nofollow noreferrer"">https://blogs.technet.microsoft.com/heyscriptingguy/2015/06/20/weekend-scripter-understanding-quotation-marks-in-powershell/</a></p>

<p>Thanks</p>
","<p>I'd like to script the management of Docker containers, but I find it difficult to pass PS variables to Docker commands, in particular due to  path format differences.</p>

<p>The following line (*) and the likes you can find  <a href=""https://stackoverflow.com/questions/45223031/powershell-call-msi-with-arguments"">here</a> do work, however they are inconvenient:</p>

<pre><code>Start-Process docker "" run --rm -v $($env:USERPROFILE -replace '\\','/'):/data alpine ls /data""
</code></pre>

<p>Indeed, PS <code>Start-Process</code> is perfect for an MSI installer where you need to see the popup and to control its level of visibility so as to understand that a silent installer is going. Instead, you don't want to start a new window each time you run a console app and particularly in Docker, where you interact back and forth with caller and called shell.</p>

<p>To <a href=""https://docs.microsoft.com/en-gb/powershell/module/microsoft.powershell.core/about/about_operators"" rel=""nofollow noreferrer"">""Run a command, script, or script block"" PS specifically  provides <code>&amp;</code>,  ""the call operator, also known as the 'invocation operator'""</a>. However, I attempted without success with:</p>

<pre><code>&amp; docker run --rm -v $($env:USERPROFILE -replace '\\','/'):/data alpine ls /data
</code></pre>

<p><code>Cmd.exe</code> would perhaps make things easier, but PowerShell is the official shell to interact with  <a href=""https://docs.docker.com/docker-for-windows"" rel=""nofollow noreferrer"">Docker for Windows</a>. Thus, there should be a reliable  way to pass variable arguments to Docker commands. </p>

<p> (*) The remove switch <code>-rm</code> is used here only for the purpose of experimenting with the answer avoiding cluttering your workspace. Of course, I do not usually destroy the container as soon as I create it, but rather interact with it passing  <code>-ti</code>. </p>

<p>EDIT</p>

<p>@AnsgarWiechers proposes <a href=""https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_splatting?view=powershell-5.1"" rel=""nofollow noreferrer"">parameters splatting</a> in a comment:</p>

<pre><code>$params = 'run',  '--rm', ""-v $($env:USERPROFILE -replace '\\','/'):/data"", 'alpine', 'ls /data'
docker  @params
</code></pre>

<p>Assuming I am implementing it properly, it doesn't work either and gives:</p>

<pre><code>C:\Program Files\Docker\Docker\Resources\bin\docker.exe: Error response from daemon: invalid mode: /data.        
See 'C:\Program Files\Docker\Docker\Resources\bin\docker.exe run --help'.                                        
</code></pre>
"
"48301257","How to install oracle-java8-installer on docker debian:jessie","<docker><java-8><debian><debian-jessie>","48332593","Installing Java8 in Dockerfile","<java><ubuntu><docker><java-8><dockerfile>","<p>I am trying to install java 8 through oracle-java8-installer on a debian:jessie docker container. The following is my Dockerfile:</p>

<pre><code>FROM debian:jessie

ENV JAVA_VERSION 1.8.0

RUN echo ""deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main"" &gt; /etc/apt/sources.list.d/webupd8team-java.list
RUN echo ""deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main"" &gt;&gt; /etc/apt/sources.list.d/webupd8team-java.list
RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
RUN echo ""debconf shared/accepted-oracle-license-v1-1 select true"" | /usr/bin/debconf-set-selections
RUN apt-get update
RUN apt-get install -y --force-yes vim
RUN apt-get install -y --force-yes oracle-java8-installer
</code></pre>

<p>Yet this gives:</p>

<pre><code>Connecting to download.oracle.com (download.oracle.com)|23.63.224.171|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2018-01-17 12:31:05 ERROR 404: Not Found.

download failed
Oracle JDK 8 is NOT installed.
dpkg: error processing package oracle-java8-installer (--configure):
 subprocess installed post-installation script returned error exit status 1
Errors were encountered while processing:
 oracle-java8-installer
E: Sub-process /usr/bin/dpkg returned an error code (1)
The command '/bin/sh -c apt-get install -y --force-yes oracle-java8-installer' returned a non-zero code: 100
</code></pre>

<p>I have found many similar issues described online, but none of the proposed solutions worked for me. Any idea?</p>
","<p>I have a Docker container in which I was installing Java/JDK in order to power Pentaho. Starting at some point withing the last ~9 days ( I believe ), I've been getting an error when trying to build this container. </p>

<p>I'm not sure if the speciic jdk has been archived by oracle, but I've spent a solid day trying to find a way around this one and hitting dead ends.</p>

<p>Looking for a PPA or active dist to get me out of this mess.</p>

<p>From the Dockerfile: </p>

<pre><code># Pentaho
ENV PDI_RELEASE=6.1 \
    PDI_VERSION=6.1.0.1-196 \
    PDI_HOME=/opt/pentaho-di \
    KETTLE_HOME=/pentaho-di

# Java
# auto validate license
RUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | /usr/bin/debconf-set-selections &amp;&amp; \
    echo ""deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main"" | tee /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \
    echo ""deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main"" | tee -a /etc/apt/sources.list.d/webupd8team-java.list &amp;&amp; \
    apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886 &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get -y install oracle-java8-installer unzip &amp;&amp; \
    apt-get clean &amp;&amp; \
    curl -sS -L -o /tmp/pdi-ce-${PDI_VERSION}.zip http://downloads.sourceforge.net/project/pentaho/Data%20Integration/${PDI_RELEASE}/pdi-ce-${PDI_VERSION}.zip &amp;&amp; \
    unzip -q /tmp/pdi-ce-${PDI_VERSION}.zip -d $PDI_HOME &amp;&amp; \
    rm /tmp/pdi-ce-${PDI_VERSION}.zip
</code></pre>

<p>Here's the failure I'm getting from the build: </p>

<pre><code>2018-01-18T17:59:31.674ZUnpacking locales (2.19-18+deb8u10) ...
2018-01-18T17:59:32.569ZSelecting previously unselected package oracle-java8-installer.
2018-01-18T17:59:32.571ZPreparing to unpack .../oracle-java8-installer_8u151-1~webupd8~0_all.deb ...
2018-01-18T17:59:32.742Zdebconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
2018-01-18T17:59:32.966Zoracle-license-v1-1 license has already been accepted
2018-01-18T17:59:32.983ZUnpacking oracle-java8-installer (8u151-1~webupd8~0) ...
2018-01-18T17:59:33.245ZProcessing triggers for mime-support (3.58) ...
2018-01-18T17:59:33.636ZProcessing triggers for hicolor-icon-theme (0.13-1) ...
2018-01-18T17:59:33.644ZProcessing triggers for shared-mime-info (1.3-1) ...
2018-01-18T17:59:36.221ZSetting up java-common (0.52) ...
2018-01-18T17:59:36.333ZSetting up locales (2.19-18+deb8u10) ...
2018-01-18T17:59:36.421Zdebconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
2018-01-18T17:59:36.839ZGenerating locales (this might take a while)...
2018-01-18T17:59:36.842ZGeneration complete.
2018-01-18T17:59:36.877ZSetting up oracle-java8-installer (8u151-1~webupd8~0) ...
2018-01-18T17:59:36.969Zdebconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
2018-01-18T17:59:37.411ZNo /var/cache/oracle-jdk8-installer/wgetrc file found.
Creating /var/cache/oracle-jdk8-installer/wgetrc and
using default oracle-java8-installer wgetrc settings for it.
Downloading Oracle Java 8...
2018-01-18T17:59:38.406Z--2018-01-18 17:59:38--  http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gz
Resolving download.oracle.com (download.oracle.com)... 
2018-01-18T17:59:38.488Z184.84.129.202
Connecting to download.oracle.com (download.oracle.com)|184.84.129.202|:80... 
2018-01-18T17:59:38.488Zconnected.
HTTP request sent, awaiting response... 
2018-01-18T17:59:38.490Z302 Moved Temporarily
Location: https://edelivery.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gz [following]
--2018-01-18 17:59:38--  https://edelivery.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gz
2018-01-18T17:59:38.501ZResolving edelivery.oracle.com (edelivery.oracle.com)... 
2018-01-18T17:59:38.635Z23.212.176.23, 2600:1408:10:397::2d3e, 2600:1408:10:3b5::2d3e
Connecting to edelivery.oracle.com (edelivery.oracle.com)|23.212.176.23|:443... 
2018-01-18T17:59:38.636Zconnected.
2018-01-18T17:59:38.644ZHTTP request sent, awaiting response... 
2018-01-18T17:59:38.738Z302 Moved Temporarily
Location: http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gz?AuthParam=1516298498_6fba300d1f634adc9f84ee54d5966fce [following]
--2018-01-18 17:59:38--  http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gz?AuthParam=1516298498_6fba300d1f634adc9f84ee54d5966fce
2018-01-18T17:59:38.738ZConnecting to download.oracle.com (download.oracle.com)|184.84.129.202|:80... 
2018-01-18T17:59:38.739Zconnected.
2018-01-18T17:59:38.739ZHTTP request sent, awaiting response... 
2018-01-18T17:59:38.758Z404 Not Found
2018-01-18 17:59:38 ERROR 404: Not Found.

2018-01-18T17:59:38.808Zdownload failed
Oracle JDK 8 is NOT installed.
</code></pre>

<p>A workaround I've been trying per SO and the internets' guidance:</p>

<pre><code>RUN apt-get -y install software-properties-common python-software-properties &amp;&amp; \
    add-apt-repository ppa:webupd8team/java &amp;&amp; \
    apt-get update
RUN cd /var/lib/dpkg/info &amp;&amp; \
    sed -i 's|JAVA_VERSION=8u151|JAVA_VERSION=8u161|' oracle-java8-installer.* &amp;&amp; \
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|' oracle-java8-installer.* &amp;&amp; \
    sed -i 's|SHA256SUM_TGZ=""c78200ce409367b296ec39be4427f020e2c585470c4eed01021feada576f027f""|SHA256SUM_TGZ=""6dbc56a0e3310b69e91bb64db63a485bd7b6a8083f08e48047276380a0e2021e""|' oracle-java8-installer.* &amp;&amp; \
    sed -i 's|J_DIR=jdk1.8.0_151|J_DIR=jdk1.8.0_161|' oracle-java8-installer.*
RUN apt-get -y install oracle-java8-installer unzip
</code></pre>

<p>Unfortunately it looks like the jessie dist is missing -- here's the error:</p>

<pre><code>(image: pre-deps) (service: rails_pre_deps) OK
(image: pre-deps) (service: rails_pre_deps) Get:1 http://security.debian.org jessie/updates InRelease [63.1 kB]
(image: pre-deps) (service: rails_pre_deps) Ign http://ppa.launchpad.net jessie InRelease
(image: pre-deps) (service: rails_pre_deps) Hit http://ppa.launchpad.net trusty InRelease
(image: pre-deps) (service: rails_pre_deps) Ign http://deb.debian.org jessie InRelease
(image: pre-deps) (service: rails_pre_deps) Hit http://deb.debian.org jessie-updates InRelease
(image: pre-deps) (service: rails_pre_deps) Hit http://deb.debian.org jessie Release.gpg
(image: pre-deps) (service: rails_pre_deps) Ign http://ppa.launchpad.net jessie Release.gpg
(image: pre-deps) (service: rails_pre_deps) Hit http://deb.debian.org jessie Release
(image: pre-deps) (service: rails_pre_deps) Ign http://ppa.launchpad.net jessie Release
(image: pre-deps) (service: rails_pre_deps) Err http://ppa.launchpad.net jessie/main amd64 Packages
(image: pre-deps) (service: rails_pre_deps)
(image: pre-deps) (service: rails_pre_deps) Err http://ppa.launchpad.net jessie/main amd64 Packages
(image: pre-deps) (service: rails_pre_deps)
(image: pre-deps) (service: rails_pre_deps) Err http://ppa.launchpad.net jessie/main amd64 Packages
(image: pre-deps) (service: rails_pre_deps)
(image: pre-deps) (service: rails_pre_deps) Err http://ppa.launchpad.net jessie/main amd64 Packages
(image: pre-deps) (service: rails_pre_deps)
(image: pre-deps) (service: rails_pre_deps) Err http://ppa.launchpad.net jessie/main amd64 Packages
(image: pre-deps) (service: rails_pre_deps)   404  Not Found
</code></pre>

<p>Apologies for the differences in output. One is coming from <code>jet</code>, and the other is from codeship. </p>

<p>Thanks for any help</p>
"
"48550843","Unable to copy file from docker-compose mount to host","<selenium><docker><protractor><docker-compose>","48548791","Copying file from docker container to host using docker compose","<docker><docker-compose>","<p>I am unable to copy a file generated by my Selenium tests in a folder inside docker container mounted to host machine.</p>

<p>Here is how my compose file look like</p>

<pre><code>  selenium:
    image: 'selenium/standalone-chrome'
    expose:
      - ""4444""
  tests:
    build:
      context: ./tests
      dockerfile: Dockerfile
    depends_on:
      - selenium
      - web
    volumes:
      - /testResultsReport:/testResultsReport
</code></pre>

<p>This is how my directory structure looks like.</p>

<pre><code>\
 - docker-compose.yml
 - build scripts
 - tests
   - testResultsReport
   - Test scripts
</code></pre>

<p>Name of the file generated inside testResultsReport folder when running docker-compose is <code>TestResultsReport.HTML</code></p>

<p>As my tests are running inside selenium server on port <code>4444</code>, should I mount my volume there? I tried doing that to but still not getting my html file copied to host machine?
I confirmed that my html file is generated when i run my tests outside compose.</p>

<p>I also tried ./testResultsReport however it added a folder in root with name of testResultsReport and did nothing. So I used ./tests/testResultsReport but still not getting file.</p>
","<p>I am trying to mount a volume from my host machine to docker using docker-compose but not getting my file copied over from docker-container.
This is what i have.</p>

<pre><code>volumes:
  - /tests/testResultsReport:/tests/testResultsReport
</code></pre>

<p>I have a file that is generated inside testResultsReport folder every time I run my tests and want my file to be copied over from docker container to my host.</p>

<p>I think I am missing something in understanding how it works so needs a little help.</p>

<p>I already checked that path /tests/testResultsReport exists in host.</p>
"
"51247609","What is the difference between docker run and docker container run","<docker>","65845677","Why docker provides two commands 'docker image load' and 'docker load' with same functionality?","<docker>","<p>can anyone help me in the understanding difference between <strong>docker run</strong> &amp; <strong>docker container run</strong>?</p>

<p>when i do <strong>docker run --help</strong> &amp; <strong>docker container run --help</strong> from docker cmd line. I see the following </p>

<p><strong>Run a command in a new container</strong>.</p>

<p>Is there any difference in how they run the container internally or both are same doing same work?</p>

<p>As per <a href=""https://forums.docker.com/t/docker-run-and-docker-container-run/30526"" rel=""noreferrer"">https://forums.docker.com/t/docker-run-and-docker-container-run/30526</a>. <strong>docker run</strong> is still the old one, which will be deprecated soon but same is not confirmed.</p>
","<p>Why does docker provides two commands <a href=""https://docs.docker.com/engine/reference/commandline/load/"" rel=""nofollow noreferrer"">docker load</a> and <a href=""https://docs.docker.com/engine/reference/commandline/image_load/"" rel=""nofollow noreferrer"">docker image load</a> doing the exact same thing? What is the history behind these commands?</p>
"
"51518087","RUN inside a conditional statement in Dockerfile","<docker>","65703289","Include If else statement in docker file","<docker><if-statement><conditional-statements>","<p>I have a <code>Dockerfile</code> which currently uses the following:</p>

<pre><code>COPY ./compose/local/django/start.sh /start.sh
RUN sed -i 's/\r//' /start.sh
RUN chmod +x /start.sh
</code></pre>

<p>As you can see it copies the file and then makes it executable. I now need to change this depending on a argument provided when I build the image. I have tried using this:</p>

<pre><code>RUN if [ ""$arg"" = ""True"" ]; then \
    COPY ./compose/local/django/new.sh /new.sh \
    RUN sed -i 's/\r//' /new.sh \
    RUN chmod +x /new.sh; else \
    COPY ./compose/local/django/start.sh /start.sh \
    RUN sed -i 's/\r//' /start.sh \
    RUN chmod +x /start.sh; fi
</code></pre>

<p>But this fails as it appears that I can't run <code>COPY</code> or <code>RUN</code> commands inside a conditional statement. It always fails with:</p>

<pre><code>/bin/sh: 1: COPY: not found
</code></pre>

<p>or</p>

<pre><code>/bin/sh: 1: RUN: not found
</code></pre>

<p>So, I think that the best course of action is to create a separate bash file which does the copies, something like:</p>

<pre><code>#!/usr/bin/env bash

if [ ""${arg}"" = ""True"" ]; then
    echo ""arg = ${arg}""
    cp ./compose/local/django/new.sh /new.sh
elif [ ""${arg}"" = ""False"" ]; then
    echo ""arg = ${arg}""
    cp ./compose/local/django/start.sh /start.sh
fi
</code></pre>

<p>But I am struggling with how to make the copied bash files executable. I know I can do it by running <code>chmod +x</code> in the command line, but as I am not the only person who is going to be building this, I was hoping that there would be a better solution, something like I was doing previously in the original <code>Dockerfile</code> script.</p>

<p>Can anyone help me with this? Any help would be much appreciated.</p>
","<p><strong>I want to create docker file with conditional statements.</strong> i get parameters from outside ( BUILD_TOOL) .
This is the code but i got error while building docker image.</p>
<p>i got this error =  <em>dockerfile parse error line 23: unknown instruction: ELSE</em></p>
<pre><code>RUN if [ &quot;$BUILD_TOOL&quot; = &quot;maven&quot; ] ; then 
    RUN mvn clean install;

#if build tool is gradle
else 
    RUN gradle clean;
fi
</code></pre>
"
"54954187","Docker images - types. Slim vs slim-stretch vs stretch vs alpine","<java><docker><dockerfile>","65776474","Difference between the docker images for aspnet (aspnet version, host os codename, ect.)","<docker><.net-core><dockerfile>","<p>I am looking to pick up a docker image to build a java app and looking at the variants of the OpenJDK images available.
I am looking here <a href=""https://github.com/docker-library/openjdk/tree/master/8/jdk"" rel=""noreferrer"">https://github.com/docker-library/openjdk/tree/master/8/jdk</a> and see alpine, slim and windows.
What are the differences between these and what does each variant give?</p>
","<p>What is the difference between the docker images below?</p>
<ul>
<li>mcr.microsoft.com/dotnet/aspnet:3.1</li>
<li>mcr.microsoft.com/dotnet/aspnet:3.1-buster</li>
<li>mcr.microsoft.com/dotnet/aspnet:3.1-buster-slim</li>
</ul>
<p>I see all these tags listed with the same docker file:
<em>3.1.11-buster-slim, 3.1-buster-slim,  3.1.11, 3.1</em></p>
<p>Reference: <a href=""https://hub.docker.com/_/microsoft-dotnet-aspnet/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/microsoft-dotnet-aspnet/</a></p>
<p>I've read some details for the java images, so i guess the format is</p>
<blockquote>
<p>[aspnet version]-[host os codename]-[features included]</p>
</blockquote>
<p>but then it's not clear to me why event the <em>[features included]</em> tag points to the same docker file description or if the naming concept is the same as the JDK one.</p>
"
"55845582","How to launch a rails console in a Fargate container","<ruby-on-rails><amazon-web-services><console><amazon-ecs><aws-fargate>","65855413","SSH on AWS ECS Fargate task docker container and run rails console","<ruby-on-rails><docker><heroku><amazon-ecs><aws-fargate>","<p>I would like to open a Rails console in a Fargate container to interact with my production installation</p>

<p>However after searching the web and posting in the AWS forum I could not find an answer to this question</p>

<p>Does anyone know how I can do this? This seems like a mandatory thing to have in any production environment and having no easy way to do it is kind of surprising coming from such a respected cloud provider as AWS</p>

<p>Thanks</p>
","<p>I'm running a vanilla Rails application with a <a href=""https://buildpacks.io/"" rel=""nofollow noreferrer"">pack</a> built container on AWS's ECS with Fargate and therefore I cannot access the host machine.</p>
<p>I'm facing an issue that only happens in production and therefore I need to be able to log into the running task to debug it.</p>
<p>If I use a EC2 provider as a host for the running task, I am able to log into with <code>docker exec -it &lt;container-id&gt; /bin/bash</code> and all ENVs will be there and I can leverage the use of <code>console</code> command that will be in <code>PATH</code> for instance and get <code>rails console</code> running.</p>
<p>As Fargate allows me to use a public IP, I've created a simple buildpack that will install <code>sshd</code> and create a private key so I can log into it.</p>
<p>I am able to log in using <code>ssh -i &lt;my-rsa-id&gt;.key heorku@&lt;task-public-ip&gt;</code> and I will get a bash as well, but nothing is on the path and all ENVs from the Task Definition are not loaded at all...</p>
<p>I'm looking into how to leverage whatever is done in the ENTRYPOINT or CMD of the <code>docker run</code> triggered by ECS which I believe is called <a href=""https://buildpacks.io/docs/app-developer-guide/run-an-app/#user-provided-shell-process"" rel=""nofollow noreferrer"">launcher</a> and be able to get a behavior just like the <code>docker exec -it ...</code> command.</p>
<p>Any clues are highly appreciated!</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","65757920","Docker container failed to spin up","<node.js><docker><docker-compose>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I am trying to run this <a href=""https://github.com/tadhglewis/issue-status"" rel=""nofollow noreferrer"">project</a> in <code>Github</code> in a <code>Docker</code> container. It is a simple reactjs status application.</p>
<p>I tried to run this in my local environment with following commands after cloning:</p>
<pre><code>// install dependencies
npm install

// and then
npm start
</code></pre>
<p>And works totally fine.</p>
<p>Then I did the same thing with a <code>Dockerfile</code>, as below:</p>
<pre><code># Use whatever version you are running locally (see node -v)
FROM node:12.13

WORKDIR /

COPY package.json /


RUN npm install

COPY . /


EXPOSE 3000

CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<p>And when I am using <code>docker-compose.yml</code> to build this as below using <code>docker-compose up --build -d</code>:</p>
<pre><code>version: &quot;3.2&quot;

services:
  frontend_common:
    build: .
    ports:
      - 80:3000
       
</code></pre>
<p>But the container EXITS and when I try to see the logs, it gives no error but following output:</p>
<pre><code>
&gt; issue-status@0.1.0 start /
&gt; react-scripts start

ℹ ｢wds｣: Project is running at http://&lt;Redacted&gt;/
ℹ ｢wds｣: webpack output is served from
ℹ ｢wds｣: Content not from webpack is served from /public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...

</code></pre>
<p>Can someone help me where I am being gone wrong?</p>
"
"1191374","Using module 'subprocess' with timeout","<python><multithreading><timeout><subprocess>","63374790","Check health of Docker using Python Script - Python 2.7","<python-2.7><docker><automation><python-2.x>","<p>Here's the Python code to run an arbitrary command returning its <code>stdout</code> data, or raise an exception on non-zero exit codes:</p>

<pre><code>proc = subprocess.Popen(
    cmd,
    stderr=subprocess.STDOUT,  # Merge stdout and stderr
    stdout=subprocess.PIPE,
    shell=True)
</code></pre>

<p><code>communicate</code> is used to wait for the process to exit:</p>

<pre><code>stdoutdata, stderrdata = proc.communicate()
</code></pre>

<p>The <code>subprocess</code> module does not support timeout--ability to kill a process running for more than X number of seconds--therefore, <code>communicate</code> may take forever to run.</p>

<p>What is the <strong>simplest</strong> way to implement timeouts in a Python program meant to run on Windows and Linux?</p>
","<p>I want to write a python script (Python 2.7) to check the health of the Docker (using command 'docker ps').</p>
<pre><code>docker ps
</code></pre>
<p>and it returns anything then it is healthy and exit the script. If the command is <strong>hung for more than 2 minutes</strong> then print &quot;The Docker is stopped&quot;</p>
<p>Kindly help me to figure out how to check whether it hung for 2 minutes.</p>
<p>Thanks in advance.</p>
"
"6865538","Solving a ""communications link failure"" with JDBC and MySQL","<java><mysql><jdbc>","63406255","Cannot connect to MySQL image: Could not create connection to database server","<mysql><spring-boot><docker><jdbc><docker-compose>","<p>I'm trying to connect to the local MySQL server but I keep getting an error.</p>

<p>Here is the code.</p>

<pre><code>public class Connect {

    public static void main(String[] args) {
        Connection conn = null;

        try {
            String userName = ""myUsername"";
            String password = ""myPassword"";

            String url = ""jdbc:mysql://localhost:3306/myDatabaseName"";
            Class.forName(""com.mysql.jdbc.Driver"").newInstance();
            conn = DriverManager.getConnection(url, userName, password);
            System.out.println(""Database connection established"");
        } catch (Exception e) {
            System.err.println(""Cannot connect to database server"");
            System.err.println(e.getMessage());
            e.printStackTrace();
        } finally {
            if (conn != null) {
                try {
                    conn.close();
                    System.out.println(""Database Connection Terminated"");
                } catch (Exception e) {}
            }
        }
    }
}
</code></pre>

<p>and the errors :</p>

<pre><code>Cannot connect to database server
Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
        at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1116)
        at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:344)
        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2333)
        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2370)
        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2154)
        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:792)
        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:381)
        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:305)
        at java.sql.DriverManager.getConnection(DriverManager.java:582)
        at java.sql.DriverManager.getConnection(DriverManager.java:185)
        at Connect.main(Connect.java:16)
    Caused by: java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at java.net.Socket.connect(Socket.java:478)
        at java.net.Socket.&lt;init&gt;(Socket.java:375)
        at java.net.Socket.&lt;init&gt;(Socket.java:218)
        at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:257)
        at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:294)
        ... 15 more
</code></pre>

<p>I've set the classpath, made sure my.cnf had the skip network option commented out. </p>

<p>java version is 1.2.0_26 (64 bit)
mysql 5.5.14
mysql connector 5.1.17</p>

<p>I made sure that the user had access to my database.</p>
","<p>I am trying to connect from my Java application to MySQL image, but for some reason, it gives me: <code>[08001] Could not create connection to database server. Attempted to reconnect 3 times. Giving up. java.net.ConnectException: Connection refused: connect.</code></p>
<p>It seems like it tries to access the database, but login info is incorrect, though it is not true. I would appreciate any help, maybe I am just blind.</p>
<p>My docker-compose.yaml:</p>
<pre><code>  version: &quot;3.3&quot;

services:

   db-mysql:
       image: mysql:8.0.21
       ports:
          - 3306:3306
       container_name: eRestaurant_db
       networks:
          - eRestaurant-network
       environment:
          MYSQL_DATABASE: happy_tummy
          MYSQL_ROOT_PASSWORD: password123
       volumes:
          - my-db:/var/lib/mysql

volumes:
   my-db:

networks:
   eRestaurant-network:
</code></pre>
<p>Stack when container is up:</p>
<pre><code>    Recreating eRestaurant_db ... done                                                                                                                      Attaching to eRestaurant_db
eRestaurant_db | 2020-08-14 03:39:19+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
eRestaurant_db | 2020-08-14 03:39:19+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
eRestaurant_db | 2020-08-14 03:39:19+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
eRestaurant_db | 2020-08-14 03:39:20+00:00 [Note] [Entrypoint]: Initializing database files
eRestaurant_db | 2020-08-14T03:39:20.036357Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.21) initializing of server in progress as process 42
eRestaurant_db | 2020-08-14T03:39:20.042898Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
eRestaurant_db | 2020-08-14T03:39:20.956247Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
eRestaurant_db | 2020-08-14T03:39:22.903509Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
eRestaurant_db | 2020-08-14 03:39:27+00:00 [Note] [Entrypoint]: Database files initialized
eRestaurant_db | 2020-08-14 03:39:27+00:00 [Note] [Entrypoint]: Starting temporary server
eRestaurant_db | 2020-08-14T03:39:27.698855Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.21) starting as process 89
eRestaurant_db | 2020-08-14T03:39:27.718821Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
eRestaurant_db | 2020-08-14T03:39:27.944323Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
eRestaurant_db | 2020-08-14T03:39:28.061636Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
eRestaurant_db | 2020-08-14T03:39:28.185890Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
eRestaurant_db | 2020-08-14T03:39:28.186075Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
eRestaurant_db | 2020-08-14T03:39:28.192291Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
eRestaurant_db | 2020-08-14T03:39:28.213335Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.21'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
eRestaurant_db | 2020-08-14 03:39:28+00:00 [Note] [Entrypoint]: Temporary server started.
eRestaurant_db | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
eRestaurant_db | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
eRestaurant_db | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
eRestaurant_db | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
eRestaurant_db | 2020-08-14 03:39:31+00:00 [Note] [Entrypoint]: Creating database happy_tummy
eRestaurant_db |
eRestaurant_db | 2020-08-14 03:39:31+00:00 [Note] [Entrypoint]: Stopping temporary server
eRestaurant_db | 2020-08-14T03:39:31.451056Z 11 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.21).
eRestaurant_db | 2020-08-14T03:39:34.401782Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.21)  MySQL Community Server - GPL.
eRestaurant_db | 2020-08-14 03:39:34+00:00 [Note] [Entrypoint]: Temporary server stopped
eRestaurant_db |
eRestaurant_db | 2020-08-14 03:39:34+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.
eRestaurant_db |
eRestaurant_db | 2020-08-14T03:39:34.689506Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.21) starting as process 1
eRestaurant_db | 2020-08-14T03:39:34.699358Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
eRestaurant_db | 2020-08-14T03:39:34.928207Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
eRestaurant_db | 2020-08-14T03:39:35.042858Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
eRestaurant_db | 2020-08-14T03:39:35.123421Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
eRestaurant_db | 2020-08-14T03:39:35.123614Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
eRestaurant_db | 2020-08-14T03:39:35.130639Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
eRestaurant_db | 2020-08-14T03:39:35.150321Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.21'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
</code></pre>
<p>application.properties file:</p>
<pre><code>spring.datasource.url=jdbc:mysql://10.0.75.1:3306/happy_tummy?allowPublicKeyRetrieval=true&amp;useSSL=false&amp;createDatabaseIfNotExist=true
spring.datasource.username=root
spring.datasource.password=password123
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.jpa.database-platform = org.hibernate.dialect.MySQL8Dialect
spring.jpa.generate-ddl=true
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto = update
</code></pre>
"
"8633461","How to keep environment variables when using sudo","<linux><environment-variables><sudo>","63448690","How to pass environment variables to docker container app user","<docker>","<p>When I use any command with sudo the environment variables are not there. For example after setting HTTP_PROXY the command <code>wget</code> works fine without <code>sudo</code>. However if I type <code>sudo wget</code> it says it can't bypass the proxy setting.</p>
","<p>Using Docker Compose, how do you pass environment variables to the app user shell such that the environment variables are always loaded when that user runs a command?</p>
<p>For example, I have tried via Dockerfile:</p>
<pre><code>FROM phusion/passenger-ruby26

ENV FOO=bar

RUN dpkg --clear-avail
RUN rm -rf /var/lib/apt/lists/*
RUN apt-get -qq update --fix-missing
RUN apt-get -qq install -f
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get clean
RUN apt-get -qq install -y build-essential apt apt-utils \
                           software-properties-common wget inetutils-ping \
                           sudo netcat

COPY --chown=app:app . .

ENTRYPOINT [&quot;sh&quot;, &quot;./startup.sh&quot;]
</code></pre>
<p>startup.sh is:</p>
<pre class=""lang-sh prettyprint-override""><code>_user=&quot;$(id -u -n)&quot;
echo &quot;User name : $_user&quot;

su - app -c 'echo &quot;**************** root printenv:&quot;'
printenv
su - app -c 'echo &quot;**************** app printenv:&quot;'
su - app -c 'printenv'
</code></pre>
<p><s>The <code>build</code>/<code>run</code> output includes a number of existing/pre-set variables -- but not <code>FOO</code>.</s> When I don't use sudo, I can see the variable in the <code>printenv</code> run as root. However, the variable is not available to app user.</p>
<p>What is the best way to make FOO available to user <code>app</code>? Any help is much appreciated.</p>
"
"10382929","How to fix java.lang.UnsupportedClassVersionError: Unsupported major.minor version","<java><jvm><incompatibility><unsupported-class-version>","66339954","running java fie on docker returns errors","<java><docker>","<p>I am trying to use <a href=""http://en.wikipedia.org/wiki/Notepad%2B%2B"" rel=""noreferrer"">Notepad++</a> as my all-in-one tool edit, run, compile, etc.</p>

<p>I have <a href=""http://en.wikipedia.org/wiki/Java_Virtual_Machine#Execution_environment"" rel=""noreferrer"">JRE</a> installed, and I have setup my path variable to the <code>.../bin</code> directory.</p>

<p>When I run my ""Hello world"" in Notepad++, I get this message:</p>

<pre class=""lang-none prettyprint-override""><code>java.lang.UnsupportedClassVersionError: test_hello_world :
 Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(Unknown Source)
       .........................................
</code></pre>

<p>I think the problem here is about versions; some versions of Java may be old or too new.</p>

<ol>
<li>How do I fix it?</li>
<li>Should I install the JDK, and setup my path variable to the JDK instead of JRE?</li>
<li>What is the difference between the <code>PATH</code> variable in JRE or JDK?</li>
</ol>
","<p>I try to run a simple java class with docker making the following steps:</p>
<ol>
<li><p>create java file (HelloWorld.java)</p>
</li>
<li><p>compile it (javac HelloWorld.java)</p>
</li>
<li><p>create dockerfile (above the content of the Dockerfile)</p>
<pre><code>FROM alpine:latest
ADD HelloWorld.class HelloWorld.class
RUN apk --update add openjdk8-jre
ENTRYPOINT [&quot;java&quot;, &quot;-Djava.security.egd=file:/dev/./urandom&quot;, &quot;HelloWorld&quot;]
</code></pre>
</li>
</ol>
<p>then I build it with this command:</p>
<pre><code>   docker build --tag &quot;docker-hello-world:latest&quot; .
</code></pre>
<p>and I got the following:</p>
<pre><code>    C:\Gradle&gt;javac HelloWorld.java

    C:\Gradle&gt;docker build --tag &quot;docker-hello-world:latest&quot; .
    =&gt; [internal] load build definition from Dockerfile
    =&gt; =&gt; transferring dockerfile: 209B
    =&gt; [internal] load .dockerignore
    =&gt; =&gt; transferring context: 2B
    =&gt; [internal] load metadata for docker.io/library/alpine:latest
    =&gt; [internal] load build context
    =&gt; =&gt; transferring context: 471B
    =&gt; [1/3] FROM docker.io/library/alpine:latest
    =&gt; CACHED [2/3] ADD HelloWorld.class HelloWorld.class
    =&gt; CACHED [3/3] RUN apk --update add openjdk8-jre
    =&gt; exporting to image
    =&gt; =&gt; exporting layers
   =&gt; =&gt; writing image sha256:e3450ba2f444321b7d9965d95443e1d394ef8c62549268059e9786bb882992ec
   =&gt; =&gt; naming to docker.io/library/docker-hello-world:latest   
</code></pre>
<p>I think everything is Ok, and I tried to run it with this command:</p>
<pre><code>    docker run docker-hello-world:latest
</code></pre>
<p>an I get this Error:</p>
<pre><code>    C:\Gradle&gt;docker run docker-hello-world:latest
    Error: A JNI error has occurred, please check your installation and try again
    Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: HelloWorld has been compiled by a more recent version of the Java Runtime (class file version 59.0), this version of the Java Runtime only recognizes class file versions up to 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:601)
</code></pre>
<p>this is my java version on my machine :</p>
<pre><code>    C:\Gradle&gt;java -version
    openjdk version &quot;15.0.1&quot; 2020-10-20
    OpenJDK Runtime Environment (build 15.0.1+9-18)
    OpenJDK 64-Bit Server VM (build 15.0.1+9-18, mixed mode, sharing)
</code></pre>
<p>and by the way I build my java file whit the same java version on my machine,therefore I don't know why the error said,the HelloWorld java file has been compiled by more recent version of java Runtime...
My intention is just to run this Java file with docker and get &lt;Hello,World&gt; in the output</p>
"
"16747021","How do you statically link a c library in go using cgo?","<go><cgo>","63241619","How can I build static Go binaries which use C","<docker><go>","<p>So there's a bunch of stuff on the group that suggests you can do this in go (although not on the cgo documentation):</p>

<pre><code>package bridge

import ""fmt""

// #cgo CFLAGS: -I/Users/doug/projects/c/go-bridge/include
// #cgo LDFLAGS: /Users/doug/projects/c/go-bridge/build/libgb.a
// #include &lt;junk.h&gt;
import ""C""

func Run() {
  fmt.Printf(""Invoking c library...\n"")
  C.x(10)
  fmt.Printf(""Done\n"")
}
</code></pre>

<p>However, it doesn't seem to work:</p>

<pre><code>/var/folders/.../bridge.a(bridge.cgo2.o)(__TEXT/__text): x: not defined
</code></pre>

<p>This seems to work fine using a dynamic library, and inspecting the generated files, it actually has the symbol 'x' in there:</p>

<pre><code>/var/folders/rg/hj4s3qlj3sz1d1b5p50ws0vc0000gn/T/go-build442792776/bridge/_obj/_cgo_.o:
0000000100001048 S _NXArgc 
0000000100001050 S _NXArgv 
0000000100001060 S ___progname 
0000000100000dc0 T __cgo_2d7eefe3d6d4_Cfunc_x
0000000100000da0 T __cgo_allocate 
0000000100000db0 T __cgo_panic
0000000100000000 T __mh_execute_header 
0000000100000d90 T _crosscall2
0000000100001058 S _environ
                 U _exit 
0000000100000d80 T _main
                 U _puts 
0000000100001000 s _pvars 
0000000100000de0 T _x                &lt;------- Exists
                 U dyld_stub_binder 
0000000100000d40 T start
</code></pre>

<p>but obviously is just a marker in bridge.cgo2.o:</p>

<pre><code>/var/folders/rg/hj4s3qlj3sz1d1b5p50ws0vc0000gn/T/go-build442792776/bridge.a(bridge.cgo2.o):
0000000000000368 s EH_frame0
0000000000000000 T __cgo_2d7eefe3d6d4_Cfunc_x
0000000000000380 S __cgo_2d7eefe3d6d4_Cfunc_x.eh
                 U _x
</code></pre>

<p>What am I doing wrong?</p>

<p>For ref, the c header:</p>

<pre><code>int x(int y);
</code></pre>

<p>And code:</p>

<pre><code>#include &lt;junk.h&gt;
#include &lt;stdio.h&gt;

int x(int y) {
  printf(""Hello World\n"");
  return y;
}
</code></pre>

<p>--</p>

<p>Edit:</p>

<p>No, -L and -l don't work either; there's actually some specific discussion on the google group that this (-l/blah/blah.a) does not work for cgo, and the correct syntax <em>is</em> in fact to omit the -l and just list the .a file... but hey, if it'd worked, I'd totally just use it. But it doesn't:</p>

<pre><code>dougs-mini:go doug$ go run test.go
# bridge
ld: library not found for -l/Users/doug/projects/c/go-bridge/build/libgb.a
collect2: ld returned 1 exit status
dougs-mini:go doug$ ls -l /Users/doug/projects/c/go-bridge/build/libgb.a
-rw-r--r--  1 doug  staff  872 25 May 14:02 /Users/doug/projects/c/go-bridge/build/libgb.a
</code></pre>

<p>verbose version:</p>

<pre><code>dougs-mini:go doug$ go build -work -x test.go
WORK=/var/folders/rg/hj4s3qlj3sz1d1b5p50ws0vc0000gn/T/go-build354497708
mkdir -p $WORK/bridge/_obj/
cd /Users/doug/projects/c/go-bridge/go/src/bridge
/Users/doug/projects/go/go/pkg/tool/darwin_amd64/cgo -objdir $WORK/bridge/_obj/ -- -I/Users/doug/projects/c/go-bridge/include -I $WORK/bridge/_obj/ bridge.go
/Users/doug/projects/go/go/pkg/tool/darwin_amd64/6c -FVw -I $WORK/bridge/_obj/ -I /Users/doug/projects/go/go/pkg/darwin_amd64 -o $WORK/bridge/_obj/_cgo_defun.6 -DGOOS_darwin -DGOARCH_amd64 $WORK/bridge/_obj/_cgo_defun.c
gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -I/Users/doug/projects/c/go-bridge/include -I $WORK/bridge/_obj/ -o $WORK/bridge/_obj/_cgo_main.o -c $WORK/bridge/_obj/_cgo_main.c
gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -I/Users/doug/projects/c/go-bridge/include -I $WORK/bridge/_obj/ -o $WORK/bridge/_obj/_cgo_export.o -c $WORK/bridge/_obj/_cgo_export.c
gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -I/Users/doug/projects/c/go-bridge/include -I $WORK/bridge/_obj/ -o $WORK/bridge/_obj/bridge.cgo2.o -c $WORK/bridge/_obj/bridge.cgo2.c
gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -o $WORK/bridge/_obj/_cgo_.o $WORK/bridge/_obj/_cgo_main.o $WORK/bridge/_obj/_cgo_export.o $WORK/bridge/_obj/bridge.cgo2.o -l/Users/doug/projects/c/go-bridge/build/libgb.a
# bridge
ld: library not found for -l/Users/doug/projects/c/go-bridge/build/libgb.a
collect2: ld returned 1 exit status
</code></pre>

<p>It's worth noting that the failure when you try to link like this (using -l) is typical of gcc failing to link because you're attempting to combine a set of object files.</p>

<p>ie. This:</p>

<pre><code>gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -o ... -l/path/libgb.a
</code></pre>

<p>Will never compile under gcc; you <em>must</em> link a static library like this:</p>

<pre><code>gcc -I . -g -O2 -fPIC -m64 -pthread -fno-common -o ... /path/libgb.a
</code></pre>

<p>ie. It is absolutely not that I'm missing a -l or -L.</p>
","<p>I have a problem with building a Go code which uses C. The idea is to build a binary inside the docker container and then create a slim docker image with that binary. Everything works fine for services which don't contain C code. I can run binary inside docker containers or on the host machine. But I can't run binary with C code neither inside the container nor on the host machine. I always  encounter the following error: <code>No such file or directory</code>. I set the env variables CGO_ENABLED=1 GOOS=linux GOARCH=amd64. Can someone please tell me where the issue is? Thanks.</p>
<p><strong>Dockerfile</strong> for $IMAGE</p>
<pre><code>FROM golang:1.13-alpine

ARG PKG_CONFIG_PATH=/usr/local/ffmpeg/pkgconfig

RUN apk update \
&amp;&amp; apk add \
    git \
    ffmpeg-dev \
    gcc \
    libc-dev \
    pkgconfig \
&amp;&amp; rm -rf /var/cache/apk/*

RUN wget -O- -nv https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s v1.24.0
</code></pre>
<p><strong>Command</strong></p>
<pre><code>docker run --rm -it -v $PWD:/go/src/project -w /go/src/project/api \
--env CGO_ENABLED=1 --env GOOS=linux --env GOARCH=amd64 \
$IMAGE go build -a -installsuffix cgo -o ../bin/api/api
</code></pre>
"
"21553353","What is the difference between CMD and ENTRYPOINT in a Dockerfile?","<docker>","63677659","CMD vs ENTRYPOINT?","<linux><docker><dockerfile><devops><linux-development>","<p>In Dockerfiles there are two commands that look similar to me: <code>CMD</code> and <code>ENTRYPOINT</code>. But I guess that there is a (subtle?) difference between them - otherwise it would not make any sense to have two commands for the very same thing.</p>

<p>The documentation states for <code>CMD</code></p>

<blockquote>
  <p>The main purpose of a CMD is to provide defaults for an executing container.</p>
</blockquote>

<p>and for <code>ENTRYPOINT</code>:</p>

<blockquote>
  <p>An ENTRYPOINT helps you to configure a container that you can run as an executable.</p>
</blockquote>

<p>So, what's the difference between those two commands?</p>
","<p>I am learning Docker. However, I can't understand what the difference between CMD, and ENTRYPOINT is, when can I use CMD, and ENTRYPOINT in a dockerfile?  It would be helpful if someone explain that to me.</p>
<p>Thanks,
Hugo</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","66362392","What is the use of Expose in docker file for ASP.NET Core application","<docker><asp.net-core><asp.net-core-mvc><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I created a dockerfile from Visual Studio for an ASP.NET Core app. The dockerfile code is:</p>
<pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.

FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;FirstDockerApp.csproj&quot;, &quot;&quot;]
RUN dotnet restore &quot;./FirstDockerApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;FirstDockerApp.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;FirstDockerApp.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;FirstDockerApp.dll&quot;]
</code></pre>
<p>There comes 2 expose ports for the container these are:</p>
<pre><code>EXPOSE 80
EXPOSE 443
</code></pre>
<p>So according to the docs, once the container is run for this app I should be able to access the app by any of these 2 url on the browser.</p>
<ol>
<li>localhost: 80</li>
<li>localhost: 443</li>
</ol>
<p>So I ran 2 command which builds the image from the dockerfile and creates a container.</p>
<ol>
<li><p>Command 1</p>
<p>docker build -t myapp -f Dockerfile .</p>
</li>
<li><p>Command 2</p>
<p>docker run myapp</p>
</li>
</ol>
<p>Now when I try to access the app on the container with the url - <code>localhost:443</code>* in the browser. Then I get a message that the website can't be reached. What is the problem here?</p>
<p><strong>Expose has no effect</strong></p>
<p>Next I removed the expose from the dockerfile.</p>
<pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging. See the dockerfile code after removing the expose.

FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;FirstDockerApp.csproj&quot;, &quot;&quot;]
RUN dotnet restore &quot;./FirstDockerApp.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;FirstDockerApp.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;FirstDockerApp.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;FirstDockerApp.dll&quot;]
</code></pre>
<p>Now I build the image and created the container. This time i used &quot;-p&quot; option with the run command. See below</p>
<ol>
<li><p>Command 1</p>
<pre><code> docker build -t myapp -f Dockerfile .
</code></pre>
</li>
<li><p>Command 2</p>
<pre><code> docker run -p 5001:80 myapp
</code></pre>
</li>
</ol>
<p>Now I can access the app from the url <code>localhost:5001</code>.</p>
<p>So what is the use of expose in the ASP.NET Core docker app? According to me it serves no purpose.</p>
<p>There is also another thing. My dockerfile has EXPOSE 443. Now if i expose the port of the container by the run command - <code>docker run -p 5004:443 firstdockerapp</code>. Next I open the url on the browser - <code>localhost:5004</code> then app could not be reached by the browser. Why it is so?</p>
"
"22466255","Is it possible to answer dialog questions when installing under docker?","<ubuntu><installation><docker><apt-get>","66333915","Docker package installation expects data entry","<docker><opencv><installation>","<p>Is it possible to somehow answer the questions that are presented as dialogs when installing some packages using apt-get?</p>

<p>For instance I'm trying to setup a container containing the <code>mail-stack-delivery</code> package with:</p>

<pre><code>FROM ubuntu

RUN apt-get install -y mail-stack-delivery
</code></pre>

<p>However that dockerfile generates dozens of errors when built that are along the lines of:</p>

<pre><code>debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (Can't locate Term/ReadLine.pm in @INC (@INC contains: /etc/perl /usr/local/lib/perl/5.14.2 /usr/local/share/perl/5.14.2 /usr/lib/perl5 /usr/share/perl5 /usr/lib/perl/5.14 /usr/share/perl/5.14 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7, &lt;&gt; line 11.)
debconf: falling back to frontend: Teletype
dpkg-preconfigure: unable to re-open stdin: 
</code></pre>

<p>From what I understand I just simply can't respond to the dialogs, but is there some way that I can pass a parameter to answer each question in advance? I know that it's just changing some configurations, so I could do it after the fact, but presumably it's better to let the install scripts do it so everything gets set properly.</p>
","<p>I'm trying to install opencv on docker image, but docker not reads stdin</p>
<p>Dockerfile</p>
<pre><code>FROM ubuntu:18.04
RUN apt-get update \
    &amp;&amp; apt-get install tesseract-ocr -y \
    tesseract-ocr-rus \
    ffmpeg \
    python3 \
    #python-setuptools \
    python3-pip \
    python3-opencv \
    &amp;&amp; apt-get clean \
    &amp;&amp; apt-get autoremove


RUN mkdir -p /usr/src/app 

WORKDIR /usr/src/app

COPY ./server /usr/src/app
COPY ./requirements.txt /usr/src/app

RUN python3 -m pip install --upgrade pip
RUN pip3 install -r requirements.txt

RUN ls

EXPOSE 8000

CMD [ &quot;python3&quot;, &quot;./manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot; ]
</code></pre>
<p>Terminal:</p>
<pre><code>...
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
</code></pre>
<p>He expects data entry</p>
<p>How can i pass data entry\ or how to input data?</p>
"
"23935141","How to copy Docker images from one host to another without using a repository","<docker>","63813136","How to locally export docker image from a machine X and import and run it on machine Y?","<docker><dockerfile><docker-container><docker-image>","<p>How do I transfer a Docker image from one machine to another one without using a repository, no matter private or public?</p>

<p>I create my own image in VirtualBox, and when it is finished I try to deploy to other machines to have real usage.</p>

<p>Since it is based on my own based image (like Red Hat Linux), it cannot be recreated from a Dockerfile. My dockerfile isn't easily portable.</p>

<p>Are there simple commands I can use? Or another solution?</p>
","<p>I have a requirement to export docker image on machine X (locally) without pushing to the docker hub or any repo, and I have to use the exported image (my_docker_image.gz or something) on machine Y as a container. How to do it? Pls help on commands</p>
"
"24309526","How to change the docker image installation directory?","<docker>","63315904","Docker does not see all disks - No space left on device","<docker><dockerfile><docker-build><linux-df>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>If i run</p>
<pre><code>df -h
</code></pre>
<p>inside the Dockerfile of an image I want to build, it sees:</p>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
overlay         118G  112G     0 100% /
tmpfs            64M     0   64M   0% /dev
tmpfs            16G     0   16G   0% /sys/fs/cgroup
shm              64M     0   64M   0% /dev/shm
/dev/sda1       118G  112G     0 100% /etc/hosts
tmpfs            16G     0   16G   0% /proc/asound
tmpfs            16G     0   16G   0% /proc/acpi
tmpfs            16G     0   16G   0% /proc/scsi
tmpfs            16G     0   16G   0% /sys/firmware
</code></pre>
<p>but if I run it outside, it sees:</p>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
udev             16G     0   16G   0% /dev
tmpfs           3.2G  355M  2.8G  12% /run
/dev/sda1       118G  112G     0 100% /
tmpfs            16G  300M   16G   2% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            16G     0   16G   0% /sys/fs/cgroup
/dev/sda3       308G  4.8G  288G   2% /mnt/ssd
/dev/sdb1       1.8T  1.2T  605G  66% /home
tmpfs           3.2G   16K  3.2G   1% /run/user/1002
tmpfs           3.2G  4.0K  3.2G   1% /run/user/1038
tmpfs           3.2G     0  3.2G   0% /run/user/1037
/dev/loop1       97M   97M     0 100% /snap/core/9436
/dev/loop0       97M   97M     0 100% /snap/core/9665
/dev/loop3      163M  163M     0 100% /snap/blender/42
/dev/loop2      163M  163M     0 100% /snap/blender/43
tmpfs           3.2G     0  3.2G   0% /run/user/1039
</code></pre>
<p>I got error &quot;No space left on device&quot; and I would like to use my /dev/sdb1 disk with 600 free GB, why docker does not see it? How can I make it use that space?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63412022","Connect Docker image to local Postgres","<java><postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a Spring app that connects to local Postgres db. It works fine.</p>
<p>I then created a Docker image from the Spring app. When running the container it throws a db connection error.</p>
<p>I have followed the solution suggested <a href=""https://gist.github.com/MauricioMoraes/87d76577babd4e084cba70f63c04b07d"" rel=""nofollow noreferrer"">here</a>, but still getting the same error.</p>
<pre><code>[ERROR ] [task-1] c.z.h.p.HikariPool c.z.h.p.HikariPool.throwPoolInitializationException(HikariPool.java:593) – HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:285)
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:217)
    at org.postgresql.Driver.makeConnection(Driver.java:458)
    at org.postgresql.Driver.connect(Driver.java:260)
</code></pre>
<p>I have run the docker image in several ways with the same result.</p>
<p><code>docker run app-image</code></p>
<p><code>docker run --network=host app-image</code></p>
<p>Dockerfile:</p>
<pre><code>FROM java:8-jdk-alpine

COPY ./build/libs/runtime-0.0.1-SNAPSHOT.jar /usr/app/
WORKDIR /usr/app

ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;runtime-0.0.1-SNAPSHOT.jar&quot;]
# Expose standard tomcat port

EXPOSE 9888
</code></pre>
<p>postgresql.conf:</p>
<pre><code>#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------

# - Connection Settings -

listen_addresses = '*'
</code></pre>
<p>pg_hba.conf</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

host        postgres        postgres     0.0.0.0/0              trust
host        postgres        postgres     172.17.0.0/16          trust
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63186064","How to connect from Docker container parse-server to a host Postgresql DB?","<postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have installed an instance of parse-server with docker. I have my DB on the Host, but when I run the docker, it says <code>error on connecting to the DB.</code></p>
<pre><code>docker run -t -d --name parse-server parseplatform/parse-server --appName appname --appId appid--masterKey masterkey --clientKey clientkey --databaseURI postgres://example:1234563@localhost/parse-server --serverURL http://sync.example.com:1337/parse --verbose --publicServerURL http://sync.example.com
</code></pre>
<p>And I am getting this error:</p>
<pre><code>Error: connect ECONNREFUSED 127.0.0.1:5432
at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1141:16) {
  errno: 'ECONNREFUSED',
  code: 'ECONNREFUSED',
  syscall: 'connect',
  address: '127.0.0.1',
  port: 5432
}
</code></pre>
<p>I tried with <code>--network host</code> but parse-server is giving me <code>{&quot;error&quot;:&quot;unauthorized&quot;}</code> using this docker.</p>
<p>I tried also using <code>-o 5432:5432</code>, but it said the port is used, because the instance running on the host.</p>
<p>How can I connect to the PostgreSQL on the host from the docker parse-server without using <code>--network host</code>?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63068066","How to share local Postgres database with docker?","<django><postgresql><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a django project with PostgreS database. I need to use my local database in docker container. How to share localhost of the machine with docker container? My <em><strong>docker-compose.yml</strong></em> :</p>
<pre><code>  version: '3'
  services:
    web:
      build: ./
      network_mode: &quot;host&quot;
      command: bash -c &quot;python manage.py makemigrations &amp;&amp; python manage.py migrate &amp;&amp; python manage.py collectstatic --noinput --i rest_framework &amp;&amp; gunicorn orion-amr.wsgi:application --bind 0.0.0.0:8000 --workers=2&quot;
      ports:
        - &quot;${webport}:8000&quot;
        - &quot;${pgport}:${pgport}&quot;
      env_file:
        - ./.env
</code></pre>
<p>Django <em><strong>settings.py</strong></em>:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.contrib.gis.db.backends.postgis',
        'NAME': os.environ.get('pgdatabase'),
        'USER': os.environ.get('pguser'),
        'PASSWORD': os.environ.get('pgpassword'),
        'HOST': os.environ.get('pghostname'),
        'PORT': os.environ.get('pgport'),
    }
}
</code></pre>
<p>I added</p>
<pre><code>listen_addresses = '*'  
</code></pre>
<p>to <em><strong>postgresql.conf</strong></em> file and below is my <em><strong>pg_hba.conf</strong></em> file:</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD

# IPv4 local connections:
host    all             all             0.0.0.0/0               trust
# IPv6 local connections:
host    all             all             ::/0                    trust
# Allow replication connections from localhost, by a user with the
# replication privilege.
host    replication     all             127.0.0.1/32            trust
host    replication     all             ::1/128                 trust
</code></pre>
<p>Now I have an error:</p>
<pre><code>web_1  | django.db.utils.OperationalError: could not connect to server: Connection refused
web_1  |        Is the server running on host &quot;127.0.0.1&quot; and accepting
web_1  |        TCP/IP connections on port 5432?
web_1  |
</code></pre>
<p>Who can help me? Please!</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","63764895","Redis connect ECONNREFUSED 127.0.0.1:6379","<javascript><node.js><docker><redis><containers>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Im running redis on my server (Redis is not running in a container)  and I am trying to connect to redis inside of my Container which has a Node.js app runninng. When starting the Docker Container with the node app from my Dockerfile in the logs there is the following error: <code>Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code></p>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","63736258","Make a new container with a combination of two separate containers","<docker><dockerfile><containers><jenkins-docker>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I want a container which have both, docker application and jenkins application installed.<br/>
I have tried building a new container from a Dockerfile -
`
FROM :docker
FROM :jenkins/jenkins</p>
<p>but it only seems to have jenkins but no docker .
`</p>
"
"27273412","Cannot install packages inside docker Ubuntu image","<docker><ubuntu-14.04>","63756071","How to use apt install in docker FROM python:3.6","<python><docker>","<p>I installed Ubuntu 14.04 image on docker. After that, when I try to install packages inside the ubuntu image, I'm getting unable to locate package error: </p>

<pre><code>apt-get install curl

Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package curl
</code></pre>

<p>How to fix this error?</p>
","<p>I make a Dockerfile and use <code>FROM python:3.6</code></p>
<p>And now, I want to use <code>apt</code> to install some <code>dependence</code> what should I do?</p>
<p>Dockerfile</p>
<pre><code>FROM python:3.6

COPY . /app

WORKDIR /app

RUN pip install -r requirements.txt

CMD python app.py
</code></pre>
"
"28662021","In fedora container systemctl gives Failed to get D-Bus connection","<docker><fedora>","63507412","Failed to get D-Bus connection: Operation not permitted [docker]","<docker><nginx><systemctl>","<p>When I in a fedora container systemctl use, I get:</p>

<p>Failed to get D-Bus connection:: Unknown error -1</p>

<p>Does someone know how to fix this? Or can systemctl not be used in a docker container? </p>
","<p>I'm trying to create a docker image (centos7) with an nginx web server running on it. I have created a <code>Dockerfile</code> which looks like this:</p>
<pre><code>FROM centos:centos7

RUN yum update -y

# -------- OPENSSL -------- 

#ADD install-openssl.sh /
#RUN chmod +x install-openssl.sh
#RUN /install-openssl.sh

# -------- NGINX --------

RUN yum install epel-release -y
RUN yum install nginx -y

# Copy a configuration file from the current directory 
ADD nginx.conf /etc/nginx/

# Append &quot;daemon off:&quot; to the beginning of the configuration
RUN echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf

# Expose ports
EXPOSE 80

RUN systemctl start nginx
</code></pre>
<p>I build my docker image with the following command:</p>
<pre><code>docker build -t nginx-img .
</code></pre>
<p>And I run it like this:</p>
<pre><code>docker -v run --name nginx-cont -p 80:80 -i  nginx-img
</code></pre>
<p>But I get the following error:</p>
<pre><code>Failed to get D-Bus connection: Operation not permitted
</code></pre>
"
"30198189","Can I run Docker directly on a non VT-X machine (no Virtual Machine used)?","<docker>","63408215","Docker without VTX or AMD-V","<docker><virtualization><amd>","<p>I would like to use Docker on servers with 80546k Xeon Irwindale CPUs. These CPUs are 64bit but don't support Intel VT-X virtualization. VirtualBox won't run on this machine. 
I'm planning to install Linux directly on the hardware (No VM layer) and use Docker to virtualize applications. Is this possible? I found a lot of discussions about Docker and VT-x, but all of them concern a Virtual Machine such as VirtualBox besides Docker. </p>

<p>Thanks,
Alan</p>
","<p>I have laptop ACER aspire E15 553g F1j2, having processor AMD-FX9800p.
My computer's bios doesn't have any option to enable virtualization.</p>
<p>I'm not able to install Android Emulator.</p>
<p>Can I run Docker for MERN stack development without enabling the virtualization ?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63471692","Imports not working with Flask in Docker (no gunicorn!)","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a flask app, with Flask RestX, that works fine with <code>flask run</code> but has import issues with Docker. I've seen some questions and answers about this but they all involve Gunicorn, but <strong>I'm not using Gunicorn</strong>.</p>
<p>Here is my folder structure. I'm running the <code>docker-compose</code> file which uses the <code>flask.dev.dockerfile</code> as well as a Redis container.</p>
<p><a href=""https://i.stack.imgur.com/91cKl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/91cKl.png"" alt=""enter image description here"" /></a></p>
<p>So, I want to use my redis wrapper, <code>RedisDatabase</code>, from my <code>database.py</code> file. PyCharm inserts this as</p>
<pre><code>from ivd_app.database import RedisDatabase
</code></pre>
<p>And the rest of the <code>__init__.py</code> file, in summary:</p>
<pre class=""lang-py prettyprint-override""><code>from flask import Flask, request
from flask_cors import CORS
from flask_restx import Api, Resource, fields

def create_app(db=None):
    app = Flask(__name__, instance_relative_config=True)

    if (db is None):
        db = RedisDatabase()

    api = Api(app, version='0.0.1', title='IVD')

    config_endpoint = api.namespace(
        'config', description='APIs to Send configuration to and from the Front End'
    )

    config_model = api.model('Configuration', {
        # the model
    })

    @config_endpoint.route('/')
    class ConfigurationEndpoint(Resource):

        @config_endpoint.expect(config_model, validate=True)
        def put(self):
            # the put endpoint

        def get(self):
            # the get endpoint

    return app

# if __name__ == &quot;__main__&quot;:
#    create_app().run(host=&quot;0.0.0.0&quot;, debug=True)
</code></pre>
<p>When I use <code>CMD flask run</code> from the dockerfile after setting the environment variables, the application runs. But for some reason I can't access the flask server, so instead I uncomment the lines at the bottom, since I know those work, and use <code>CMD python ivd_app/__init__.py</code> in the Dockerfile. But the container exits:</p>
<pre><code>flask_1  | Traceback (most recent call last):
flask_1  |   File &quot;ivd_app/__init__.py&quot;, line 38, in &lt;module&gt;
flask_1  |     from ivd_app.database import RedisDatabase
flask_1  | ModuleNotFoundError: No module named 'ivd_app'
</code></pre>
<p>If I remove the import statement and put the <code>RedisDatabase</code> class in the <code>__init__.py</code> file, the entire app works, of course. But I want to separate it into a different file.</p>
<p>How do I fix this?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63201965","Container works but localhost says unable to connect","<docker>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Hi I have made my flask app and I have exposed port 5001 in Docker file.
I pushed it to dockerhub repo and ran on different machine by</p>
<pre><code>docker container run --name XYZ &lt;username&gt;/&lt;repo_name&gt;:&lt;tag&gt;
</code></pre>
<p>The log says that app is running on <a href=""http://127.0.0.1:5001/"" rel=""nofollow noreferrer"">http://127.0.0.1:5001/</a></p>
<p>But if I open that localtion in browser its says</p>
<pre><code>Unable to connect
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM ubuntu:18.04

RUN apt-get update &amp;&amp; apt-get -y upgrade \
    &amp;&amp; apt-get -y install python3.8 \
    &amp;&amp; apt -y install python3-pip \
    &amp;&amp; pip3 install --upgrade pip

WORKDIR /app

COPY . /app

RUN pip3 --no-cache-dir install -r requirements.txt 

EXPOSE 5001

ENTRYPOINT  [&quot;python3&quot;]

CMD [&quot;app.py&quot;]
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63712527","Python package run in Docker results in ERR_EMPTY_RESPONSE","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have created a python package which is a Flask application. I want to run that application in a Docker container. This is my Dockerfile:</p>
<pre><code>FROM python:3.7
FROM pytorch/pytorch

MAINTAINER Nikolay Valkov nikolay1499@gmail.com

# set a directory for the app
WORKDIR /usr/app/

# copy all the files to the container
COPY . .

# install dependencies
RUN pip install --no-cache-dir -r requirements.txt

WORKDIR /usr/app/src/

RUN pip install -e .

WORKDIR /usr/app

# tell the port number the container should expose
EXPOSE 5000

ENV FLASK_APP packagename

# run the command
CMD flask run

</code></pre>
<p>My app is bound to 0.0.0.0 so it can be accessed from outside the Docker container like this:</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == '__main__':
    app.run(host='0.0.0.0')
</code></pre>
<p>The app variable is declared in <code>app.py</code> and imported in <code>__init__.py</code> if that information is required.
When I run the flask package locally without Docker everything works but when I run the container localhost:5000 gives me ERR_EMPTY_RESPONSE. I used the command <code>docker run -p 5000:5000 nameofimage</code>.
Any ideas why this happens? What am I missing?</p>
<p>Edit I was asked to post the python code:
<code>setup.py</code></p>
<pre><code>from setuptools import setup

def readme():
    with open(&quot;README.rst&quot;) as f:
        return f.read()

setup(name = &quot;generateme&quot;,
      version = &quot;0.1.2&quot;,
      description = &quot;Flask application to generate images with Generative Adversarial networks&quot;,
      long_description = readme(),
      url = &quot;https://github.com/Nikolay1499/GenerateMe&quot;,
      author = &quot;Nikolay Valkov&quot;,
      author_email = &quot;nikolay1499@gmail.com&quot;,
      license = &quot;MIT&quot;,
      packages = [&quot;generateme&quot;],
      install_requires = [
          &quot;flask&quot;,
          &quot;gevent&quot;,
          &quot;numpy&quot;,
          &quot;Pillow&quot;,
          &quot;matplotlib&quot;,
          &quot;future&quot;,
      ],
      zip_safe = False,
      include_package_data = True,
)
</code></pre>
<p><code>__init__.py</code></p>
<pre><code>from flask import Flask
import os
from generateme.app import app
from generateme.app import index, showImageConv, showImageLinear, showImageStyle
IMAGE_FOLDER = os.path.join(&quot;static&quot;, &quot;Photos&quot;)

def create_app():
  app = Flask(__name__)
  app.config[&quot;UPLOAD_FOLDER&quot;] = IMAGE_FOLDER
  app.config[&quot;TEMPLATES_AUTO_RELOAD&quot;] = True
  app.add_url_rule(&quot;/&quot;, &quot;index&quot;, index)
  app.add_url_rule(&quot;/index&quot;, &quot;index&quot;, index)
  app.add_url_rule(&quot;/getStyleImage&quot;, &quot;showImageStyle&quot;, showImageStyle)
  app.add_url_rule(&quot;/getConvImage&quot;, &quot;showImageConv&quot;, showImageConv)
  app.add_url_rule(&quot;/getLinearImage&quot;, &quot;showImageLinear&quot;, showImageLinear)
  return app
</code></pre>
<p><code>app.py</code></p>
<pre><code>from flask import Flask, render_template, send_file, Response, url_for
from PIL import Image
import numpy as np
import io
import os
from generateme.LinearGan import getLinearImage
from generateme.DCGan import getConvImage
from generateme.StyleGan import getStyleImage

IMAGE_FOLDER = os.path.join(&quot;static&quot;, &quot;Photos&quot;)

app = Flask(__name__)
app.config[&quot;UPLOAD_FOLDER&quot;] = IMAGE_FOLDER
app.config[&quot;TEMPLATES_AUTO_RELOAD&quot;] = True
folder = os.path.dirname(os.path.abspath(__file__))

@app.route(&quot;/&quot;)
@app.route(&quot;/index&quot;)
def index():
    return render_template(&quot;index.html&quot;)

@app.route(&quot;/getStyleImage&quot;)
def showImageStyle():
    getStyleImage()
    return getImage()
    
@app.route(&quot;/getConvImage&quot;)
def showImageConv():
    getConvImage()
    return getImage()
    
@app.route(&quot;/getLinearImage&quot;)
def showImageLinear():
    getLinearImage()
    return getImage()

def getImage():
    file = my_file = os.path.join(folder, &quot;static/Photos/image.png&quot;)
    img = Image.open(file)
    file_object = io.BytesIO()

    img.save(file_object, &quot;PNG&quot;)  
    file_object.seek(0)

    response = send_file(file_object, mimetype=&quot;image/PNG&quot;)
    response.headers[&quot;Cache-Control&quot;] = &quot;no-store, no-cache, must-revalidate, max-age=0&quot;
    return response
    
    
if __name__ == '__main__':
    app.run(host=&quot;0.0.0.0&quot;,port=5000)
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63652631","Flask app not exposed properly on localhost","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a flask app, that i want to dockerize, so i have used the following Dockerfile</p>
<pre><code>FROM python:3.8.0-alpine
WORKDIR /project
ADD . /project
RUN pip install -r requirements.txt
EXPOSE 5000
CMD [&quot;python&quot;,&quot;application.py&quot;]
</code></pre>
<p>I built the container with a tag:
<code>docker build -t flask-sms-service:latest .</code>
and run it with the following command:
<code>docker run -d -p 5000:5000 flask-sms-service</code></p>
<p>I made sure to expose port 5000, for the server to be accessible. But when i access it on <code>127.0.0.1:5000</code> i can't access the container.</p>
<p>What did i do wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","63816090","Can't access React and Flask services created by Docker-Compose on Windows 10","<reactjs><docker><flask><docker-compose><dockerfile>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have a docker-compose file that defines services for both a Flask backend and a React frontend. It looks like that everything is working correctly, but I can't access either applications. I get no responses returned for both services.</p>
<p>The backend and frontend services both build and run successfully. Here is the flask service output when I run <code>docker-compose up</code>:</p>
<pre><code>flask_1     |  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
flask_1     |  * Restarting with stat
flask_1     |  * Debugger is active!
flask_1     |  * Debugger PIN: 319-453-363
</code></pre>
<p>But when I try to access <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a>, I get the following</p>
<p><a href=""https://i.stack.imgur.com/OR9JG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OR9JG.png"" alt=""enter image description here"" /></a></p>
<p>It looks like Docker is exposing the ports correctly:</p>
<p><a href=""https://i.stack.imgur.com/CFJSF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CFJSF.png"" alt=""enter image description here"" /></a></p>
<p>docker-compose.yml</p>
<pre><code>version: '3.8'

services:
  flask:
    build: 
      context: ./backend/
      dockerfile: Dockerfile
    ports:
      - &quot;5000:5000&quot;
  frontend:
    build: ./frontend/webapp/
    stdin_open: true
    volumes:
      - './frontend/webapp:/app/webapp'
      - '/app/frontend/node_modules'
    ports:
      - &quot;3002:3002&quot;
</code></pre>
<p>Dockerfile for frontend:</p>
<pre><code>FROM node:latest
WORKDIR /app/frontend
COPY package.json /app/frontend
COPY . /app/frontend
RUN npm install
EXPOSE 3002
CMD [&quot;npm&quot;, &quot;start&quot;]

</code></pre>
<p>Dockerfile for backend:</p>
<pre><code>FROM anaconda3:5.0.1
WORKDIR /app/Backend
COPY . .
EXPOSE 5000
CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p><strong>UPDATE:</strong></p>
<p>Turned out the reason that this wasn't working was because React runs default on port 3000. In my docker-compose file, I had</p>
<pre><code>ports:
      - &quot;3002:3002&quot;
</code></pre>
<p>I changed the first '3002' to '3000' and was able to access it.</p>
"
"31324981","How to access host port from docker container","<docker><docker-container>","63653049","How would I connect to a MySQL on the host machine from inside a docker kubernetes pod?","<mysql><docker><kubernetes><windows-subsystem-for-linux><kind>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<p>I have a MySQL server running on my local machine, accessible at <code>localhost:3306</code>.</p>
<p>I also have a KinD cluster set up on WSL2, with Docker integration. How would I access the server from a python application inside a kubernetes pod?</p>
"
"31961934","Why docker login command saves unencrypted password on my computer?","<docker><docker-machine>","63357792","Unecrypted Docker password","<docker><dockerfile>","<p>I've used <code>docker login</code> command and passed my credentials and I got warning <code>WARNING: login credentials saved in /Users/{my_username}/.docker/config.json</code>. I've checked that file and indeed my password is there unencrypted (base64 encoded only).</p>

<p>Why docker saved my password unencrypted? Shouldn't it save it in my key-chain (I am on Mac OS) or instead of saving password just generate some access token or something like that?</p>
","<p>I recently built a docker image and upon completion docker throws a message that states:</p>
<p>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</p>
<p>I tried to navigate to the folder to see whether this was true but the file path doesn't exist.What could be the issue here? Is my password safe?</p>
"
"32723111","How to remove old and unused Docker images","<docker><docker-image>","66361693","How do I delete multiple images in docker?","<docker>","<p>When running Docker for a long time, there are a lot of images in system. How can I remove all unused Docker images at once safety to free up the storage?</p>

<p>In addition, I also want to remove images pulled months ago, which have the correct <code>TAG</code>.</p>

<p>So, I'm not asking for removing untagged images only. I'm searching for a way to remove general unused images, which includes both untagged and other images such as pulled months ago with correct <code>TAG</code>.</p>
","<p>I have created a lot of images but I don't know how to delete them efficiently.</p>
<p>Does anyone know good command to remove the images smartly?</p>
<p>Thank you very much!</p>
"
"33322103","Multiple FROMs - what it means","<docker><dockerfile>","63367929","Dockerfile: Are multiple FROM (s) allowed?","<docker><dockerfile><cypress>","<p>I want to build a docker image for the <a href=""https://github.com/Linkurious/linkurious.js"" rel=""noreferrer"">Linkurious</a> project on github, which requires both the Neo4j database, and Node.js to run.</p>

<p>my first approach was to declare a base image for my image, containing Neo4j.  The reference docs do not define ""base image"" in any helpful manner:</p>

<blockquote>
  <p>Base image:
  An image that has no parent is a base image</p>
</blockquote>

<p>from which I read that I may only have a base image if that image has no base image itself.</p>

<p>but what is a base image? does it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?</p>

<p>reading the Docker reference (see: <a href=""https://docs.docker.com/reference/builder/#from"" rel=""noreferrer"">https://docs.docker.com/reference/builder/#from</a>) I see: </p>

<blockquote>
  <p>FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command.</p>
</blockquote>

<p>do I want to create multiple images? it would seem what I want is to have a single image that contains the contents of other images e.g. neo4j and node.js</p>

<p>I've found no directive to declare dependencies in the reference manual.  are there no dependencies like in RPM where in order to run my image the calling context must first install the images it needs?</p>

<p>I'm confused...</p>
","<p>I am creating a Docker image that needs to contain both:</p>
<ol>
<li>Ex: Company Foo's image of Jenkins support functions: <code>docker.foo.com/jenkins-build:latest</code></li>
<li><a href=""https://github.com/cypress-io/cypress-docker-images/tree/master/included/4.12.1"" rel=""nofollow noreferrer"">cypress/included:4.12.1</a></li>
</ol>
<p>This image will be labeled <code>docker.foo.com/cypress-build</code></p>
<p><code>Dockerfile</code> for this image is scripted as follows:</p>
<pre><code># Image containing company Foo's Jenkins build
FROM docker.foo.com/jenkins-build:latest

# Main image that include all operating system dependencies necessary to run Cypress,
# but NOT the test runner itself.
FROM cypress/included:4.12.1
</code></pre>
<p>Is this <code>Dockerfile</code> valid in scripting with multiple <code>FROM</code>?</p>
<p>If not, then what must be done to correct it?</p>
<p>Thank you, much appreciate any feedback!</p>
"
"34048838","Pod Communication","<kubernetes>","63448591","How two kubernetes pods can communicate?","<python><docker><kubernetes><deployment><data-science>","<p>How does the communication between two different pods happen in Kubernetes?</p>

<p>In my case I have two pods: <em>frontend</em> and <em>backend</em>, both have different containers.
I want my frontend pod to communicate with the backend pod but I don't want to use backend pod's IP( i.e.  hard coded). </p>

<p>Is it possible through services? </p>
","<p>I have created 2 PODS. In one POD I have kept all the machine learning model pickle file and in another one I have kept flask api code(app.py). How flaskapi POD will communicate with mlPickleFile POD?</p>
"
"35218194","what is 'z' flag in docker container's volumes-from option?","<docker>","63634293","docker volume on file system","<docker><docker-compose>","<p>While going through the docker docs, I came across volumes-from (<a href=""https://docs.docker.com/engine/reference/commandline/run/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/commandline/run/</a>)   option for docker run command. 
I didn't understand the differences between <code>ro, rw, and z</code> option provided as-<br>
 <code>$ docker run --volumes-from ba8c0c54f0f2:ro -i -t ubuntu pwd</code><br>
In the above command the <code>ro</code> option is replaced with <code>z</code>. I will be thankful if anyone explores on differences of using these options.</p>
","<p>I have a docker-compose file like that :</p>
<pre><code>version: &quot;2&quot;
services:
  orthanc:
    build: orthanc
    restart: unless-stopped
    ports: [&quot;${PORT}:8042&quot;]
    volumes: [&quot;orthanc-storage:/var/lib/orthanc/db:Z&quot;]
    [...]
volumes:
  orthanc-storage:
</code></pre>
<p>1/ I can't understand how to define <code>orthanc-storage</code> with something like <code>C:/tmp/db</code></p>
<p>2/ what means the <code>Z</code> ?</p>
<p>Thank you for your answers</p>
"
"35560894","Is Docker ARG allowed within CMD instruction","<docker><docker-compose>","63122324","docker argument not getting detected","<docker><kubernetes><jenkins-pipeline>","<p>I have a Dockerfile where an <code>ARG</code> is used in the <code>CMD</code> instruction:</p>

<pre><code>ARG MASTER_NAME
CMD spark-submit --deploy-mode client --master ${MASTER_URL}
</code></pre>

<p>The arg is passed via docker-compose:</p>

<pre><code>  spark:
    build:
      context: spark
      args:
        - MASTER_URL=spark://master:7077
</code></pre>

<p>However, the <code>ARG</code> does not seem to get expanded for <code>CMD</code>. After I <code>docker-compose up</code>.</p>

<p>Here's what inspect shows:</p>

<pre><code>docker inspect  -f ""{{.Name}} {{.Config.Cmd}}"" $(docker ps -a -q)
/spark {[/bin/sh -c spark-submit --deploy-mode client --master ${MASTER_URL}]}
</code></pre>
","<p><strong>Problem</strong></p>
<p>I am trying to send a argument to dockerfile using <code>--build-arg</code> but my final <code>CMD</code> is not recognizing it.</p>
<p><strong>Code:</strong></p>
<pre><code>docker image build -t *image_name* --build-arg env=${ENVIRONMENT} .
</code></pre>
<p><strong>Dockerfile:</strong></p>
<pre><code>FROM openjdk:11-slim
VOLUME /tmp
ARG env
RUN echo ${env}
RUN echo &quot;config-${env}.yml&quot;

COPY configs/* ./
EXPOSE 8080
EXPOSE 8081
CMD java -jar something server config-$env.yml
</code></pre>
<p>The kubernetes clusters are giving <code>CrashLoopBackOff</code></p>
<p><strong>What I did:</strong></p>
<pre><code>kubectl -n abc logs -p xyz
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>java.io.FileNotFoundException: File config-.yml not found
</code></pre>
"
"37461868","Difference between RUN and CMD in a Dockerfile","<docker><dockerfile>","63138328","I was creating a simple DOCKERFILE . I have attached code below . Can someone specify to start service in DOCKERFILE?","<apache><docker><containers><webserver>","<p>I'm confused about when should I use <code>CMD</code> vs <code>RUN</code>. For example, to execute bash/shell commands (i.e. <code>ls -la</code>) I would always use <code>CMD</code> or is there a situation where I would use <code>RUN</code>? Trying to understand the best practices about these two similar <code>Dockerfile</code> directives.</p>
","<pre><code>FROM centos:latest AS build

RUN echo $USER

RUN yum update -y

RUN yum install  -y  httpd 

USER root

ADD index.html /var/www/html/index.html

ENTRYPOINT [ &quot;/usr/sbin/httpd&quot; ]

CMD [ &quot;-D&quot;,&quot;BACKGROUND&quot; ]

EXPOSE 80
</code></pre>
<p>Can someone please explain me difference between <code>CMD</code>, <code>RUN</code>, <code>ENTRYPOINT</code>. It is so confusing. And How do I start httpd service inside the <code>Dockerfile</code>.</p>
"
"39468841","Is it possible to start a stopped container from another container","<docker><docker-container>","66324634","Deploy django app that uses docker run command","<python><django><docker><heroku><docker-compose>","<p>There are two containers A and B. Once container A starts, one process will be executed, then the container will stop. Container B is just an web application (say expressjs). Is it possible to kickstart A from container B ?</p>
","<p>I have a Django app that I want to deploy on Heroku. The application calls the command <code>docker run hello-world</code> in views.py when the button is pressed. As far as I know, on Heroku you can set up a stack on Ubuntu or Docker.</p>
<ol>
<li>When I use Ubuntu, I install docker using <code>heroku-buildpack-apt</code>. Unfortunately I then get the following error:</li>
</ol>
<pre><code>docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
See 'docker run --help'.
</code></pre>
<p>I tried using <code>docker run -v /var/run/docker.sock:/var/run/docker.sock hello-world</code> but to no effect.</p>
<p>Aptfile</p>
<pre><code>uidmap
iptables
daemon
apt-transport-https
ca-certificates
curl
gnupg-agent
software-properties-common
http://security.ubuntu.com/ubuntu/pool/universe/d/docker.io/docker.io_19.03.8-0ubuntu1.20.04.1_amd64.deb
</code></pre>
<ol start=""2"">
<li>When I try to dockerize the django app (and apply docker in docker), I have the same problem. I tested it locally. After typing <code>django-compose up</code> and clicking button, I get</li>
</ol>
<pre><code>docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
See 'docker run --help'.
</code></pre>
<p>Dockerfile</p>
<pre><code>FROM python:3.8-buster

ENV PATH=&quot;/scripts:${PATH}&quot;
ENV LD_LIBRARY_PATH=/usr/local/lib

COPY ./requirements.txt /requirements.txt

RUN apt-get update
RUN pip install --upgrade pip
RUN pip install -r /requirements.txt

# install Docker
RUN apt-get remove docker docker-engine docker.io runc
RUN apt-get install -yq --no-install-recommends apt-utils
RUN apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common 

RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -

RUN add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/debian \
   $(lsb_release -cs) \
   stable&quot;

RUN apt-get update
RUN apt-get install -y docker-ce docker-ce-cli containerd.io

RUN usermod -aG docker $(whoami)


RUN mkdir /app
COPY ./app /app
WORKDIR /app
COPY ./scripts /scripts

RUN chmod +x /scripts/*

CMD [&quot;entrypoint.sh&quot;]
</code></pre>
<p>entrypoint.sh</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh

set -e

python manage.py collectstatic --noinput

uwsgi --socket :8000 --master --enable-threads --module app.wsgi
</code></pre>
<p>docker-compose.yml</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.7'

services:
  app:
    build:
      context: .
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - ./app:/app
    command: sh -c &quot;python manage.py runserver 0.0.0.0:8000&quot;
    environment:
      - DEBUG=1
</code></pre>
<p>Is it even possible to deploy such an app? In both cases docker is installed, because I can type <code>docker --help</code>.</p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","63164968","Combine multiple docker containers without copying","<docker>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p><del>Suppose, for example, I have a project that needs to run both Python and Go. For Python I can use the <code>python:latest</code> container in DockerHub and for Go I can use <code>golang:latest</code>. However, how can I combine the two containers in order to access both languages (i.e. applications) at the same time? </del></p>
<p><del>Some possible solutions is to install using <code>apt-get install python3</code> in the go container or to do <code>COPY --from=python</code> in the Dockerfile. However, The actual problem I have involves something much more complicated (I have multiple applications and multiple versions to deal with, and they are large and difficult to copy). So ideally I can link all the files, instead of installing again or copying the files. </del></p>
<p>Edit: I have a project that is based on Python, go, swi-prolog, and some other programming languages. That project uses all these programming languages and is sensitive to the version of each programming language used. So I need to be able to change the version of each programming language quickly (e.g. from Python 3.8.5 to 3.7.8, while still using golang 1.15rc1).</p>
<p>I have limited amount of disk space for docker images (only around 1GB). Python takes 350 MB, Go takes 300MB, swi-prolog takes 50MB, etc. It is possible for me to pull the docker images, but I CANNOT use <code>COPY --from=</code> in my docker file (since copy will consume disk space, and I run out of disk space before I can finish a multi-stage build. So multi-stage builds is NOT a viable solution for my problem). I also want to just download container images from dockerhub, instead of building the images myself.</p>
<p>The ideal case is to have something similar to <code>COPY --from=</code> but hard-link all the files instead of copying. Or maybe mount a container inside another container. Is it possible?</p>
"
"39657058","Installing GD in Docker","<php><docker><dockerfile>","63795830","configure: error: Package requirements (libpng) were not met when install gd on docker image","<docker>","<p>I am a complete Docker novice but am having to maintain an existing system. The Dockerfile I am using is as below:</p>

<pre><code>FROM php:5.6-apache

RUN docker-php-ext-install mysql mysqli

RUN apt-get update -y &amp;&amp; apt-get install -y sendmail

RUN apt-get update &amp;&amp; \
    apt-get install -y \
        zlib1g-dev 

RUN docker-php-ext-install mbstring

RUN docker-php-ext-install zip

RUN docker-php-ext-install gd
</code></pre>

<p>When I run 'docker build [sitename]' everything seems ok until I get the error:</p>

<pre><code>configure: error: png.h not found.
The command '/bin/sh -c docker-php-ext-install gd' returned a non-zero code: 1
</code></pre>

<p>What is the cause of this error?</p>
","<p>I want to create an php image and install gd component on it. I have used following commands:</p>
<pre><code>RUN apt-get update &amp;&amp; \
    apt-get install -y \
        zlib1g-dev 

RUN docker-php-ext-install gd

</code></pre>
<p>but I get this error :</p>
<pre><code>configure: error: Package requirements (libpng) were not met:

No package 'libpng' found

Consider adjusting the PKG_CONFIG_PATH environment variable if you
installed software in a non-standard prefix.

Alternatively, you may set the environment variables PNG_CFLAGS
and PNG_LIBS to avoid the need to call pkg-config.
See the pkg-config man page for more details.

</code></pre>
"
"5836674","Why does DEBUG=False setting make my django Static Files Access fail?","<django><django-views><django-staticfiles>","61918466","Django (wagtail) serving static with Docker and docker-compose","<django><docker><gunicorn><wagtail>","<p>Am building an app using Django as my workhorse. All has been well so far - specified db settings, configured static directories, urls, views etc. But trouble started sneaking in the moment I wanted to render my own beautiful and custom 404.html and 500.html pages.</p>

<p>I read the docs on custom error handling, and set necessary configurations in UrlsConf, created corresponding views and added the 404.html and the 500.html to my app's template directory (specified in the settings.py too).</p>

<p>But the docs say <code>you can actually view custom error views until Debug is Off</code>, so I did turn it off to test my stuff, and that's when stuff goes berserk!</p>

<p>Not only do I fail to view the custom 404.html (actually, it loads, but because my error pages each contain a graphic error message -as some nice image), the source of the error page loads, but nothing else loads! Not even linked CSS or Javascript!</p>

<p>Generally, once I set <code>DEBUG = False</code>, all views will load, but any linked content (CSS, Javascript, Images, etc) wont load! What's happening? Is there something am missing, concerning static files and the <code>DEBUG</code> setting?</p>
","<p><strong>Dockerfile</strong></p>

<pre><code>FROM python:3.7

# Set environment variables
ENV PYTHONUNBUFFERED 1
ENV DJANGO_SETTINGS_MODULE project.settings.production

RUN apt-get update
RUN apt-get -y install apt-utils build-essential libsass-dev
RUN apt-get -y install postgresql
RUN apt-get -y install postgresql-client

COPY ./requirements.txt /code/requirements.txt
RUN pip install --upgrade pip
# Install any needed packages specified in requirements.txt
RUN pip install -r /code/requirements.txt
RUN pip install gunicorn

# Copy the current directory contents into the container at /code/
COPY . /code/

# Set the working directory to /code/
WORKDIR /code/

RUN useradd my_user
RUN chown -R my_user /code
USER my_user

# collect static files
RUN python manage.py collectstatic --noinput --clear

CMD exec gunicorn project.wsgi:application --bind 0.0.0.0:8000 --workers 3
EXPOSE 8000
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3'
services:
    db:
        # db configuration here
    web:
        build: .
        depends_on:
            - db
        volumes:
            - .:/code
        environment:
            - DJANGO_SETTINGS_MODULE=project.settings.dev
        command: bash -c ""
            python manage.py collectstatic --noinput --clear &amp;&amp;
            python manage.py runserver 0:8000 --insecure""
        ports:
            - '80:8000'
</code></pre>

<p><strong>settings/base.py</strong></p>

<pre><code>STATICFILES_FINDERS = [
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
]

STATICFILES_DIRS = [
    os.path.join(PROJECT_DIR, 'static'),
]

# ManifestStaticFilesStorage is recommended in production, to prevent outdated
# Javascript / CSS assets being served from cache (e.g. after a Wagtail upgrade).
# See https://docs.djangoproject.com/en/3.0/ref/contrib/staticfiles/#manifeststaticfilesstorage
STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'

# Application static files are collected here
STATIC_ROOT = os.path.join(BASE_DIR, 'static')
STATIC_URL = '/static/'
</code></pre>

<p>Now, if I run this project with <code>DEBUG=True</code> it will serve the static files normally the problem happens when I do <code>./manage.py collectstatic</code> and set <code>DEBUG=False</code>, when I run <code>docker-compose up</code> it will spin up the services and I can see the website working but no static file is being served, all of them fail with 404, the url that django/wagtail is generating for <code>/static</code> is not working.</p>

<p>What am I missing ?</p>
"
"9968916","print every nth line into a row using gawk","<printing><awk><line><gawk>","62368803","print nth, (n+3)rd, ((n+3)+3)rd,.... lines using awk","<bash><docker><awk>","<p>I have a very huge file in which I need to obtain every nth line and print it into a row.</p>

<p>My data:</p>

<pre><code>1      937  4.320194
2      667  4.913314
3      934  1.783326
4      940  -0.299312
5      939  2.309559
6      936  3.229496
7      611  -1.41808
8      608  -1.154019
9      606  2.159683
10     549  0.767828
</code></pre>

<p>I want my data to look like this:</p>

<pre><code>1      937  4.320194
3      934  1.783326
5      939  2.309559
7      611  -1.41808
9      606  2.159683
</code></pre>

<p>This is of course an example, I want every 10th line for my huge data file.  I tried this so far:</p>

<pre><code> NF == 6 {
     if(NR%10) {print;}
     }
</code></pre>
","<pre><code>1
ADD file ... in /
21.44 MB
2
CMD [""bash""]
0 B
3
ENV DEBIAN_FRONTEND=noninteractive
0 B
4
COPY file:6229791a987783682b536a308b0d287c470ec1bfa0be9b6b3471feab66706874 in /install.sh
925 B
5
/bin/sh -c /install.sh &amp;&amp; rm
38.37 MB
6
ARG SPLUNK_FILENAME
0 B
7
ARG SPLUNK_BUILD_URL
0 B
8
ARG SPLUNK_DEFAULTS_URL
0 B
9
ENV SPLUNK_HOME=/opt/splunkforwarder SPLUNK_GROUP=splunk SPLUNK_USER=splunk
0 B
10
|1 SPLUNK_BUILD_URL=http://releases.splunk.com/dl/orangeswirl_builds/7.2.1-20181119-0400/splunkforwarder-7.2.1-be11b2c46e23-Linux-x86_64.tgz /bin/sh -c groupadd
23.32 MB
11
USER splunk
0 B
12
COPY dir:6e993b24f9f98057163ef8bd8c62e4861acd5bfa0b2053e0e38f2945df2a8e2a in /opt/ansible
20.04 KB
13
COPY multi:96216845d7aa8a7c7fe1625c00b24c9cf8c2077b7bfb6cd2170777135f01fa20 in /sbin/
2.92 KB
14
COPY multi:fb03ac2df458b42167388f14efe28124d79c80127da9acdba91b01e5ef99d67d in /tmp/
2.94 KB
15
EXPOSE 8088 8089 9997
0 B
16
VOLUME [/opt/splunkforwarder/etc /opt/splunkforwarder/var]
0 B
17
HEALTHCHECK &amp;{[""CMD-SHELL"" ""/sbin/checkstate.sh ||
0 B
18
ENTRYPOINT [""/sbin/entrypoint.sh""]
0 B
19
CMD [""start-service""]
0 B
Command

</code></pre>

<p>I have copied the dockerfile contentfrom dockerhub but it comes with line number and sizes, I want to create an awk control file to extract just the dockerfile content without the numbers or sizes. That is, I want to print 2nd,5th,8th,11th and so on lines. I can do it with bash scripting but I'm learning awk and want to do it that way.</p>

<p>Edit:</p>

<pre><code>ADD file ... in /
CMD [""bash""]
ENV DEBIAN_FRONTEND=noninteractive
COPY file:6229791a987783682b536a308b0d287c470ec1bfa0be9b6b3471feab66706874 in /install.sh
/bin/sh -c /install.sh &amp;&amp; rm
ARG SPLUNK_FILENAME
ARG SPLUNK_BUILD_URL
ARG SPLUNK_DEFAULTS_URL
ENV SPLUNK_HOME=/opt/splunkforwarder SPLUNK_GROUP=splunk SPLUNK_USER=splunk
|1 SPLUNK_BUILD_URL=http://releases.splunk.com/dl/orangeswirl_builds/7.2.1-20181119-0400/splunkforwarder-7.2.1-be11b2c46e23-Linux-x86_64.tgz /bin/sh -c groupadd
USER splunk
COPY dir:6e993b24f9f98057163ef8bd8c62e4861acd5bfa0b2053e0e38f2945df2a8e2a in /opt/ansible
COPY multi:96216845d7aa8a7c7fe1625c00b24c9cf8c2077b7bfb6cd2170777135f01fa20 in /sbin/
COPY multi:fb03ac2df458b42167388f14efe28124d79c80127da9acdba91b01e5ef99d67d in /tmp/
EXPOSE 8088 8089 9997
VOLUME [/opt/splunkforwarder/etc /opt/splunkforwarder/var]
HEALTHCHECK &amp;{[""CMD-SHELL"" ""/sbin/checkstate.sh ||
ENTRYPOINT [""/sbin/entrypoint.sh""]
CMD [""start-service""]
Command
</code></pre>

<p>this should be the final output.</p>

<p>I'm at </p>

<pre><code>awk 'BEGIN{NR==2 print $0}{NR==5,NR==8,... print $0}' 
</code></pre>

<p>I'm not sure if we can sort in BEGIN block and how to use increment logic with NR varaible.</p>
"
"18318076","Python-magic installation error - ImportError: failed to find libmagic","<python-2.7><python-magic>","66158406","Python Docker Image ImportError: python-magic","<python><docker><pip><python-magic>","<p>I am trying to install python-magic for Windows and I have followed all the instructions in <a href=""https://github.com/ahupp/python-magic"" rel=""nofollow"">https://github.com/ahupp/python-magic</a> and repeated the process several times but I am still getting this error:</p>

<pre><code>ImportError: failed to find libmagic. Check your installation
</code></pre>

<p>I have magic1.dll (along with the two other files the docs specified) in C:\Windows\System32 so I am not sure what the issue is. I would appreciate any help or workarounds.</p>
","<p>Trying to create a very basic python image with a Dockerfile:</p>
<pre><code>FROM python:3.8-slim

RUN pip3 install python-magic
</code></pre>
<p>The build works fine but when I try to run the container and simply <code>import magic</code>, python gives:</p>
<pre><code>Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
File &quot;/usr/local/lib/python3.8/site-packages/magic.py&quot;, line 201, in &lt;module&gt;
    raise ImportError('failed to find libmagic.  Check your installation')
ImportError: failed to find libmagic.  Check your installation
</code></pre>
<p>I have googled and tried all different types of installtions such as:</p>
<pre><code>RUN pip3 install python-magic
RUN pip3 install python-magic==0.4.18
RUN pip3 install python-libmagic
</code></pre>
<p>I don't know what is so special about this package but for some reason I can't seem to access it in my docker image. Works fine on my local mac computer.</p>
"
"18460016","Connect from one Docker container to another","<rabbitmq><celery><docker>","62167356","Communicate between two Docker-Container in Docker for Windows","<docker><networking><port><docker-for-windows>","<p>I want to run rabbitmq-server in one docker container and connect to it from another container using celery (<a href=""http://celeryproject.org/"" rel=""noreferrer"">http://celeryproject.org/</a>)</p>

<p>I have rabbitmq running using the below command...</p>

<pre><code>sudo docker run -d -p :5672 markellul/rabbitmq /usr/sbin/rabbitmq-server
</code></pre>

<p>and running the celery via</p>

<pre><code>sudo docker run -i -t markellul/celery /bin/bash
</code></pre>

<p>When I am trying to do the very basic tutorial to validate the connection on <a href=""http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html"" rel=""noreferrer"">http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html</a></p>

<p>I am getting a connection refused error:</p>

<blockquote>
  <p>consumer: Cannot connect to amqp://guest@127.0.0.1:5672//: [Errno 111]
  Connection refused.</p>
</blockquote>

<p>When I install rabbitmq on the same container as celery it works fine.</p>

<p>What do I need to do to have container interacting with each other?</p>
","<p>this is my first question and I hope I summarize my problem as good as needed.</p>

<p>I am using docker for windows, which is, well known, not running on a VM. Windows uses Hyper-V.</p>

<p>I want to allow communication betweetn 2 running containers.</p>

<p>Container 1 (Jboss EAP, with AMQ): <br>
<code>docker run -p 9990:9990 -d jboss1</code></p>

<p>Container 2 (Jboss EAP, with AMQ): <br>
<code>docker run -p 9555:9555 -d jboss2</code></p>

<p>Booth are accessible with my local browser. I am using the base Image ""ubi7/ubi7-init"" version 7.6.</p>

<p>What should I do, to allow communication? curl to each other does not work. Do I need to expose ports? If I need to, how can I do that?</p>

<p>Thanks</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","66180061","Dynamically bind and unbind TCP port in a running docker container","<docker><networking><docker-compose>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>When using docker and docker-compose, ports are binded and reserved at the beginning of the execution process.
Since some applications outside the container can compete (and potentially crash) for the same ports, it would be interesting a solution to dynamically bind / unbind port for running docker containers.</p>
<p>One existing solution is is using variables to define the binding ports in docker-compose, but once started the container can not have any port binded or unbinded.</p>
<pre><code>ports:
  # Service1 
  - 14443:${PORTSERVICE1}
  # Service2
  - 13306:{PORTSERVICE2}
</code></pre>
<p>Is there any way to change ports binding dynamically after container initialization?</p>
"
"20813486","Exploring Docker container's file system","<linux><docker><filesystems>","66141391","How to view files inside docker image without running it?","<docker>","<p>I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.</p>

<p>What would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.</p>
","<p>Sometimes running the docker image fails so ssh’ing into the container is not an option. in that cases how do we see the content inside container?</p>
<p>There is a existing question but mistakenly marked as duplicate. <a href=""https://stackoverflow.com/questions/53243471/how-to-browse-docker-image-without-running-it"">how to browse docker image without running it?</a></p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","61849319","Difference between -p during image run and EXPOSE in dockerfile","<docker><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I've a website running on node and it is hosted at port 8080 inside a docker container. To use it in the host machine, I need to use -p against a relevant port of the host machine. There isn't any Web Server like ngInx here. It's just npm start.</p>

<p>Now, when should I use EXPOSE in docker file and why is it required ? How is it different from -p during run ?</p>

<p>In this case, is EXPOSE required ? I can directly run with -p and i can access the site with out EXPOSE as well. </p>
"
"23558425","How do I get the local IP address in Go?","<go>","62101269","Getting Container IP in a Go application","<docker><go><dockerfile><containers>","<p>I want to get the computer's IP address. I used the code below, but it returns <code>127.0.0.1</code>.</p>

<p>I want to get the IP address, such as <code>10.32.10.111</code>, instead of the loopback address. </p>

<pre><code>name, err := os.Hostname()
if err != nil {
     fmt.Printf(""Oops: %v\n"", err)
     return
}

addrs, err := net.LookupHost(name)
if err != nil {
    fmt.Printf(""Oops: %v\n"", err)
    return
}

for _, a := range addrs {
    fmt.Println(a)
}  
</code></pre>
","<p>I have a simple Go application running inside a docker container. I wish to get the container IP of that container inside the Go application itself. How can I do that? I am a newbie so please help me out, if this question sounds silly</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61773184","How to connect from docker-compose to host","<php><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>How can I connect from docker-compose to the host. In the docker-compose there is an nginx and php running and I would like for the php container to connect to a mysql database that is running on the host. the nginx and php containers are connected via a docker-compose network.</p>

<p>Here is the docker-compose file:</p>

<pre><code>version: '3'

networks:
    test:

services:
    nginx:
        some stuff
        networks:
            - test

    php:
        some stuff
        networks:
            - test
            - HOST?????
</code></pre>

<p>I think in the php application I would then connect to ""localhost"" and docker connects this the host.</p>

<p>Is this possible? I have read, that it is possible for individual containers to be connected to the host, but don't know if it is possible when docker-compose is used.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","61986076","how does containered springboot image connect itself to mysql5.7 which runs locally?","<java><mysql><spring-boot><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have written a very simple demo project, you can download it by clicking <a href=""https://d613.top/filebedcc/static/d874ca730b814f3ba5bb90d67.zip"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The above java project will be containerized as an docker image. But it needs to connect to the local mysql database which is not containerized.</p>

<p>I tried changing the value of <code>spring.datasource.url</code> from <code>jdbc:mysql://localhost:3306/db?characterEncoding=utf8&amp;useSSL=false</code> to <code>jdbc:mysql://192.168.1.105:3306/db?characterEncoding=utf8&amp;useSSL=false</code> in the <code>application.yaml</code>. This change seems to bring success.</p>

<p>so I Change the value of <code>spring.datasource.url</code> back to the original and tried <code>docker run --network=host .....</code>,but this didn't success.. why?</p>

<p>Thanks a lot.</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","62080978","Dockerfile copy to one folder up","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I have below structure</p>

<pre><code>folder1
    Dockerfile
    start.sh
folder2
    config.ini
someotherfiles
</code></pre>

<p>inside the Dockerfile</p>

<p>i have added</p>

<pre><code>COPY ../folder2/config.ini /
COPY ../somotherfiles /
</code></pre>

<p>Now when I build the docker, i'm getting error <code>file not found.</code> </p>

<p>how can I copy the files which is one folder up where the Dockerfile exists? </p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","66442522","Running docker when dockerfile is in folder inside a project and the solution includes multiple projects","<docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I added a docker support to a dot.net core application, this is what I got</p>
<blockquote>
<p>#See <a href=""https://aka.ms/containerfastmode"" rel=""nofollow noreferrer"">https://aka.ms/containerfastmode</a> to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.</p>
</blockquote>
<pre><code>FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build
WORKDIR /src
COPY [&quot;SMSys.csproj&quot;, &quot;&quot;]
COPY [&quot;../DataLayer/DataLayer.csproj&quot;, &quot;../DataLayer/&quot;]
COPY [&quot;../Utilities/Utilities.csproj&quot;, &quot;../Utilities/&quot;]
COPY [&quot;../ServiceLayer/ServiceLayer.csproj&quot;, &quot;../ServiceLayer/&quot;]
RUN dotnet restore &quot;./SMSys.csproj&quot;
COPY . .
WORKDIR &quot;/src/.&quot;
RUN dotnet build &quot;SMSys.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;SMSys.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;SMSys.dll&quot;]
</code></pre>
<p>My docker file is where 'SMSys.csproj' is.</p>
<p>If I run docker inside that directory, I get:</p>
<pre><code>COPY failed: forbidden path outside the build context:
</code></pre>
<p>If I change some of the references of the project after the copy commands and run docker from the outside of the directory where all my projects are, I get multiple over a thousand of errors relating to my solution. The errors relate to missing things (assemblies and packages) as if all the projects are oblivious to one another when in fact they should be well referenced to one another as they do when I launch the project through visual studio.</p>
<p>This is an example of a solution that I followed that didnt work.
<a href=""https://www.jamestharpe.com/docker-include-files-outside-build-context/"" rel=""nofollow noreferrer"">https://www.jamestharpe.com/docker-include-files-outside-build-context/</a></p>
<p>Whats the best solution to implement in order to run my project through docker?</p>
<p>UPDATE</p>
<pre><code>    FROM mcr.microsoft.com/dotnet/aspnet:5.0.3-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

 
FROM mcr.microsoft.com/dotnet/sdk:5.0.103-buster-slim AS build 
WORKDIR /src 

# Prevent 'Warning: apt-key output should not be parsed (stdout is not a terminal)'
ENV APT_KEY_DONT_WARN_ON_DANGEROUS_USAGE=1

# install NodeJS 13.x
# see https://github.com/nodesource/distributions/blob/master/README.md#deb
RUN apt-get update -y
RUN apt-get upgrade -y
RUN apt-get install -y mc
#RUN apt-get install curl gnupg -yq 
#RUN curl -sL https://deb.nodesource.com/setup_12.x | bash -
RUN apt-get install -y npm
 
COPY [&quot;SMSysSolution/SMSys.csproj&quot;, &quot;SMSysSolution/&quot;]
COPY [&quot;DataLayer/DataLayer.csproj&quot;, &quot;DataLayer/&quot;]
COPY [&quot;Utilities/Utilities.csproj&quot;, &quot;Utilities/&quot;]
COPY [&quot;ServiceLayer/ServiceLayer.csproj&quot;, &quot;ServiceLayer/&quot;]
COPY [&quot;XUnitIntegrationTests/XUnitIntegrationTests.csproj&quot;, &quot;XUnitIntegrationTests/&quot;]
COPY [&quot;XUnitTestProject1/XUnitTestProject1.csproj&quot;, &quot;XUnitTestProject1/&quot;]
RUN dotnet restore &quot;./SMSysSolution/SMSys.csproj&quot;
COPY . .
WORKDIR &quot;/src/SMSysSolution&quot;
RUN dotnet build &quot;SMSys.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;SMSys.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;SMSys.dll&quot;]
</code></pre>
<p>I had to put a few changes to the paths to make it work.</p>
"
"29905953","How to correctly link php-fpm and Nginx Docker containers?","<php><nginx><docker><dockerfile><docker-compose>","62262377","Nginx and php-fpm most optimal setup configuration with docker","<php><docker><nginx><fpm>","<p>I am trying to link 2 separate containers:</p>

<ul>
<li><a href=""https://registry.hub.docker.com/_/nginx/"">nginx:latest</a></li>
<li><a href=""https://registry.hub.docker.com/_/php/"">php:fpm</a></li>
</ul>

<p>The problem is that php scripts do not work. Perhaps the php-fpm configuration is incorrect.
Here is the source code, which is in my <a href=""https://github.com/bocharsky-bw/docker"">repository</a>. Here is the file <code>docker-compose.yml</code>:</p>

<pre><code>nginx:
    build: .
    ports:
        - ""80:80""
        - ""443:443""
    volumes:
        - ./:/var/www/test/
    links:
        - fpm
fpm:
    image: php:fpm
    ports:
        - ""9000:9000""
</code></pre>

<p>and <code>Dockerfile</code> which I used to build a custom image based on the nginx one:</p>

<pre><code>FROM nginx

# Change Nginx config here...
RUN rm /etc/nginx/conf.d/default.conf
ADD ./default.conf /etc/nginx/conf.d/
</code></pre>

<p>Lastly, here is my custom Nginx virtual host config:</p>

<pre><code>server {
    listen  80;

    server_name localhost;
    root /var/www/test;

    error_log /var/log/nginx/localhost.error.log;
    access_log /var/log/nginx/localhost.access.log;

    location / {
        # try to serve file directly, fallback to app.php
        try_files $uri /index.php$is_args$args;
    }

    location ~ ^/.+\.php(/|$) {
        fastcgi_pass 192.168.59.103:9000;
        fastcgi_split_path_info ^(.+\.php)(/.*)$;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param HTTPS off;
    }
}
</code></pre>

<p>Could anybody help me configure these containers correctly to execute php scripts?</p>

<p><strong>P.S.</strong>
I run containers via docker-composer like this:</p>

<p><code>docker-compose up</code></p>

<p>from the project root directory.</p>
","<p>I have a question regarding the most optimal way to set up NGINX and PHP-FPM for my application with docker. So in my set up, my application code and PHP-FPM live in one docker container (the web container), and Ngnix on a separate docker container. </p>

<p>The trick here is that Nginx does not have access to the application files, because they are in the web container, which resulted in some configuration issues with Nginx.
One of those issues is that I can't use the 'root' directive in the Nginx config file, I had to use 'alias' instead. But I managed to get Nginx to point to the proper 'index.php' file in the end (see the config file at the end).</p>

<p>But I was wondering what is the most optimal way to define the relationship between this services?</p>

<ul>
<li>Should I just give access to the application code to Nginx?</li>
<li>Should I set up one more Nginx service in the web container, so that it will accept the requests that come from the Nginx container?</li>
<li>Or maybe there is some other option that would be even better for my case?</li>
</ul>

<p>This is my current nginx confg file (I've remove some irrelevant parts):</p>

<pre><code>upstream phpserver {
  server web:9000;
}
server {
    listen 443 ssl http2;
    server_name app;
    gzip off;   

    location ~ ^/index\.php(/|$) {
        alias /web/index.php;  

        fastcgi_pass phpserver;
        fastcgi_split_path_info ^(.+\.php)(/.*)$;
        #include fastcgi_params;

        fastcgi_param   QUERY_STRING        $query_string;
        fastcgi_param   REQUEST_METHOD      $request_method;
        fastcgi_param   CONTENT_TYPE        $content_type;
        fastcgi_param   CONTENT_LENGTH      $content_length;

        fastcgi_param   SCRIPT_FILENAME     $request_filename;
        fastcgi_param   SCRIPT_NAME     $fastcgi_script_name;
        fastcgi_param   REQUEST_URI     $request_uri;
        fastcgi_param   DOCUMENT_URI        $document_uri;
        fastcgi_param   DOCUMENT_ROOT       $document_root;
        fastcgi_param   SERVER_PROTOCOL     $server_protocol;

        fastcgi_param   GATEWAY_INTERFACE   CGI/1.1;
        fastcgi_param   SERVER_SOFTWARE     nginx/$nginx_version;

        fastcgi_param   REMOTE_ADDR     $remote_addr;
        fastcgi_param   REMOTE_PORT     $remote_port;
        fastcgi_param   SERVER_ADDR     $server_addr;
        fastcgi_param   SERVER_PORT     $server_port;
        fastcgi_param   SERVER_NAME     $server_name;

        internal;
        http2_push_preload on;
    }
</code></pre>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","62197514","How to make sure the database has been created before application connection","<postgresql><docker><docker-compose><fastapi><testdriven.io>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>This is the scenario: There is the <code>docker-compose.yml</code> which spins up a web application (fastapi + tortoise) and a Postgres database server. at the first <code>docker-compose up</code>, the Postgres server takes some time to initialize, and therefore, the ORM can't connect to the DB server and the web app fails but the databases are created. When we restart via <code>docker-compose restart</code>, the containers are restarted and the web app can connect to the database.</p>

<p>How can we make the application wait for the DB to finish creating before the ORM connect's to it for the very first time?</p>

<p>The following is a snippet of the error when the database hasn't been created. </p>

<pre><code>web_1     | Waiting for postgres...
web_1     | PostgreSQL started
web_1     | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
web_1     | INFO:     Started reloader process [1] using statreload
web_1     | INFO:     Started server process [8]
web_1     | INFO:     Waiting for application startup.
web_1     | INFO:     Tortoise-ORM startup
web_1     |     connections: {'default': {'engine': 'tortoise.backends.asyncpg', 'credentials': {'port': 5432, 'database': 'web_dev', 'host': 'web-db', 'user': 'postgres', 'password': 'postgres'}}}
web_1     |     apps: {'models': {'models': ['app.models.tortoise'], 'default_connection': 'default'}}
web_1     | ERROR:    Traceback (most recent call last):
web_1     |   File ""/usr/local/lib/python3.8/site-packages/starlette/routing.py"", line 517, in lifespan
web_1     |     await self.startup()
web_1     |   File ""/usr/local/lib/python3.8/site-packages/starlette/routing.py"", line 494, in startup
web_1     |     await handler()
web_1     |   File ""/usr/local/lib/python3.8/site-packages/tortoise/contrib/fastapi/__init__.py"", line 92, in init_orm
web_1     |     await Tortoise.init(config=config, config_file=config_file, db_url=db_url, modules=modules)
web_1     |   File ""/usr/local/lib/python3.8/site-packages/tortoise/__init__.py"", line 555, in init
web_1     |     await cls._init_connections(connections_config, _create_db)
web_1     |   File ""/usr/local/lib/python3.8/site-packages/tortoise/__init__.py"", line 385, in _init_connections
web_1     |     await connection.create_connection(with_db=True)
web_1     |   File ""/usr/local/lib/python3.8/site-packages/tortoise/backends/asyncpg/client.py"", line 90, in create_connection
web_1     |     self._pool = await asyncpg.create_pool(None, password=self.password, **self._template)
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/pool.py"", line 398, in _async__init__
web_1     |     await self._initialize()
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/pool.py"", line 426, in _initialize
web_1     |     await first_ch.connect()
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/pool.py"", line 125, in connect
web_1     |     self._con = await self._pool._get_new_connection()
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/pool.py"", line 468, in _get_new_connection
web_1     |     con = await connection.connect(
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/connection.py"", line 1668, in connect
web_1     |     return await connect_utils._connect(
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/connect_utils.py"", line 652, in _connect
web_1     |     con = await _connect_addr(
web_1     |   File ""/usr/local/lib/python3.8/site-packages/asyncpg/connect_utils.py"", line 631, in _connect_addr
web_1     |     await asyncio.wait_for(connected, timeout=timeout)
web_1     |   File ""/usr/local/lib/python3.8/asyncio/tasks.py"", line 483, in wait_for
web_1     |     return fut.result()
web_1     | asyncpg.exceptions.CannotConnectNowError: the database system is starting up
web_1     | 
web_1     | ERROR:    Application startup failed. Exiting.
</code></pre>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","61929503","Waiting for database connection before starting up","<node.js><docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I actually have been stuck at waiting for the database to be ready before launching services. Am using <code>docker-compose.yml</code> with <code>NodeJS</code>. Am  using <a href=""https://www.npmjs.com/package/wait-on"" rel=""nofollow noreferrer"">wait-on</a> and all my connections happen through links <code>nats://nats:4222</code>, <code>redis://redis:6379/2</code> and <code>mysql:3306</code>. How do use the <code>wait-on</code> package to wait for these services before bootstrapping the service. Any help would be greatly appreciated, thanks!</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","66421280","Docker - need a delay before starting the container","<docker><docker-compose><docker-container>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>How do you &quot;tell&quot; the docker-compose to delay before starting the container?</p>
<p>The container is already up and the service is not ready yet.
Docker does not take into account the readiness of the service (the container is raised) and it starts the next one. The next one looks at the availability of the service and it is not there yet - it falls.</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","66148548","How to prevent an application from starting before its dependencies are available","<docker><rabbitmq>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>My Dockerfile starts the application using the following command</p>
<pre><code>CMD [&quot;/home/app/start-app.sh&quot;]
</code></pre>
<p>start-app.sh contains the following, it waits until RabbitMQ server is available. Is there a better way of achieving this for applications running using docker-compose, k8s</p>
<pre><code>while ! nc -z &quot;$RABBITMQ_HOSTNAME&quot; &quot;$SPRING_RABBITMQ_PORT&quot;; do sleep 10; done
</code></pre>
"
"37242217","Access docker container from host using containers name","<docker><docker-compose>","62279012","How do I publish and access jenkins docker container from browser?","<docker>","<p>I am developing a service and using there docker compose to spin services like postgres, redis, elasticsearch. I have a web application that is based on RubyOnRails and writes and reads from all those services.</p>

<p>Here is my <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      - frontapp

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      - frontapp

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      - frontapp

networks:
  frontapp:
    driver: bridge
</code></pre>

<p>And i can ping containers within this network</p>

<pre><code>$ docker-compose run redis /bin/bash
root@777501e06c03:/data# ping postgres
PING postgres (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.346 ms
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.047 ms
...
</code></pre>

<p>So far so good. Now I want to run ruby on rails application on my host machine but be able to access postgres instance with url like <code>postgresql://username:password@postgres/database</code> currently that is not possible</p>

<pre><code>$ ping postgres
ping: unknown host postgres
</code></pre>

<p>I can see my network in docker</p>

<pre><code>$ docker network ls
NETWORK ID          NAME                DRIVER
ac394b85ce09        bridge              bridge              
0189d7e86b33        elephant_default    bridge              
7e00c70bde3b        elephant_frontapp   bridge              
a648554a72fa        host                host                
4ad9f0f41b36        none                null 
</code></pre>

<p>And I can see an interface to it</p>

<pre><code>$ ifconfig 
br-0189d7e86b33 Link encap:Ethernet  HWaddr 02:42:76:72:bb:c2  
          inet addr:172.18.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:76ff:fe72:bbc2/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:36 errors:0 dropped:0 overruns:0 frame:0
          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2000 (2.0 KB)  TX bytes:8792 (8.7 KB)

br-7e00c70bde3b Link encap:Ethernet  HWaddr 02:42:e7:d1:fe:29  
          inet addr:172.20.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e7ff:fed1:fe29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1584 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1597 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:407137 (407.1 KB)  TX bytes:292299 (292.2 KB)
...
</code></pre>

<p>But i am not sure what should I do next. I tried to play a bit with <code>/etc/resolv.conf</code>, mainly with <code>nameserver</code> directive, but that had no effect.</p>

<p>I would appreciate any help of suggestions how to configure this setup correctly.</p>

<p><strong>UPDATE</strong></p>

<p>After browsing through Internet resources I managed to assign static IP addresses to boxes. For now it is enough for me to continue development. Here is my current <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      frontapp:
        ipv4_address: 172.25.0.11

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      frontapp:
        ipv4_address: 172.25.0.12

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      frontapp:
        ipv4_address: 172.25.0.10

networks:
  frontapp:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1
</code></pre>
","<p><strong>What I did:</strong></p>
<p>Created 2 docker containers using the below command and successfully navigated from the browser.</p>
<p><strong>Docker Setup:</strong></p>
<p><strong>Host Machine:</strong> Linux VM Image(Assume IP is <em>10.1.1.1</em>)</p>
<p><strong>Jenkins Docker Command:</strong></p>
<pre><code>sudo docker run --detach \
  --hostname jenkins.local \
  --publish 8080:8080 \
  --publish 50000:50000 \
  --name jenkins \
  --restart always \
  jenkins/jenkins
</code></pre>
<p><strong>GitLab CE Docker Command:</strong></p>
<pre><code>sudo docker run --detach \
  --hostname gitlab.local \
  --publish 50443:443 \
  --publish 7070:80 \
  --publish 5022:22 \
  --name gitlab \
  --restart always \
  --volume $GITLAB_HOME/gitlab/config:/etc/gitlab:Z \
  --volume $GITLAB_HOME/gitlab/logs:/var/log/gitlab:Z \
  --volume $GITLAB_HOME/gitlab/data:/var/opt/gitlab:Z \
  gitlab/gitlab-ce:latest
</code></pre>
<p><strong>Browser Navigation:</strong></p>
<ul>
<li>Jenkins: <a href=""http://10.1.1.1:8080/"" rel=""nofollow noreferrer"">http://10.1.1.1:8080/</a> - Success</li>
<li>GitLab : <a href=""http://10.1.1.1:7070/"" rel=""nofollow noreferrer"">http://10.1.1.1:7070/</a> - Success</li>
</ul>
<p><strong>What I need to achieve?</strong></p>
<p>I need to access the docker applications using a friendly URL without the port as given below.</p>
<ul>
<li><a href=""http://10.1.1.1:8080/"" rel=""nofollow noreferrer"">http://10.1.1.1:8080/</a> by --&gt; <a href=""http://gitlab.test.local"" rel=""nofollow noreferrer"">http://gitlab.test.local</a></li>
<li><a href=""http://10.1.1.1:7070/"" rel=""nofollow noreferrer"">http://10.1.1.1:7070/</a> by --&gt; <a href=""http://jenkins.test.local"" rel=""nofollow noreferrer"">http://jenkins.test.local</a></li>
</ul>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","62357876","devilbox_httpd_1 error address already in use","<apache><docker><docker-compose><port>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I am using macOS Catalina 10.15 </p>

<p>I do: </p>

<pre><code>docker-compose up -d httpd php mysql 
</code></pre>

<p>And I get: </p>

<pre><code>Creating devilbox_httpd_1 ... error

ERROR: for devilbox_httpd_1  Cannot start service httpd: Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use

ERROR: for httpd  Cannot start service httpd: Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use
</code></pre>

<p>When I go to Network Utility Monitor/ Port Scan for localhost and also for my IP address I find:</p>

<pre><code>Port Scanning host: 127.0.0.1

     Open TCP Port:     80          http
     Open TCP Port:     1053        remote-as
</code></pre>

<p>Firewall is not activated. </p>

<p>What else can I do/check to solve this error?? </p>

<p>Thanks a ton </p>
"
"38396139","Docker: change folder where to store docker volumes","<mongodb><amazon-ec2><docker><ubuntu-14.04><volumes>","62170193","Create docker volume in directory","<docker><docker-compose><docker-volume>","<p>On my <code>Ubuntu EC2</code> I host an application using docker containers. <code>db</code> data and <code>upload</code> data is being stored in volumes <code>CaseBook-data-db</code> and <code>CaseBook-data-uploads</code> which are being created with this commands:</p>

<pre><code>docker volume create --name=CaseBook-data-db
docker volume create --name=CaseBook-data-uploads
</code></pre>

<p>Volumes being attached through <code>docker-compose</code> file:</p>

<pre><code>version: '2'
services:
    mongo:
        container_name: ""CaseBook-db""
        restart: always
        image: mongo:3.2.7
        ports:
            - ""27017""
        volumes:
            - data_db:/data/db
        labels:
            - ""ENVIRONMENT_TYPE=meteor""

    app:
        container_name: ""CaseBook-app""
        restart: always
        image: ""meteor/casebook""
        build: .
        depends_on:
            - mongo
        environment:
            - MONGO_URL=mongodb://mongo:27017/CaseBook
        ports:
            - ""80:3000""
        volumes:
            - data_uploads:/Meteor-CaseBook-Container/.uploads
        labels:
            - ""ENVIRONMENT_TYPE=meteor""
volumes:
     data_db:
        external:
            name: CaseBook-data-db
     data_uploads:
        external:
            name: CaseBook-data-uploads
</code></pre>

<p>I need to store those docker volumes in different folder(for example <code>/home/ubuntu/data/</code>) of the host machine. How to change docker storage folder for volumes? Or there is a better way in doing this? Thank you in advance.</p>
","<p>It is possible to create docker volume in my destined directory? </p>

<p>I was not able to found it in documentation, I just saw some mountpoints, but I want to mount it localy, from my defined directory. For example <code>/data/docker/volume_name/</code></p>

<p><code>docker volume create</code>  won't let me do that.</p>
"
"39404280","Make container accessible only from localhost","<docker><portforwarding>","62120030","Prevent public access to MySQL port running using Docker-compose","<mysql><docker><docker-compose>","<p>I have Docker engine installed on Debian Jessie and I am running there container with nginx in it. My ""run"" command looks like this:</p>

<pre><code>docker run -p 1234:80 -d -v /var/www/:/usr/share/nginx/html nginx:1.9
</code></pre>

<p>It works fine, problem is that now content of this container is accessible via <code>http://{server_ip}:1234</code>. I want to run multiple containers (domains) on this server so I want to setup reverse proxies for them. </p>

<p>How can I make sure that container will be only accessible via reverse proxy and not directly from <code>IP:port</code>? Eg.:</p>

<pre><code>http://{server_ip}:1234  # not found, connection refused, etc...
http://localhost:1234  # works fine
</code></pre>

<p><strong>//EDIT:</strong> Just to be clear - I am not asking how to setup reverse proxy, but how to run Docker container to be accessible only from localhost.</p>
","<p>I'm running MySQL using Docker-compose on my server.</p>

<p>This is the related part of my <code>docker-compose.yml</code>:</p>

<pre class=""lang-yaml prettyprint-override""><code>mysql:
    image: mysql:5.7.27
    environment:
      - MYSQL_DATABASE=app
      - MYSQL_ROOT_PASSWORD=secret
    ports:
      - 3306:3306
</code></pre>

<p>Now, it's accessible from the Internet ([Server Static IP]:3306) and it seems so insecure.</p>

<p>I want to <strong>prevent</strong> it from being accessible from the <strong>Internet</strong> and let it only be accessible <strong>inside the server</strong> and <strong>through SSH</strong>.</p>
"
"39780422","Docker not mapping port using gunicorn","<docker><ports>","63134004","GUnicorn not showing up locally, not using any ports (Django)","<python><django><docker><dockerfile><gunicorn>","<p>I'm running gunicorn inside a docker container. I know this works because sshing into it and curling localhost:8000/things in docker container gives me the response I want, however, I am not able to reach this on my host, despite docker telling me the port has been mapped. What gives?</p>

<p>I ran </p>

<pre><code>docker run -d -p 80:8000 myapp:version1.1 /bin/bash -c 'gunicorn things:app'
</code></pre>

<p><code>docker ps</code> gives me</p>

<pre><code>CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                            NAMES
614df1f2708e        myapp:version1.1  ""/bin/bash -c 'gunico""   6 minutes ago       Up 6 minutes        5000/tcp, 0.0.0.0:80-&gt;8000/tcp   evil_stallman
</code></pre>

<p>On my host, curling locahost/things gives me</p>

<pre><code>curl: (52) Empty reply from server
</code></pre>

<p>However, when I <code>docker exec -t -i 614df1f2708e /bin/bash</code> and then <code>curl localhost:8000/things</code>, I succesfully get my correct response.</p>

<p>Why isn't docker mapping my port 8000 correctly?</p>
","<p>I am trying to run a Docker container via the command:</p>
<p><code>docker run --rm -d  projectname:latest</code></p>
<p>The gunicorn command which is executing is:</p>
<p><code>gunicorn projectname.wsgi 0.0.0.0:$PORT</code></p>
<p>And I get the following gunicorn lines:</p>
<pre><code>[2020-07-28 11:56:50 +0000] [10] [INFO] Listening at: http://127.0.0.1:8000 (10)
[2020-07-28 11:56:50 +0000] [10] [INFO] Using worker: sync
[2020-07-28 11:56:50 +0000] [12] [INFO] Booting worker with pid: 12
</code></pre>
<p>From this I thought it was working fine. However, nothing is showing up in 127.0.0.1:8000. After using Network Utility's port scanner, no ports above or including 8000 are being used.</p>
<p>When I right click on the container in Visual Studio Code (with Docker extension installed) and click 'Open in Browser' it states &quot;No valid ports are available. Source: Docker (Extension)&quot;.</p>
<p>How can I see the app running locally? What could the issue possibly be?</p>
"
"40714970","Escaping forward slashes in sed command","<bash><shell><sed>","63080844","Sed in Dockerfile not working due to slahes in replacement","<docker><sed>","<p>With <code>Bash</code> and <code>SED</code> I'm trying to replace two strings in a js file with URL's.</p>

<p>The two urls that should be inserted is input params when I run the .sh script.</p>

<pre><code>./deploy.sh https://hostname.com/a/index.html https://hostname2.com/test
</code></pre>

<p>However to make this usable in my sed command I have to escape all forward slashes with: <code>\\</code> ?</p>

<pre><code>./deploy.sh https:\\/\\/hostname.com\\/a\\/index.html https:\\/\\/hostname2.com\\/test
</code></pre>

<p>If they are escaped this SED command works on Mac OSX Sierra</p>

<pre><code>APP_URL=$1
API_URL=$2

sed ""s/tempAppUrl/$APP_URL/g;s/tempApiUrl/$API_URL/g"" index.src.js &gt; index.js
</code></pre>

<p>Now I don't want to insert escaped urls as params, I want the script it self to escape the forward slashes.</p>

<p>This is what I've tried:</p>

<pre><code>APP_URL=$1
API_URL=$2

ESC_APP_URL=(${APP_URL//\//'\\/'})
ESC_API_URL=(${API_URL//\//'\\/'})

echo 'Escaped URLS'
echo $ESC_APP_URL
#Echos result: https:\\/\\/hostname.com\\/a\\/index.html 
echo $ESC_API_URL
#Echos result: https:\\/\\/hostname2.com\\/test

echo ""Inserting app-URL and api-URL before dist""
sed ""s/tempAppUrl/$ESC_APP_URL/g;s/tempApiUrl/$ESC_API_URL/g"" index.src.js &gt; index.js
</code></pre>

<p>The params looks the same but in this case the SED throws a error </p>

<pre><code>sed: 1: ""s/tempAppUrl/https:\\/\ ..."": bad flag in substitute command: '\'
</code></pre>

<p>Could anyone tell me the difference here? The Strings looks the same but gives different results.</p>
","<p>My Dockerfiles is using sed to update a .env file.</p>
<p>I am trying to replace <strong>wasabi-url</strong> with <strong><a href=""http://s3.eu-central-1.wasabisys.com"" rel=""nofollow noreferrer"">http://s3.eu-central-1.wasabisys.com</a></strong></p>
<pre><code>REACT_APP_WASABI_URL=http://s3.eu-central-1.wasabisys.com 
RUN sed -i &quot;s/wasabi-url/$REACT_APP_WASABI_URL/&quot; ./.env 
</code></pre>
<p>Sed errors I am guessing due to the <code>/</code> in the replacement text.</p>
"
"40905761","How do I mount a host directory as a volume in docker compose","<docker><docker-compose><docker-volume>","66321985","debugging, constant reload","<python><docker><flask>","<p>I have a development environment I'm dockerizing and I would like the ability to livereload my changes without having to rebuild docker images. I'm using docker compose because redis is one of my app's dependencies and I like being able to link a redis container</p>
<p>I have two containers defined in my <code>docker-compose.yml</code>:</p>
<pre class=""lang-yml prettyprint-override""><code>node:
  build: ./node
  links:
    - redis
  ports:
    - &quot;8080&quot;
  env_file:
    - node-app.env

redis:
  image: redis
  ports:
    - &quot;6379&quot;
</code></pre>
<p>I've gotten to the point in my <code>node</code> app's dockerfile where I add a volume, but how do I mount the the host's directory in the volume so that all my live edits to the code are reflected in the container?</p>
<p>Here's my current Dockerfile:</p>
<pre class=""lang-dockerfile prettyprint-override""><code># Set the base image to Ubuntu
FROM    node:boron

# File Author / Maintainer
MAINTAINER Amin Shah Gilani &lt;amin@gilani.me&gt;

# Install nodemon
RUN npm install -g nodemon

# Add a /app volume
VOLUME [&quot;/app&quot;]

# TODO: link the current . to /app

# Define working directory
WORKDIR /app

# Run npm install
RUN npm install

# Expose port
EXPOSE  8080

# Run app using nodemon
CMD [&quot;nodemon&quot;, &quot;/app/app.js&quot;]
</code></pre>
<p>My project looks like this:</p>
<pre><code>/
- docker-compose.yml
- node-app.env
- node/
  - app.js
  - Dockerfile.js
</code></pre>
","<p>hello i would like to use debug inside container but i cannot add working &quot;volumes&quot; into docker comopse.xml</p>
<p>file structure in the piscture:</p>
<p><a href=""https://i.stack.imgur.com/q70LS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q70LS.png"" alt=""enter image description here"" /></a></p>
<pre><code>FROM python:3.9-alpine
WORKDIR /planner
ENV FLASK_APP=app
ENV FLASK_RUN_HOST=0.0.0.0
RUN apk add --no-cache gcc musl-dev linux-headers
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt
EXPOSE 5000
COPY . .
CMD [&quot;flask&quot;, &quot;run&quot;]
</code></pre>
<p>compose:</p>
<pre><code>   version: &quot;3.7&quot;
    services:
      web:
        build: .
        restart: always
        ports:
          - &quot;5000:5000&quot;
        volumes:
          - .:/planner
    
      rq-dashboard:
        image: jaredv/rq-docker:0.0.2
        command: rq-dashboard -H rq-server
        ports:
          - 9181:9181
    
      rq-worker:
        image: jaredv/rq-docker:0.0.2
        command: rq worker -u redis://rq-server:6379 high normal low
        deploy:
          replicas: 3
    
      rq-server:
        image: redis:alpine
        ports:
          - 6379:6379
</code></pre>
<p>problem is with:</p>
<pre><code>volumes:
  - .:/planner
</code></pre>
<p><a href=""https://i.stack.imgur.com/FXtWj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXtWj.png"" alt=""enter image description here"" /></a></p>
<p>could you help me with it?</p>
"
"42885538","Raspberry-pi docker error: standard_init_linux.go:178: exec user process caused ""exec format error""","<linux><docker><raspberry-pi><raspbian>","63401867","docker image error: standard_init_linux.go:211: exec user process caused “exec format error”","<reactjs><postgresql><spring-boot><docker><docker-compose>","<p>I've installed docker in rapsbian according to the official instructions (i.e., running <code>curl -sSL https://get.docker.com | sh</code>) but I'm not able to run the hello-world example (I've also tried other examples without success). This is the error I'm getting:</p>

<pre><code>pi@raspberrypi2:~ $ docker run hello-world
standard_init_linux.go:178: exec user process caused ""exec format error""
</code></pre>

<p>My environment is Raspberry Pi 2 Model B with Raspbian GNU/Linux 8 (jessie) and Docker version 17.03.0-ce, build 60ccb22. </p>

<p>Any hint about the problem or possible directions to solve the problem? </p>

<p>Many thanks!</p>
","<p>I created a few docker images on windows and when I try to run them (with or without docker-compose) on Raspbian I get the error described in the title. They work on windows perfectly but fail on linux. I'm totally new to docker. I looked up the error but didn't find anything that helped. Thanks for your answers.</p>
<p><strong>Dockerfiles:</strong></p>
<pre><code>FROM openjdk:8-jdk-alpine
COPY *.jar app.jar
CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]
</code></pre>
<pre><code>FROM node:12.7-alpine
COPY package.json ./
COPY package-lock.json ./
RUN npm install
RUN npm install react-scripts@3.4.0 -g
COPY . .
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<pre><code>FROM postgres
COPY sql ./docker-entrypoint-initdb.d
ENV POSTGRES_DB {db}
ENV POSTGRES_USER {user}
ENV POSTGRES_PASSWORD {pwd}
</code></pre>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: '3'
services:
  database:
    image: {database-image}
    ports:
      - 5432:5432
  service:
    image: {service-image}
    ports:
      - 8080:8080
    depends_on:
      - database
  web:
    image: {web-image}
    ports:
      - 3000:3000
    stdin_open: true
</code></pre>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","66369850","I have created volumes with Docker Desktop, I see the volumes using ""docker volume ls"" but the folder in windows are empy","<docker><docker-compose><docker-volume><docker-desktop>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I have got a docker compose file that uses a volume, then I check that the volume is created while my container is running.</p>
<p><a href=""https://i.stack.imgur.com/q46kG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q46kG.png"" alt=""enter image description here"" /></a></p>
<p>Then I go to the folder where the volumes are stored in docker desktop, but the folder is empty.</p>
<p><a href=""https://i.stack.imgur.com/G3XBW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G3XBW.png"" alt=""enter image description here"" /></a></p>
<p>this is my docker-compose.yml</p>
<pre><code>version: &quot;3.3&quot;

services:

  backnode:
    restart: always
    build:
      context: .
    image: foo-backnode:1.0.0
    env_file: docker-compose.env
    volumes: 
      - html:/var/log/foo
    networks:
      - internal
    ports: 
      - &quot;3001:3001&quot;

networks:
  internal:
    internal: false

volumes: 
  html:
    external: false
</code></pre>
<p>I expect to see the log files produced by my application.</p>
<p><strong>Edit:</strong></p>
<p>All this works properly under linux enviroment (ubuntu).</p>
<p><a href=""https://i.stack.imgur.com/ApzxG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ApzxG.png"" alt=""enter image description here"" /></a></p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","63072408","is it possible to call command belonging to another container in docker","<php><docker><nginx><docker-compose><volumes>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>Is it possible to call a command inside docker which belongs to another container.</p>
<p>An example would be:</p>
<p>php container which contains php
app container which contains nginx</p>
<p>I would like to somehow share the php executable inside the nginx container.</p>
<p>Currently I have tried sharing the volumes with no luck.</p>
<p>Any ideas?</p>
"
"44658101","How to run docker containers in host network mode using docker-compose?","<docker-compose><docker-swarm>","63777655","Docker ERROR: only one instance of ""host"" network is allowed","<docker><docker-compose><centos7><docker-swarm>","<p>I need to run containers from docker-compose in host network mode.
For example, if I run single container, I write:</p>

<pre><code>docker run --net=host my_image
</code></pre>

<p>How to do this part <code>--net=host</code> in docker-compose?</p>
","<p>I am trying to run a container where I need to user network driver as &quot;host&quot; instead of &quot;bridge&quot;. I am running it on Centos machine and my docker-compose.yml is</p>
<pre><code>version: '3.4'

services:
  testContainer:
    build:
      context: .
      args: 
        HADOOP_VERSION: 2.6.0
        HIVE_VERSION: 1.1.0
    image: testcontainer
    container_name: testcontainer
    hostname: testcontainer
    ports:
      - 9200:9200
      - 9300:9300
      - 5601:5601
      - 9001:9001
    ulimits:
      memlock:
        soft: -1
        hard: -1  
    networks:
      - elknet  

networks:
  elknet:
    driver: host      
</code></pre>
<p>But i am getting the following error when I fire &quot;<em><strong>docker-compose up</strong></em>&quot; :</p>
<blockquote>
<p>ERROR: only one instance of &quot;host&quot; network is allowed</p>
</blockquote>
<p>Can anyone please suggest how can I use host network using docker-compose.yml.</p>
<p>Also note that if I use network_host as suggested by @larsks, I am still getting error</p>
<pre><code>version: '3.4'

services:
  testContainer:
    build:
      context: .
      args: 
        HADOOP_VERSION: 2.6.0
        HIVE_VERSION: 1.1.0
    image: testcontainer
    container_name: testcontainer
    hostname: testcontainer
    ports:
      - 9200:9200
      - 9300:9300
      - 5601:5601
      - 9001:9001
    ulimits:
      memlock:
        soft: -1
        hard: -1  
    network_mode: host
</code></pre>
<p>I am getting following error</p>
<blockquote>
<p>ERROR: The Compose file './docker-compose.yml' is invalid because:
Unsupported config option for services: 'testContainer'</p>
</blockquote>
"
"39525820","Docker port forwarding not working","<docker><portforwarding><docker-container>","61973619","Are there any port mapping specific features in a Spring application using Docker Compose?","<java><spring><docker><nginx><docker-compose>","<p>I have setup Docker container for access my machine docker container to another machine in local.</p>

<p>Create a container below command:</p>

<pre><code>    docker run -it -d --name containerName -h www.myhost.net -v /var/www/html -p 7000:8000 --net mynetwork --ip 172.11.0.10 --privileged myimagename bash
</code></pre>

<p>After Create A Container Details:</p>

<pre><code>        CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES

        1e1e5e9b74b4        myimgaename         ""bash""              21 minutes ago      Up 6 minutes        0.0.0.0:7000-&gt;8000/tcp   containername
</code></pre>

<p>NetWork Details:</p>

<pre><code>     ""NetworkSettings"": {
        ""Bridge"": """",
        ""SandboxID"": ""fe357c54c816fff0f9d642037dc9a173be7f7e42a80776d006572f6a1395969e"",
        ""HairpinMode"": false,
        ""LinkLocalIPv6Address"": """",
        ""LinkLocalIPv6PrefixLen"": 0,
        ""Ports"": {
            ""8000/tcp"": [
                {
                    ""HostIp"": ""0.0.0.0"",
                    ""HostPort"": ""7000""
                }
            ]
        }
</code></pre>

<p>if I access docker  ipaddr(172.11.0.10) or hostname(www.myhost.net) in mymachine(hostmachine) it working  </p>

<p>But if I access with <strong>Port</strong> doesn't work: hostmachine ip: 192.168.1.1</p>

<pre><code>  go to the browser  192.168.1.1:7000  hostmachine and locally connected anoter machine also.
</code></pre>

<p>But My 7000 port are listen in hostmachine:      </p>

<pre><code>        # ps aux | grep 7000
        root     10437  0.0  0.2 194792 24572 pts/0    Sl+  12:33   0:00 docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 7000 -container-ip 172.11.0.10 -container-port 8000
        root     10941  0.0  0.0 118492  2324 pts/3    R+   12:44   0:00 grep --color=auto 7000
</code></pre>

<p><strong>update 1:</strong></p>

<pre><code>      $ docker version
        Client:
         Version:      1.11.2
         API version:  1.23
         Go version:   go1.5.4
         Git commit:   b9f10c9
         Built:        Wed Jun  1 21:39:21 2016
         OS/Arch:      linux/amd64

        Server:
         Version:      1.11.2
         API version:  1.23
         Go version:   go1.5.4
         Git commit:   b9f10c9
         Built:        Wed Jun  1 21:39:21 2016
         OS/Arch:      linux/amd64
</code></pre>

<p>Suggest me Why Cannot access my Container to another machine. How to Resolve this Problem</p>
","<p>I have a problem with dockerizing Spring Boot app. My docker-compose project consists of 4 parts:</p>

<ul>
<li>back - it`s just Spring Boot application with Tomcat on 8080. Here are my controllers for front app.</li>
<li>front - Nginx + Angular</li>
<li>core - mainly consists of a TCP server for receiving some information to DB in database-app, implemented on a simple Java Socket.</li>
<li>database - Postgres, which I just download from the Docker Hub and configure to create the database necessary for the back-application.</li>
</ul>

<p>My goal is to use my front-app, which is open in the browser on the host machine, manipulate data from the database from the database-app, through the controllers of my back-app.</p>

<p>So, I don't have any problems with building and running. Ports mapping for core, database and front apps works excellent. But not for back. I don't have any access from host to back-container from <code>localhost:8080</code>(<code>curl</code> requests from the host to container return an empty response, but <code>curl</code> in container bash works fine). In back-app I used Spring Security, so CORS is configured to allow all requests, and CSRF is disabled, if it's matter.</p>

<p>Generously apologize for my broken English!</p>

<p>Back Dockerfile</p>

<pre><code>FROM maven:3.5-jdk-8 AS build
COPY src /usr/src/app/src  
COPY pom.xml /usr/src/app  
RUN mvn -f /usr/src/app/pom.xml clean package

FROM gcr.io/distroless/java  
ARG JAR_FILE=target/*.jar
COPY --from=build /usr/src/app/${JAR_FILE} /usr/app/back.jar
ENTRYPOINT [""java"",""-jar"",""/usr/app/back.jar""]
</code></pre>

<p>Core Dockerfile</p>

<pre><code>FROM maven:3.5-jdk-8 AS build
COPY src /usr/src/app/src  
COPY pom.xml /usr/src/app  
RUN mvn -f /usr/src/app/pom.xml clean package

FROM gcr.io/distroless/java  
ARG JAR_FILE=target/*.jar
COPY --from=build /usr/src/app/${JAR_FILE} /usr/app/core.jar
ENTRYPOINT [""java"",""-jar"",""/usr/app/core.jar""]
</code></pre>

<p>Front Dockerfile</p>

<pre><code>FROM node:12 as builder
COPY package.json package-lock.json ./
RUN npm install &amp;&amp; mkdir /app &amp;&amp; mv ./node_modules ./app
WORKDIR /app
COPY . .
RUN npm run ng build -- --deploy-url=/ --prod

FROM nginx
COPY ./.nginx/nginx.conf /etc/nginx/nginx.conf
RUN rm -rf /usr/share/nginx/html/*
COPY --from=builder /app/dist/snsr-front-app /usr/share/nginx/html
ENTRYPOINT [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>UPDATED 1</strong>:
Dockerfile(s) are still the same.</p>

<p>docker-compose.yml</p>

<pre><code>version: '3'

services:

    snsr-front-app:
        build: ./snsr-front-app
        ports: 
            - 4200:80
        depends_on: 
            - snsr-back-app
        image: mxmtrms/snsr-front-app
        networks: 
            - front-net

    snsr-back-app:
        build: ./snsr-back-app
        depends_on: 
            - database
        image: mxmtrms/snsr-back-app
        networks: 
            - back-net
            - front-net
        expose: 
            - 8080
        environment: 
            DB_URL: database
            DB_PORT: 5432


    snsr-core-app:
        build: ./snsr-core-app
        ports: 
            - 3000:3000
        depends_on: 
            - database
        image: mxmtrms/snsr-core-app
        networks:
            - back-net

    database: 
        image: postgres
        environment: 
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: masterkey
            POSTGRES_DB: snsr
        ports: 
            - 5432:5432
        networks: 
            - back-net
networks:
  back-net:
  front-net:
</code></pre>

<p>nginx.conf</p>

<pre><code>worker_processes 4;

events { worker_connections 1024; }

http {

    upstream frontend {
        server 0.0.0.0:80;
    }
    upstream backend {
        server snsr-back-app:8080;
    }

    server {

        listen 80;
        root  /usr/share/nginx/html;
        include /etc/nginx/mime.types;

        location / {
            proxy_pass http://frontend;
            try_files $uri /index.html;
        }

        location /api {
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_pass http://backend;
        }
    }
}
</code></pre>

<p><strong>UPDATE 2</strong>
Backend logs: <a href=""https://gist.github.com/mxmtrms/ff12e2481d0ccc2781f15a961de6eab9"" rel=""nofollow noreferrer"">https://gist.github.com/mxmtrms/ff12e2481d0ccc2781f15a961de6eab9</a></p>

<p><code>docker ps</code>:
<a href=""https://gist.github.com/mxmtrms/2baaadc0e4873fc8bb28453d5c6d04f4"" rel=""nofollow noreferrer"">https://gist.github.com/mxmtrms/2baaadc0e4873fc8bb28453d5c6d04f4</a></p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","66445929","Best way to inherit from multiple Docker images","<docker><dockerfile>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p>My OS is Ubuntu, but i want to grab very large components + dependencies from other, let's say 3-4 base images.</p>
<p>All images that i need to derive from have, as i've mentioned, very large components + some dependencies.</p>
<p>Is there a way to incrementally build on an Ubuntu image, by copying stuff from other downloaded images ?</p>
<p>The most stupid think i can thing at the moment is to do it like this:</p>
<pre><code>FROM my_base_1 AS base_1
FROM my_base_2 AS base_2
FROM ubuntu:18:04
COPY --from=base_1 relevant relevant_1
COPY --from=base_2 relevant relevant_2

apt-get install relevant-dependencies

etc
```
</code></pre>
"
"39632038","Cannot run webpack-dev-server inside docker","<docker><webpack><webpack-dev-server>","66173709","Working with Docker, but site isn't showing on localhost","<reactjs><typescript><docker><docker-compose><dockerfile>","<p>I have created a docker image which serves a simple react app using webpack from inside the container, but I get nothing in the browser.</p>

<p>Here are my config files</p>

<p><code>package.json</code></p>

<pre><code>{
  ""name"": ""invas_client"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""start"": ""webpack --inline --content-base .""
  },
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""react"": ""^0.14.7"",
    ""react-dom"": ""^0.14.7"",
    ""react-router"": ""^2.0.0""
  },
  ""devDependencies"": {
    ""babel-core"": ""^6.5.1"",
    ""babel-loader"": ""^6.2.2"",
    ""babel-preset-es2015"": ""^6.5.0"",
    ""babel-preset-react"": ""^6.5.0"",
    ""http-server"": ""^0.8.5"",
    ""webpack"": ""^1.12.13"",
    ""webpack-dev-server"": ""^1.14.1""
  }
}
</code></pre>

<p><code>webpack.config.js</code></p>

<pre><code>module.exports = {
    entry: './index.js',

    output: {
        filename: 'bundle.js',
        publicPath: ''
    },

    module: {
        loaders: [
            { test: /\.js$/, exclude: /node_modules/, loader: 'babel-loader?presets[]=es2015&amp;presets[]=react' }
        ]
    }
}
</code></pre>

<p><code>Dockerfile</code></p>

<pre><code># Use starter image
FROM node:argon

# Create app directory
RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

# Install app dependencies
COPY package.json /usr/src/app/
RUN npm install

# Bundle app source
COPY . /usr/src/app

# Expose port
EXPOSE 8080

# Default command to run
CMD [""npm"", ""start""]
</code></pre>

<h2>What's working fine</h2>

<p>When I run <code>npm start</code>, the <code>webpack-dev-server</code> runs normally, and when I go to <a href=""http://localhost:8080"" rel=""noreferrer"">http://localhost:8080</a>, I see my page.</p>

<h2>What isn't working</h2>

<p>When I run my server using docker, with the following command:</p>

<p><code>docker build -t anubhav756/app . &amp;&amp; docker run -p 80:8080 anubhav756/app</code> </p>

<p>the logs show everything working normally from inside the container, but when I point my browser to <a href=""http://localhost"" rel=""noreferrer"">http://localhost</a>, I get <code>ERR_CONNECTION_RESET</code></p>

<p>Sample code's over <strong><a href=""https://github.com/anubhav756/webpack-docker"" rel=""noreferrer"">here</a></strong></p>
","<p>I'm a bit new to Docker and was able to get one site up using create-react-app, but at the moment, I'm trying to get one working from scratch using Webpack.  The site compiles with warnings, but I'm not seeing any result in the browser.</p>
<p>This is the output I'm getting when compiling, but I'm not seeing it in the browser.</p>
<pre><code>Successfully built 190902aff07e
Successfully tagged climbtrack_web:latest
Recreating climbtrack ... done
Attaching to climbtrack
climbtrack | 
climbtrack | &gt; climbtrack@1.0.0 start
climbtrack | &gt; webpack serve --open
climbtrack | 
climbtrack | ℹ ｢wds｣: Project is running at http://localhost:3100/
climbtrack | ℹ ｢wds｣: webpack output is served from /
climbtrack | ℹ ｢wds｣: Content not from webpack is served from ./dist
climbtrack | ℹ ｢wds｣: 404s will fallback to /index.html
climbtrack | ℹ ｢wdm｣: Compiled with warnings.
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:15.5.0

WORKDIR /app

COPY package*.json ./

RUN npm install

COPY . .

EXPOSE 3100

CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<p><strong>docker-compose.yml</strong></p>
<pre><code>version: &quot;3.8&quot;

services:
  web:
    container_name: climbtrack
    restart: always
    build: .
    volumes:
      - .:/app
    ports:
      - &quot;3100:3100&quot;
</code></pre>
<p><strong>webpack.config.js</strong></p>
<pre><code>const path = require(&quot;path&quot;);
const HtmlWebPackPlugin = require(&quot;html-webpack-plugin&quot;);
const { CleanWebpackPlugin } = require(&quot;clean-webpack-plugin&quot;);
module.exports = {
  resolve: {
    extensions: [&quot;.js&quot;, &quot;.jsx&quot;, &quot;.ts&quot;, &quot;.tsx&quot;],
  },
  entry: &quot;./src/index.tsx&quot;,
  module: {
    rules: [
      {
        test: /\.(ts|tsx)$/,
        exclude: /node_modules/,
        use: &quot;ts-loader&quot;,
      },
      {
        test: /\.(js|jsx)$/,
        exclude: /node_modules/,
        use: &quot;babel-loader&quot;,
      },
      {
        test: /\.scss$/,
        use: [&quot;style-loader&quot;, &quot;css-loader&quot;, &quot;sass-loader&quot;],
      },
      {
        test: /\.(jpg|png|svg)$/,
        use: &quot;file-loader&quot;,
      },
    ],
  },
  plugins: [
    new CleanWebpackPlugin({ cleanStaleWebpackAssets: false }),
    new HtmlWebPackPlugin({
      template: &quot;./public/index.html&quot;,
    }),
  ],
  output: {
    filename: &quot;[name].bundle.js&quot;,
    path: path.resolve(__dirname, &quot;dist&quot;)
  },
  devServer: {
    contentBase: &quot;./dist&quot;,
    historyApiFallback: true,
    port: 3100
  },
  stats: &quot;errors-only&quot;,
};
</code></pre>
<p><strong>package.json</strong></p>
<pre><code>{
  &quot;name&quot;: &quot;climbtrack&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;description&quot;: &quot;Track your every climb&quot;,
  &quot;main&quot;: &quot;index.js&quot;,
  &quot;scripts&quot;: {
    &quot;watch&quot;: &quot;webpack --watch&quot;,
    &quot;start&quot;: &quot;webpack serve --open&quot;,
    &quot;test&quot;: &quot;npm test&quot;
  },
  &quot;author&quot;: &quot;Brannon Glover&quot;,
  &quot;license&quot;: &quot;ISC&quot;,
  &quot;devDependencies&quot;: {
    &quot;@types/styled-components&quot;: &quot;^5.1.7&quot;,
    &quot;babel-loader&quot;: &quot;^8.2.2&quot;,
    &quot;clean-webpack-plugin&quot;: &quot;^3.0.0&quot;,
    &quot;css-loader&quot;: &quot;^5.0.2&quot;,
    &quot;file-loader&quot;: &quot;^6.2.0&quot;,
    &quot;html-webpack-plugin&quot;: &quot;^5.0.0&quot;,
    &quot;sass-loader&quot;: &quot;^11.0.1&quot;,
    &quot;style-loader&quot;: &quot;^2.0.0&quot;,
    &quot;styled-components&quot;: &quot;^5.2.1&quot;,
    &quot;ts-loader&quot;: &quot;^8.0.17&quot;,
    &quot;typescript&quot;: &quot;^4.1.5&quot;,
    &quot;webpack&quot;: &quot;^5.21.2&quot;,
    &quot;webpack-cli&quot;: &quot;^4.5.0&quot;,
    &quot;webpack-dev-server&quot;: &quot;^3.11.2&quot;
  },
  &quot;dependencies&quot;: {
    &quot;@types/html-webpack-plugin&quot;: &quot;^3.2.4&quot;,
    &quot;@types/react&quot;: &quot;^17.0.1&quot;,
    &quot;@types/react-dom&quot;: &quot;^17.0.0&quot;,
    &quot;react&quot;: &quot;^17.0.1&quot;,
    &quot;react-dom&quot;: &quot;^17.0.1&quot;
  }
}
</code></pre>
"
"40356259","Mount linux image in docker container","<linux><docker><mount>","62178026","Is it possible to mount ISO file while building image by Dockerfile?","<docker>","<p>For a project I need to mount a linux image inside a docker container running <a href=""https://hub.docker.com/_/ubuntu/"" rel=""noreferrer"">ubuntu</a>. The image I want to mount is Raspbian. I need to access the linux filesystem of the image and add a file.</p>

<p>I access the image by mounting the folder with the volume flag:</p>

<p><code>docker run -it -v /path/to/image/folder:/default ubuntu /bin/bash</code></p>

<p>With <code>fdisk -l raspbian.img</code> I found the offset:</p>

<pre><code>Disk raspbian.img: 1.3 GiB, 1389363200 bytes, 2713600 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x5a7089a1

Device        Boot  Start     End Sectors  Size Id Type
raspbian.img1        8192  137215  129024   63M  c W95 FAT32 (LBA)
raspbian.img2      137216 2713599 2576384  1.2G 83 Linux
</code></pre>

<p>Now when I try to mount the image with <code>mount -o loop,offset=$((137216*512)) raspbian.img /mnt/</code> I get <code>mount: /mnt/: mount failed: Unknown error -1</code>. Can someone explain if I can mount a linux image in a running docker container and if so how?</p>

<p><strong>Edit</strong></p>

<p>Doing the same mount operations in vagrant works perfectly. Are there some limitations to docker mounting filesystems?</p>
","<p>I'm trying to mount the image file in Dockerfile,
but it occurred error.</p>

<p>I think the error is related to there are no loop device in docker, is this suggestion right?</p>

<p>How can I resolve it, is it possible?</p>

<p>This is my Dockerfile as below:</p>

<pre><code>FROM ubuntu:bionic

RUN mkdir -p /mnt/rpi-sysroot                                                   
COPY ubuntu.img /root                  
RUN OFFSET=$(fdisk -l /root/ubuntu.img | grep Linux | awk '{print $2 ""* 512""}' | bc) 
RUN mount -o ro,loop,offset=$OFFSET -t auto /root/ubuntu.img /mnt/rpi-sysroot
</code></pre>
"
"40454470","How can I use a variable inside a Dockerfile CMD?","<docker><dockerfile>","62322343","How to use ENV variables within Dockerfile CMD command","<docker><dockerfile>","<p>Inside my Dockerfile:</p>

<pre><code>ENV PROJECTNAME mytestwebsite
CMD [""django-admin"", ""startproject"", ""$PROJECTNAME""]
</code></pre>

<p>Error:</p>

<pre><code>CommandError: '$PROJECTNAME' is not a valid project name
</code></pre>

<p>What is the quickest workaround here?  Does Docker have any plan to ""fix"" or introduce this functionality in later versions of Docker?</p>

<p>NOTE:  If I remove the CMD line from the Docker file and then run the Docker container, I am able to manually run Django-admin startproject $PROJECTNAME from inside the container and it will create the project...</p>
","<p>Dockerfile</p>

<pre><code>ENV APP_JAR = ""/app/service/lib/service-1.0.0.jar""

# CMD [""java"", ""-jar"", ""/app/service/lib/service-1.0.0.jar""]
CMD [""java"", ""-jar"", $APP_JAR]
</code></pre>

<p>The above fails to start my jar, what should I do to use ENV var properly</p>
"
"41972328","Docker History Base Image Add:sha256hash","<linux><docker><hash><dockerfile><sha256>","62315791","In a Docker image's history what is ""ADD file:<digest> in /""?","<docker><dockerfile><containers><dockerhub>","<p>I'm trying to better understand the <code>docker history</code> output. When I run <code>docker history nginx:latest</code> I get output that nearly matches the <a href=""https://github.com/nginxinc/docker-nginx/blob/e950fa7dfcee74933b1248a7fe345bdbc176fffb/mainline/jessie/Dockerfile"" rel=""noreferrer"">Dockerfile</a>:</p>

<pre><code>/bin/sh -c #(nop) CMD [""nginx"" ""-g"" ""daemon off;""]
/bin/sh -c #(nop) EXPOSE 443/tcp 80/tcp/bin/sh -c ln -sf /dev/stdout /var/log/nginx/access.log  &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log
/bin/sh -c apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62  &amp;&amp; echo ""deb http://nginx.org/packages/mainline/debian/ jessie nginx"" &gt;&gt; /etc/apt/sources.list &amp;&amp; apt-get update &amp;&amp; apt-get install --no-install-recommends --no-install-suggests -y       ca-certificates nginx=${NGINX_VERSION} nginx-module-xslt nginx-module-geoip nginx-module-image-filter nginx-module-perl nginx-module-njs gettext-base  &amp;&amp; rm -rf /var/lib/apt/lists/*
/bin/sh -c #(nop) ENV NGINX_VERSION=1.11.9-1~jessie
/bin/sh -c #(nop) MAINTAINER NGINX Docker Maintainers ""docker-maint@nginx.com""
/bin/sh -c #(nop) CMD [""/bin/bash""]
/bin/sh -c #(nop) ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /
</code></pre>

<p>with three notable exceptions</p>

<ol>
<li>All of the lines start with <code>/bin/sh -c #(nop)</code> except for the third line which is the <code>RUN</code> command in the Dockerfile - no big deal</li>
<li>The commands are in reverse (the last command in the Dockerfile is the first command listed with <code>docker history</code>) - also no big deal</li>
<li><p>This one's the kicker - The <code>FROM debian:jessie</code> line from the Dockerfile is translated to:</p>

<p><code>ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /</code>
<code>CMD [""/bin/bash""]</code></p></li>
</ol>

<p>It took me a little while to realize that the last two commands above (the <code>ADD</code> and <code>CMD [""/bin/bash""]</code> lines) were carried over from the base image <code>debian:jessie</code>. Once I figured that out, I thought to myself, ""self, the <code>file:89ec...da957</code> must be the sha256 hash of the <a href=""https://github.com/tianon/docker-brew-debian/blob/b39d31635ca26c8b1f3d982090ba8d54167c4d85/jessie/rootfs.tar.xz"" rel=""noreferrer"">rootfs.tar.xz</a> included as the file system. But no, the sha256 hash of the rootfs.tar.xz is <code>467328e24c316fd058f086eb8eb77706f3f448ad8886d202e7c9687d30692eca</code>.</p>

<p>Herein lies my question: <strong>Where does the hash listed in <code>docker history</code> come from? And why is it different than the actual hash of rootfs.tar.xz?</strong></p>

<p>I've thoroughly reviewed much of Docker's documentation, with no luck, including:</p>

<ol>
<li><a href=""https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/"" rel=""noreferrer"">https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/</a></li>
<li><a href=""https://docs.docker.com/engine/reference/commandline/history/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/commandline/history/</a></li>
<li><a href=""https://docs.docker.com/engine/reference/builder/"" rel=""noreferrer"">https://docs.docker.com/engine/reference/builder/</a></li>
</ol>

<p>The hash is consistent across all images that use <code>debian:jessie</code> as the base image. Even <code>docker history debian:jessie</code> shows the same hash:</p>

<pre><code>/bin/sh -c #(nop) CMD [""/bin/bash""]
/bin/sh -c #(nop) ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da957 in /
</code></pre>

<p>and I think you might agree, that there is only one file that could possibly have a hash in the <code>debian:jessie</code> Dockerfile:</p>

<pre><code>FROM scratch
ADD rootfs.tar.xz /
CMD [""/bin/bash""]
</code></pre>

<p>If anyone could provide some insight or point me to a resource I have yet to find, it would be much appreciated.</p>
","<p>On Docker Hub when viewing the ""Image History"" for an image, I commonly see this as the first line:</p>

<pre><code>ADD file:&lt;SHA256 digest&gt; in /
</code></pre>

<p>For example:</p>

<pre><code>ADD file:c3e6bb316dfa6b81dd4478aaa310df532883b1c0a14edeec3f63d641980c1789 in / 
</code></pre>

<p>I'm guessing it's some other docker image being used as the base, but I can't find any docs for that, and I can't find a way to easily search for that digest to better understand what it is.</p>

<ol>
<li>Can you please reference any docs that can help explain? </li>
<li>Is there a way to search for that digest to help find what it is?</li>
</ol>
"
"42153759","MySQL container crash after /etc/mysql/my.cnf change, how to edit back?","<mysql><docker><crash><debian><containers>","62379708","How to access the inside of the container without running the container","<macos><docker><docker-compose><mariadb>","<p>I changed some mysql config settings and set something wrong, now Docker container keeps restarting and I cannot find the <code>my.cnf</code> file to edit in host filesystem. I have tried aufs/diff folders but so far unable to find it. Also tried:</p>

<pre><code>find / -name my.cnf -exec nano {} \;
</code></pre>

<p>But it does not bring up the file I changed. And I tried to change <code>config.v2.json</code> to start <code>/bin/bash</code> instead of <code>mysqld</code> and restarted docker, but yet it started <code>mysqld</code> (due supervisor or something?) using official mysql container image.</p>
","<p>Good morning.
I am currently using Docker version 19.03 on Mac OS X Catalina.
MariaDB 10.3 was installed in Docker, vim was installed to set the character-set, and /etc/mysql/my.cnf file was modified.</p>

<p>After modification, an attempt was made to restart to reflect, but it was not executed normally.
When I checked it with docker ps -a command, STATUS showed an Exited (1) error.</p>

<p>When I checked the log showing the error, I could check the following log.</p>

<p>unknown variable'collection-server=utf8_unicode_ci'</p>

<p>Stupidly there was a typo in the settings.</p>

<p>So I am trying to modify this setting, but there is no way to modify it because the docker container is not loaded.</p>

<p>docker-compose.yml is not in use.</p>

<p>The simplest way is to delete the Docker Container, reset it, but I don't think this is the right way.</p>

<p>Is there a way to modify /etc/mysql/my.cnf inside Docker Container without using docker-compose.yml?</p>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","66421681","How can I run windows docker containers on linux?","<docker><virtual-machine><docker-machine>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I have a docker container that was built using another windows container that requires the host OS be windows.</p>
<p>Is there a way I can get this to run on linux?</p>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","62167766","Will(or When) Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><docker-desktop><windows-container>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I'm using Wine to host windows appilcation on centos server. I want to replace it into docker. But currently, <a href=""https://stackoverflow.com/questions/42158596/can-windows-containers-be-hosted-on-linux"">it is impossible</a>. I just want to know if docker team has any plan for hosting Windows container on linux system.</p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","61963319","Where Docker store volume on Windows or Mac?","<docker>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I created a docker volume</p>

<pre><code>docker inspect mongodata                                                                                                [
{
    ""CreatedAt"": ""2020-05-22T20:02:02Z"",
    ""Driver"": ""local"",
    ""Labels"": {},
    ""Mountpoint"": ""/var/lib/docker/volumes/mongodata/_data"",
    ""Name"": ""mongodata"",
    ""Options"": {},
    ""Scope"": ""local""
}
</code></pre>

<p>]</p>

<p>Do you know where the docker stores this volume on Windows?</p>

<p>OS: Windows10 64x</p>

<p>Docker Desktop Version: 2.2.0.3</p>
"
"43746782","Running nuxt js application in Docker","<javascript><docker><vue.js><nuxt.js>","62352284","Docker mapped port is not accessible outside","<docker><dockerfile><nuxt.js>","<p>I'm trying to run nuxt application in docker container. In order to do so, I created the following Dockerfile:</p>

<pre><code>FROM node:6.10.2

RUN mkdir -p /app

EXPOSE 3000

COPY . /app
WORKDIR /app
RUN npm install
RUN npm run build

CMD [ ""npm"", ""start"" ]
</code></pre>

<p>However, when I build the image and run the container (<code>docker run -p 3000:3000 &lt;image-id&gt;</code>) I get nothing while hitting <code>localhost:3000</code> in my browser. What could be the cause?</p>
","<p>I'm running:</p>

<p><code>sudo docker run -d -p 9001:9001 --rm --name &lt;cname&gt; &lt;img&gt;</code></p>

<p>then I go to my browser at <code>localhost:9001</code>, no connection.</p>

<p>If I run:</p>

<p><code>sudo docker run -d --network=host --rm --name &lt;cname&gt; &lt;img&gt;</code></p>

<p>I can access the application at <code>localhost:9001</code> from my browser.</p>

<p>Running the first command, I can verify it's running properly inside docker by running:</p>

<p><code>sudo docker exec &lt;cname&gt; wget localhost:9001</code> which returns a page as expected.</p>

<p>If it is useful: the application running is a standard <code>nuxt.js</code> that listens on port <code>9001</code>, the dockerfile used to generate the image is (ran npm build before docker image build)</p>

<pre><code>FROM node:lts-alpine

WORKDIR /app/
COPY . /app/

EXPOSE 9001

ENTRYPOINT npm start
</code></pre>

<p>The docker version I'm using is <code>19.03.8-ce</code>. How would I fix this ?</p>
"
"44262919","Selenium Server Setup Errors","<selenium><command-prompt><selenium-grid><qa>","62046636","Address already in use using RemoteWebDriver Selenium","<docker><selenium><remotewebdriver>","<p>I am trying to start a Selenium server in the command line, but it returns the following message: </p>

<pre><code>    Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\a607270&gt;java -jar selenium-server-standalone-3.4.0.jar -role hub
09:49:50.198 INFO - Selenium build info: version: '3.4.0', revision: 'unknown'
09:49:50.199 INFO - Launching Selenium Grid hub
2017-05-30 09:49:52.134:INFO::main: Logging initialized @2625ms to org.seleniumh
q.jetty9.util.log.StdErrLog
09:49:52.167 INFO - Will listen on 4444
2017-05-30 09:49:52.276:INFO:osjs.Server:main: jetty-9.4.3.v20170317
2017-05-30 09:49:52.313:INFO:osjs.session:main: DefaultSessionIdManager workerNa
me=node0
2017-05-30 09:49:52.313:INFO:osjs.session:main: No SessionScavenger set, using d
efaults
2017-05-30 09:49:52.320:INFO:osjs.session:main: Scavenging every 660000ms
2017-05-30 09:49:52.364:INFO:osjsh.ContextHandler:main: Started o.s.j.s.ServletC
ontextHandler@5e955596{/,null,AVAILABLE}
Usage: &lt;main class&gt; [options]
  Options:
    --version, -version
       Displays the version and exits.
       Default: false
    -browserTimeout
       &lt;Integer&gt; in seconds : number of seconds a browser session is allowed to
       hang while a WebDriver command is running (example: driver.get(url)). If
the
       timeout is reached while a WebDriver command is still processing, the ses
sion
       will quit. Minimum value is 60. An unspecified, zero, or negative value m
eans
       wait indefinitely.
       Default: 0
    -matcher, -capabilityMatcher
       &lt;String&gt; class name : a class implementing the CapabilityMatcher
       interface. Specifies the logic the hub will follow to define whether a re
quest can
       be assigned to a node. For example, if you want to have the matching proc
ess
       use regular expressions instead of exact match when specifying browser
       version. ALL nodes of a grid ecosystem would then use the same capability
Matcher,
       as defined here.
       Default: org.openqa.grid.internal.utils.DefaultCapabilityMatcher@3439f68d

    -cleanUpCycle
       &lt;Integer&gt; in ms : specifies how often the hub will poll running proxies
       for timed-out (i.e. hung) threads. Must also specify ""timeout"" option
       Default: 5000
    -custom
       &lt;String&gt; : comma separated key=value pairs for custom grid extensions.
       NOT RECOMMENDED -- may be deprecated in a future revision. Example: -cust
om
       myParamA=Value1,myParamB=Value2
       Default: {}
    -debug
       &lt;Boolean&gt; : enables LogLevel.FINE.
       Default: false
    -host
       &lt;String&gt; IP or hostname : usually determined automatically. Most commonly

       useful in exotic network configurations (e.g. network with VPN)
       Default: 10.56.130.102
    -hubConfig
       &lt;String&gt; filename: a JSON file (following grid2 format), which defines
       the hub properties
    -jettyThreads, -jettyMaxThreads
       &lt;Integer&gt; : max number of threads for Jetty. An unspecified, zero, or
       negative value means the Jetty default value (200) will be used.
    -log
       &lt;String&gt; filename : the filename to use for logging. If omitted, will log

       to STDOUT
    -maxSession
       &lt;Integer&gt; max number of tests that can run at the same time on the node,
       irrespective of the browser used
    -newSessionWaitTimeout
       &lt;Integer&gt; in ms : The time after which a new test waiting for a node to
       become available will time out. When that happens, the test will throw an

       exception before attempting to start a browser. An unspecified, zero, or
negative
       value means wait indefinitely.
       Default: -1
    -port
       &lt;Integer&gt; : the port number the server will use.
       Default: 4444
    -prioritizer
       &lt;String&gt; class name : a class implementing the Prioritizer interface.
       Specify a custom Prioritizer if you want to sort the order in which new s
ession
       requests are processed when there is a queue. Default to null ( no priori
ty = FIFO
       )
    -role
       &lt;String&gt; options are [hub], [node], or [standalone].
       Default: hub
    -servlet, -servlets
       &lt;String&gt; : list of extra servlets the grid (hub or node) will make
       available. Specify multiple on the command line: -servlet tld.company.Ser
vletA
       -servlet tld.company.ServletB. The servlet must exist in the path:
       /grid/admin/ServletA /grid/admin/ServletB
       Default: []
    -timeout, -sessionTimeout
       &lt;Integer&gt; in seconds : Specifies the timeout before the server
       automatically kills a session that hasn't had any activity in the last X
seconds. The
       test slot will then be released for another test to use. This is typicall
y
       used to take care of client crashes. For grid hub/node roles, cleanUpCycl
e
       must also be set.
       Default: 1800
    -throwOnCapabilityNotPresent
       &lt;Boolean&gt; true or false : If true, the hub will reject all test requests
       if no compatible proxy is currently registered. If set to false, the requ
est
       will queue until a node supporting the capability is registered with the
grid.
       Default: true
    -withoutServlet, -withoutServlets
       &lt;String&gt; : list of default (hub or node) servlets to disable. Advanced
       use cases only. Not all default servlets can be disabled. Specify multipl
e on
       the command line: -withoutServlet tld.company.ServletA -withoutServlet
       tld.company.ServletB
       Default: []

java.net.BindException: Address already in use: bind
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Unknown Source)
        at sun.nio.ch.Net.bind(Unknown Source)
        at sun.nio.ch.ServerSocketChannelImpl.bind(Unknown Source)
        at sun.nio.ch.ServerSocketAdaptor.bind(Unknown Source)
        at org.seleniumhq.jetty9.server.ServerConnector.open(ServerConnector.jav
a:298)
        at org.seleniumhq.jetty9.server.AbstractNetworkConnector.doStart(Abstrac
tNetworkConnector.java:80)
        at org.seleniumhq.jetty9.server.ServerConnector.doStart(ServerConnector.
java:236)
        at org.seleniumhq.jetty9.util.component.AbstractLifeCycle.start(Abstract
LifeCycle.java:68)
        at org.seleniumhq.jetty9.server.Server.doStart(Server.java:431)
        at org.seleniumhq.jetty9.util.component.AbstractLifeCycle.start(Abstract
LifeCycle.java:68)
        at org.openqa.grid.web.Hub.start(Hub.java:184)
        at org.openqa.grid.selenium.GridLauncherV3$2.launch(GridLauncherV3.java:
259)
        at org.openqa.grid.selenium.GridLauncherV3.main(GridLauncherV3.java:99)
</code></pre>

<p>Can anyone help me resolve this issue? If more information is needed, I can provide it. </p>

<p>I am using Windows 7 Professional, and am working in a corporate setting, so there may be proxy issues.</p>
","<p>I have a container running Selenium Hub </p>

<pre><code>2020-05-27 14:28:37.746:INFO::main: Logging initialized @1075ms to org.seleniumhq.jetty9.util.log.StdErrLog
14:28:38.098 INFO [Hub.start] - Selenium Grid hub is up and running
14:28:38.099 INFO [Hub.start] - Nodes should register to http://172.21.0.5:4444/grid/register/
14:28:38.099 INFO [Hub.start] - Clients should connect to http://172.21.0.5:4444/wd/hub
14:28:40.751 INFO [DefaultGridRegistry.add] - Registered a node http://172.21.0.7:5555
14:28:40.763 INFO [DefaultGridRegistry.add] - Registered a node http://172.21.0.8:5555
</code></pre>

<p>that is fine 
I have 2 nodes Firefox and Chrome on each docker container   </p>

<p>So when running a simple test,  I've got port 4444 is busy </p>

<pre><code>public static void main(String[] args) throws MalformedURLException {
        //DesiredCapabilities
        DesiredCapabilities dc = new DesiredCapabilities();
        dc.setBrowserName(""Chrome"");
        dc.setPlatform(Platform.LINUX);
        WebDriver driver = new RemoteWebDriver(new URL(""http://seleniumhub:4444/wd/hub""),dc);
        driver.get(""http://petclinic.com"");
    }
</code></pre>

<pre><code>15:07:42.381 INFO [GridLauncherV3.parse] - Selenium server version: 3.141.59, revision: e82be7d358
15:07:42.468 INFO [GridLauncherV3.lambda$buildLaunchers$3] - Launching a standalone Selenium Server on port 4444
2020-05-27 15:07:42.515:INFO::main: Logging initialized @363ms to org.seleniumhq.jetty9.util.log.StdErrLog
15:07:42.730 INFO [WebDriverServlet.&lt;init&gt;] - Initialising WebDriverServlet
15:07:42.810 ERROR [BaseServer.start] - Port 4444 is busy, please choose a free port and specify it using -port option
Exception in thread ""main"" java.lang.RuntimeException: java.net.BindException: Address already in use
        at org.openqa.selenium.grid.server.BaseServer.start(BaseServer.java:221)


</code></pre>

<p>That is obvious that port is busy because the hub is running, but why I can't get my test running to the relevant chrome node?
Thanks</p>
"
"44331836","apt-get install tzdata noninteractive","<bash><ubuntu><dockerfile><apt-get>","66169138","How to select an option when running Docker Build","<docker>","<p>When I try to </p>

<pre><code>apt-get install -y tzdata
</code></pre>

<p>the command line option for picking timezone shows up. I am trying to use this in a script to do some setup, how can I make the apt-get run without user input?</p>

<p>I know to reconfigure the tzdata I can do </p>

<pre><code>echo ""America/New_York"" &gt; /etc/timezone
dpkg-reconfigure -f noninteractive tzdata
</code></pre>

<p>But when installing I need it to run fully even if it doesn't set the right timezone, I can always reconfigure it. </p>

<p>I tried </p>

<pre><code>echo 5 | apt-get install -y tzdata
</code></pre>

<p>but it is not working as expected.</p>
","<p>So I am running a docker build command from my command line..and it runs for a little bit and then waits at following prompt:</p>
<pre><code>=&gt; [ 4/19] RUN apt-get update &amp;&amp; apt-get install -y --allow-unauthenticat  3758.2s
 =&gt; =&gt; #   1. Africa        6. Asia            11. System V timezones
 =&gt; =&gt; #   2. America       7. Atlantic Ocean  12. US
 =&gt; =&gt; #   3. Antarctica    8. Europe          13. None of the above
 =&gt; =&gt; #   4. Australia     9. Indian Ocean
 =&gt; =&gt; #   5. Arctic Ocean  10. Pacific Ocean
 =&gt; =&gt; # Geographic area:
</code></pre>
<p>I am assuming that it is waiting for me to select a region. However no matter what I type in the command prompt, it does not proceed further.</p>
<p>I have tried options like <code>1</code>, <code>1.</code>, <code>#1</code>, <code>=&gt;=&gt;1</code> and all other weird things I can. Please help me, this is driving me crazy!! :)</p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","62064151","How to use Docker in Jenkins from Docker (DIND: Docker in Docker)?","<docker><jenkins><docker-compose><jenkins-pipeline>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I create a jenkins with this docker-compose.yml:</p>

<pre><code>version: '3'
services:
  dind:
    image: docker:dind
    privileged: true
    expose:
      - 2375
      - 2376
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
  jenkins:
    image: jenkins/jenkins:lts
    environment:
      DOCKER_HOST: tcp://dind:2375
    ports:
      - 8081:8080
    links:
      - dind
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
      - .myProject:/home/project
</code></pre>

<p>My Jenkins is ready to use. After I use this JenkinsFile in my <code>.myProject</code> Git repository:</p>

<pre><code>pipeline {

    agent none

    stages {
        stage('Docker node test') {
          agent {
            docker {
              image 'node:7-alpine'
              args '--name docker-node' // list any args
            }
          }
          steps {
            // Steps run in node:7-alpine docker container on docker slave
            sh 'node --version'
          }
        }
        stage('Build') { 
            steps {
                sh 'npm install' 
            }
        }
    }
}
</code></pre>

<p>My result is <code>docker: not found</code>:</p>

<pre><code>[Pipeline] withEnv
[Pipeline] {
[Pipeline] isUnix
[Pipeline] sh
+ docker inspect -f . node:7-alpine
/var/jenkins_home/workspace/foo@tmp/durable-d4681405/script.sh: 1: /var/jenkins_home/workspace/foo@tmp/durable-d4681405/script.sh: docker: not found
[Pipeline] isUnix
[Pipeline] sh
+ docker pull node:7-alpine
/var/jenkins_home/workspace/foo@tmp/durable-6312e2a5/script.sh: 1: /var/jenkins_home/workspace/foo@tmp/durable-6312e2a5/script.sh: docker: not found
[Pipeline] }
[Pipeline] // withEnv
[Pipeline] }
[Pipeline] // node
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Build)
Stage ""Build"" skipped due to earlier failure(s)
[Pipeline] }
[Pipeline] // stage
[Pipeline] End of Pipeline
ERROR: script returned exit code 127
Finished: FAILURE
</code></pre>

<h2>EDIT:</h2>

<p>I custom my Jenkins:</p>

<pre><code>version: '3'
services:
  dind:
    image: docker:dind
    privileged: true
    expose:
      - 2375
      - 2376
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
  jenkins:
    build: .docker
    environment:
      DOCKER_HOST: tcp://dind:2375
    ports:
      - 8081:8080
      - 50000:50000
    links:
      - dind
    volumes:
      - .docker/jenkins_data:/var/jenkins_home
      - .myProject:/home/project
</code></pre>

<p>Dokerfile:</p>

<pre><code>FROM jenkins/jenkins:lts

LABEL maintainer=""Antoine Descamps &lt;antoine.descamps@ineat-conseil.fr&gt;""

USER root

RUN echo ""deb http://ppa.launchpad.net/ansible/ansible/ubuntu trusty main"" &gt;&gt; /etc/apt/sources.list
RUN apt-key adv --keyserver https://keyserver.ubuntu.com:443 --recv-keys 93C4A3FD7BB9C367

RUN apt-get update

RUN curl -fsSL https://get.docker.com | bash -
RUN curl -L ""https://github.com/docker/compose/releases/download/1.22.0/docker-compose-Linux-x86_64"" -o /usr/local/bin/docker-compose
RUN chmod +x /usr/local/bin/docker-compose

RUN usermod -aG docker jenkins

USER jenkins
</code></pre>

<p>Now, my error is:</p>

<pre><code>docker inspect -f . node:7-alpine

Cannot connect to the Docker daemon at tcp://dind:2375. Is the docker daemon running?
</code></pre>
"
"45173574","Docker-compose.yml file that builds a base image, then children based on it?","<docker><docker-compose><dockerfile><docker-stack>","66444428","Service to be only use for docker-compose build, but not up","<docker><docker-compose>","<p>For clarification, when I say base image, I mean the parent image that has all the common configurations, so that the children based on it don't need to download the dependencies individually.</p>

<p>From my understanding, docker-compose.yml files are the run-time configurations, while Dockerfiles are the build-time configurations. However, there is a <code>build</code> option using docker-compose, and I was wondering how I could use this to build a base image.</p>

<p>As of right now, I use a shellscript that runs other shellscripts. One builds all my images, from a base image that it also creates. The other runs them as containers with the necessary configurations. However, the base image is never ran as a container.</p>

<p>Currently, the shellscript I hope to change into a docker-compose file, looks like so:</p>

<pre><code>echo ""Creating docker network net1""
docker network create net1

echo ""Running api as a container with port 5000 exposed on net1""
docker run --name api_cntr --net net1 -d -p 5000:5000 api_img

echo ""Running redis service with port 6379 exposed on net1""
docker run --name message_service --net net1 -p 6379:6379 -d redis

echo ""Running celery worker on net1""
docker run --name celery_worker1 --net net1 -d celery_worker_img

echo ""Running flower HUD on net1 with port 5555 exposed""
docker run --name flower_hud --net net1 -d -p 5555:5555 flower_hud_img
</code></pre>

<p>The shellscript that makes the images, is as follows:</p>

<pre><code>echo ""Building Base Image""
docker build -t base ../base-image

echo ""Building api image from Dockerfile""
docker build -t api_img  ../api

echo ""Building celery worker image""
docker build -t celery_worker_img ../celery-worker

echo ""Building celery worker HUD""
docker build -t flower_hud_img ../flower-hud
</code></pre>

<p>My questions comes down to one thing, can I create this Base image without ever running it in a container with docker-compose. (All the Dockerfiles start with <code>FROM base:latest</code> other than the base itself). I'm looking to make it as easy as possible for other people, so that they only have to run a single command.</p>

<p>EDIT: I am using version 3, and acording to the docs, <code>build:</code> is ignored, and docker-compose only accepts pre-built images.</p>
","<p>Is there a way to specify one service which would not be initialised when <code>docker-compose up</code> is called, but will be built when <code>docker-compose build</code> is run.</p>
<p>This might be a weird thing I am trying to do, but the goal is to use that built image as a base for other multiple services.</p>
"
"47394428","docker-compose run and exec: No container found","<docker-compose>","62330882","docker compose exec changing my parameter value","<docker><docker-compose>","<p>I was trying to open a second terminal un a docker with docker-compose.</p>

<p>First run the container with </p>

<pre><code>docker-compose run my-centos bash
</code></pre>

<p>And when I try to open a second terminal</p>

<pre><code>docker-compose exec my-centos bash
</code></pre>

<p>I get the message</p>

<pre><code>ERROR:No container found for my_centos_1
</code></pre>

<p>If I search the name of running container I get</p>

<pre><code>CONTAINER ID        IMAGE                                 COMMAND                  CREATED             STATUS                    PORTS                    NAMES
34a95b44f0a2        centos6   ""bash""                   9 minutes ago       Up 9 minutes                                       docker_my-centos_run_1
</code></pre>

<p>why docker-compose exec search <code>docker_my_centos_1</code> and not <code>docker_my-centos_run_1</code>?</p>
","<p>I have a docker-compose.yml file like this:</p>

<pre><code>version: ""3.7""
services:
    server:
        build: .
        ports:
            - ""3000:3000""
</code></pre>

<p>so my service name is <code>server</code>. When i try to do this:</p>

<pre><code>docker-compose exec server sh
</code></pre>

<p>it gives me an error:</p>

<blockquote>
  <p>ERROR: No container found for server_1</p>
</blockquote>

<p>why is it changing my param from <code>server</code> to <code>server_1</code>? and what's the correct way to run this command? I tried this too:</p>

<pre><code>docker-compose ps

// output: name: website-service-api_server_run_dfb252d8a1fc
// then

docker-compose exec website-service-api_server_run_dfb252d8a1fc sh
</code></pre>

<p>same error:</p>

<blockquote>
  <p>ERROR: No such service: website-service-api_server_run_dfb252d8a1fc</p>
</blockquote>
"
"48593016","Postgresql Docker role does not exist","<database><postgresql><docker>","62084379","Not able to connect to the user using Postgres python package in docker container","<python><postgresql><docker><docker-compose>","<p>I downloaded the docker container for postgres: <a href=""https://hub.docker.com/r/library/postgres/"" rel=""noreferrer"">https://hub.docker.com/r/library/postgres/</a>, and did the following:</p>

<pre><code>$ docker run --name satgres -e POSTGRES_PASSWORD=mysecretpassword -d -p 5432:5432 postgres
satgres | ""docker-entrypoint.s…"" | 5 seconds ago | Up 6 seconds | 0.0.0.0:5432-&gt;5432/tcp | satgres
$ docker exec -it c8f1b22cc41e /bin/bash        
# psql -U postgres
psql (10.1)
postgres=# CREATE DATABASE mytest;
postgres=# \password postgres
postgres=# CREATE ROLE ming WITH LOGIN PASSWORD 'pass1234';
postgres=# ALTER ROLE ming CREATEDB;
postgres=# CREATE DATABASE sjk;
postgres=# GRANT ALL PRIVILEGES ON DATABASE youtube TO ming;
postgres=# \connect sjk
youtube=# ALTER ROLE ming lOGIN;  
</code></pre>

<p>My plan is to use this database on docker with Django, so first want to check I can connect, but I cant.</p>

<pre><code>$ psql -h localhost -p 5432 -U postgres
$ psql: FATAL:  role ""postgres"" does not exist
$ psql -h localhost -p 5432 -U ming
$ psql: FATAL:  role ""ming"" does not exist
</code></pre>

<p>I do the same with PSequal.app, says the same thing,
Im running on Mac High Sierra. </p>

<p>Why am I getting this error? The role exists, or at least it seems it to me...</p>
","<p>I have created a docker image of postgres along with Kafka stream using docker-compose.</p>

<pre><code>version: ""3""

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - stream-network
  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    networks: 
        - stream-network
  streamer:
    build:
      context: ./streamingProducer/
    networks: 
      - stream-network
    depends_on:
      - kafka
  consumer:
    build:
      context: ./streamingConsumer/
    networks: 
      - stream-network
    depends_on:
      - kafka
      - postgres  
  postgres:
    # image: postgres
    # restart: always
    # environment:
    #   POSTGRES_USER: docker
    #   POSTGRES_PASSWORD: docker
    #   POSTGRES_DB: test
    #   PGDATA: /var/lib/postgresql/data/pgdata
    # ports:
    #   - ""5432:5432""
    # healthcheck:
    #   test: [""CMD-SHELL"", ""pg_isready -U docker""]
    # volumes:
    #   - ./data/postgres/pgdata:/var/lib/postgresql/data/pgdata
      build: './db'
      restart: always
      environment:
        - ""FILLA_DB_USER=filla""
        - ""FILLA_DB_PASSWORD=filla""
        - ""FILLA_DB_DATABASE=filladb1""
        - ""POSTGRES_USER=postgres""
        - ""POSTGRES_PASSWORD=password""
      ports:
        - ""5432:5432""
      networks: 
        - stream-network 
</code></pre>

<p>The docker image of Postgres seems to run fine, the log looks completely good</p>

<pre><code>`postgres_1 | creating configuration files … ok
zookeeper_1 | ===&gt; User
postgres_1 | running bootstrap script … ok
zookeeper_1 | uid=0(root) gid=0(root) groups=0(root)
zookeeper_1 | ===&gt; Configuring …
postgres_1 | performing post-bootstrap initialization … sh: locale: not found
postgres_1 | 2020-05-27 11:52:45.209 UTC [29] WARNING: no usable system locales were found
slimmerai_consumer_1 exited with code 0
postgres_1 | ok
postgres_1 | syncing data to disk … ok
postgres_1 | initdb: warning:
postgres_1 | enabling “trust” authentication for local connections
postgres_1 | You can change this by editing pg_hba.conf or using the option -A, or
postgres_1 | --auth-local and --auth-host, the next time you run initdb.
postgres_1 |
postgres_1 | Success. You can now start the database server using:
postgres_1 |
postgres_1 | pg_ctl -D /var/lib/postgresql/data -l logfile start
postgres_1 |
postgres_1 | waiting for server to start…2020-05-27 11:52:46.729 UTC [34] LOG: starting PostgreSQL 12.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 9.2.0) 9.2.0, 64-bit
postgres_1 | 2020-05-27 11:52:46.736 UTC [34] LOG: listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”
postgres_1 | 2020-05-27 11:52:46.767 UTC [35] LOG: database system was shut down at 2020-05-27 11:52:46 UTC
postgres_1 | 2020-05-27 11:52:46.771 UTC [34] LOG: database system is ready to accept connections
postgres_1 | done
postgres_1 | server started
postgres_1 |
postgres_1 | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/01-filladb.sh
postgres_1 | CREATE ROLE
postgres_1 | CREATE DATABASE
postgres_1 | GRANT
postgres_1 |
postgres_1 | 2020-05-27 11:52:46.989 UTC [34] LOG: received fast shutdown request
postgres_1 | waiting for server to shut down…2020-05-27 11:52:46.991 UTC [34] LOG: aborting any active transactions
postgres_1 | 2020-05-27 11:52:46.996 UTC [34] LOG: background worker “logical replication launcher” (PID 41) exited with exit code 1
postgres_1 | 2020-05-27 11:52:46.996 UTC [36] LOG: shutting down
postgres_1 | 2020-05-27 11:52:47.010 UTC [34] LOG: database system is shut down
postgres_1 | done
postgres_1 | server stopped
postgres_1 |
postgres_1 | PostgreSQL init process complete; ready for start up.
postgres_1 |
postgres_1 | 2020-05-27 11:52:47.100 UTC [1] LOG: starting PostgreSQL 12.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 9.2.0) 9.2.0, 64-bit
postgres_1 | 2020-05-27 11:52:47.101 UTC [1] LOG: listening on IPv4 address “0.0.0.0”, port 5432
postgres_1 | 2020-05-27 11:52:47.101 UTC [1] LOG: listening on IPv6 address “::”, port 5432
postgres_1 | 2020-05-27 11:52:47.104 UTC [1] LOG: listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”
postgres_1 | 2020-05-27 11:52:47.125 UTC [45] LOG: database system was shut down at 2020-05-27 11:52:47 UTC
postgres_1 | 2020-05-27 11:52:47.129 UTC [1] LOG: database system is ready to accept connections
`
</code></pre>

<p>And when I log into the database docker container, I can see the users and all the db being created.</p>

<p>But when I try to access the db using python package</p>

<pre><code>from psycopg2 import Error

try:
connection = psycopg2.connect(user = ""postgres"",
password = ""password"",
host = ""localhost"",
port = ""5432"",
database = ""filladb1"")


cursor = connection.cursor()

print(""Table connected successfully in PostgreSQL "")
except (Exception, psycopg2.DatabaseError) as error :
print (“Error while creating PostgreSQL table”, error)`
</code></pre>

<p>It throws an error:</p>

<p><strong>Error while creating PostgreSQL table FATAL: role “postgres” does not exist</strong></p>

<p>Not sure, what am I doing wrong, any help would be appreciated. Thanks.</p>
"
"48745936","Many <none> images created after build a docker image","<docker><dockerfile>","62061332","How to avoid Docker images with Repository and tag <none> while building an new Image","<docker><docker-compose><dockerfile>","<pre><code>FROM scratch
MAINTAINER Aario &lt;AarioAi@gmail.com&gt;
ENV SHARED_GROUP docker
</code></pre>

<p>I build a docker image with above dockerfile.
After runing <code>docker build -t ""aario/centos"" .</code> 
It creates this image aario/centos and the <code>&lt;none&gt;</code> image:</p>

<p><a href=""https://i.stack.imgur.com/CDdH4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CDdH4.png"" alt=""enter image description here""></a></p>

<p>Is it ok? And how to solve it?</p>

<p>When I run <code>docker rmi 2f???????</code> to remove the aario/centeros image. The <code>&lt;none&gt;</code> image will be removed on the same time.</p>
","<p>I am trying to build a new docker image. And this is how my Dockerfile look like</p>

<pre><code>FROM docker-standard-all/dev_env_g3_g5:latest

WORKDIR /root/

COPY controller.pom.xml .
COPY docker_settings.xml .
COPY stage.sh .

RUN mkdir ~/.m2

RUN ln -sf ~/docker_settings.xml ~/.m2/settings.xml

# Initialize Maven repository
RUN mvn dependency:resolve -f controller.pom.xml -q

ENTRYPOINT [""./stage.sh""]
</code></pre>

<p>After building a docker</p>

<pre><code>docker build -t bc/controller-in-docker:1.0.2"" ""${PATH}""
</code></pre>

<p>When i perform </p>

<pre><code>docker images -a
REPOSITORY                                          TAG                 IMAGE ID            CREATED             SIZE
bc/controller-in-docker                             1.0.2               a32e212bc0f7        2 hours ago         2.41GB
&lt;none&gt;                                              &lt;none&gt;              f4e68f5a2720        2 hours ago         2.41GB
&lt;none&gt;                                              &lt;none&gt;              b71751242efd        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              26e3a086889c        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              4acf1759940a        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              0d66510e6a67        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              adc5c7038a4b        2 hours ago         2.4GB
&lt;none&gt;                                              &lt;none&gt;              a994c54e00af        2 hours ago         2.4GB
docker-standard-all/dev_env_g3_g5                   1.17.4              6a83d123f497        6 months ago        2.4GB
</code></pre>

<p>After a bit googling and stack over-overflow, I just realized that : containers are not dangling containers. In fact they appeared to be formed after running the commands like COPY, RUN etc in Dockerfile.</p>

<p>Questions:</p>

<ol>
<li><p>What is the overall size of my new docker images? My each none:none container appears to occupy ~2.41GB from terminal output. In total they all add unto 2.4 * 7 GB</p></li>
<li><p>Can I delete these none:none Images ? They are not dangling images for sure. But they are occupying 2.4 GB each (not sure though) and also confusing name associated with them.</p></li>
<li><p>Is it possible to avoid these none:none images while building the new docker Image ? Any suggestions/modifications to the Dockerfile in order to avoid them?</p></li>
</ol>

<p>Thanks</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","62137599","Docker requires sudo to run","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>When I try to run any docker command, for example <code>docker ps</code>, I'm getting this error:</p>

<blockquote>
  <p>Got permission denied while trying to connect to the Docker daemon
  socket at unix:///var/run/docker.sock: Get
  <a href=""http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json"" rel=""nofollow noreferrer"">http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json</a>: dial unix
  /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<p>So I manage to run it using sudo:</p>

<pre><code>sudo docker ps
</code></pre>

<p>But I want to be able to run that without sudo.</p>
"
"51034120","Docker Could not resolve 'deb.debian.org'","<ubuntu><docker><debian>","61893153","Docker build in Jenkins problem with apt-get install","<docker><jenkins><jenkins-pipeline>","<p>I was trying to build a container in Docker for some experiments. Here is my <code>Dockerfile</code>.</p>

<pre><code>FROM debian

RUN mkdir -p /var/run/sshd

RUN apt-get update
RUN apt-get install -y openssh-server

RUN apt-get install -y sudo

RUN echo AddressFamily inet &gt;&gt; /etc/ssh/sshd_config

ARG username=Rivers
ARG userpasswd=perfectXJ2017

RUN useradd -ms /bin/bash $username &amp;&amp; (echo $username:$userpasswd | chpasswd)

RUN adduser $username sudo

CMD /usr/sbin/sshd -D
</code></pre>

<p>I tried to build my image with the command <code>sudo docker build -t ics-image .</code>.</p>

<p>But then I got some error messages and the whole process stopped. Here is the error messages.</p>

<pre><code>Sending build context to Docker daemon  2.048kB
Step 1/11 : FROM debian
 ---&gt; 8626492fecd3
Step 2/11 : RUN mkdir -p /var/run/sshd
 ---&gt; Running in 1e1f2dbbe5ca
Removing intermediate container 1e1f2dbbe5ca
 ---&gt; dd4bec2f81d4
Step 3/11 : RUN apt-get update
 ---&gt; Running in 022301215bfb
Get:1 http://security.debian.org/debian-security stretch/updates InRelease [94.3 kB]
Get:2 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [450 kB]
Err:3 http://deb.debian.org/debian stretch InRelease
  Could not resolve 'deb.debian.org'
Err:4 http://deb.debian.org/debian stretch-updates InRelease
  Could not resolve 'deb.debian.org'
Fetched 544 kB in 20s (26.9 kB/s)
Reading package lists...
W: Failed to fetch http://deb.debian.org/debian/dists/stretch/InRelease  Could not resolve 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/stretch-updates/InRelease  Could not resolve 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
Removing intermediate container 022301215bfb
 ---&gt; 054d32710b62
Step 4/11 : RUN apt-get install -y openssh-server
 ---&gt; Running in 802d2fa37b8d
Reading package lists...
Building dependency tree...
Reading state information...
Package openssh-server is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Package 'openssh-server' has no installation candidate
The command '/bin/sh -c apt-get install -y openssh-server' returned a non-zero code: 100
</code></pre>

<p>I did all this on Ubuntu 18.04. I cannot understand why this happened and how to solve this problem. Can anyone help?</p>
","<p>i am trying to build a docker image inside of jenkins with a JENKINSFILE.
It works fine when i build it local, but on jenkins i get fellow ouput</p>

<p>Local i use the same command but it works.</p>

<p>Has Anyone a clue why it doesn't works on jenkins?</p>

<p>UPDATE: </p>

<pre><code>+ docker build --no-cache -t app:build .
Sending build context to Docker daemon  11.87MB

Step 1/17 : FROM golang:1.14.3-stretch
 ---&gt; cc1f903b6fe7
Step 2/17 : RUN mkdir -p  /app
 ---&gt; Running in b8a0f9f5c73f
Removing intermediate container b8a0f9f5c73f
 ---&gt; b555e33481ca
Step 3/17 : RUN echo nameserver 8.8.8.8 &gt;&gt; /etc/resolv.conf &amp;&amp; cat /etc/resolv.conf
 ---&gt; Running in fb7779fc731a
# This file is managed by man:systemd-resolved(8). Do not edit.
#
# This is a dynamic resolv.conf file for connecting local clients directly to
# all known uplink DNS servers. This file lists all configured search domains.
#
# Third party programs must not access this file directly, but only through the
# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,
# replace this symlink by a static file or a different symlink.
#
# See man:systemd-resolved.service(8) for details about the supported modes of
# operation for /etc/resolv.conf.

nameserver 8.8.8.8
Removing intermediate container fb7779fc731a
 ---&gt; 8445167d56f7
Step 4/17 : RUN chmod 777 /etc/resolv.conf
 ---&gt; Running in d910167101cf
Removing intermediate container d910167101cf
 ---&gt; eeb553e7fb83
Step 5/17 : RUN apt-get update &amp;&amp; yes | apt-get install libxml2
 ---&gt; Running in 3c09069d085c
Err:1 http://security.debian.org/debian-security stretch/updates InRelease
  Could not resolve 'security.debian.org'
Err:2 http://deb.debian.org/debian stretch InRelease
  Could not resolve 'deb.debian.org'
Err:3 http://deb.debian.org/debian stretch-updates InRelease
  Could not resolve 'deb.debian.org'
Reading package lists...
[91mW: Failed to fetch http://deb.debian.org/debian/dists/stretch/InRelease  Could not resolve 'deb.debian.org'[0m[91m
W: Failed to fetch http://security.debian.org/debian-security/dists/stretch/updates/InRelease  Could not resolve 'security.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/stretch-updates/InRelease  Could not resolve 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
[0mReading package lists...
Building dependency tree...
Reading state information...
Package libxml2 is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

[91mE: Package 'libxml2' has no installation candidate[0m[91m
[0mThe command '/bin/sh -c apt-get update &amp;&amp; yes | apt-get install libxml2' returned a non-zero code: 100
</code></pre>

<p>Thanks!</p>
"
"51388921","Pass command line args to npm scripts in package.json","<npm><package.json><npm-scripts>","66175711","pass variable to npm command string (not the script run by npm command)","<node.js><docker><npm><npm-scripts>","<p>I have the below scripts in my package.json:</p>

<pre><code>""scripts"": {
    ""vumper"": ""node node_modules/vumper/index.js"",
    ""format"": ""prettier --single-quote -width=80 --write package.json""
 },
</code></pre>

<p>The 'vumper' package takes in a command line argument (such as 'dv'). What I would like to be able to do is have a command that runs both of these in succession.</p>

<p>Essentially, I would like to be able to run:</p>

<pre><code>npm run vumber dv
</code></pre>

<p>and then</p>

<pre><code>npm run format
</code></pre>

<p>but in one command, something like </p>

<pre><code>npm run my-build dv
</code></pre>

<p>which would run both of the above commands, correctly accepting the command line argument 'dv' and passing it to the first npm run vumper. Is this possible?</p>
","<p>Is it possible to pass an argument to the npm command string? The info might be somewhere in docs, but brief digging didn't get me any results, moreover, it is probably beneficial to the large audience.</p>
<p>For example, i have a command <code>build:docker</code> in <code>package.json</code>:</p>
<pre><code>   &quot;scripts&quot;: {
      &quot;build:docker&quot;: &quot;tsc -b &amp;&amp; sh dist.sh &amp;&amp; docker build -t repo_name/image .&quot;,
   },
</code></pre>
<p>I want to pass the <code>tag</code> variable to the command string, ex:</p>
<pre><code>&quot;build:docker&quot;: &quot;tsc -b &amp;&amp; sh dist.sh &amp;&amp; docker build -t repo_name/image:$tag .&quot;
</code></pre>
<p>and run it as <code>npm run build:docker --tag '1.0'</code>.</p>
<p>Is something like this possible? Thanks!</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","66476303","java with kafka docker image Broker may not be available.Error","<spring><spring-boot><docker><spring-mvc><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i want run kafka with java spring boot application
but when run my application kafka cannot connection with my application</p>
<p>there error when run application is :</p>
<pre><code> 2021-03-04 15:32:12.787  WARN 14381 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group_id-1, groupId=group_id] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2021-03-04 15:32:12.787  WARN 14381 --- [ntainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group_id-1, groupId=group_id] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected
</code></pre>
<p>my application.yml is:</p>
<pre><code> spring:
  kafka:
    consumer:
      bootstrap-servers: localhost:9092
      group-id: group_id
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    producer:
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
</code></pre>
<p>the spring cannot see kafka container
in kafka execute i can add new topic but from spring i dont send message</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62367752","Can't Connect to Kafka running on Docker","<docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I deployed Kafka using Docker Compose to a server but I am unable to connect to Kafka. I used the bitnami/kafka images.</p>

<pre><code>$ curl -sSL https://raw.githubusercontent.com/bitnami/bitnami-docker-kafka/master/docker-compose.yml &gt; docker-compose.yml
$ docker-compose up -d
</code></pre>

<p>Using Kafka Tool I've tried connecting to Zookeeper and using bootstrap to try connecting directly to Kafka. It always times out however I have verified I can hit port 2181 and 9092 on the docker host and ping from Kafka tool works as well which leads me to believe those services are up and listening removing the possibility of firewall/network issues.</p>

<p><a href=""https://i.stack.imgur.com/fTyO1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fTyO1.png"" alt=""enter image description here""></a></p>

<p>My docker compose file</p>

<pre><code>version: '2'

services:
  zookeeper:
    image: 'bitnami/zookeeper:3'
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper_data:/bitnami'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: 'bitnami/kafka:2'
    ports:
      - '9092:9092'
      - '9093:9093'
    volumes:
      - 'kafka_data:/bitnami'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","66156962","Spring boot and Apache Kafka via Docker compose throw LEADER_NOT_AVAILABLE of topic and Send failed error","<spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I just wanted to implement Spring boot and Apache Kafka via docker-compose.
Firstly I installed the Apache Kafka Docker image via this link: <a href=""https://hub.docker.com/r/ches/kafka/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/ches/kafka/</a> and then install jplock/zookeeper via this command shown below.</p>
<p><code>docker run -d — name zookeeper — publish 2181:2181 jplock/zookeeper:latest</code></p>
<p>Then I run this command (<code>docker ps</code>) to determine if all images work. their status of all these images is up. That's why they work flawlessly.</p>
<p><img src=""https://i.stack.imgur.com/F6YaP.png"" alt=""Images"" /></p>
<p>Then I run the app and sent a message via Postman. The post url is <code>localhost:8080/kafkamessage</code>.</p>
<p>JSON object is here</p>
<pre><code>{
    &quot;message&quot; : &quot;Hello World&quot;
}
</code></pre>
<p>When I sent a request, it throws an error shown below.</p>
<pre><code>&quot;message&quot;: &quot;Send failed; nested exception is org.apache.kafka.common.errors.TimeoutException: Topic k-topic not present in metadata after 60000 ms.&quot;
</code></pre>
<p>Expect for this, I defined an auto-created topic but the consumer and producer couldn't produce the topic in the console</p>
<pre><code>[Consumer clientId=consumer-k-group-1, groupId=k-group] Error while fetching metadata with correlation id 728 : {k-topic=LEADER_NOT_AVAILABLE}
[Producer clientId=producer-1] Error while fetching metadata with correlation id 681 : {k-topic=LEADER_NOT_AVAILABLE}
</code></pre>
<p>Here is my error</p>
<p><img src=""https://i.stack.imgur.com/yU6Es.png"" alt=""enter image description here"" /></p>
<p>Here is the link: <a href=""https://github.com/Rapter1990/springbootkafka"" rel=""nofollow noreferrer"">https://github.com/Rapter1990/springbootkafka</a></p>
<p>How can I fix the issue?</p>
<p><img src=""https://i.stack.imgur.com/NcIWS.png"" alt=""enter image description here"" /></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","61990336","Kafka - Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092","<docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to get one of my services up and running locally within the docker network, but keep getting <strong>""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092""</strong> exception.</p>

<pre><code>zookeeper:
  image: wurstmeister/zookeeper:3.4.6
  restart: always
  expose:
      - ""2181""



kafka:
      depends_on:
          - zookeeper
      environment:
          KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
          KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      hostname: kafka
      image: wurstmeister/kafka:2.11-2.0.0
      restart: always
      ports:
          - ""9092:9092""
      expose:
          - ""9093""
      links:
          - zookeeper:zookeeper
</code></pre>

<p>That's what I've set in my service's docker-compose for the consumers:</p>

<pre><code>  consumers:
    environment:
      - BROKER_URL=host.docker.internal:9092
</code></pre>

<p>That's the error message:</p>

<pre><code>..`block in execute'"",""/usr/local/bundle/gems/rake-12.3.2/lib/rake/task.rb:273:in `each'""],
""message"":""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092""},
""message"":""Consumer failed to start: Connection refused - connect(2) for 127.0.0.1:9092"",
""@timestamp"":""2020-05-24T18:08:09.434+00:00"",""@version"":""1"",
""severity"":""ERROR"",""host"":""87c5265c6a39""}
</code></pre>

<p><strong>UPDATE:</strong> When checking Kafkacat:</p>

<pre><code>kafkacat -L -b 127.0.0.1:9092
Metadata for all topics (from broker 0: 127.0.0.1:9092/0):
 1 brokers:
  broker 0 at 127.0.0.1:9092 (controller)
 7 topics: ...
</code></pre>

<p>When checking Zookeeper CLI (within Docker):</p>

<pre><code>[zk: localhost:2181(CONNECTED) 2] ls /brokers/ids
[1001]
</code></pre>

<p>I've tried already pretty much everything I could find, but still not able to understand what am I missing.</p>

<p>Any help would be appreciated, thanks!</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62136489","Spring can't find running kafka image","<java><spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am new to Kafka and I am trying to send messages via publisher using spring application and kafka docker container. Kafka and zookeeper is up and running but when I start springboot application it shows:</p>

<pre><code>2020-06-01 18:19:05.738  WARN 4824 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Connection to node -1 (/127.0.0.1:9092) could not be established. Broker may not be available.

2020-06-01 18:19:12.203  WARN 4824 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Bootstrap broker 127.0.0.1:9092 (id: -1 rack: null) disconnected
</code></pre>

<p>Here you can see both containers running:</p>

<pre><code>4f74ceac73ba   wurstmeister/kafka:2.12-2.5.0   ""start-kafka.sh""  0.0.0.0:9092-&gt;9092/tcp kafka 

fc958792a4a0  zookeeper:3.6.1 ""/docker-entrypoint.…"" 2888/tcp, 3888/tcp, 0.0.0.0:2181 &gt;2181/tcp, 8080/tcp   zookeeper
</code></pre>

<p>Here you can see zookeeper logs:</p>

<pre><code>2020-06-01 13:17:39,518 [myid:1] - INFO  [main:SnapStream@61] - zookeeper.snapshot.compression.method = CHECKED
2020-06-01 13:17:39,519 [myid:1] - INFO  [main:FileTxnSnapLog@470] - Snapshotting: 0x0 to /data/version-2/snapshot.0
2020-06-01 13:17:39,536 [myid:1] - INFO  [main:ZKDatabase@289] - Snapshot loaded in 45 ms, highest zxid is 0x0, digest is 1371985504
2020-06-01 13:17:39,543 [myid:1] - INFO  [main:FileTxnSnapLog@470] - Snapshotting: 0x0 to /data/version-2/snapshot.0
2020-06-01 13:17:39,544 [myid:1] - INFO  [main:ZooKeeperServer@519] - Snapshot taken in 2 ms
2020-06-01 13:17:39,571 [myid:1] - INFO  [main:RequestThrottler@74] - zookeeper.request_throttler.shutdownTimeout = 10000
2020-06-01 13:17:39,671 [myid:1] - INFO  [main:ContainerManager@83] - Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0
2020-06-01 13:17:39,674 [myid:1] - INFO  [main:ZKAuditProvider@42] - ZooKeeper audit is disabled.
2020-06-01 13:17:40,943 [myid:1] - INFO  [SyncThread:0:FileTxnLog@284] - Creating new log file: log.1
</code></pre>

<p>and docker-compose file to bootstrap containers:</p>

<pre><code>version: ""3.7""

networks:
  kafka-net:
    name: kafka-net
    driver: bridge

services:
  zookeeper:
    image: zookeeper:3.6.1
    container_name: zookeeper
    restart: always
    networks:
      - kafka-net
    ports:
      - ""2181:2181""
    volumes:
      - c:/kafka/docker-data/zookeeper:/bitnami/zookeeper

  kafka:
    image: wurstmeister/kafka:2.12-2.5.0
    container_name: kafka
    restart: always
    networks:
      - kafka-net
    ports:
      - ""9092:9092""
    volumes:
      - c:/kafka/docker-data/kafka:/bitnami/kafka
    environment:
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: DOCKER_INTERNAL:PLAINTEXT,DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: DOCKER_INTERNAL://:29092,DOCKER_EXTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: DOCKER_INTERNAL://kafka:29092,DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: ""zookeeper:2181""
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
</code></pre>

<p>There is no additional configuraion in application-properties. Any help would be appreciated, I am pretty much stuck with it :(</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62280748","kafka container - Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm following <a href=""https://github.com/wurstmeister/kafka-docker/wiki/Connectivity"" rel=""nofollow noreferrer"">this guide</a> in order to start a single kafka container .I'm working on win10 pro and my cli is gitbash.</p>

<p>docker-compose-single-broker.yml : </p>

<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""
  kafka:
    image: wurstmeister/kafka 
    ports:
      - ""9092:9092""
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_CREATE_TOPICS: ""test:1:1""
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>

<p>The containers are starting without any problem : </p>

<pre><code>$ docker-compose -f docker-compose-single-broker.yml up -d
Creating kafka-docker_zookeeper_1 ... done
Creating kafka-docker_kafka_1     ... done

$ docker ps
CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                NAMES
19929c6c3297        wurstmeister/kafka       ""start-kafka.sh""         17 seconds ago      Up 15 seconds       0.0.0.0:9092-&gt;9092/tcp                               kafka-docker_kafka_1
d343a8ecf7ed        wurstmeister/zookeeper   ""/bin/sh -c '/usr/sb…""   17 seconds ago      Up 15 seconds       22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   kafka-docker_zookeeper_1
</code></pre>

<p>However, when I try to run any kafka command(create topic,list topics) from inside the container like the following : </p>

<pre><code>&gt; $KAFKA_HOME/bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server `broker-list.sh`
&gt; $KAFKA_HOME/bin/kafka-topics.sh --describe --topic test --bootstrap-server `broker-list.sh`
</code></pre>

<p>I'm getting the following warnings and afterwards a timeout exception : </p>

<pre><code>$ ./start-kafka-shell.sh localhost
bash-4.4# $KAFKA_HOME/bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server `broker-list.sh`
[2020-06-09 11:47:26,241] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (/172.17.54.145:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2020-06-09 11:47:29,306] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (/172.17.54.145:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
</code></pre>

<p>The content of the start-kafka-shell.sh : </p>

<pre><code>$ cat start-kafka-shell.sh
#!/bin/bash
docker run --rm -v //var/run/docker.sock:/var/run/docker.sock -e HOST_IP=$1 -e ZK=$2 -i -t wurstmeister/kafka /bin/bash
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62213671","How to solve kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs?","<docker><apache-kafka><kafka-python>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>My docker-compose.yml code is:</p>

<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
</code></pre>

<p>and my dockerfile is</p>

<pre><code>FROM python
MAINTAINER Shubham Joshi 
ADD hello.py /
ADD transactions_producer.py /
COPY requirements.txt .
RUN pip3 install -r requirements.txt
CMD [""python"",""./transactions_producer.py""]
</code></pre>

<p>python code for the producer.py is:</p>

<pre><code>from kafka import KafkaProducer
from time import sleep
from kafka.errors import KafkaError
import json

producer = KafkaProducer(bootstrap_servers=['localhost:9092'],api_version=(0, 10, 0),value_serializer=lambda v: json.dumps(v).encode('utf-8'))


for i in range(10):
    producer.send('shubham', create_random_transaction())
    sleep(5)
    print(""Success"",i)

producer.close()
</code></pre>

<p>shubham is the topic I created but I am not able to produce/publish messages in the topic and getting this timeout error.</p>

<p>step 1: Created an image:</p>

<pre><code>docker build -t python_producer2 .
</code></pre>

<p>it built successfully, the I ran</p>

<pre><code>step 2:docker run python_producer2
</code></pre>

<p>this is where I am getting this error</p>

<pre><code>Traceback (most recent call last):
  File ""./transactions_producer.py"", line 52, in &lt;module&gt;
    producer.send('shubham', create_random_transaction())
  File ""/usr/local/lib/python3.8/site-packages/kafka/producer/kafka.py"", line 564, in send
    self._wait_on_metadata(topic, self.config['max_block_ms'] / 1000.0)
  File ""/usr/local/lib/python3.8/site-packages/kafka/producer/kafka.py"", line 690, in _wait_on_metadata
    raise Errors.KafkaTimeoutError(
kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Failed to update metadata after 60.0 secs.
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62228060","How to fix error: kafka.errors.NoBrokersAvailable: NoBrokersAvailable?","<docker><apache-kafka><docker-compose><kafka-python><confluent-platform>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have setup docker-compose.yml as </p>

<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_HOST_NAME: kafka

  client:
    image: python_consumer2
    container_name: python_consumer2
    depends_on: 
      - kafka
</code></pre>

<p>While doing docker-compose -f docker-compose.yml up, I am getting below error, except this kafka and zookeeper sevices are up. </p>

<pre><code>python_consumer2 |   File ""./transactions_consumer.py"", line 8, in &lt;module&gt;
python_consumer2 |     consumer = KafkaConsumer(bootstrap_servers=['localhost:9092'],
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/consumer/group.py"", line 355, in __init__
python_consumer2 |     self._client = KafkaClient(metrics=self._metrics, **self.config)
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/client_async.py"", line 242, in __init__
python_consumer2 |     self.config['api_version'] = self.check_version(timeout=check_timeout)
python_consumer2 |   File ""/usr/local/lib/python3.8/site-packages/kafka/client_async.py"", line 898, in check_version
python_consumer2 |     raise Errors.NoBrokersAvailable()
python_consumer2 | kafka.errors.NoBrokersAvailable: NoBrokersAvailable
</code></pre>

<p>My python_consumer.py contains below configurations: </p>

<pre><code>consumer = KafkaConsumer(bootstrap_servers=['localhost:9092'],
                                 auto_offset_reset='earliest',
                                 value_deserializer=lambda m: json.loads(m.decode('utf-8')),consumer_timeout_ms=10000)
consumer.subscribe(['shubham'])
</code></pre>

<p>I have already published data in the kafka topic 'shubham' I am subscribing to and I have tried changing bootstrap_servers = ['kafka:29092'], still I am getting the same error. How to fix this?</p>
"
"44769315","How to see docker image contents","<docker>","66332689","Explore content of files of nginx container on my host machine","<docker><docker-compose><docker-container>","<p>I did a docker pull and can list the image that's downloaded. I want to see the contents of this image. Did a search on the net but no straight answer.</p>
","<p>How can i see content of files of all my nginx container on my host machine ?
I want to see all the configurations files for exemple.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","62240417","Unable to create Kafka topics with Kafka and Zookeeper running on Docker","<docker><apache-kafka><apache-zookeeper>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have Kafka and Zookeeper running on two separate Docker containers:</p>

<pre><code>&lt;private-domain&gt;/wurstmeister-kafka:0.10.1.0-2
&lt;private-domain&gt;/wurstmeister-zookeeper:3.4.9
</code></pre>

<p>Both containers seem to be up, but when I try to create Kafka topics by getting in to the first container:</p>

<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
</code></pre>

<p>I get this error:</p>

<pre><code>java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2020-06-07 03:10:55,293] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
</code></pre>

<p>Please notice that I did read other related questions and tried adding arguments to the command, such as <code>-e ZK_HOSTS=""localhost:2181""</code>. I know of other people working in the environment as mine who were able to run the commands successfully, so I suspect this might be a configuration issue on my side. Can you please guide?</p>

<p><strong>EDIT:</strong> Here are the Docker Compose files:</p>

<pre><code>version: '2'
services:
 kafka:
   image: &lt;private-domain&gt;/wurstmeister-kafka:0.10.1.0-2
   container_name: kafka
   ports:
      - 9092:9092
   environment:
     KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
     KAFKA_ADVERTISED_PORT: 9092
     KAFKA_ZOOKEEPER_CONNECT: 127.0.0.1:2181

   restart:
       ""unless-stopped""
</code></pre>

<p>and</p>

<pre><code>version: '2'
services:
 zk:
  image: &lt;private-domain&gt;/wurstmeister-zookeeper:3.4.9
  container_name: zk
  ports:
      - ""2181:2181""
  restart:
   ""unless-stopped""
</code></pre>

<p>and the output of <code>docker ps</code>:</p>

<pre><code>CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                                                NAMES
bf67a49da57a        wurstmeister-kafka:0.10.1.0-2   ""start-kafka.sh""         5 months ago        Up 29 minutes       0.0.0.0:9092-&gt;9092/tcp                               kafka
ef3e908d82b3        wurstmeister-zookeeper:3.4.9    ""/bin/sh -c '/usr/sbin/sshd &amp;&amp; bash /usr/bin/start-zk.sh'""   5 months ago        Up 29 minutes       22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   zk
</code></pre>
"
"47977699","ADD failed : No such file/Directory while building docker image","<docker>","63410551","Can't build php docker image","<php><docker><docker-compose><dockerfile><docker-container>","<p>I have below Dockerfile:</p>

<pre><code>FROM python:3

RUN mkdir -p /test/code/
RUN mkdir -p /test/logs/
RUN mkdir -p /test/configs/

ADD test.py /test/code/
ADD test_output.txt /test/code/
ADD test_input.txt /test/configs/
ADD logfile.log /test/logs/


CMD [ ""python3"", ""/test/code/test.py"" ]
</code></pre>

<p>My directory structure is:</p>

<pre><code>/home/&lt;username&gt;/test/
                 |-&gt; code/Dockerfile, test_output.txt, test.py
                 |-&gt; logs/logfile.log
                 |-&gt; configs/test_input.txt
</code></pre>

<p>when I am building the docker image using below command:</p>

<pre><code>sudo docker build -t myimage .
</code></pre>

<p>It shows below error:</p>

<pre><code>Step 7/9 : ADD test_input.txt /test/configs/
ADD failed: stat /var/lib/docker/tmp/docker-builder562406652/test_input.txt: no such file or directory
</code></pre>

<p>Why it shows this error when I have the directory and my file is also present.</p>
","<p>I got this error when building docker php image</p>
<blockquote>
<p>Step 13/25 : ADD php.ini /usr/local/etc/php/php.ini</p>
<p>ERROR: Service 'phpt3' failed to build: ADD failed: stat
/var/lib/docker/tmp/docker-builder310748204/php.ini: no
such file or directory</p>
</blockquote>
<p>Below is the docker file:</p>
<pre><code>FROM php:7.3-fpm

# install the PHP extensions we need
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends msmtp mailutils vim curl debconf subversion git apt-transport-https apt-utils \
    build-essential locales acl mailutils wget nodejs \
    gnupg gnupg1 gnupg2 \
    zlib1g-dev zlib1g-dev libicu-dev g++ \
    sudo

# Install GD
RUN apt-get install -y libfreetype6-dev libjpeg62-turbo-dev libpng-dev
RUN docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ 
RUN docker-php-ext-install -j$(nproc) gd
RUN docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/
RUN docker-php-ext-install gd

# MYSQLI 
RUN docker-php-ext-install mysqli &amp;&amp; docker-php-ext-enable mysqli

# Install ext-zip
RUN apt-get install -y unzip libzip-dev
RUN docker-php-ext-install zip

RUN docker-php-ext-configure intl
RUN docker-php-ext-install pdo_mysql json calendar intl

ADD php.ini /usr/local/etc/php/php.ini
COPY additionnal.ini /usr/local/etc/php/conf.d/

COPY php-fpm-pool.conf  /usr/local/etc/php/pool.d/www.conf

RUN rm -rf /var/lib/apt/lists/* \
    echo &quot;en_US.UTF-8 UTF-8&quot; &gt; /etc/locale.gen &amp;&amp; \
    echo &quot;fr_FR.UTF-8 UTF-8&quot; &gt;&gt; /etc/locale.gen &amp;&amp; \
    locale-gen

# Install Composer
RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer
RUN composer --version

# set up sendmail config, see http://linux.die.net/man/5/ssmtp.conf for options
RUN echo &quot;hostname=localhost.localdomain&quot; &gt; /etc/msmtp/msmtp.conf
RUN echo &quot;mailhub=maildevt3&quot; &gt;&gt; /etc/msmtp/msmtp.conf
# The above 'maildevt3' is the name you used for the link command
# in your docker-compose file or docker link command.
# Docker automatically adds that name in the hosts file
# of the container you're linking MailDev to.

# Set up php sendmail config
RUN echo &quot;sendmail_path=sendmail -i -t&quot; &gt;&gt; /usr/local/etc/php/conf.d/php-sendmail.ini

# Fully qualified domain name configuration for sendmail on localhost.
# Without this sendmail will not work.
# This must match the value for 'hostname' field that you set in ssmtp.conf.
RUN echo &quot;localhost localhost.localdomain&quot; &gt;&gt; /etc/hosts 

WORKDIR /var/www/

EXPOSE 9000
CMD [&quot;php-fpm&quot;]
</code></pre>
<p>Can anyone help me to overcome this bug.</p>
<p>I'm doing this tutorial
<a href=""https://passions.miary.dev/2019/08/30/docker-maildev-fr/"" rel=""nofollow noreferrer"">https://passions.miary.dev/2019/08/30/docker-maildev-fr/</a></p>
"
"53650594","Accessing Host Java from Docker container","<java><docker><dockerfile><docker-machine>","62370968","how to mount the Java runtime from the host into the container","<docker><docker-compose><docker-volume>","<p>I have Java installed in my docker host. Now I want that to be able to my docker containers. I don't want to install again on the containers. Do we have any workaround for that?</p>
","<p>At present i am adding a jdk file into my image like below</p>

<p>FROM centos</p>

<p>ADD jdk-11.0.7_linux-x64_bin.tar.gz /opt/java</p>

<p>ENV JAVA_HOME /opt/java/jdk-11.0.7</p>

<p>ENV PATH $PATH:/opt/java/jdk-11.0.7/bin</p>

<p>RUN ls -l /opt/java/jdk-11.0.7</p>

<p>RUN java -version</p>

<p>ADD build/libs/CatalogModel-1.0.jar CatalogModel-1.0.jar</p>

<p>EXPOSE 9081</p>

<p>ENTRYPOINT [""java"", ""-jar"", ""CatalogModel-1.0.jar""]</p>

<hr>

<p>Instead can i mount the Java runtime from the host into the container?</p>
"
"47993443","Selenium ""selenium.common.exceptions.NoSuchElementException"" when using Chrome","<python><selenium><selenium-webdriver><webdriver><nosuchelementexception>","63262440","django docker selenium headless setup fails with element not found error","<python><django><docker><selenium><selenium-webdriver>","<p>I'm trying to play <a href=""http://www.foddy.net/Athletics.html?webgl=true"" rel=""noreferrer"">QWOP</a> using Selenium on Chrome but I keep getting the following error:</p>

<pre><code>selenium.common.exceptions.NoSuchElementException: 
Message: no such element: Unable to locate element
{""method"":""id"",""selector"":""window1""
(Session info: chrome=63.0.3239.108
(Driver info: chromedriver=2.34.522913
(36222509aa6e819815938cbf2709b4849735537c), platform=Linux 4.10.0-42-generic x86_64)
</code></pre>

<p>while using the following code:</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
import time

browser = webdriver.Chrome()
browser.set_window_size(640, 480)
browser.get('http://www.foddy.net/Athletics.html?webgl=true')
browser.implicitly_wait(10)

canvas = browser.find_element_by_id(""window1"")

canvas.click()

while (True):
    action = ActionChains(browser)
    action.move_to_element(canvas).perform()
    canvas.click()
    canvas.send_keys(""q"")
</code></pre>

<p>The same code works perfectly on Firefox, but because I want to use chrome's capability to run an webgl game in headless mode I can't really switch to Firefox.</p>

<p>Any workarounds to get this working?</p>
","<p>I tried printing a get req to google and it printed None. I believe it is a problem with the chrome driver setup itself. here is the code</p>
<pre><code>
----------------------------------------------------------------------
Ran 1 test in 7.654s

FAILED (errors=1)
Destroying test database for alias 'default'...

tests&gt;docker-compose run --rm app sh -c &quot;python manage.py test functional_tests&quot;
Starting public-site-tests_db_1       ... done
Starting public-site-tests_selenium_1 ... done
Creating test database for alias 'default'...
System check identified no issues (0 silenced).
None
E
======================================================================
ERROR: test_login (src.functional_tests.test_logIn.LoginTest)
Lets test the user login feature
----------------------------------------------------------------------
Traceback (most recent call last):
  File &quot;/src/functional_tests/test_logIn.py&quot;, line 67, in test_login
    email_field = self.browser.find_element_by_name('username')
  File &quot;/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 496, in find_element_by_name
    return self.find_element(by=By.NAME, value=name)
  File &quot;/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 978, in find_element
    'value': value})['value']
  File &quot;/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 321, in execute
    self.error_handler.check_response(response)
  File &quot;/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py&quot;, line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {&quot;method&quot;:&quot;css selector&quot;,&quot;selector&quot;:&quot;[name=&quot;username&quot;]&quot;}
  (Session info: headless chrome=84.0.4147.105)

</code></pre>
<p>and here is the setup</p>
<pre><code>chrome_options = Options()
chrome_options.add_argument(&quot;--no-sandbox&quot;)
chrome_options.add_argument(&quot;--headless&quot;)
chrome_options.add_argument(&quot;--window-size=1920,1080&quot;)
chrome_options.add_argument(&quot;--disable-extensions&quot;)
chrome_options.add_argument(&quot;--proxy-server='direct://'&quot;)
chrome_options.add_argument(&quot;--proxy-bypass-list=*&quot;)
chrome_options.add_argument(&quot;--start-maximized&quot;)
chrome_options.add_argument(&quot;--disable-gpu&quot;)
chrome_options.add_argument(&quot;--disable-dev-shm-usage&quot;)
chrome_options.add_argument(&quot;--ignore-certificate-errors&quot;)


class LoginTest(StaticLiveServerTestCase):
    live_server_url = 'http://{}:8000'.format(
        socket.gethostbyname(socket.gethostname())
    )

    def setUp(self):
        # if os.environ.get('CI_JOB_NAME'):
        #     browser_name = webdriver.Chrome(chrome_options=True)
        #     if os.environ.get('CI_JOB_NAME') == 'e2e:chrome' else 'Firefox'
        # self.browser = webdriver.Chrome(options=chrome_options)

        settings.DEBUG = True
        self.browser = webdriver.Remote(
            command_executor=&quot;http://selenium:4444/wd/hub&quot;,
            desired_capabilities=DesiredCapabilities.CHROME,
            options=chrome_options)

        self.login_url = self.live_server_url + '/accounts/login/'
        self.LogIn = reverse('accounts:login')
        self.user = get_user_model().objects.create_user(
            mobile_number='999999910',
            first_name='Clerk',
            last_name='Kent',
            email='realdork@superman.com',
            password='test123123',
        )
        self.user.save()

    def test_login(self):
        &quot;&quot;&quot;Lets test the user login feature&quot;&quot;&quot;
        login_page = self.browser.get(self.login_url)
        time.sleep(10)
        print(login_page)
        email_field = self.browser.find_element_by_name('username')
</code></pre>
<p>finally the docker-compose</p>
<pre><code>version: '3'

services:
  app:
    build:
      context: .
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - .:/src
    command:
      sh -c &quot;python manage.py migrate &amp;&amp;
             python manage.py runserver 0.0.0.0:8000&quot;
    depends_on:
      - db
      - selenium

  db:
    image: postgres:12-alpine
    environment:
      - POSTGRES_DB=''
      - POSTGRES_USER=''
      - POSTGRES_PASSWORD=''


  selenium:
    image: selenium/standalone-chrome-debug
    ports:
      - &quot;5900:5900&quot;
    environment:
      - SELENIUM_HOST=http://selenium:4444/wd/hub
      - TEST_SELENIUM='yes'
</code></pre>
<p>i teried to add the environment in the app service too but the result is the same. I tried to print google.com and it printe None. I had a similar problem in local when the chrome path was wring and it would print out similar traceback error. Would really appriciate some help. Thnak you in advance.</p>
"
"54471608","Unable to connect from Intellij to mySql running in docker container - ""specified database user/password combination is rejected""","<mysql><docker><intellij-idea>","62355441","How to establish a MySQL(dockerized) connection in Intellij?","<java><mysql><docker><intellij-idea>","<p>Currently unable to connect from Intellij to mySql running locally on docker container on ubuntu.</p>

<pre><code>+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| tasklogs           |
+--------------------+

+----------+-----------+------+
| DATABASE | HOST      | USER |
+----------+-----------+------+
| tasklogs | localhost | dev  |
| tasklogs | localhost | root |
+----------+-----------+------+

+-----------------------------------------------------------+
| Grants for dev@localhost                                  |
+-----------------------------------------------------------+
| GRANT USAGE ON *.* TO `dev`@`localhost`                   |
| GRANT ALL PRIVILEGES ON `tasklogs`.* TO `dev`@`localhost` |
+-----------------------------------------------------------+
</code></pre>

<p>docker ps -a:</p>

<p><a href=""https://i.stack.imgur.com/eJaM1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eJaM1.png"" alt=""enter image description here""></a></p>

<p>When I connect via intellij:</p>

<p><a href=""https://i.stack.imgur.com/WVnvu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WVnvu.png"" alt=""enter image description here""></a></p>

<p>i.e. <strong>""The specified database user/password combination is rejected: [28000][1045] Access denied for user 'dev'@'localhost' (using password: YES)""</strong></p>

<p>I am putting in the right password.</p>

<p>Any help really appreciated.</p>

<p>Thanks,</p>
","<p>I'm on Windows 10, running Intellij Ultimate Edition, running MySQL 8 in docker -(I'm new to docker, I'm familiarizing myself with the technology because of its relevance).</p>

<p>MySQL is using port 3306 and I'm able to access it otherwise normally using bash and able to modify the the entirety of its content as the root user. I created a new Java project in Intellij and established the connection with the database manager on the right of the window. I've also ran several SQLs to set up the foundation of my new db. </p>

<p>I've created a new user in MySQL specifically for the development of this project. I've granted it full control over <em>only</em> this new db. I'm able to login to this user using bash and am able to modify the contents of my new db to its entirety. </p>

<p>Here's my problem: I wanted to begin the actual coding so first thing I'd thought to do was to test the connection to my db through Java. Unfortunately, I'm unable to. </p>

<pre><code>Connection con = DriverManager.getConnection(""jdbc:mysql://localhost:3306/myDB"",""myUser"",""password"");
</code></pre>

<p>I am thrown <code>java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:3306/myDB</code> which is odd because when I click on Data Source and navigate to MySQL, I see clearly that I have <strong>MySQL Connecter/J ver 8.0.15[latest]</strong>.</p>

<p>Any help is appreciated. Thanks</p>

<p>EDIT: Okay so I've created a maven project instead and added the JDBC dependency. Now I get the infamous <strong>Access denied for user 'myUser'@'172.17.0.1' (using password: YES)</strong>. How would I fix this? </p>

<p>EDIT2: Found my answers in these two threads: 
<a href=""https://stackoverflow.com/questions/54471608/unable-to-connect-from-intellij-to-mysql-running-in-docker-container-specifie"">Unable to connect from Intellij to mySql running in docker container - &quot;specified database user/password combination is rejected&quot;</a></p>

<p><a href=""https://stackoverflow.com/questions/11634084/are-users-user-and-userlocalhost-not-the-same"">Are Users &#39;User&#39;@&#39;%&#39; and &#39;User&#39;@&#39;localhost&#39; not the same?</a></p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","63070313","New user account - permission denied to enter a docker","<docker><user-accounts>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I work on a server (Ubuntu 20.04 LTS (GNU/Linux 5.4.0-40-generic x86_64)).
Somebody else set-up some folders and mysql data bases in a docker <em>redcap-lamp</em> on this server.
In addition, he created a user account for me to enter the server, and the docker.</p>
<p>Every thing works fine with my account.<br />
As login on the server I use: <code>ssh [my user account]@[our server]</code>.<br />
To enter the docker I (was told to) use: <code>docker exec -it redcap-lamp bash</code>.</p>
<p>I now had to create a user account for somebody else (and am not used to working in terminals and dockers).</p>
<p>I created a new user account by using: <code>sudo adduser [newUser]</code>, what worked fine.
It is possible to log in with this new account to have access to our server.
Unfortunately, when I use <code>docker exec -it redcap-lamp bash</code> to enter the docker with the new account, the permission is always denied. I get the output:</p>
<p><code>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/redcap-lamp/json: dial unix /var/run/docker.sock: connect: permission denied</code></p>
<p>Do I have to give new users some 'privileges' or something? Does anybody know where the problem lies and how it can be fixed?</p>
<p>Thanks,<br />
Danny</p>
"
"56649582","Substitute environment variables in NGINX config from docker-compose","<docker><nginx><docker-compose><environment-variables>","61932742","Include docker compose startup command in Dockerfile or docker run","<shell><docker><nginx><docker-compose><dockerfile>","<p>I am trying to start an NGINX server within a docker container configured through docker-compose. The catch is, however, that I would like to substitute an environment variable inside of the http section, specifically within the ""upstream"" block. </p>

<p>It would be awesome to have this working, because I have several other containers that are all configured through environment variables, and I have about 5 environments that need to be running at any given time. I have tried using ""envsubst"" (as suggested by the official NGINX docs), perl_set, and set_by_lua, however none of them appear to be working.</p>

<p>Below is the NGINX config, as it is after my most recent trial</p>

<pre><code>user  nginx;
worker_processes  1;
env NGINXPROXY;

load_module modules/ngx_http_perl_module.so;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}

http {
    perl_set $nginxproxy 'sub { return $ENV{""NGINXPROXY""}; }';

    upstream api-upstream {
        server ${nginxproxy};
    }

    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    access_log  /var/log/nginx/access.log  main;

    sendfile        off;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
}

</code></pre>

<p>Below is the NGINX dockerfile</p>

<pre><code># build stage
FROM node:latest
WORKDIR /app
COPY ./ /app
RUN npm install
RUN npm run build

# production stage
FROM nginx:1.17.0-perl
COPY --from=0 /app/dist /usr/share/nginx/html
RUN apt-get update &amp;&amp; apt-get install -y gettext-base
RUN rm /etc/nginx/conf.d/default.conf
RUN rm /etc/nginx/nginx.conf
COPY default.conf /etc/nginx/conf.d
COPY nginx.conf /etc/nginx
RUN mkdir /certs
EXPOSE 80 443
CMD [""nginx"", ""-g"", ""daemon off;""]

</code></pre>

<p>Below is the section of the docker-compose.yml for the NGINX server (with names and IPs changed). The envsubst command is intentionally commented out at this point in my troubleshooting.</p>

<pre><code>front-end:
        environment:
            - NGINXPROXY=172.31.67.100:9300
        build: http://gitaccount:password@gitserver.com/group/front-end.git#develop
        container_name: qa_front_end
        image: qa-front-end
        restart: always
        networks:
            qa_network:
                ipv4_address: 172.28.0.215
        ports:
            - ""9080:80""
        # command: /bin/bash -c ""envsubst '$$NGINXPROXY' &lt; /etc/nginx/nginx.conf &gt; /etc/nginx/nginx.conf &amp;&amp; nginx -g 'daemon off;'""
</code></pre>

<p>What appears to be happening is when I reference the $nginxproxy variable in the upstream block (right after ""server""), I get output that makes it look like it's referencing the string literal ""$nginxproxy"" rather than substituting the value of the variable.</p>

<pre><code>qa3_front_end       | 2019/06/18 12:35:36 [emerg] 1#1: host not found in upstream ""${nginx_upstream}"" in /etc/nginx/nginx.conf:19
qa3_front_end       | nginx: [emerg] host not found in upstream ""${nginx_upstream}"" in /etc/nginx/nginx.conf:19
qa3_front_end exited with code 1
</code></pre>

<p>When I attempt to use envsubst, I get an error that makes it sound like the command messed with the format of the nginx.conf file</p>

<pre><code>qa3_front_end       | 2019/06/18 12:49:02 [emerg] 1#1: no ""events"" section in configuration
qa3_front_end       | nginx: [emerg] no ""events"" section in configuration
qa3_front_end exited with code 1
</code></pre>

<p>I'm pretty stuck, so thanks in advance for your help.</p>
","<p>I am using following command in docker-compose to copy environment values into nginx configuration file at startup of nginx container.</p>

<pre><code>command: /bin/sh -c ""envsubst &lt; /etc/nginx/conf.d/site.template &gt; /etc/nginx/conf.d/default.conf &amp;&amp; exec nginx -g 'daemon off;'""
</code></pre>

<p>I want to get rid of docker-compose and use it with the docker run. I tried to inlcude the command in Dockerfile and use CMD. But the ngix container immediately exit.</p>

<pre><code>FROM mydocker:latest
COPY ./site.template /etc/nginx/conf.d/site.template

CMD envsubst &lt; /etc/nginx/conf.d/site.template &gt; /etc/nginx/conf.d/default.conf &amp;&amp; exec nginx -g 'daemon off;'
</code></pre>

<p>Can someone please help me here.
Thanks in advance</p>
"
"57516777","How can I delete docker images older than X and not in use","<docker>","62247063","remove docker images which are older than 10 minutes","<linux><docker><ubuntu>","<p>I'm running out of disk space on a server and <code>docker images</code> shows some containers from 6 months ago but as old as 2 years ago.  I'd like to remove all the ones older than 8 months.  What magic can I add to <code>docker rmi $(MAGIC)</code> that'll accomplish this?</p>
","<p>I am deleting the docker images by searching a pattern leaving only 2 docker images behind. </p>

<p><code>sudo docker rmi -f $(docker images | grep $search_docker_image | awk '{ print $3 }' | awk '!/'$base_docker_image_id'/ &amp;&amp; !/'$recent_docker_image_id'/')</code>   </p>

<p>In the same command i want also delete the docker images which are older than 10 mins .Is it possible or is there any other way of achieving it.</p>
"
"49037742","Why does it take ages to install Pandas on Alpine Linux","<pandas><numpy><docker><alpine>","63517400","Installing pandas=1.1.1 in a Docker container takes forever","<python><pandas><docker><raspberry-pi>","<p>I've noticed that installing Pandas and Numpy (it's dependency) in a Docker container using the base OS Alpine vs. CentOS or Debian takes much longer. I created a little test below to demonstrate the time difference. Aside from the few seconds Alpine takes to update and download the build dependencies to install Pandas and Numpy, why does the setup.py take around 70x more time than on Debian install?</p>

<p>Is there any way to speed up the install using Alpine as the base image or is there another base image of comparable size to Alpine that is better to use for packages like Pandas and Numpy?</p>

<p><strong>Dockerfile.debian</strong></p>

<pre><code>FROM python:3.6.4-slim-jessie

RUN pip install pandas
</code></pre>

<p><strong>Build Debian image with Pandas &amp; Numpy:</strong></p>

<pre><code>[PandasDockerTest] time docker build -t debian-pandas -f Dockerfile.debian . --no-cache
    Sending build context to Docker daemon  3.072kB
    Step 1/2 : FROM python:3.6.4-slim-jessie
     ---&gt; 43431c5410f3
    Step 2/2 : RUN pip install pandas
     ---&gt; Running in 2e4c030f8051
    Collecting pandas
      Downloading pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)
    Collecting numpy&gt;=1.9.0 (from pandas)
      Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)
    Collecting pytz&gt;=2011k (from pandas)
      Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)
    Collecting python-dateutil&gt;=2 (from pandas)
      Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
    Collecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)
      Downloading six-1.11.0-py2.py3-none-any.whl
    Installing collected packages: numpy, pytz, six, python-dateutil, pandas
    Successfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0
    Removing intermediate container 2e4c030f8051
     ---&gt; a71e1c314897
    Successfully built a71e1c314897
    Successfully tagged debian-pandas:latest
    docker build -t debian-pandas -f Dockerfile.debian . --no-cache  0.07s user 0.06s system 0% cpu 13.605 total
</code></pre>

<p><strong>Dockerfile.alpine</strong></p>

<pre><code>FROM python:3.6.4-alpine3.7

RUN apk --update add --no-cache g++

RUN pip install pandas
</code></pre>

<p><strong>Build Alpine image with Pandas &amp; Numpy:</strong></p>

<pre><code>[PandasDockerTest] time docker build -t alpine-pandas -f Dockerfile.alpine . --no-cache
Sending build context to Docker daemon   16.9kB
Step 1/3 : FROM python:3.6.4-alpine3.7
 ---&gt; 4b00a94b6f26
Step 2/3 : RUN apk --update add --no-cache g++
 ---&gt; Running in 4b0c32551e3f
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz
(1/17) Upgrading musl (1.1.18-r2 -&gt; 1.1.18-r3)
(2/17) Installing libgcc (6.4.0-r5)
(3/17) Installing libstdc++ (6.4.0-r5)
(4/17) Installing binutils-libs (2.28-r3)
(5/17) Installing binutils (2.28-r3)
(6/17) Installing gmp (6.1.2-r1)
(7/17) Installing isl (0.18-r0)
(8/17) Installing libgomp (6.4.0-r5)
(9/17) Installing libatomic (6.4.0-r5)
(10/17) Installing pkgconf (1.3.10-r0)
(11/17) Installing mpfr3 (3.1.5-r1)
(12/17) Installing mpc1 (1.0.3-r1)
(13/17) Installing gcc (6.4.0-r5)
(14/17) Installing musl-dev (1.1.18-r3)
(15/17) Installing libc-dev (0.7.1-r0)
(16/17) Installing g++ (6.4.0-r5)
(17/17) Upgrading musl-utils (1.1.18-r2 -&gt; 1.1.18-r3)
Executing busybox-1.27.2-r7.trigger
OK: 184 MiB in 50 packages
Removing intermediate container 4b0c32551e3f
 ---&gt; be26c3bf4e42
Step 3/3 : RUN pip install pandas
 ---&gt; Running in 36f6024e5e2d
Collecting pandas
  Downloading pandas-0.22.0.tar.gz (11.3MB)
Collecting python-dateutil&gt;=2 (from pandas)
  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
Collecting pytz&gt;=2011k (from pandas)
  Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)
Collecting numpy&gt;=1.9.0 (from pandas)
  Downloading numpy-1.14.1.zip (4.9MB)
Collecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)
  Downloading six-1.11.0-py2.py3-none-any.whl
Building wheels for collected packages: pandas, numpy
  Running setup.py bdist_wheel for pandas: started
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/e8/ed/46/0596b51014f3cc49259e52dff9824e1c6fe352048a2656fc92
  Running setup.py bdist_wheel for numpy: started
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: still running...
  Running setup.py bdist_wheel for numpy: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/9d/cd/e1/4d418b16ea662e512349ef193ed9d9ff473af715110798c984
Successfully built pandas numpy
Installing collected packages: six, python-dateutil, pytz, numpy, pandas
Successfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0
Removing intermediate container 36f6024e5e2d
 ---&gt; a93c59e6a106
Successfully built a93c59e6a106
Successfully tagged alpine-pandas:latest
docker build -t alpine-pandas -f Dockerfile.alpine . --no-cache  0.54s user 0.33s system 0% cpu 16:08.47 total
</code></pre>
","<p>I am trying to create a <code>Dockerfile</code> with <code>streamlit</code> and <code>pandas</code> but the building of pandas takes forever. This is my <code>Dockerfile</code>.</p>
<pre><code># For more information, please refer to https://aka.ms/vscode-docker-python
FROM python:3.8-slim

# This is needed for argon2-cffi==20.1.0
RUN apt-get update &amp;&amp; apt-get install libssl-dev libffi-dev -y

# Apparently the installation of pandas in Docker takes forever.
RUN apt-get install python3-pandas -y

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE 1

# Exposing Streamlit Default Port for Streamlit
EXPOSE 8501

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED 1

ADD requirements.txt .
RUN python -m pip install -r requirements.txt
</code></pre>
<p>But the building of the Dockerfile takes forever. I have tried to install pandas with <code>sudo apt-get install python3-pandas -y</code> and removed pandas=1.1.1 from the requirements.txt file but apparently the installed version of pandas is not 1.1.1 and this is the required version from <code>Streamlit</code> so it is still trying to install the latest <code>pandas</code>.</p>
<p>Do you have any ideas, how I can speed up the process of the building of the docker image? By the way, I am building it on top of a Raspberry Pi 4B.</p>
"
"58395566","lsb_release: command not found in latest Ubuntu Docker container","<linux><docker><ubuntu><containers><version>","61972822","Interesting problem with GithubActions+Docker","<docker><ubuntu><github-actions>","<p>I just wanted to test something out real quick. So I ran a docker container and I wanted to check which version I was running:</p>

<pre><code>$ docker run -it ubuntu    
root@471bdb08b11a:/# lsb_release -a
bash: lsb_release: command not found
root@471bdb08b11a:/# 
</code></pre>

<p>So I tried installing it (as <a href=""http://www.andryhacks.com/bash-lsb_release-command-not-found/"" rel=""noreferrer"">suggested here</a>):</p>

<pre><code>root@471bdb08b11a:/# apt install lsb_release
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package lsb_release
root@471bdb08b11a:/# 
</code></pre>

<p>Anybody any idea why this isn't working?</p>
","<p>So it turns out that the Github team <a href=""https://github.com/actions/virtual-environments/issues/228"" rel=""nofollow noreferrer"">hasn't added yet support for Ubuntu 20.04 to GithubActions</a>.</p>

<p>However, <a href=""https://help.github.com/en/actions/creating-actions/creating-a-docker-container-action"" rel=""nofollow noreferrer"">GithubActions has native support for Docker</a>, so I thought, why not use that?</p>

<p>Then I used <a href=""https://github.com/knocte/hello-world-docker-action/commit/159ad202b285af5fae9f593a35db762e9f65caf7"" rel=""nofollow noreferrer"">their hello-world template</a>, <a href=""https://github.com/knocte/hello-world-docker-action/commit/d69e535fcc7ae26c88a8f88753c5ceb324d7e677"" rel=""nofollow noreferrer"">added a workflow to make use of it</a>, and <a href=""https://github.com/knocte/hello-world-docker-action/commit/a97e3f487361082515838b0939bf2ad5903fc2c1"" rel=""nofollow noreferrer"">modified it to use Ubuntu 20.04 instead of Alpine Linux</a>.</p>

<p>However, <a href=""https://github.com/knocte/hello-world-docker-action/runs/702035401"" rel=""nofollow noreferrer"">the logs of it</a> show the error:</p>

<pre><code>Reading package lists...
E: Unable to locate package lsb-release
</code></pre>

<p>How is that possible? lsb-release apt package should exist in Ubuntu 20.04. Am I missing something obvious about Docker?</p>

<p><strong>UPDATE:</strong> Not really a duplicate because I was already installing the <code>lsb-release</code> package in my script, what I was missing is a call to <code>apt update</code> before doing it.</p>
"
"49492697","Remote WebDriver UnreachableBrowserException: Could not start a new session","<selenium-webdriver><webdriver><selenium-grid><remotewebdriver>","63226084","docker selenium stand alone chrome or firefox issue, org.openqa.selenium.remote.UnreachableBrowserException: Could not start a new session","<docker><selenium-webdriver>","<p>I got this exception for all browsers. For example, I create a remote webdriver on chrome like this:</p>

<pre><code>caps = DesiredCapabilities.chrome();
ChromeOptions options = new ChromeOptions();
options.addArguments(""disable-infobars"");
caps.setCapability(ChromeOptions.CAPABILITY, options);
webDriver = new RemoteWebDriver(new URL(""http://myIP:5555/wd/hub""), caps);
</code></pre>

<p>And I got UnreachableBrowserException as follow:</p>

<pre><code>org.openqa.selenium.remote.DesiredCapabilities chrome
INFO: Using `new ChromeOptions()` is preferred to `DesiredCapabilities.chrome()`

org.openqa.selenium.remote.UnreachableBrowserException: Could not start a new session. Possible causes are invalid address of the remote server or browser start-up failure.
</code></pre>

<p>But I check my selenium hub at <code>http://myIP:4444/grid/console</code>, everything is fine, the node is stil registered. I then check my node at <code>http://myIP:5555/wd/hub/static/resource/hub.html</code>, I still can click ""Create Session"" to create a session for all browsers.</p>

<p>I just got this exception today, it still worked few days ago. I am using Selenium 3.11.0, IntelliJ 2017.3, all drivers and browsers are latest versions. </p>

<p>I googled here, but I can't find a solution for this while my gird is still running. Any help much appreciated. </p>
","<p>OS:windows7</p>
<p>Docker image: docker run -d -p 4444:4444 -v /dev/shm:/dev/shm selenium/standalone-chrome:4.0.0-alpha-6-20200730</p>
<p>I am trying to run the following code, throwing the following error:</p>
<p>java - selenium</p>
<pre><code>public static void main(String[] args)
    {
        
        URL url = null;
        try {
             url= new URL(&quot;http://localhost:4444/wd/hub&quot;);
        } catch (MalformedURLException e) {
                e.printStackTrace();
        }
    WebDriver driver=new RemoteWebDriver(url,DesiredCapabilities.chrome());
        driver.get(&quot;http://www.google.com&quot;);
           System.out.println(driver.getTitle());
        Assert.assertEquals(driver.getTitle(), &quot;Google&quot;);
        driver.quit();
    }
</code></pre>
<p>error:Aug 03, 2020 1:25:11 PM org.openqa.selenium.remote.DesiredCapabilities chrome
INFO: Using <code>new ChromeOptions()</code> is preferred to <code>DesiredCapabilities.chrome()</code>
Exception in thread &quot;main&quot; org.openqa.selenium.remote.UnreachableBrowserException: Could not start a new session. Possible causes are invalid address of the remote server or browser start-up failure.
Build info: version: '3.141.59', revision: 'e82be7d358', time: '2018-11-14T08:17:03'
System info: host: 'BALAJI-PC', ip: '192.168.56.1', os.name: 'Windows 7', os.arch: 'amd64', os.version: '6.1', java.version: '1.8.0_231'
Driver info: driver.version: RemoteWebDriver
at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:573)
at org.openqa.selenium.remote.RemoteWebDriver.startSession(RemoteWebDriver.java:213)
at org.openqa.selenium.remote.RemoteWebDriver.(RemoteWebDriver.java:131)
at org.openqa.selenium.remote.RemoteWebDriver.(RemoteWebDriver.java:144)
at com.init.Google_TC.main(Google_TC.java:26)
Caused by: java.net.ConnectException: Failed to connect to localhost/0:0:0:0:0:0:0:1:4444
at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:247)
at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:165)
at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257)
at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:200)
at okhttp3.RealCall.execute(RealCall.java:77)
at org.openqa.selenium.remote.internal.OkHttpClient.execute(OkHttpClient.java:103)
at org.openqa.selenium.remote.ProtocolHandshake.createSession(ProtocolHandshake.java:105)
at org.openqa.selenium.remote.ProtocolHandshake.createSession(ProtocolHandshake.java:74)
at org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:136)
at org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:552)
... 4 more
Caused by: java.net.ConnectException: Connection refused: connect
at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)
at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:606)
at okhttp3.internal.platform.Platform.connectSocket(Platform.java:129)
at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:245)
... 26 more</p>
"
"58457140","Dependencies Between Workflows on Github Actions","<github><continuous-integration><continuous-deployment><github-actions>","62367778","How can I make a GitHub Workflow depend on the success of another workflow?","<docker><dependencies><publish><github-actions>","<p>I have a monorepo with two workflows:</p>

<p><code>.github/workflows/test.yml</code></p>

<pre><code>name: test

on: [push, pull_request]

jobs:
  test-packages:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v1
      - name: test packages
        run: |
          yarn install
          yarn test
...
</code></pre>

<p><code>.github/workflows/deploy.yml</code></p>

<pre><code>  deploy-packages:
    runs-on: ubuntu-latest
    needs: test-packages
    steps:
      - uses: actions/checkout@v1
      - name: deploy packages
        run: |
          yarn deploy
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
...
</code></pre>

<p>This doesn't work, I can't reference a job in another workflow:</p>

<pre><code>### ERRORED 19:13:07Z

- Your workflow file was invalid: The pipeline is not valid. The pipeline must contain at least one job with no dependencies.
</code></pre>

<p><strong>Is there a way to create a dependency between workflows?</strong></p>

<p>What I want is to run <code>test.yml</code> then <code>deploy.yml</code> on tags, and <code>test.yml</code> only on push and pull request. I don't want to duplicate jobs between workflows.</p>
","<p>I've made a new project on GitHub with two workflows:</p>

<ul>
<li><a href=""https://github.com/pgxn/docker-pgxn-tools/blob/main/.github/workflows/ci.yml"" rel=""noreferrer"">One</a> to test the project</li>
<li>And <a href=""https://github.com/pgxn/docker-pgxn-tools/blob/main/.github/workflows/publish.yml"" rel=""noreferrer"">another</a> to publish the resulting Docker image</li>
</ul>

<p>I'd like the second workflow to trigger only if the first one succeeds. Is there a way to do that? Or would I need to create a single workflow file where the second job depends on the first?</p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","63700454","Restrict user from accessing code inside docker","<docker><docker-compose><dockerfile><password-protection><docker-machine>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I have created a docker image for my code which is an Django and Deeplearning application and I have to give that image to some client. What I want is that the client should not be permitted to access the code. So is there any chance to password protect the docker image so that the user has to enter the username and password from command line each time before trying to execute any docker command?
Docker version - 19.03.12 / June 18, 2020
Operating system - Ubuntu</p>
"
"60235353","Docker is not running on Colab","<docker><google-colaboratory>","61822982","Create docker on Google colab","<docker><google-colaboratory>","<p>I have tried  to install Docker on google Colab through the following ways:</p>

<p>(1)<a href=""https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04</a> </p>

<p>(2)<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04</a></p>

<p>(3)<a href=""https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP"" rel=""noreferrer"">https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP</a></p>

<p>I started the docker service and saw the status, but it showed 'Docker is not running'. Maybe the docker can not work on the Colab.
<a href=""https://i.stack.imgur.com/Xxs7O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xxs7O.png"" alt=""enter image description here""></a></p>

<p>I feel confused and want to know the reason.</p>

<p>Thanks</p>
","<p>Is their any way to actually create a docker container on google colab?
I currently have a CPU based docker container on my local and I want to create a GPU based docker and since i developed my code on colab, I am thinking if I can directly create a docker on colab and leverage that.</p>

<p>I found a way that if we clone our Git repository on Google(colab), then we can initialize the docker but my docker size is 3 GB and git doesn't allow that.</p>

<p>Is it possible to do that?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63570332","Accessing Kafka broker in Docker container","<docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I can't for the life of me find information on this so forgive me if this is obvious.</p>
<p>I am publishing a Kafka broker's port to my host machine (MacOS) and cannot connect from the host.  Then, even when I <code>exec</code> into the container I can't <code>ping localhost:9092</code> or <code>curl</code> it.</p>
<p>I've also tried hitting the <code>KAFKA_ADVERTISED_LISTENERS</code> value which is <code>PLAINTEXT://kafka1:9092</code> and that gives me a <code>ping: cannot resolve PLAINTEXT://kafka1:9092</code></p>
<p>This is the relevant bit of my docker-compose file:</p>
<pre><code>version: '3'
services:
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_OPTS: &quot;-Dzookeeper.4lw.commands.whitelist=*&quot;
    ports:
      - &quot;2181:2181&quot;

  kafka1:
    build: .
    container_name: &quot;kafka1&quot;
    depends_on:
      - zookeeper
    ports:
      - &quot;8778&quot;
      - &quot;9092:9092&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OPTS: '-javaagent:/usr/jolokia/agents/jolokia-jvm.jar=host=0.0.0.0'
</code></pre>
<p>Dockerfile is as follows:</p>
<pre><code>FROM confluentinc/cp-kafka:latest

ENV JOLOKIA_VERSION 1.3.5
ENV JOLOKIA_HOME /usr/jolokia-${JOLOKIA_VERSION}
RUN curl -sL --retry 3 \
  &quot;https://github.com/rhuss/jolokia/releases/download/v${JOLOKIA_VERSION}/jolokia-${JOLOKIA_VERSION}-bin.tar.gz&quot; \
  | gunzip \
  | tar -x -C /usr/ \
 &amp;&amp; ln -s $JOLOKIA_HOME /usr/jolokia \
 &amp;&amp; rm -rf $JOLOKIA_HOME/client \
 &amp;&amp; rm -rf $JOLOKIA_HOME/reference
</code></pre>
<p>Any ideas?</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","61807252","Docker Compose : Unable to start react app server","<reactjs><docker><docker-compose>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I am trying to launch a react-springboot application using docker on my mac machine. I am able to individually launch the server and client urls using docker run commands but when docker compose start up fails in the react application with the following error logs:</p>

<p>app-client_1        | ℹ ｢wds｣: Project is running at <a href=""http://172.20.0.2/"" rel=""nofollow noreferrer"">http://172.20.0.2/</a>
app-client_1        | ℹ ｢wds｣: webpack output is served from 
app-client_1        | ℹ ｢wds｣: Content not from webpack is served from /app/public
app-client_1        | ℹ ｢wds｣: 404s will fallback to /
app-client_1        | Starting the development server...
app-client_1        | 
react-proj_app-client_1 exited with code 0
app-client_1        | 
app-client_1        | > my-app@0.1.0 start /app
app-client_1        | > react-scripts start
app-client_1        | </p>

<p>The following is docker-compose.yml file:</p>

<pre><code>version: '3.7'
services:
  # App backend service
  app-server:
    # Configuration for building the docker image for the backend service
    build:
      context: demo
      dockerfile: Dockerfile
    ports:
      - ""8080:8080"" # Forward the exposed port 8080 on the container to port 8080 on the host machine
    restart: always
    depends_on:
      - mysql-standalone # This service depends on mysql. Start that first.
    environment: # Pass environment variables to the service
      SPRING_DATASOURCE_URL: jdbc:mysql://mysql-standalone:3306/my_db
      SPRING_DATASOURCE_USERNAME: sa
      SPRING_DATASOURCE_PASSWORD: password
    networks:
      - backend
      - frontend
  # Frontend Service
  app-client:
    build:
      context: my-app
      dockerfile: Dockerfile
      args:
          REACT_APP_API_BASE_URL: http://localhost:8080
    ports:
      - 3000:3000
    restart: always
    depends_on:
      - app-server
    networks:
      - frontend
    command: [""npm"", ""start""]
  # Database Service (Mysql)
  mysql-standalone:
    image: mysql:5.6
    ports:
      - ""3306:3306""
    restart: always
    environment:
      MYSQL_DATABASE: my_db
      MYSQL_USER: sa
      MYSQL_PASSWORD: password
      MYSQL_ROOT_PASSWORD: some_password
    networks:
      - backend


  # Volumes
volumes:
  db-data:

  # Networks to be created to facilitate communication between containers
networks:
  backend:
  frontend:

</code></pre>

<p>What might be the issue here?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63596455","Kafdrop - Cannot connect to Kafka Cluster setup using bitnami/kafka","<docker><apache-kafka><docker-compose><kafka-cluster>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I setup a kafka cluster using bitnami kafka and zookeeper and I wanted to view this cluster or at least one broker using kafdrop. I used docker compose to build all the components. I initially followed this <a href=""https://itnext.io/how-to-install-kafka-using-docker-a2b7c746cbdc"" rel=""nofollow noreferrer"">tutorial</a> and then added the kafdrop config in the docker-compose.yml</p>
<pre><code>version: '2'

networks:
  kafka-net:
    driver: bridge

services:
  zookeeper-server:
    image: 'bitnami/zookeeper:latest'
    networks:
      - kafka-net
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafdrop:
    image: obsidiandynamics/kafdrop
    networks:
      - kafka-net
    restart: &quot;no&quot;
    ports:
      - &quot;9000:9000&quot;
    environment:
      KAFKA_BROKERCONNECT: &quot;PLAINTEXT://localhost:9092,PLAINTEXT://localhost:9093,PLAINTEXT://localhost:9094&quot;
      JVM_OPTS: &quot;-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify&quot;
    depends_on:
      - &quot;kafka-server1&quot;
      - &quot;kafka-server2&quot;
      - &quot;kafka-server3&quot;
  kafka-server1:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
  kafka-server2:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9093:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9093
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
  kafka-server3:
    image: 'bitnami/kafka:latest'
    networks:
      - kafka-net    
    ports:
      - '9094:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9094
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper-server
</code></pre>
<p>My main issue is that kafdrop always throw this error:</p>
<pre><code>020-08-26 10:53:53.517  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -3 (localhost/127.0.0.1:9094) could not be established. Broker may not be available.
2020-08-26 10:53:53.522  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -2 (localhost/127.0.0.1:9093) could not be established. Broker may not be available.
2020-08-26 10:53:53.526  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdrop-admin] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-08-26 10:53:53.627  WARN 1 [| kafdrop-admin] o.a.k.c.NetworkClient                    : [AdminClient clientId=kafdro
</code></pre>
<p>I have tried changing the value of <code>KAFKA_BROKERCONNECT</code> with the ff values but all didn't work.</p>
<ul>
<li>PLAINTEXT://localhost:9092,PLAINTEXT://localhost:9093,PLAINTEXT://localhost:9094</li>
<li>localhost:9092,localhost:9093,localhost:9094</li>
<li>PLAINTEXT://kafka-server1:9092,PLAINTEXT://kafka-server2:9093,PLAINTEXT://kafka-server3:9094</li>
<li>kafka-server1:9092,kafka-server2:9093,kafka-server3:9094</li>
</ul>
<p>I am actually just guessing the correct config syntax so any explanation with this one is appreciated :).</p>
<p>Also, is adding <code>networks</code> property needed on kafdrop config? Kafdrop have sample <a href=""https://github.com/obsidiandynamics/kafdrop/tree/master/docker-compose/kafka-kafdrop"" rel=""nofollow noreferrer"">docker-compose</a> file and this one doesn't have network config so I wonder why/if <code>network</code> is needed.</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","62243616","Node container is crashing without error output","<node.js><docker>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I'm trying to run Node container named Client.</p>

<p>Dockerfile:</p>

<pre><code>FROM node:14.4.0-alpine

WORKDIR /usr/src/app

ENV PATH /usr/src/app/node_modules/.bin:$PATH

COPY package.json /usr/src/app/package.json
COPY package-lock.json /usr/src/app/package-lock.json
RUN npm ci
RUN npm install react-scripts@2.1.8 -g --silent --no-optional

CMD [""npm"", ""start""]
</code></pre>

<p>docker-compose:</p>

<pre><code>version: '3.7'

services:

  users:
    build:
      context: ./services/users
      dockerfile: Dockerfile
    volumes:
      - './services/users:/usr/src/app'
    ports:
      - 5001:5000
    environment:
      - FLASK_ENV=development
      - APP_SETTINGS=project.config.DevelopmentConfig
      - DATABASE_URL=postgres://postgres:postgres@users-db:5432/users_dev
      - DATABASE_TEST_URL=postgres://postgres:postgres@users-db:5432/users_test
    depends_on:
      - users-db

...
...

  client:
    build:
      context: ./services/client
      dockerfile: Dockerfile
    volumes:
      - './services/client:/usr/src/app'
      - '/usr/src/app/node_modules'
    ports:
      - 3007:3000
    environment:
      - NODE_ENV=development
      - REACT_APP_USERS_SERVICE_URL=${REACT_APP_USERS_SERVICE_URL}
    depends_on:
      - users
</code></pre>

<p>All services work just fine on dev server but when I try to containerize Node it crashes as soon as I spin the container. Log gives me this:</p>

<pre><code>ℹ ｢wds｣: Project is running at http://172.18.0.5/
ℹ ｢wds｣: webpack output is served from 
ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...
</code></pre>

<p>Not very helpful to me as you can imagine. What steps I should take to diagnose where is the issue ?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63603430","How to access Docker Kafka from host machine","<java><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>looking for help as I have been stuck for a week now. I have a Kafka (0.8.2.1_11) and Zookeeper instance running inside of docker containers, and I am trying to get a Java app to write data to Kafka. I see that my topic is created, but no data is being saved.</p>
<p>My Java app is pretty straightforward, for loop that writes the iteration number. I also included a snippet of my docker-compose.yml file for the two services.</p>
<p>I start up my Kafka and Zookeeper by issuing a <code>docker-compose -f docker-compose.yml up -d</code> command. Then I run my Java file through intelliJ</p>
<pre><code>import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

public class MainFake {

    private static final Logger LOGGER = LoggerFactory.getLogger(MainFake.class);
    private static Properties kafkaProperties = new Properties();

    static {

        // set up Kafka
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);
        kafkaProperties.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        kafkaProperties.put(&quot;retries&quot;, 0);
        kafkaProperties.put(&quot;batch.size&quot;, 16384);
        kafkaProperties.put(&quot;linger.ms&quot;, 1);
        kafkaProperties.put(&quot;buffer.memory&quot;, 33554432);
    }

    public static void main(String[] args) {
        LOGGER.info(&quot;Started fake Main&quot;);
        KafkaProducer kafkaProducer = new KafkaProducer(kafkaProperties);
        try {
            for (int i = 0; i &lt; 30; i++) {
                LOGGER.info(&quot;logging &quot; + i);
                String kafkaPayload = String.valueOf(i);
                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test&quot;, generateKey(kafkaPayload), kafkaPayload);
                LOGGER.info(&quot;sending...&quot;);
                kafkaProducer.send(record).get();
                LOGGER.info(&quot;sent.&quot;);
            }
        } catch (Exception e) {
            LOGGER.error(String.valueOf(e));
        } finally {
            LOGGER.info(&quot;closing producer...&quot;);
            kafkaProducer.close();
            LOGGER.info(&quot;closed producer.&quot;);
        }
        LOGGER.info(&quot;Finished fake Main&quot;);
    }

    private static String generateKey(String payload) {
        return String.valueOf(System.nanoTime() + payload.hashCode());
    }

}

</code></pre>
<pre><code>zk:
    image: registry.cloud.cas.org/osd-platform/baseimage-zookeeper:3.4.8_11
    restart: always
    ports:
      - &quot;2181:2181&quot;
    networks:
      - chad

  kafka:
    image: registry.cloud.cas.org/osd-platform/kafka:0.8.2.1_11
    restart: always
    ports:
      - &quot;9092:9093&quot;
    depends_on:
      - zk
    environment:
      KAFKA_NAME: 1
      LOG_LEVEL: INFO
      KAFKA_ZOOKEEPER_CONNECT: localhost:2181
      KAFKA_LISTENERS: INTERNAL://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    hostname: ${VM_HOSTNAME}
    networks:
      chad:
        aliases:
          - kafka
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","63416186","Can not connect to Kafka outside container","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I'm trying to implement blackbox testing of my app. So I created compose file</p>
<pre><code>version: '3.4'
services:  
  kafka:
    image: &quot;spotify/kafka&quot;
    environment:
      - ADVERTISED_HOST=kafka
      - ADVERTISED_PORT=9092
    ports:
      - &quot;9092:9092&quot;
      - &quot;2181:2181&quot;
    networks:
      testing_net:
        ipv4_address: 172.17.0.2
  app:
    build: ./blackbox
    image: app    
    networks:
      testing_net:
        ipv4_address: 172.17.0.4
networks:
  testing_net:
    ipam:
      driver: default
      config:
        - subnet: &quot;172.17.0.0/16&quot;
</code></pre>
<p>It works fine, application inside app container can connect to kafka using kafka:9092 host.
But app is consumer, and producer is located outside of containers. When I try to populate data outside containers I get error:</p>
<pre><code>[kafka-producer-network-thread | producer] WARN o.apache.kafka.clients.NetworkClient - [Producer clientId=producer] Error connecting to node kafka:9092 (id: 0 rack: null) 
</code></pre>
<p>I could add &quot;kafka&quot; host to hosts file and it maybe will work, but I have to run this scenario not only at local machine but at CI\CD</p>
<p>I tried to set ADVERTISED_HOST=172.17.0.2. It doesn't work as well.</p>
<p>I can not edit any kafka config files, I can only use docker-compose file</p>
<p>Any suggestions would be appreciated. Thanks</p>
<p>UPD.</p>
<p>Solved this issue using dirty hack. For windows for sure, for other systems need to check Docker adds following host to hosts file: kubernetes.docker.internal:127.0.0.1 In that case inside container I use</p>
<pre><code>kafka:
    image: &quot;spotify/kafka&quot;
    environment:
      - ADVERTISED_HOST=kubernetes.docker.internal
      - ADVERTISED_PORT=9092
    ports:
      - &quot;9092:9092&quot;
      - &quot;2181:2181&quot;
    networks:
      testing_net:
        ipv4_address: 172.16.238.102
    extra_hosts:
      - &quot;kubernetes.docker.internal:172.16.238.102&quot;
</code></pre>
<p>And kafka broker works for outside application because kubernetes.docker.internal is known host</p>
"
"61589254","why docker image with elasticsearch status restarting always?","<docker><elasticsearch>","61844868","run elasticsearch from docker on ec2","<amazon-web-services><docker><elasticsearch><amazon-ec2><jvm>","<p>unbuntu 16.04, ram 1gb, on aws instance</p>

<p>I had to run old instance of elasticsearch so I wanted use a docker image of elasticsearch 5.3.3 version. by seeing the stackoverflow multiple links with same title, i have modified my installation of docker image based elasticsearch as below</p>

<pre><code>sudo docker run -p 9200:9200 -p 9300:9300 -d -e ""http.host=0.0.0.0"" -e ""transport.host=127.0.0.1"" -e ""xpack.security.enabled=false"" --restart=unless-stopped --name careerassistant-elastic  docker.elastic.co/elasticsearch/elasticsearch:5.3.3
</code></pre>

<p>the installation is finished and have problem with accessing the elasticsearch, though I had mutliple modifications as above command, I couldnt resolve the issue. when on</p>

<pre><code>sudo docker ps
</code></pre>

<p>the status is still --> restarting(1) 48 seconds ago</p>

<p>when i was checking the log of the docker i couldnt understand anything as i am new to docker and its utilization</p>

<pre><code>&gt; docker logs --tail 50 --follow --timestamps careerassistant-elastic
</code></pre>

<p>i got the following output</p>

<pre><code>2020-05-04T09:36:00.552415247Z CmaTotal:              0 kB
2020-05-04T09:36:00.552418314Z CmaFree:               0 kB
2020-05-04T09:36:00.552421364Z HugePages_Total:       0
2020-05-04T09:36:00.552424343Z HugePages_Free:        0
2020-05-04T09:36:00.552427401Z HugePages_Rsvd:        0
2020-05-04T09:36:00.552430358Z HugePages_Surp:        0
2020-05-04T09:36:00.552433336Z Hugepagesize:       2048 kB
2020-05-04T09:36:00.552436334Z DirectMap4k:       67584 kB
2020-05-04T09:36:00.552439415Z DirectMap2M:      980992 kB
2020-05-04T09:36:00.552442390Z 
2020-05-04T09:36:00.552445460Z 
2020-05-04T09:36:00.552448777Z CPU:total 1 (initial active 1) (1 cores per cpu, 1 threads per core) family 6 model 63 stepping 2, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, lzcnt, tsc, bmi1, bmi2
2020-05-04T09:36:00.552452312Z 
2020-05-04T09:36:00.552455227Z /proc/cpuinfo:
2020-05-04T09:36:00.552458471Z processor    : 0
2020-05-04T09:36:00.552461695Z vendor_id    : GenuineIntel
2020-05-04T09:36:00.552464872Z cpu family   : 6
2020-05-04T09:36:00.552467992Z model        : 63
2020-05-04T09:36:00.552471311Z model name   : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
2020-05-04T09:36:00.552474616Z stepping : 2
2020-05-04T09:36:00.552477715Z microcode    : 0x43
2020-05-04T09:36:00.552480781Z cpu MHz      : 2400.040
2020-05-04T09:36:00.552483934Z cache size   : 30720 KB
2020-05-04T09:36:00.552486978Z physical id  : 0
2020-05-04T09:36:00.552490023Z siblings : 1
2020-05-04T09:36:00.552493103Z core id      : 0
2020-05-04T09:36:00.552496146Z cpu cores    : 1
2020-05-04T09:36:00.552511390Z apicid       : 0
2020-05-04T09:36:00.552515457Z initial apicid   : 0
2020-05-04T09:36:00.552518523Z fpu      : yes
2020-05-04T09:36:00.552521677Z fpu_exception    : yes
2020-05-04T09:36:00.552524702Z cpuid level  : 13
2020-05-04T09:36:00.552527802Z wp       : yes
2020-05-04T09:36:00.552531691Z flags        : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single kaiser fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
2020-05-04T09:36:00.552535638Z bugs     : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit
2020-05-04T09:36:00.552538954Z bogomips : 4800.08
2020-05-04T09:36:00.552545171Z clflush size : 64
2020-05-04T09:36:00.552548419Z cache_alignment  : 64
2020-05-04T09:36:00.552551514Z address sizes    : 46 bits physical, 48 bits virtual
2020-05-04T09:36:00.552554916Z power management:
2020-05-04T09:36:00.552558030Z 
2020-05-04T09:36:00.552561090Z 
2020-05-04T09:36:00.552564141Z 
2020-05-04T09:36:00.552567135Z Memory: 4k page, physical 1014424k(76792k free), swap 0k(0k free)
2020-05-04T09:36:00.552570458Z 
2020-05-04T09:36:00.552573441Z vm_info: OpenJDK 64-Bit Server VM (25.131-b11) for linux-amd64 JRE (1.8.0_131-b11), built on Jun 16 2017 13:51:29 by ""buildozer"" with gcc 6.3.0
2020-05-04T09:36:00.552576947Z 
2020-05-04T09:36:00.552579894Z time: Mon May  4 09:36:00 2020
2020-05-04T09:36:00.552582956Z elapsed time: 0 seconds (0d 0h 0m 0s)
2020-05-04T09:36:00.552586052Z
</code></pre>

<p>can someone help me out to figure what could be the problem for docker status to be restarting ?</p>
","<p>im trying to start a elasticsearch instance for dev porpuse(the smallest possible , dont care), is inside ec2 t2.micro (in order to avoid cost running the elasticsearch service from aws)</p>

<p>so ... i pulled the instance </p>

<pre><code>docker pull docker.elastic.co/elasticsearch/elasticsearch:6.8.9
</code></pre>

<p><a href=""https://i.stack.imgur.com/LSdxz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LSdxz.png"" alt=""enter image description here""></a></p>

<p>now , when i try to run the image</p>

<pre><code>docker run -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" docker.elastic.co/elasticsearch/elasticsearch:6.8.9
</code></pre>

<p>the following error apperars</p>

<pre><code>OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) 

    <code>failed; error='Not enough space' (errno=12)
    <code>#
    <code># There is insufficient memory for the Java Runtime Environment to continue.
    <code># Native memory allocation (mmap) failed to map 1073741824 bytes for committing reserved memory.
    <code># An error report file with more information is saved as:
    <code># logs/hs_err_pid1.log</code></code></code></code></code></code>

</pre>

<p>i heard in some places that you have two options 
Either give more memory to your VM (is this posible for a t2.micro instance), how can achive that?
 or change Elasticsearch JVM settings<code>/etc/elasticsearch/jvm.options</code> and lower the values of the following parameters -Xms512m -Xmx512m</code> but how can i do this if i the image its not available to get in </p>

<p>i also tried to install elasticserach via rpm but requires coreutils >= 8.4 , so another failure trying to get the instance up</p>

<p>using -Xms512m -Xmx512m on jvm.options with avaible space (i guess)
i got the same error <strong>Not enough space</strong>
<a href=""https://i.stack.imgur.com/tekPy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tekPy.png"" alt=""enter image description here""></a></p>
"
"61797514","Docker container not hot-reloading Angular","<angular><docker><docker-compose><containers>","61799549","Trying to make hot-reload work Angular Dockerized","<angular><docker><docker-compose><containers><livereload>","<p>I'm configuring a dockerized Angular app but when i modify something, the page does not refresh automatically as it does on my local setup.</p>

<p>Based on this: <a href=""https://stackoverflow.com/questions/44176922/docker-container-doesnt-reload-angular-app/44180317"">Docker container doesn&#39;t reload Angular app</a></p>

<p>I should expose port 49153 but this does not work.</p>

<p>Here's my configuration:</p>

<p>Dockerfile</p>

<pre><code>FROM node:14-slim
RUN npm install @angular/cli@latest -g
RUN mkdir -p /home/boilerplate
WORKDIR /home/boilerplate
EXPOSE 4200 49153
CMD ng serve --port 4200 --poll 1
</code></pre>

<p>docker-compose.yaml</p>

<pre><code>version: ""3""
services:
  app:
    image: project
    build:
      context: .
      dockerfile: font-app/Dockerfile
    volumes:
      - ./font-app:/home/boilerplate
    ports:
      - 4200:4200  #live reload server is 49153 while normal server is 4200 where changes refresh when browser refreshed
      - 49153:49153
</code></pre>

<p>localhost:4200 works fine, and when i do changes, and refresh it works. 
I have also exposed port 49153 like another stackoverflow post, but it does not work. Port 49153 has nothing in it.</p>

<p>Any thoughts what is wrong?</p>

<p>Logs:</p>

<pre><code>192-168-0-243:Spring-and-springboot Jeff$ docker-compose up
Starting spring-and-springboot_app_1 ... done
Attaching to spring-and-springboot_app_1
app_1  | Your global Angular CLI version (9.1.6) is greater than your local
app_1  | version (8.3.26). The local Angular CLI version is used.
app_1  | 
app_1  | To disable this warning use ""ng config -g cli.warnings.versionMismatch false"".
app_1  | WARNING: This is a simple server for use in testing or debugging Angular applications
app_1  | locally. It hasn't been reviewed for security issues.
app_1  | 
app_1  | Binding this server to an open connection can result in compromising your application or
app_1  | computer. Using a different host than the one passed to the ""--host"" flag might result in
app_1  | websocket connection issues. You might need to use ""--disableHostCheck"" if that's the
app_1  | case.
app_1  | ℹ ｢wds｣: Project is running at http://0.0.0.0:4200/webpack-dev-server/
app_1  | ℹ ｢wds｣: webpack output is served from /
app_1  | ℹ ｢wds｣: 404s will fallback to //index.html
app_1  | 
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 270 kB [initial] [rendered]
app_1  | chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
app_1  | chunk {styles} styles.js, styles.js.map (styles) 9.69 kB [initial] [rendered]
app_1  | chunk {vendor} vendor.js, vendor.js.map (vendor) 3.91 MB [initial] [rendered]
app_1  | Date: 2020-05-14T12:24:26.717Z - Hash: d3844cd515038df15e5c - Time: 19389ms
app_1  | ** Angular Live Development Server is listening on 0.0.0.0:4200, open your browser on http://localhost:4200/ **
app_1  | ℹ ｢wdm｣: Compiled successfully.
app_1  | ℹ ｢wdm｣: Compiling...
app_1  | ℹ ｢wdm｣: wait until bundle finished: /
app_1  | ℹ ｢wdm｣: wait until bundle finished: /runtime.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /styles.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /polyfills.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /main.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /vendor.js
app_1  | 
app_1  | Date: 2020-05-14T12:29:46.595Z - Hash: 1d9ec15738fbcb9e6453
app_1  | 4 unchanged chunks
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | Time: 3435ms
app_1  | ℹ ｢wdm｣: Compiled successfully.
</code></pre>
","<p>I'm configuring a dockerized Angular app but when i modify something, <strong>the page does not refresh automatically as it does on my local setup.</strong></p>

<p>Based on this: <a href=""https://stackoverflow.com/questions/44176922/docker-container-doesnt-reload-angular-app/44180317"">Docker container doesn&#39;t reload Angular app</a> , I should expose port 49153 but this does not work.</p>

<p>Here's my configuration:</p>

<p>Dockerfile</p>

<pre><code>FROM node:14-slim
RUN npm install @angular/cli@latest -g
RUN mkdir -p /home/boilerplate
WORKDIR /home/boilerplate
EXPOSE 4200 49153
CMD ng serve --port 4200 --live-reload-port 49153 --poll 1
</code></pre>

<p>docker-compose.yaml</p>

<pre><code>version: ""3""
services:
  app:
    image: project
    build:
      context: .
      dockerfile: font-app/Dockerfile
    volumes:
      - ./font-app:/home/boilerplate
    ports:
      - 4200:4200  
      - 49153:49153
</code></pre>

<p>localhost:4200 works fine but if i change something in my code and save, the page does not automatically refresh.</p>

<p>Any thoughts on what is wrong?</p>

<p>Logs:</p>

<pre><code>192-168-0-243:Spring-and-springboot Jeff$ docker-compose up
Starting spring-and-springboot_app_1 ... done
Attaching to spring-and-springboot_app_1
app_1  | Your global Angular CLI version (9.1.6) is greater than your local
app_1  | version (8.3.26). The local Angular CLI version is used.
app_1  | 
app_1  | To disable this warning use ""ng config -g cli.warnings.versionMismatch false"".
app_1  | WARNING: This is a simple server for use in testing or debugging Angular applications
app_1  | locally. It hasn't been reviewed for security issues.
app_1  | 
app_1  | Binding this server to an open connection can result in compromising your application or
app_1  | computer. Using a different host than the one passed to the ""--host"" flag might result in
app_1  | websocket connection issues. You might need to use ""--disableHostCheck"" if that's the
app_1  | case.
app_1  | ℹ ｢wds｣: Project is running at http://0.0.0.0:4200/webpack-dev-server/
app_1  | ℹ ｢wds｣: webpack output is served from /
app_1  | ℹ ｢wds｣: 404s will fallback to //index.html
app_1  | 
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | chunk {polyfills} polyfills.js, polyfills.js.map (polyfills) 270 kB [initial] [rendered]
app_1  | chunk {runtime} runtime.js, runtime.js.map (runtime) 6.15 kB [entry] [rendered]
app_1  | chunk {styles} styles.js, styles.js.map (styles) 9.69 kB [initial] [rendered]
app_1  | chunk {vendor} vendor.js, vendor.js.map (vendor) 3.91 MB [initial] [rendered]
app_1  | Date: 2020-05-14T12:24:26.717Z - Hash: d3844cd515038df15e5c - Time: 19389ms
app_1  | ** Angular Live Development Server is listening on 0.0.0.0:4200, open your browser on http://localhost:4200/ **
app_1  | ℹ ｢wdm｣: Compiled successfully.
app_1  | ℹ ｢wdm｣: Compiling...
app_1  | ℹ ｢wdm｣: wait until bundle finished: /
app_1  | ℹ ｢wdm｣: wait until bundle finished: /runtime.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /styles.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /polyfills.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /main.js
app_1  | ℹ ｢wdm｣: wait until bundle finished: /vendor.js
app_1  | 
app_1  | Date: 2020-05-14T12:29:46.595Z - Hash: 1d9ec15738fbcb9e6453
app_1  | 4 unchanged chunks
app_1  | chunk {main} main.js, main.js.map (main) 51.7 kB [initial] [rendered]
app_1  | Time: 3435ms
app_1  | ℹ ｢wdm｣: Compiled successfully.
</code></pre>
"
"54269973","How to convert a Spring-Boot web service into a Docker image?","<spring-boot><docker><dockerfile>","63567621","Docker container port unreachable Spring Boot App","<spring><spring-boot><docker>","<p>I want to access my website from a Docker container but I can't. The steps I am trying to implement are as follows. After all the steps I can't access the <code>http://localhost:8080/index</code> page, where did I make a mistake?</p>

<p>The Spring-Boot project name is <code>demo</code>. Parts of my code:</p>

<pre><code>package com.qwerty.demo.rest;

import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class FunRestService {

    @GetMapping(""/index"")
    public String setIndex() {
        return ""HELLO WORLD!"";
    }
}
</code></pre>

<p>My <code>dockerfile</code> code:</p>

<pre><code>FROM openjdk:8
COPY . /usr/var/www/MYPROJECT
WORKDIR /usr/var/www/MYPROJECT
EXPOSE 8080
CMD ./mvnw spring-boot:run
</code></pre>

<p>I build the Spring-Boot project to <code>myimage1</code> with this command.</p>

<pre><code>docker build -t myimage1 .
</code></pre>

<p>Then, I create a new container from <code>myimage1</code> with this command.</p>

<pre><code>docker run --name mycontainer1 myimage1
</code></pre>

<p>And Maven downloads necessary files and starts my app for me. Last output:</p>

<pre><code>  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.2.RELEASE)

2019-01-19 17:40:32.604  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : Starting DemoApplication on 8086b6e010fb with PID 6 (/usr/var/www/MYPROJECT/target/classes started by root in /usr/var/www/MYPROJECT)
2019-01-19 17:40:32.613  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : No active profile set, falling back to default profiles: default
2019-01-19 17:40:34.119  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2019-01-19 17:40:34.170  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-01-19 17:40:34.171  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.14]
2019-01-19 17:40:34.186  INFO 6 --- [           main] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib]
2019-01-19 17:40:34.288  INFO 6 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2019-01-19 17:40:34.289  INFO 6 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1559 ms
2019-01-19 17:40:34.602  INFO 6 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2019-01-19 17:40:34.882  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''2019-01-19 17:40:34.888  INFO 6 --- [           main] com.qwerty.demo.DemoApplication     : Started DemoApplication in 3.176 seconds (JVM running for 69.839)
</code></pre>

<p>What should we do to convert one such Spring-Boot project (using a <code>Dockerfile</code>) into an image? How can I access my web page?</p>
","<p>I run my application in my host.</p>
<pre><code>java -jar docker-spring-boot.war


 http://localhost:8083/
</code></pre>
<p>On the browser, it works fine.</p>
<p>In the docker container, it didn't work.</p>
<pre><code> docker run docker-spring-boot -p 8083:8083

 http://localhost:8083/
 http://192.168.99.100:8083/
</code></pre>
<p>I have this message:</p>
<p><code>This site is inaccessible</code></p>
<p>Dockerfile</p>
<pre><code>FROM openjdk:8-jdk-alpine
EXPOSE 8083
ADD target/docker-spring-boot.war docker-spring-boot.war
ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/docker-spring-boot.war&quot;]
</code></pre>
<p>Have you an idea about the solution ?</p>
"
"62169568","Docker Alpine Linux python (missing)","<docker><alpine>","62169844","ERROR: unsatisfiable constraints - while installing python using APK","<python><docker><dockerfile><alpine>","<p>I have a pipeline which deploys my container from GitLab. Last deployment was 5 days ago and went without any problems. Today I deploy it and get the following error:</p>
<pre><code>$ apk add --no-cache curl python py-pip
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
 fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
 ERROR: unsatisfiable constraints:
   python (missing):
     required by: world[python]
</code></pre>
<p>My job definition is:</p>
<pre><code>my-deploy:
  type: my-deploy
  image: docker:stable
  script:
    - apk update
    - apk add --no-cache curl python py-pip &lt;-- Here the erorr happens
    ...
</code></pre>
","<p>My dockerfile looks like this:</p>

<pre><code>FROM alpine

RUN apk add --no-cache \
    ca-certificates \
    curl \
    python \
    py-pip
RUN pip install awscli

# remaining code...
</code></pre>

<p>But when I build this, I get the following logs with an error:</p>

<pre><code>RUN apk add --no-cache  ca-certificates     curl    python  py-pip

fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz

fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz

ERROR: unsatisfiable constraints:

  python (missing):

    required by: world[python]

The command '/bin/sh -c apk add --no-cache  ca-certificates     curl    python  py-pip' returned a non-zero code: 1
</code></pre>

<p>I have found some questions which described the same error but the solution doesn't help me.</p>

<p>Do I need to mention the <strong>python</strong> version? I would really like to use to install latest version only.</p>
"
"55419927","Permission denied when changing permissions on PV with init-container","<kubernetes><openshift>","63132158","Kubernetes permissions with initContainer: Operation not Permitted","<docker><kubernetes><openshift>","<p>I am trying to run an deployment config on OpenShift. Part of my deployment config runs an init container which sets up permissions on persistent volume with chown. When the init-container fires up, it fails and the logs print out ""permission denied""</p>

<p>Here is my init-container:</p>

<pre><code>  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
        name: ${NAME}-primary
        namespace: ${NAMESPACE}

    spec:
        replicas: 1
        strategy:
            type: Recreate
        template:
            metadata:
                labels:
                    name: ${NAME}-primary
                    test-app: ${NAME}
            spec:
                serviceAccountName: ${SERVICE_ACCOUNT}

                initContainers:
                  - name: set-volume-ownership
                    image: alpine:3.6
                    command: [""sh"", ""-c"", ""chown -R 1030:1030 /var/opt""]
                    volumeMounts:
                      - name: test-app-data
                        mountPath: /var/opt
</code></pre>

<p>I also have chmod 777 set on the nfs mount which has my persistent volume.</p>

<p>So, I know OpenShift runs the pod as a random UID by default. I know I can add the service account from my deployment config to scc anyuid and this will work but I dont want to do that as that is a security concern and my cluster admin will not allow that. How can I get around this? I have been reading about fsGroups but they havent made sense to me. Any opinions?</p>
","<p>I currently have the problem that I want to deploy clickhouse-server for another application called sentry in openshift / kubernetes. This image needs 2 volume mounts for some stuff but the mounts do not have the correct permissions so the <code>clickhouse</code> user is not able to write to this directory. My initial thought was to use <code>initContainer</code> and use <code>alpine:3.12.0</code> to do a simple <code>chown</code> on my mounts as you can see in the following config:</p>
<pre class=""lang-yaml prettyprint-override""><code>apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: ../kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: clickhouse
  name: clickhouse
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: clickhouse
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        kompose.cmd: ../kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: clickhouse
    spec:
      initContainers:
        - name: volume-permissions
          volumeMounts:
            - mountPath: /var/lib/clickhouse
              name: sentry-clickhouse
            - mountPath: /var/log/clickhouse-server
              name: sentry-clickhouse-log
          image: alpine:3.12.0
          # image: busybox
          imagePullPolicy: IfNotPresent
          # command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
          command:
            - chown
            - -R
            - 2000020900:2000020900
            - /var/log/clickhouse-server
            - /var/lib/clickhouse
      containers:
      - image: yandex/clickhouse-server:19.17
        imagePullPolicy: IfNotPresent
        name: clickhouse
        resources: 
          requests:
            memory: 64Mi
            cpu: 25m
          limits:
            memory: 128Mi
            cpu: 100m
        volumeMounts:
        - mountPath: /var/lib/clickhouse
          name: sentry-clickhouse
          # subPath: clickhouse
        - mountPath: /var/log/clickhouse-server
          name: sentry-clickhouse-log
          # subPath: clickhouse-server
      restartPolicy: Always
      serviceAccountName: &quot;&quot;
      volumes:
      - name: sentry-clickhouse
        # persistentVolumeClaim:
        #     claimName: default
      - name: sentry-clickhouse-log
        # persistentVolumeClaim:
        #     claimName: default
      # securityContext:
      #     fsGroup: 2000020900
      #     runAsUser: 2000020900
      #     runAsGroup: 2000020900
      
status: {}

</code></pre>
<p>The error I get when looking at the logs of initContainer is: <code>chown: Operation not permitted!</code> Anyone got an idea how to solve this?</p>
"
"66456627","Docker image run in m1 processor","<mysql><macos><docker><docker-compose><apple-m1>","66468991","What is the correct Docker Baseimage for a manylinux1_x86_64.whl?","<python><linux><docker><pip><gurobi>","<p>I can only play in my macbook air m1 with docker preview and i can't run an image of mysql with version 8.0.22 through a docker-compose file.</p>
<p><a href=""https://i.stack.imgur.com/z20rc.png"" rel=""nofollow noreferrer"">docker-compose set</a></p>
<p>The command i run is : <code>docker-compose up -d mysql</code></p>
<p>How can I solve this problem?</p>
","<p>I'm trying to pip install gurobipy in my docker. When I used pip install with the given index <code>pip install -i https://pypi.gurobi.com gurobipy</code>, i got a &quot;no matching distribution found&quot; error when building the image. So I decided to use the correct wheel file directly. This left me with <code>ERROR: gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl is not a supported wheel on this platform.</code></p>
<ul>
<li><p>I tried using <code>python:3.6</code>, <code>python:3.8</code> and ubuntu latest with python3 as base images with the corresponding .whl files</p>
<ul>
<li>Besides <code>pip install -i *url*</code>, I used the following .whl files for the corresponding python versions(all can be found here: <a href=""https://pypi.gurobi.com/gurobipy/"" rel=""nofollow noreferrer"">https://pypi.gurobi.com/gurobipy/</a>):
<ul>
<li>python:3.8: gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl</li>
<li>python:3.6: gurobipy-9.1.1-cp36-cp36m-manylinux1_x86_64.whl</li>
</ul>
</li>
</ul>
</li>
<li><p>I checked the python architecture - it's 64. Bit. When I used <code>pip install -i *url*</code> in my pycharm Python 3.8 venv (64-Bit), everything worked out perfectly fine.</p>
</li>
<li><p>The directorys inside the .whl file can be found in the attached screenshot.<a href=""https://i.stack.imgur.com/sQn1Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sQn1Y.png"" alt=""file structure inside whl file"" /></a></p>
</li>
</ul>
<p>A similar issue was adressed at <a href=""https://stackoverflow.com/questions/54945866/montagepy-1-0-1-cp36-cp36m-manylinux1-x86-64-whl-is-not-a-supported-wheel-on-thi"">MontagePy-1.0.1-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform</a> without a sufficient anwser.</p>
<p>Gurobi is widely used and I couldn't find others with similar problems regarding gurobi, so I'm pretty sure i messed something up in my dockerfile. Am I using the wrong baseimage, Python version or missing any requirements?</p>
<p>I'm using the following dockerfile and requirements.</p>
<p>Dockerfile:
The outcommented lines i usually run in the terminal of the running container.</p>
<pre><code>FROM python:3.8

WORKDIR /

COPY . ./Wueexam_optimization
RUN pip install --upgrade pip
RUN pip install -r Wueexam_optimization/optimization/requirements.txt
#RUN pip install --extra-index-url https://pypi.gurobi.com gurobipy
#RUN pip install https://pypi.gurobi.com/gurobipy/gurobipy-9.1.1-cp38-cp38-manylinux1_x86_64.whl

ENV PYTHONPATH /

WORKDIR /Wueexam_optimization/optimization
CMD [&quot;python3&quot;,&quot;-u&quot;,&quot;app.py&quot;]
</code></pre>
<p>Requirements:</p>
<pre><code>flask
flask-cors
flask-sqlalchemy
Flask-SQLAlchemy
numpy == 1.19.3
pandas
mysqlclient
openpyxl
pulp
pymysql
cryptography

</code></pre>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","63742046",":cached and :delegated mounted volumed performance in OSX","<macos><performance><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>Has anyone been able to solve the filesystem performance issues with mounted volumes on OSX?</p>
<p>I’m running <code>time du -d0</code> in some directory, and my baseline (on the host) is this:</p>
<pre><code>&gt; time du -d0
117496  .
________________________________________________________
Executed in   87.45 millis    fish           external
   usr time    7.43 millis   88.00 micros    7.34 millis
   sys time   75.34 millis  735.00 micros   74.60 millis
</code></pre>
<p>When doing that inside a container, on a mounted volume:</p>
<pre><code>&gt; docker run -ti --rm -v $PWD:$PWD -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 6.42s
user    0m 0.06s
sys 0m 0.29s
</code></pre>
<p>After reading <a href=""https://docs.docker.com/docker-for-mac/osxfs-caching/"" rel=""nofollow noreferrer"">this</a>, I also tried the <code>:cached</code> and <code>:delegated</code> options, to no avail:</p>
<pre><code>&gt; docker run -ti --rm -v $PWD:$PWD:cached -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 6.09s
user    0m 0.06s
sys 0m 0.36s
&gt; docker run -ti --rm -v $PWD:$PWD:delegated -w $PWD alpine:3.11 time du -d0
58748   .
real    0m 5.28s
user    0m 0.06s
sys 0m 0.49s
</code></pre>
<p>Tested on:
macOS Catalina 10.15.6
Docker desktop 2.3.0.4
Docker engine: 19.03.12</p>
"
"57331050","How can I install package from private repository using docker","<docker><npm><npm-install>","63672427","npm install failed on compilation dockerfile","<docker><npm><svelte>","<p>I am installing a package from my private repository. I am able to install it using:
<code>npm i -S git+https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git</code></p>

<p>I am using docker container but while installation process I am getting an error:</p>

<pre><code>npm ERR! path git
npm ERR! code ENOENT
npm ERR! errno ENOENT
npm ERR! syscall spawn git
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t https://oauth2:XXXXXXX@gitlab.com/mygroup/acl-api.git
npm ERR! enoent
npm ERR! enoent
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent
</code></pre>

<p>How can I solve it?</p>

<p>My docker file:</p>

<pre><code>FROM node:alpine

COPY package.json package.json
COPY src src
COPY .babelrc .babelrc

RUN npm install  
RUN npm run gitlab-build

RUN ls
EXPOSE 8080
CMD [""npm"", ""run"", ""docker-start""]
</code></pre>
","<p>I have my dockerfile:</p>
<pre><code>FROM node:14-alpine
 
WORKDIR /usr/src/app
 
COPY rollup.config.js ./
COPY package.json ./
 
RUN npm install
 
COPY ./src ./src
COPY ./static ./static
 
RUN npm run-script build
 
EXPOSE 4000
 
ENV HOST=0.0.0.0
 
CMD [ &quot;npm&quot;, &quot;start&quot; ]
</code></pre>
<p>When i execute</p>
<pre><code>docker build -t Dockerfile .
</code></pre>
<p>I have this error:</p>
<pre><code>npm ERR! code ENOENT
npm ERR! syscall spawn git
npm ERR! path git
npm ERR! errno -2
npm ERR! enoent Error while executing:
npm ERR! enoent undefined ls-remote -h -t http://username:password@192.168.0.117:3000/username/example-component.git
npm ERR! enoent 
npm ERR! enoent 
npm ERR! enoent spawn git ENOENT
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent 
</code></pre>
<p>Because some of my package on my package.json are</p>
<pre><code>&quot;package&quot;:  &quot;git+http://username:password@192.168.0.117:3000/username/example-component.git&quot;,
</code></pre>
<p>that is reachable only with a vpn. When i execute docker build i'm under vpn, i don't understand why is unreachable</p>
"
"59186984","selenium.common.exceptions.SessionNotCreatedException: Message: session not created from tab crashed using ChromeDriver Chrome Selenium Python","<python-3.x><selenium><google-chrome><selenium-chromedriver>","63063417","Selenium: (Chrome) session deleted because of page crash / (Firefox) failed to decode response from marionette (Mac OSX)","<r><docker><selenium><web-scraping>","<p>I am having this error apparently when trying to access a url that the script requests, does not have a specific. I don't understand exactly why this error, but I want to treat it so as not to abort the script when it occurs.</p>

<p>This make duplicate, but not solution my problem: <a href=""https://stackoverflow.com/questions/59069926/how-to-avoid-the-error-selenium-common-exceptions-sessionnotcreatedexception-m"">How to avoid the error: selenium.common.exceptions.SessionNotCreatedException: Message: session not created from tab crashed</a></p>

<p>code:</p>

<pre><code>from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--ignore-certificate-errors')
chrome_options.add_argument('--incognito')
chrome_options.add_argument('--headless')
driver = webdriver.Chrome(""/driver/chromedriver"", options=chrome_options)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
  File ""scripts/page11.py"", line 15, in &lt;module&gt;
    driver = webdriver.Chrome(BASE_WEB_DRIVER, options=chrome_options)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py"", line 76, in __init__
    RemoteWebDriver.__init__(
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created
from tab crashed
  (Session info: headless chrome=78.0.3904.108)
</code></pre>

<p>Chrome: <code>Google Chrome 78.0.3904.108</code> and driver: <code>ChromeDriver 78.0.3904.105 (60e2d8774a8151efa6a00b1f358371b1e0e07ee2-refs/branch-heads/3904@{#877})</code> are compatible.</p>
","<p>I'm trying to navigate to a webpage in (R)Selenium using Docker on OSX. Certain webpages sometimes produce the following error using Chrome:</p>
<pre><code>Selenium message:unknown error: session deleted because of page crash
from tab crashed
(Session info: chrome=84.0.4147.89)
Build info: version: '3.141.59', revision: 'e82be7d358', time: '2018-11-14T08:25:53'
System info: host: '9ea7fbbb76dc', ip: '172.17.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '4.19.76-linuxkit', java.version: '1.8.0_252'
Driver info: driver.version: unknown
    
Error:   Summary: UnknownError
     Detail: An unknown server-side error occurred while processing the command.
     class: org.openqa.selenium.WebDriverException
     Further Details: run errorDetails method
</code></pre>
<p>Those same webpages tend to produce the following (related?) error using Firefox:</p>
<pre><code>Selenium message:Failed to decode response from marionette
Build info: version: '3.141.59', revision: 'e82be7d358', time: '2018-11-14T08:25:53'
System info: host: 'e06f2e9f8c00', ip: '172.17.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '4.19.76-linuxkit', java.version: '1.8.0_252'
Driver info: driver.version: unknown

Error:   Summary: UnknownError
     Detail: An unknown server-side error occurred while processing the command.
     class: org.openqa.selenium.WebDriverException
     Further Details: run errorDetails method
</code></pre>
<p>Here are my Docker settings:</p>
<p><a href=""https://i.stack.imgur.com/ItJNs.png"" rel=""nofollow noreferrer"">Docker settings</a></p>
<p>I couldn't figure out if it was some sort of disk space issue—digging around for answers has suggested that. Weirdly, the error happens almost randomly—the same webpage might throw it on one run but not the next time. <a href=""https://stackoverflow.com/questions/53902507/unknown-error-session-deleted-because-of-page-crash-from-unknown-error-cannot"">It also appears like the proposed fixes for this (like increasing /dev/shm size) aren't applicable for OSX.</a> Thanks in advance.</p>
"
"59299133","How to silent install Postgresql in Ubuntu via. Dockerfile?","<postgresql><docker><dockerfile>","63737359","Trying to create a custom Docker image but getting asked for question during a RUN step","<python><docker><opencv>","<p>I have the following docker file, and I am using the command <code>docker build -t demo:v1 .</code> to build the image.</p>

<pre><code>FROM ubuntu:18.04
WORKDIR /app
RUN apt update \
    &amp;&amp; apt -y upgrade \
    &amp;&amp; apt install -y python3 \
    &amp;&amp; apt install -y python3-pip \
    &amp;&amp; apt install -y poppler-utils \
    &amp;&amp; apt install -y libsm6 libxext6 libxrender-dev

RUN apt install -y postgresql

COPY requirements.txt /app/requirements.txt

RUN pip3 install -r requirements.txt

COPY . /app

CMD gunicorn -t 300 --workers 5 --bind  0.0.0.0:8080 wsgi
</code></pre>

<p>When I build an image using this, while installing postgresql, it expects input and stops the building process like this </p>

<pre><code>.
.
.
.
Setting up libpopt0:amd64 (1.16-11) ...
Setting up tzdata (2019c-0ubuntu0.18.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area:
</code></pre>

<p>So, how can I setup postgresql inside my image, so that it builds without expecting this input? Also, surprisingly, even after I input my option, nothing happens further, and the process gets stuck. </p>
","<p>I'm trying to create a custom Docker image and during one of the <code>RUN</code> steps I'm getting asked to select some 'menu' choice .. which hangs/stops the image from continuing/getting made.</p>
<p>Here's the Dockerfile:</p>
<pre><code>FROM ubuntu:latest

RUN apt update

# install OpenCV
RUN apt install python3-opencv -y

# Test that OpenCV has been installed
RUN python3 -c &quot;import cv2; print(cv2.__version__)&quot;
</code></pre>
<p>and here's the last few lines in the build output:</p>
<pre><code>....
Setting up libsnappy1v5:amd64 (1.1.8-1build1) ...
Setting up poppler-data (0.4.9-2) ...
Setting up libkrb5support0:amd64 (1.17-6ubuntu4) ...
Setting up libsasl2-modules-db:amd64 (2.1.27+dfsg-2) ...
Setting up tzdata (2020a-0ubuntu0.20.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area: 
</code></pre>
<p>This is part of the 3rd step =&gt; <code>RUN p3 p3-opencv</code></p>
<p>How can I get this step to sorta auto completed? I'm <em>guessing</em> that when trying to install <code>python-opencv</code> I am missing dependencies and/or needing some dependencies to be updated. This is fine, and it's trying to update/install these, but needs some user-interaction.</p>
"
"60895246","React app exiting in docker container with exit code 0","<reactjs><docker><nginx><docker-compose>","63265549","Problem running 'react-scripts start' from docker-compose","<reactjs><docker>","<p>I am trying to create a docker-compose setup with nginzx, flask, and react. I started my react app with react-create-app (<a href=""https://github.com/facebook/create-react-app"" rel=""noreferrer"">https://github.com/facebook/create-react-app</a>) and haven't changed anything from it yet.</p>

<p>My Dockerfile for the react app is:</p>

<pre><code>FROM node:10

WORKDIR /usr/src/app

# Install app dependencies
# A wildcard is used to ensure both package.json AND package-lock.json are copied
COPY package*.json ./
RUN npm install --verbose

# Bundle app source
COPY . .


EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>The compose script is:</p>

<pre><code>version: '3.1'

services:
    nginx:
        image: nginx:1.15
        container_name: nginx
        volumes:
            - ../:/var/www
            - ./nginx-dev.conf:/etc/nginx/conf.d/default.conf
        ports:
            - 80:80
        networks:
            - my-network
        depends_on:
            - flask
            - react
    react:
        build:
            context: ../react-app/
            dockerfile: ./Dockerfile
        container_name: react
        volumes:
            - ../react-app:/usr/src/app
        networks:
            my-network:
                aliases:
                    - react-app
        expose:
            - 3000
        ports:
            - ""3000:3000""
    flask:
        ...
networks:
    my-network:
</code></pre>

<p>The flask and nginx containers start fine, the output for react is:</p>

<pre><code>react    | 
react    | &gt; react-app@0.1.0 start /usr/src/app
react    | &gt; react-scripts start
react    | 
react    | ℹ ｢wds｣: Project is running at http://my-ip-address/
react    | ℹ ｢wds｣: webpack output is served from 
react    | ℹ ｢wds｣: Content not from webpack is served from /usr/src/app/public
react    | ℹ ｢wds｣: 404s will fallback to /
react    | Starting the development server...
react    | 
react    | 
react    | npm verb lifecycle react-app@0.1.0~start: unsafe-perm in lifecycle true
react    | npm verb lifecycle react-app@0.1.0~start: PATH: /usr/local/lib/node_modules/npm/node_modules/npm-lifecycle/node-gyp-bin:/usr/src/app/node_modules/.bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
react    | npm verb lifecycle react-app@0.1.0~start: CWD: /usr/src/app
react    | npm info lifecycle react-app@0.1.0~poststart: react-app@0.1.0
react    | npm verb exit [ 0, true ]
react    | npm timing npm Completed in 1727ms
react    | npm info ok 
react exited with code 0
</code></pre>
","<p>I like to use Docker as development environment and it worked quite well until react 2.1.8, but after upgrade the react to version 3.4.1, it stopped to work.</p>
<p>Now, the Docker container with the React application close with status exit 0, just after start the server.</p>
<p>It is strange because there is no log error. Any suggestion?</p>
<p>There is no error, just this log when I try <code>docker run react-frontend</code>:</p>
<pre><code>ℹ ｢wds｣: Project is running at http://172.17.0.2/
ℹ ｢wds｣: webpack output is served from 
ℹ ｢wds｣: Content not from webpack is served from /app/public
ℹ ｢wds｣: 404s will fallback to /
Starting the development server...

marcos@marcos-Inspiron-7472:~/Projetos/Virtus/Softex/edgeframework/maestro/frontend$ 
</code></pre>
"
"61762219","Kubernetes services are not accessible through nodeport with Desktop Docker setup","<docker><kubernetes><docker-desktop><kubernetes-service>","63225266","Can't access my node in k8s via browser (with service & deployment)","<docker><kubernetes><deployment>","<p>I am using Docker Desktop on windows 10. And I generate kubernetes NodePort Service to access from client web browser (<a href=""http://10.110.201.24:30008/hello/praveen"" rel=""nofollow noreferrer"">http://10.110.201.24:30008/hello/praveen</a>) but service is not accessible.</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  selector:
    matchLabels:
      run: spring-app
  replicas: 1
  template:
    metadata:
      labels:
        run: spring-app
    spec:
      containers:
        - name: spring-boot
          image: praveen1233/spring-boot:v1
          ports:
            - containerPort: 80
</code></pre>

<p>service </p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: service
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30008
</code></pre>

<p>DockerFile </p>

<pre><code>FROM openjdk:8u212-jdk-slim
LABEL maintainer=""praveen.ambati33@gmail.com""
VOLUME /tmp
EXPOSE 5001
ARG JAR_FILE=target/spring-boot-app.jar
ADD ${JAR_FILE} spring-boot-app.jar
ENTRYPOINT [""java"",""-Djava.security.egd=file:/dev/./urandom"",""-jar"",""/spring-boot-app.jar""]
</code></pre>

<p>I have wrote simple web based spring boot application as below </p>

<pre><code>@RestController
public class HelloController {
    @GetMapping( value = ""hello/{input}"")
    public String getMessage(@PathVariable String input){
        return ""Hi ""+input;
    }
}
</code></pre>

<p>application.properties</p>

<pre><code>server.port=5000
</code></pre>

<p>I could see everything looks good in terms of Deployment, Pod and Services status. However I am not able to access the application. </p>

<pre><code>kubectl get all
NAME                            READY   STATUS             RESTARTS   AGE
pod/my-app-7b77675f79-wwbfl     1/1     Running            0          32m

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        6d1h
service/service      NodePort    10.110.201.24   &lt;none&gt;        80:30008/TCP   23m

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-app     1/1     1            1           32m

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/my-app-7b77675f79     1         1         1       32m
</code></pre>

<p>I am guessing their could be different way of accessing ip in Docker Hub which I am not able figure it out! Could you please help. Appreciated !</p>

<p>Docker and k8s versions as below </p>

<pre><code>Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b
 Built:             Wed Mar 11 01:23:10 2020
 OS/Arch:           windows/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.8
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.17
  Git commit:       afacb8b
  Built:            Wed Mar 11 01:29:16 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.2.13
  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429
 runc:
  Version:          1.0.0-rc10
  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
</code></pre>
","<p>There is the deployment:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-task-tracker-deployment
spec:
  selector:
    matchLabels:
      app: my-task-tracker
  replicas: 5
  template:
    metadata:
      labels:
        app: my-task-tracker
    spec:
      containers:
        - name: hello-world
          image: shaikezam/task-tracker:1.0
          ports:
            - containerPort: 8080
              protocol: TCP
</code></pre>
<p>This is the service (NodePort):</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: my-task-tracker-service
  labels:
    app: my-task-tracker
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8085
      nodePort: 30001
      protocol: TCP
  selector:
    app: my-task-tracker
</code></pre>
<p>Now, I try to access localhost:8085 or localhost:30001, and nothing happened.</p>
<p>I'm running using K8S in docker desktop.</p>
<p>Any suggestion what I'm doing wrong?</p>
"
"63177219","Docker: How to access files from another container from a given container?","<docker><docker-compose><dockerfile><grpc><grpc-node>","63182856","Can One Container use the file of another Container?","<docker><docker-compose><dockerfile><docker-registry><docker-volume>","<p>Basically I have a main directory and Books Directory (General file structure, there's more but these are the important pieces). So when I fire a request from main to booksServer, it doesn't work because the node modules are missing.</p>
<p>That's because the node modules are inside the docker container at a specific path:  <strong>'/usr/src/app'</strong></p>
<p>How can I have main.js see that books (service/container) does have the proper node packages inside this specific path?</p>
<p>I think I can use docker-compose, but I wanted to test it individually without docker-compose first.</p>
<pre><code>**-Main Directory (Individual Service, has its own container)**
  -Initiator (Fires commands) 
  -DockerFile

**-Books Directory (Individual Service, has its own container)**
  -Stubs
     -BooksStub.js (NEED THIS!, but it won't work because needs npm modules which is located in its container @/usr/src/app. How can I access the nodemodules that it's using?)

  -booksServer.js
  -Package*.json (lock and package.json)
  -DockerFile
</code></pre>
<p>Inside the</p>
<p><strong>Error:</strong></p>
<pre><code>internal/modules/cjs/loader.js:800
    throw err;
    ^

Error: Cannot find module 'grpc'
</code></pre>
<p><strong>Books Dockerfile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 30043
CMD [&quot;node&quot;, &quot;booksServer.js&quot;]
</code></pre>
<p><strong>Main DockerFile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 4555
CMD [&quot;node&quot;, &quot;main.js&quot;]
</code></pre>
","<p>Basically I have a main directory and Books Directory (General file structure, there's more but these are the important pieces). So when I fire a request from main to booksServer, it doesn't work because the node modules are missing.</p>
<p>That's because the node modules are inside the docker container at a specific path:  <strong>'/usr/src/app'</strong></p>
<p>How can I have main.js see that books (service/container) does have the proper node packages inside this specific path?</p>
<p>I think I can use docker-compose, but I wanted to test it individually without docker-compose first.</p>
<pre><code>**-Main Directory (Individual Service, has its own container)**
  -Initiator (Fires commands) 
  -DockerFile

**-Books Directory (Individual Service, has its own container)**
  -Stubs
     -BooksStub.js (NEED THIS!, but it won't work because needs npm modules which is located in its container @/usr/src/app. How can I access the nodemodules that it's using?)

  -booksServer.js
  -Package*.json (lock and package.json)
  -DockerFile
</code></pre>
<p>Inside the</p>
<p><strong>Error:</strong></p>
<pre><code>internal/modules/cjs/loader.js:800
    throw err;
    ^

Error: Cannot find module 'grpc'
</code></pre>
<p><strong>Books Dockerfile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 30043
CMD [&quot;node&quot;, &quot;booksServer.js&quot;]
</code></pre>
<p><strong>Main DockerFile</strong></p>
<pre><code>FROM node:12.14.0
WORKDIR /usr/src/app
COPY package*.json ./
COPY . /usr/src/app
RUN npm install
EXPOSE 4555
CMD [&quot;node&quot;, &quot;main.js&quot;]
</code></pre>
"
"63280779","npm ERR! 404 Not Found - GET https://registry.npmjs.org/@typescript-eslint%2feslint-plugin - Not found","<reactjs><npm><create-react-app>","63281275","Kubernetes/Docker/Create React App + nginx not returning static files?","<reactjs><docker><kubernetes>","<p>When trying to create a new project with 'create-react-app', I get this error</p>
<pre><code>Installing packages. This might take a couple of minutes.
Installing react, react-dom, and react-scripts with cra-template...

npm ERR! code E404
npm ERR! 404 Not Found - GET https://registry.npmjs.org/@typescript-eslint%2feslint-plugin - Not found
npm ERR! 404
npm ERR! 404  '@typescript-eslint/eslint-plugin@^2.10.0' is not in the npm registry.
npm ERR! 404 You should bug the author to publish it (or use the name yourself!)
npm ERR! 404 It was specified as a dependency of 'react-scripts'
npm ERR! 404
npm ERR! 404 Note that you can also install from a
npm ERR! 404 tarball, folder, http url, or git url.
</code></pre>
<p>How can I fix this?</p>
","<p>i cant seem to find what causes this, This is the result in an incognito window:
<a href=""https://i.stack.imgur.com/zkzgl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zkzgl.png"" alt=""enter image description here"" /></a>
and i have this errors in my console:</p>
<pre><code>main.83e12943.chunk.js:1 Failed to load resource: the server responded with a status of 404 ()
main.5f361e03.chunk.css:1 Failed to load resource: the server responded with a status of 404 ()
logo.5d5d9eef.svg:1 Failed to load resource: the server responded with a status of 404 ()
DevTools failed to load SourceMap: Could not load content for https://ticketing.dev/static/js/2.a925df73.chunk.js.map: HTTP error: status code 404, net::ERR_HTTP_RESPONSE_CODE_FAILURE
DevTools failed to load SourceMap: Could not load content for https://ticketing.dev/static/css/main.5f361e03.chunk.css.map: HTTP error: status code 404, net::ERR_HTTP_RESPONSE_CODE_FAILURE
</code></pre>
<p>I am using the latest create react app version and this is my Docker file:</p>
<pre><code># build environment
FROM node:13.12.0-alpine as build
WORKDIR /app
ENV PATH /app/node_modules/.bin:$PATH
COPY package.json ./
COPY package-lock.json ./
RUN npm ci --silent
RUN npm install react-scripts@3.4.1 -g --silent
COPY . ./
RUN npm run build

# production environment
FROM nginx:stable-alpine
COPY --from=build /app/build /usr/share/nginx/html
# new
COPY nginx/nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 3000
CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]
</code></pre>
<p>my nginx.conf inspired from this post: <a href=""https://mherman.org/blog/dockerizing-a-react-app/"" rel=""nofollow noreferrer"">https://mherman.org/blog/dockerizing-a-react-app/</a></p>
<pre><code>server {

  listen 3000;

  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
    try_files $uri $uri/ /index.html;
  }

  error_page   500 502 503 504  /50x.html;

  location = /50x.html {
    root   /usr/share/nginx/html;
  }

}
</code></pre>
<p>This is Kubernetes Deployment/Service:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: service
  name: client-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: client
  template:
    metadata:
      labels:
        app.kubernetes.io/component: service
        app: client
    spec:
      containers:
        - name: client
          image: asia.gcr.io/udemy-ticketing-dev-285506/client
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: service
  name: client-srv
spec:
  ports:
    - name: client
      protocol: TCP
      port: 3000
      targetPort: 3000
  selector:
    app: client
  type: NodePort
</code></pre>
<p>and lastly is my ingress service:</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/use-regex: 'true'
spec:
  rules:
    - host: ticketing.dev
      http:
        paths:
          - path: /api/users/?(.*)
            backend:
              serviceName: auth-srv
              servicePort: 3000
          - path: /?(.*)
            backend:
              serviceName: client-srv
              servicePort: 3000
</code></pre>
<p>where did i go wrong?</p>
"
"63325112","torch.cuda.is_avaiable returns False with nvidia-smi not working","<docker><gpu><nvidia><torch>","63324124","torch.cuda.is_available() returns false - torch.version.cuda different from torch._C._cuda_getDriverVersion()","<docker><cuda><dockerfile><nvidia><torch>","<p>I'm trying to build a docker image that can run using GPUS, this my situation:
<a href=""https://i.stack.imgur.com/vZivE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vZivE.png"" alt=""situation inside docker images"" /></a></p>
<p>I have python 3.6 and I am starting from image nvidia/cuda:10.0-cudnn7-devel.
Torch does not see my GPUs.</p>
<p><code>nvidia-smi</code> is not working too, returning error:</p>
<pre><code>&gt; Failed to initialize NVML: Unknown Error
&gt; The command '/bin/sh -c nvidia-smi' returned a non-zero code: 255
</code></pre>
<p>I installed nvidia toolkit and nvidia-smi with</p>
<pre><code> RUN apt install nvidia-cuda-toolkit -y
 RUN apt-get install nvidia-utils-410 -y
</code></pre>
","<p>My problem is that I can't se my GPUs, as torch.cuda.device_count() returns 0 and torch.cuda.is_available() False .</p>
<p>I found out that torch._C._cuda_getDriverVersion() returns 10000, nvcc --version is</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
</code></pre>
<p>BUT torch.version.cuda is 10.2.</p>
<p>Moreover, <code>nvidia-smi</code> does not work inside the container, even if I installed cuda toolkit and utils with:</p>
<pre><code>RUN apt install nvidia-cuda-toolkit -y
RUN apt-get install nvidia-utils-410 -y
</code></pre>
<p>The <code>nvidia-smi</code> error is: Failed to initialize NVML: Unknown Error</p>
<p>My base image is nvidia/cuda:10.0-cudnn7-devel.</p>
<p>Edit:
I tried <code>docker run --gpus all nvidia/cuda:10.0-cudnn7-devel nvidia-smi</code> and it works, showing the same nvidia-smi as the local machine</p>
"
"63764895","Redis connect ECONNREFUSED 127.0.0.1:6379","<javascript><node.js><docker><redis><containers>","63765804","Can't connect to DB from container","<javascript><node.js><docker><redis><containers>","<p>Im running redis on my server (Redis is not running in a container)  and I am trying to connect to redis inside of my Container which has a Node.js app runninng. When starting the Docker Container with the node app from my Dockerfile in the logs there is the following error: <code>Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code></p>
","<p>I have a Docker container with a Node.js app and a Redis Database which is running on the server (not in a container). In the container when the app starts I can't connect to the DB and I am getting the following errror <code>Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379</code> but I dont understand why because if I open redis-cli on the server and ping it answers with pong so the db is up.</p>
"
"6795350","Nginx 403 forbidden for all files","<nginx><centos><http-status-code-403><php>","60490795","Docker php nginx mariadb","<php><docker><nginx><docker-compose><dockerfile>","<p>I have nginx installed with PHP-FPM on a CentOS 5 box, but am struggling to get it to serve any of my files - whether PHP or not.</p>

<p>Nginx is running as www-data:www-data, and the default ""Welcome to nginx on EPEL"" site (owned by root:root with 644 permissions) loads fine.</p>

<p>The nginx configuration file has an include directive for <i>/etc/nginx/sites-enabled/*.conf,</i> and I have a configuration file <em>example.com.conf</em>, thus:</p>

<pre><code>server {
 listen 80;

 Virtual Host Name
 server_name www.example.com example.com;


 location / {
   root /home/demo/sites/example.com/public_html;
   index index.php index.htm index.html;
 }

 location ~ \.php$ {
  fastcgi_pass   127.0.0.1:9000;
  fastcgi_index  index.php;
  fastcgi_param  PATH_INFO $fastcgi_script_name;
  fastcgi_param  SCRIPT_FILENAME  /home/demo/sites/example.com/public_html$fastcgi_script_name;
  include        fastcgi_params;
 }
}
</code></pre>

<p>Despite public_html being owned by www-data:www-data with 2777 file permissions, this site fails to serve any content -</p>

<pre><code> [error] 4167#0: *4 open() ""/home/demo/sites/example.com/public_html/index.html"" failed (13: Permission denied), client: XX.XXX.XXX.XX, server: www.example.com, request: ""GET /index.html HTTP/1.1"", host: ""www.example.com""
</code></pre>

<p>I've found numerous other posts with users getting 403s from nginx, but most that I have seen involve either more complex setups with Ruby/Passenger (which in the past I've actually succeeded with) or are only receiving errors when the upstream PHP-FPM is involved, so they seem to be of little help.</p>

<p>Have I done something silly here?</p>
","<p>I am new to Docker and I'm trying to convert my config.php into docker-compose and dockerfile but I can't run a server...</p>

<p>default.conf:</p>

<pre><code>upstream php {
        server php:9000;
}

server {
        root /var/www/monsite;
        index index.php index.html;

        location / {

                try_files $uri $uri/ /index.php?$args;
        }

        location ~ \.php$ {
                fastcgi_pass php;
                fastcgi_split_path_info ^(.+\.php)(/.*)$;
                include fastcgi_params;
                fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
                fastcgi_param HTTPS off;
        }

        location ~* \.(js|css|png|jpg|jpeg|gif|ico)$ {
                expires max;
                log_not_found off;
        }
}

</code></pre>
"
"1177238","change a config.properties file in a jar / war file in runtime and hotdeploy the changes?","<java><properties><jar><webserver><war>","66639244","Map a file in Docker using Docker Volume","<docker><docker-volume>","<p>change a config.properties file in a jar / war file in runtime and hotdeploy the changes ?
 my requirement is something as follows, we have a ""config.properties"" in a jar/war file , i have to open the file through a webpage and after the user has made necessary changes to it, i have to update the ""config.properties"" in jar/war file and hot deploy it. can we achieve this feat ? if so can you please point me to relevant sites/documents so that i can jumpstart on this.</p>
","<p>I've created a WAR file and running it as an Image using Tomcat in Docker</p>
<p>So in the WAR file contain a Java Web App</p>
<p>I got a config.properties file in the WAR.</p>
<p>I want to be able to modify the config.properties file when the container is running and map the modified file in Docker Volume for storage</p>
"
"7120426","How to invoke bash, run commands inside the new shell, and then give control back to user?","<shell><bash>","60515881","how to keep bash shell after finishing processing `-c` commands","<bash><docker>","<p>This must either be really simple or really complex, but I couldn't find anything about it... I am trying to open a new bash instance, then run a few commands inside it, and give the control back to the user <em>inside that same instance</em>.</p>

<p>I tried:</p>

<pre><code>$ bash -lic ""some_command""
</code></pre>

<p>but this executes <code>some_command</code> inside the new instance, then closes it. I want it to stay open.</p>

<p>One more detail which might affect answers: if I can get this to work I will use it in my <code>.bashrc</code> as alias(es), so bonus points for an <code>alias</code> implementation!</p>
","<p>Generally, my <code>/bin/bash</code> shell exits after finishing executing the commands specified in <code>-c</code> list. Is there a way to keep the shell still open after all the commands are done?</p>
"
"8224898","Java JDBC Access denied for user","<java><mysql><jdbc>","60235408","Trouble connecting to mysql in docker using jdbc","<mysql><docker><jdbc>","<p>I am trying make a connection to MySQL from my java application and it keeps on saying :</p>

<blockquote>
  <p>java.sql.SQLException: Access denied for user 'vincent'@'x.x.x.x'
  (using password: YES)</p>
</blockquote>

<p>I have checked in phpmyadmin that vincent can conect from any host and I also have a python script who can connect with the same username/password without any problem</p>

<p>What is the problem ?</p>

<p>Thank you very much</p>

<p>Regards.</p>
","<p>I am new to both Docker and Java.I am trying to connect to mysql server inside a docker using JDBC from my local Ubuntu machine.The command that I use to start the mysql server in docker is:</p>

<pre><code>docker network create --subnet=172.18.0.0/16 master_network    
docker run --net master_network --ip 172.18.0.22 --name docker-mysql -e MYSQL_ROOT_PASSWORD=password -d -p=3306:3306   mysql:latest 
</code></pre>

<p>The command that I am using to connect to mysql server from docker is as follows:</p>

<pre><code>String connectionUrl = ""jdbc:mysql://172.18.0.22:3306/hb_student_tracker?useSSL=false&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true"";
Connection conn =  DriverManager.getConnection(connectionUrl,user,password);
</code></pre>

<p>But I am getting the below error:</p>

<pre><code>java.sql.SQLException: Access denied for user 'hbstudent'@'172.18.0.1' (using password: YES)
at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)
at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:456)
at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)
at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)
at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:228)
at HibernateBasics.TestingConnectionToMysql.main(TestingConnectionToMysql.java:15)
</code></pre>

<p>The docker inspect command gives me the following information</p>

<pre><code>[
{
    ""Id"": ""90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5"",
    ""Created"": ""2020-02-15T01:19:14.021480835Z"",
    ""Path"": ""docker-entrypoint.sh"",
    ""Args"": [
        ""mysqld""
    ],
    ""State"": {
        ""Status"": ""running"",
        ""Running"": true,
        ""Paused"": false,
        ""Restarting"": false,
        ""OOMKilled"": false,
        ""Dead"": false,
        ""Pid"": 5105,
        ""ExitCode"": 0,
        ""Error"": """",
        ""StartedAt"": ""2020-02-15T01:19:14.526528758Z"",
        ""FinishedAt"": ""0001-01-01T00:00:00Z""
    },
    ""Image"": ""sha256:791b6e40940cd550af522eb4ffe995226798204504fe495743445b900e417a51"",
    ""ResolvConfPath"": ""/var/lib/docker/containers/90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5/resolv.conf"",
    ""HostnamePath"": ""/var/lib/docker/containers/90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5/hostname"",
    ""HostsPath"": ""/var/lib/docker/containers/90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5/hosts"",
    ""LogPath"": ""/var/lib/docker/containers/90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5/90c8f3d5191fd207578a0b40e8a2369de949dc3a9c6fd2fdcf2ba044c32bdca5-json.log"",
    ""Name"": ""/docker-mysql"",
    ""RestartCount"": 0,
    ""Driver"": ""overlay2"",
    ""Platform"": ""linux"",
    ""MountLabel"": """",
    ""ProcessLabel"": """",
    ""AppArmorProfile"": ""docker-default"",
    ""ExecIDs"": null,
    ""HostConfig"": {
        ""Binds"": null,
        ""ContainerIDFile"": """",
        ""LogConfig"": {
            ""Type"": ""json-file"",
            ""Config"": {}
        },
        ""NetworkMode"": ""master_network"",
        ""PortBindings"": {
            ""3306/tcp"": [
                {
                    ""HostIp"": """",
                    ""HostPort"": ""3306""
                }
            ]
        },
        ""RestartPolicy"": {
            ""Name"": ""no"",
            ""MaximumRetryCount"": 0
        },
        ""AutoRemove"": false,
        ""VolumeDriver"": """",
        ""VolumesFrom"": null,
        ""CapAdd"": null,
        ""CapDrop"": null,
        ""Capabilities"": null,
        ""Dns"": [],
        ""DnsOptions"": [],
        ""DnsSearch"": [],
        ""ExtraHosts"": null,
        ""GroupAdd"": null,
        ""IpcMode"": ""private"",
        ""Cgroup"": """",
        ""Links"": null,
        ""OomScoreAdj"": 0,
        ""PidMode"": """",
        ""Privileged"": false,
        ""PublishAllPorts"": false,
        ""ReadonlyRootfs"": false,
        ""SecurityOpt"": null,
        ""UTSMode"": """",
        ""UsernsMode"": """",
        ""ShmSize"": 67108864,
        ""Runtime"": ""runc"",
        ""ConsoleSize"": [
            0,
            0
        ],
        ""Isolation"": """",
        ""CpuShares"": 0,
        ""Memory"": 0,
        ""NanoCpus"": 0,
        ""CgroupParent"": """",
        ""BlkioWeight"": 0,
        ""BlkioWeightDevice"": [],
        ""BlkioDeviceReadBps"": null,
        ""BlkioDeviceWriteBps"": null,
        ""BlkioDeviceReadIOps"": null,
        ""BlkioDeviceWriteIOps"": null,
        ""CpuPeriod"": 0,
        ""CpuQuota"": 0,
        ""CpuRealtimePeriod"": 0,
        ""CpuRealtimeRuntime"": 0,
        ""CpusetCpus"": """",
        ""CpusetMems"": """",
        ""Devices"": [],
        ""DeviceCgroupRules"": null,
        ""DeviceRequests"": null,
        ""KernelMemory"": 0,
        ""KernelMemoryTCP"": 0,
        ""MemoryReservation"": 0,
        ""MemorySwap"": 0,
        ""MemorySwappiness"": null,
        ""OomKillDisable"": false,
        ""PidsLimit"": null,
        ""Ulimits"": null,
        ""CpuCount"": 0,
        ""CpuPercent"": 0,
        ""IOMaximumIOps"": 0,
        ""IOMaximumBandwidth"": 0,
        ""MaskedPaths"": [
            ""/proc/asound"",
            ""/proc/acpi"",
            ""/proc/kcore"",
            ""/proc/keys"",
            ""/proc/latency_stats"",
            ""/proc/timer_list"",
            ""/proc/timer_stats"",
            ""/proc/sched_debug"",
            ""/proc/scsi"",
            ""/sys/firmware""
        ],
        ""ReadonlyPaths"": [
            ""/proc/bus"",
            ""/proc/fs"",
            ""/proc/irq"",
            ""/proc/sys"",
            ""/proc/sysrq-trigger""
        ]
    },
    ""GraphDriver"": {
        ""Data"": {
            ""LowerDir"": ""/var/lib/docker/overlay2/794c33dc3dba3bd3f3b59c2fa0751abb4a08d256716791442e048eba8dad4718-init/diff:/var/lib/docker/overlay2/08cff1074288c594a797ee81fc4f8e6d65e56e3815ed779c59d6a12b69c4bef3/diff:/var/lib/docker/overlay2/d4965f2d4dc55cc40e6b33b7370ea8f735e06ba02bc6e9d9f302a023e2f0ab40/diff:/var/lib/docker/overlay2/2c4fa859d3bf60b43e2ad97889fa850cc02d82e55a3c61d951a1fa7fa8a37a23/diff:/var/lib/docker/overlay2/e0f465f72028a167ef41bf541b693e70c004053d429ee8c8992e690a7138d419/diff:/var/lib/docker/overlay2/28b6d7f4d734c945bc670e4d9741012bcf0a4e779e1797e0687f76969c413a95/diff:/var/lib/docker/overlay2/3d0721a625aca9d5a9c11d2d07a9ce365f0d03cf8033975557435ed43a6f0465/diff:/var/lib/docker/overlay2/58393e75098f068ca6b95c79f62f31f0774af97ebe53dc191b8b34c9c79b0ce4/diff:/var/lib/docker/overlay2/c656f68c525873355be411b15e2af708f47758f0efcc55e0dcf0de87f4f3859a/diff:/var/lib/docker/overlay2/a9b4a61a797212e7ebc6816a7b2f94fbfe74a9572e61fbf803748590eee89a5a/diff:/var/lib/docker/overlay2/e2db5f0234d764243d37f466e65b2e87913ece5744d806ecafcb596c937094ce/diff:/var/lib/docker/overlay2/06cc3c7e20d47b12dffed61bb7b95bf8055468bc65c981cb4f0d0a7080cc9880/diff:/var/lib/docker/overlay2/72eb0da71371fe94e46795d36ad0d103b92c6ee0e0aec48ee3888c16a1e72837/diff"",
            ""MergedDir"": ""/var/lib/docker/overlay2/794c33dc3dba3bd3f3b59c2fa0751abb4a08d256716791442e048eba8dad4718/merged"",
            ""UpperDir"": ""/var/lib/docker/overlay2/794c33dc3dba3bd3f3b59c2fa0751abb4a08d256716791442e048eba8dad4718/diff"",
            ""WorkDir"": ""/var/lib/docker/overlay2/794c33dc3dba3bd3f3b59c2fa0751abb4a08d256716791442e048eba8dad4718/work""
        },
        ""Name"": ""overlay2""
    },
    ""Mounts"": [
        {
            ""Type"": ""volume"",
            ""Name"": ""63948ece9ac10c7e0f6a5761684a5861051cfad101dec1374529ba016bb6973a"",
            ""Source"": ""/var/lib/docker/volumes/63948ece9ac10c7e0f6a5761684a5861051cfad101dec1374529ba016bb6973a/_data"",
            ""Destination"": ""/var/lib/mysql"",
            ""Driver"": ""local"",
            ""Mode"": """",
            ""RW"": true,
            ""Propagation"": """"
        }
    ],
    ""Config"": {
        ""Hostname"": ""90c8f3d5191f"",
        ""Domainname"": """",
        ""User"": """",
        ""AttachStdin"": false,
        ""AttachStdout"": false,
        ""AttachStderr"": false,
        ""ExposedPorts"": {
            ""3306/tcp"": {},
            ""33060/tcp"": {}
        },
        ""Tty"": false,
        ""OpenStdin"": false,
        ""StdinOnce"": false,
        ""Env"": [
            ""MYSQL_ROOT_PASSWORD=password"",
            ""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",
            ""GOSU_VERSION=1.7"",
            ""MYSQL_MAJOR=8.0"",
            ""MYSQL_VERSION=8.0.19-1debian9""
        ],
        ""Cmd"": [
            ""mysqld""
        ],
        ""Image"": ""mysql:latest"",
        ""Volumes"": {
            ""/var/lib/mysql"": {}
        },
        ""WorkingDir"": """",
        ""Entrypoint"": [
            ""docker-entrypoint.sh""
        ],
        ""OnBuild"": null,
        ""Labels"": {}
    },
    ""NetworkSettings"": {
        ""Bridge"": """",
        ""SandboxID"": ""45769ba1596268ba02b3d6f2e50eaadd950b1113faed28f25f981408370272ab"",
        ""HairpinMode"": false,
        ""LinkLocalIPv6Address"": """",
        ""LinkLocalIPv6PrefixLen"": 0,
        ""Ports"": {
            ""3306/tcp"": [
                {
                    ""HostIp"": ""0.0.0.0"",
                    ""HostPort"": ""3306""
                }
            ],
            ""33060/tcp"": null
        },
        ""SandboxKey"": ""/var/run/docker/netns/45769ba15962"",
        ""SecondaryIPAddresses"": null,
        ""SecondaryIPv6Addresses"": null,
        ""EndpointID"": """",
        ""Gateway"": """",
        ""GlobalIPv6Address"": """",
        ""GlobalIPv6PrefixLen"": 0,
        ""IPAddress"": """",
        ""IPPrefixLen"": 0,
        ""IPv6Gateway"": """",
        ""MacAddress"": """",
        ""Networks"": {
            ""master_network"": {
                ""IPAMConfig"": {
                    ""IPv4Address"": ""172.18.0.22""
                },
                ""Links"": null,
                ""Aliases"": [
                    ""90c8f3d5191f""
                ],
                ""NetworkID"": ""25371b13fc9c6f11c23e1d795364bbd5cc5a160de866335b058b807c9f6ce552"",
                ""EndpointID"": ""b3cb70c1cbee99170886dd9a60f2ca87534831694f947949fed596919712d668"",
                ""Gateway"": ""172.18.0.1"",
                ""IPAddress"": ""172.18.0.22"",
                ""IPPrefixLen"": 16,
                ""IPv6Gateway"": """",
                ""GlobalIPv6Address"": """",
                ""GlobalIPv6PrefixLen"": 0,
                ""MacAddress"": ""02:42:ac:12:00:16"",
                ""DriverOpts"": null
            }
        }
    }
}
</code></pre>

<p>]</p>

<p>I am connected to mysql running in the docker from mysql workbench application using the ip 172.18.0.22 and port 3306.This is the script I am using to create the users in mysql using mysql workbench application.</p>

<pre><code>CREATE USER 'hbstudent'@'localhost' IDENTIFIED BY 'hbstudent';
GRANT ALL PRIVILEGES ON * . * TO 'hbstudent'@'localhost';
ALTER USER 'hbstudent'@'localhost' IDENTIFIED WITH mysql_native_password BY 'hbstudent';
</code></pre>

<p><strong>Also ,my-sql server in docker is running at 172.18.0.22.But the error says that user is denied access at 172.18.0.1 although my connection string in jdbc is 172.18.0.22</strong></p>

<p>I have already referred to similar stackoverflow questions and yet not able to figure out what my mistake is.</p>
"
"9581064","Why should there be spaces around '[' and ']' in Bash?","<bash><if-statement><syntax>","60645034","getting sh: missing ] error while building Docker for React built image is built successfully","<reactjs><bash><docker><dockerfile><sh>","<p>I was trying to write a Bash script that uses an <code>if</code> statement.</p>

<pre><code>if[$CHOICE -eq 1];
</code></pre>

<p>The script was giving me errors until I gave a space before and after <code>[</code> and before <code>]</code> as shown below:</p>

<pre><code>if [ $CHOICE -eq 1 ];
</code></pre>

<p>My question here is, why is the space around the square brackets so important in Bash?</p>
","<p>I am getting <code>sh: missing ]</code> error when trying to build the Docker for the React app. The image is successfully built but when I run the Docker image it is not working.</p>
<p>Below is my Dockerfile:</p>
<pre><code>FROM node:12.2.0-alpine

# Create a work directory and copy over our dependency manifest files.
RUN mkdir /app
WORKDIR /app
COPY /src /app/src
COPY /public /app/public
COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;start.sh&quot;, &quot;./&quot;]
RUN chmod +x ./start.sh
ARG DOCKER_ENV=staging
ENV REACT_APP_NODE_ENV=&quot;${DOCKER_ENV}&quot;
RUN npm install --production --silent
RUN yarn global add serve
RUN yarn build
RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;development&quot;]; then npm start; fi
RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;test&quot;]; then yarn predeploy; fi
RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;preProd&quot;]; then yarn preprod; fi

CMD [&quot;./start.sh&quot;,&quot;run&quot;]

# Expose PORT 3000 and 5000 on our virtual machine so we can run our server
EXPOSE 3000 5000

# Run the docker
# docker build -t ci-onedashui:latest .
# docker run -e REACT_APP_NODE_ENV=development -itd -p 1100:3000 ci-onedashui:latest
# docker run -e REACT_APP_NODE_ENV=test -itd -p 2200:5000 ci-onedashui:latest
# docker run -e REACT_APP_NODE_ENV=preProd -itd -p 3300:5000 ci-onedashui:latest
</code></pre>
<p>Below is my start.sh file:</p>
<pre><code># #!/usr/bin/env bash
set -eo pipefail

case $1 in
  start)
    # The '| cat' is to trick Node that this is an non-TTY terminal
    # then react-scripts won't clear the console.
    yarn start | cat
    ;;
  build)
    yarn build
    ;;
  test)
    yarn test $@
    ;;
  *)
    exec &quot;$@&quot;
    ;;
esac
</code></pre>
<p>Below is the result after the docker image is built</p>
<pre><code>$ docker build -t ci-onedashui:latest .
Sending build context to Docker daemon  382.7MB
Step 1/17 : FROM node:12.2.0-alpine
 ---&gt; f391dabf9dce
Step 2/17 : RUN mkdir /app
 ---&gt; Using cache
 ---&gt; f19895e14198
Step 3/17 : WORKDIR /app
 ---&gt; Using cache
 ---&gt; 1d055992105e
Step 4/17 : COPY /src /app/src
 ---&gt; Using cache
 ---&gt; 18d79c639d05
Step 5/17 : COPY /public /app/public
 ---&gt; Using cache
 ---&gt; 1afadec4f9da
Step 6/17 : COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;start.sh&quot;, &quot;./&quot;]
 ---&gt; 82ea71d10c35
Step 7/17 : RUN chmod +x ./start.sh
 ---&gt; Running in fc0e593e2040
Removing intermediate container fc0e593e2040
 ---&gt; 05420d4df0bc
Step 8/17 : ARG DOCKER_ENV=staging
 ---&gt; Running in c91c53a346a2
Removing intermediate container c91c53a346a2
 ---&gt; d8b0eba554aa
Step 9/17 : ENV REACT_APP_NODE_ENV=&quot;${DOCKER_ENV}&quot;
 ---&gt; Running in ad754a94628e
Removing intermediate container ad754a94628e
 ---&gt; 50c279a8bb3f
Step 10/17 : RUN npm install --production --silent
 ---&gt; Running in bb660fb39754
added 1621 packages from 839 contributors and audited 2114964 packages in 40.897s
found 44 moderate severity vulnerabilities
  run `npm audit fix` to fix them, or `npm audit` for details
Removing intermediate container bb660fb39754
 ---&gt; 9c1dd40bd618
Step 11/17 : RUN yarn global add serve
 ---&gt; Running in 46dab04c9d0b
yarn global v1.15.2
[1/4] Resolving packages...
[2/4] Fetching packages...
[3/4] Linking dependencies...
[4/4] Building fresh packages...
success Installed &quot;serve@11.3.0&quot; with binaries:
      - serve
Done in 4.55s.
Removing intermediate container 46dab04c9d0b
 ---&gt; b792f9347d6d
Step 12/17 : RUN yarn build
 ---&gt; Running in 72af5962707b
yarn run v1.15.2
$ react-scripts build
Creating an optimized production build...
Compiled successfully.

File sizes after gzip:

  360.6 KB  build/static/js/2.d5cfe191.chunk.js
  33.55 KB  build/static/css/2.e2ee0b36.chunk.css
  32.34 KB  build/static/js/main.8eef7758.chunk.js
  2.98 KB   build/static/css/main.61726d20.chunk.css
  771 B     build/static/js/runtime-main.f5dd81f8.js

The project was built assuming it is hosted at the server root.
You can control this with the homepage field in your package.json.
For example, add this to build it for GitHub Pages:

  &quot;homepage&quot; : &quot;http://myname.github.io/myapp&quot;,

The build folder is ready to be deployed.
You may serve it with a static server:

  npm install -g serve
  serve -s build

Find out more about deployment here:

  bit.ly/CRA-deploy

Done in 61.41s.
Removing intermediate container 72af5962707b
 ---&gt; 9072d8a887a0
Step 13/17 : RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;development&quot;]; then npm start; fi
 ---&gt; Running in c7c5a7fdcf14
sh: missing ]
Removing intermediate container c7c5a7fdcf14
 ---&gt; 0e278de7d770
Step 14/17 : RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;test&quot;]; then yarn predeploy; fi
 ---&gt; Running in acfb683cabb1
sh: missing ]
Removing intermediate container acfb683cabb1
 ---&gt; 06ead152621c
Step 15/17 : RUN if [ &quot;${REACT_APP_NODE_ENV}&quot; = &quot;preProd&quot;]; then yarn preprod; fi
 ---&gt; Running in abcfa702b7d8
sh: missing ]
Removing intermediate container abcfa702b7d8
 ---&gt; 669a14f20590
Step 16/17 : CMD [&quot;./start.sh&quot;,&quot;run&quot;]
 ---&gt; Running in 1527008685b3
Removing intermediate container 1527008685b3
 ---&gt; 7c4b22b6fbd4
Step 17/17 : EXPOSE 3000 5000
 ---&gt; Running in 444d1732d360
Removing intermediate container 444d1732d360
 ---&gt; 3467e21a6717
Successfully built 3467e21a6717
Successfully tagged ci-onedashui:latest
SECURITY WARNING: You are building a Docker image from Windows against a non-Windows Docker host. All files and directories added to build context will have '-rwxr-xr-x' permissions. It is recommended to double check and reset permissions for sensitive files and directories.
</code></pre>
"
"9607702","Does not contain a static 'main' method suitable for an entry point","<c#><visual-studio-2010>","66546809","getting error Program does not contain a static 'Main' method due to path issue in docker configuration file","<docker><.net-core><containers>","<p>I began organizing my code to day into seperarate .cs files, and in order to allow the methods that work with the UI to continue to do so I would create the .cs code under the same namespace and public partial class name so the methods could be inter-operable. </p>

<p>My header look like this in four files, including my main core file that calls: </p>

<pre><code>public shell()
{
InitializeComponent(); 
}
</code></pre>

<p>Header area of .cs files that work with the UI (and seem to be causing this new conflict):</p>

<pre><code>using System;
using System.Windows.Forms;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.Linq;
using System.Text;
using System.Text.RegularExpressions;
using System.IO;
using System.Data.SqlServerCe;
using System.Diagnostics;
using System.Threading;
using System.Collections.Specialized;
using System.Net;
using System.Runtime.InteropServices;
using watin = WatiN.Core;
using WatiN.Core.Native.InternetExplorer;
using System.Web; 


namespace WindowsFormsApplication1
{

    public partial class shell : Form
    {
</code></pre>

<p>Now when I try to debug/preview my application (BTW this is a Windows Application within Visual Studio 2010 Express) I get this error message:</p>

<blockquote>
  <p>Does not contain a static 'main' method suitable for an entry point</p>
</blockquote>

<p>I looked in the application properties in Application->Startup object, but it offers me no options. How can I inform the application to begin at the .cs file that has my InitializeComponent(); command? </p>

<ul>
<li>I've looked around so far without a solution.</li>
<li>The properties on each .cs file are set to 'Compile'.</li>
<li>I do not see an App.xaml file in my Solutions explorer but I do see a app.config file.</li>
</ul>

<p>I'm still very new and this is my first attempt at an organizing method with c# code.</p>
","<p>I have placed a dotnet core console app with the directory &quot;C:\TEMP\ConsoleDocker&quot; and place dockerfile here only.</p>
<p><a href=""https://i.stack.imgur.com/CPu0o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CPu0o.png"" alt=""enter image description here"" /></a></p>
<p>and here is the docker file,</p>
<pre><code>FROM mcr.microsoft.com/dotnet/runtime:3.1 AS base
WORKDIR /app

FROM mcr.microsoft.com/dotnet/sdk:3.1 AS build
WORKDIR /src
COPY ConsoleDocker.csproj ConsoleDocker/
RUN dotnet restore ConsoleDocker/ConsoleDocker.csproj
COPY . .
WORKDIR /src/ConsoleDocker
RUN dotnet build ConsoleDocker.csproj -c Release -o /app

FROM build AS publish
RUN dotnet publish ConsoleDocker.csproj -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [&quot;dotnet&quot;, &quot;ConsoleDocker.dll&quot;]
</code></pre>
<p>When I am trying to build docker using command <code>docker build -t test -f Dockerfile .</code>, I am getting error like, <code>Program does not contain a static 'Main' method suitable for an entry point </code> which indicates some path issue within my docker file configuration. What is I am missing here? Please suggest</p>
"
"12120598","Syntax error in shell script with process substitution","<linux><bash><shell><process-substitution>","60424822","entrypoint.sh - line 9: syntax error: unexpected ""(""","<bash><docker><entry-point>","<p>I have this shell script which I use to back up my system. There is a line:</p>

<pre><code>tar -Pzcpf /backups/backup.tar.gz --directory=/ --exclude=proc --exclude=sys --exclude=dev/pts --exclude=backups --exclude=var/log / 2&gt; &gt;(grep -v 'socket ignored' &gt;&amp;2)
</code></pre>

<p>As you can see, I have been trying to filter out the annoying, useless ""socket ignored"" error by tar, using <a href=""http://wundsam.net/2011/03/27/filtering-stderr-or-how-to-get-rid-of-tars-socket-ignored-message/"" rel=""noreferrer"">this blog post</a>.</p>

<p>What I get from shell upon execution is:</p>

<blockquote>
  <p>/bin/sysback: line 45: syntax error near unexpected token <code>&gt;'
  /bin/sysback: line 45:</code>tar -Pzcpf /backups/backup --directory=/
  --exclude=proc --exclude=sys --exclude=dev/pts --exclude=backups --exclude=var/log / 2> >(grep -v 'socket ignored' >&amp;2)'</p>
</blockquote>
","<p>I am trying to run the following script from docker (based on alpine image)</p>

<pre><code>#!/bin/sh

echo ""test""

export USERNAME=""AQICAHj456mvH8iSJofL46Xtr7KP6Ng3Vn5k6BpZbkAAAAZTBjBgkqhkiG9w0BBwagVjBUAgEAME8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMwCm8C+wSLRm/+sSuAgEQgCJHCFbrIwCQuH0x2iGp13j9SuxMtfrcE6c4SmrHRVkkX24f""
export AWS_REGION=""us-east-1""
echo ""$AWS_REGION""

decrypt=$(aws kms decrypt --ciphertext-blob fileb://&lt;(echo ""$USERNAME"" | base64 -d))
export $key=""$(echo $decrypt | jq .Plaintext -r | base64 -d)""

exec ""$@""
</code></pre>

<p>I am getting the below output</p>

<pre><code>test
us-east-1
/bin/entrypoint.sh: line 9: syntax error: unexpected ""(""
</code></pre>

<p>I am not sure how to resolve this syntax error. Any help is appreciated.</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","60466005","Docker port networking","<docker><docker-networking>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>How is possible to open ports on existing docker container, without creating a new configured one?</p>

<p>I have the container with the OpenVAS configuration, just need to open the ports for the Greenbone assistant being opened in the local browser from the container. No IPTables, no additional configuration were made to the container. Just Openvas and Greenbone assistant configuration. By default using host network, also have tried to bridge the connection between localhost and the container.</p>
"
"20073168","Warning: mysqli_connect(): (HY000/2002): No such file or directory","<php><mysql><mysqli>","66959923","PHP can't connect MySQL with localhost","<php><mysql><docker>","<p>I'm trying to install vanilla forums on my Mac, and for this I just created a database and a user from the MySQL command line:</p>

<pre><code>mysql&gt; CREATE DATABASE vanilla;
Query OK, 1 row affected (0.00 sec)

mysql&gt; create user 'vanilla_user3'@'localhost' IDENTIFIED BY 'vanilla_password';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; GRANT ALL PRIVILEGES ON * . * TO 'vanilla_user3'@'localhost';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)
</code></pre>

<p>So I try to connect using the following code:</p>

<pre><code>$con=mysqli_connect(""localhost"",""vanilla_user3"",""vanilla_password"",""vanilla"");
if (mysqli_connect_errno($con)) {
  echo ""Failed to connect to MySQL: "" . mysqli_connect_error();
}
</code></pre>

<p>but unfortunately, I get an error saying </p>

<blockquote>
  <p>Warning: mysqli_connect(): (HY000/2002): No such file or directory in /Users/kramer65/Sites/vanilla/info.php on line 3
  Failed to connect to MySQL: No such file or directory</p>
</blockquote>

<p>Any idea where I'm going wrong?</p>
","<p>I am using Docker-compose with PHP7.4 and MySQL.
And im creating form in PHP and when connecting to my BD with localhost or 127.0.0.1
I have error</p>
<blockquote>
<p>Warning: mysqli::__construct(): (HY000/2002): No such file or directory in</p>
</blockquote>
<p>But im conecting with the public IP from server the connection is ok.
I add in /etc/hosts my public IP with localhost and doesn't work</p>
<p>And I need the connection to be with localhost
Could it be a problem to use a vps so I can't use localhost? or Docker config?</p>
<p>Thanks.</p>
<p>EDIT: I test and i can use IP from container (172.18.0.1) to connect
How do I configure it to work only locally?</p>
<p>This respose is helpme
<a href=""https://stackoverflow.com/questions/51566115/docker-compose-using-php-mysqli-to-connect-to-mysql-database"">Docker-Compose Using PHP (MySQLi) to Connect to MySQL database</a></p>
"
"22049212","Docker: Copying files from Docker container to host","<docker><docker-container><file-copying>","66307489","Copy folder from Dockerfile to host","<docker><dockerfile>","<p>I'm thinking of using Docker to build my dependencies on a Continuous Integration (CI) server, so that I don't have to install all the runtimes and libraries on the agents themselves. </p>

<p>To achieve this I would need to copy the build artifacts that are built inside the container back into the host. Is that possible?</p>
","<p>I have a docker file</p>
<pre><code>FROM ubuntu:20.04
################################
### INSTALL Ubuntu build tools and prerequisites
################################
# Install build base

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update &amp;&amp; apt-get install -y \
    build-essential \
    git \
    subversion \
    sharutils \
    vim \
    asciidoc \
    binutils \ 
    bison \
    flex \
    texinfo \
    gawk \
    help2man \
    intltool \
    libelf-dev \
    zlib1g-dev \
    libncurses5-dev \
    ncurses-term \
    libssl-dev \
    python2.7-dev \
    unzip \
    wget \
    rsync \
    gettext \
    xsltproc &amp;&amp; \
    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*
 
ARG  FORCE_UNSAFE_CONFIGURE=1
RUN  git clone https://git.openwrt.org/openwrt/openwrt.git
WORKDIR /openwrt
RUN ./scripts/feeds update -a &amp;&amp; ./scripts/feeds install -a
COPY .config /openwrt/.config
RUN mkdir files
WORKDIR /files
RUN mkdir etc
WORKDIR /etc
RUN mkdir uci-defaults
WORKDIR /uci-defaults
COPY xx_custom /openwrt/files/etc/uci-defaults/xx_custom
WORKDIR /openwrt
RUN make -j 4
RUN ls /openwrt/bin/targets/ramips/mt76x8

WORKDIR /root
CMD [&quot;bash&quot;]
</code></pre>
<p>I want to copy all the files inside the folder mt76x8 to the host. I want to that inside the dockerfile so that when I run the docker file I should get the generated files in my host.</p>
<p>How can I do that?</p>
"
"2821043","Allowed characters in Linux environment variable names","<linux><syntax><environment-variables>","65625003","Dockerfile ENV directive: can key contain a period?","<docker><dockerfile>","<p>What characters are allowed in Linux environment variable names? My cursory search of man pages and the web did only produce information about how to work with variables, but not which names are allowed.</p>

<p>I have a Java program that requires an defined environment variable containing a dot, like <code>com.example.fancyproperty</code>. With Windows I can set that variable, but I had no luck setting it in linux (tried in SuSE and Ubuntu). Is that variable name even allowed?</p>
","<p>Docker version: <code>20.10.2, build 2291f61</code></p>
<p>In a Dockerfile, is it possible to use an <code>ENV</code> directive for which the key contains embedded periods?  For example:</p>
<pre><code>ENV story.paragraph.port 2029
</code></pre>
<p>And if it is possible to declare ENV variables with keys that have periods, then is it later possible to reference them, using familiar shell-interpolation, in the same Dockerfile?</p>
<pre><code>EXPOSE $story.paragraph.port
</code></pre>
<p>The latter <code>EXPOSE</code> directive breaks for me.</p>
<p>My use-case:  I have a python script that loads its configuration from an INI file.  Eg, I might have configuration properties like these:</p>
<pre><code>story.paragraph.word=helloworld
story.paragraph.length=256
</code></pre>
<p>The python logic recognizes configuration settings both from the INI file (by default) or, alternatively, in overrides specified in the environment.  The idea is that a container instance could specify its own environment variables for <code>story.paragraph.word</code> or <code>story.paragraph.length</code>, and that those values would override the default configuration.</p>
"
"5533050",".gitignore exclude folder but include specific subfolder","<git><gitignore>","51760051","Is there a way to tell .dockerignore to ignore all but certain packages from node_modules?","<node.js><docker><npm>","<p>I have the folder <code>application/</code> which I add to the <code>.gitignore</code>. Inside the  <code>application/</code> folder is the folder <code>application/language/gr</code>. How can I include this folder?</p>

<p>I've tried this</p>

<pre><code>application/
!application/language/gr/
</code></pre>

<p>with no luck...</p>
","<p>I have the case that I can not use npm install in my Dockerfile for all of my packages because some of them are private and using the technique described <a href=""https://docs.npmjs.com/private-modules/docker-and-private-modules"" rel=""nofollow noreferrer"" title=""NPM Docker and private packages"">here</a> on npm using an auth token is not working in this scenario.</p>

<p>So I'm now following these instructions <a href=""https://stackoverflow.com/a/38782109/9931013"">https://stackoverflow.com/a/38782109/9931013</a></p>

<p>But as it would be way faster and more convenient to just copy the private npm packages instead of all I was wondering if anyone had an idea on how to do this.</p>

<p>p.s. Of course I could just add all node_modules subfolders except for the private ones to .dockerignore I guess but as Joey from Friends once said: ""There's got to be a better way!!""</p>
"
"20164466","What would be a good docker webdev workflow?","<docker>","51915550","advantages of separated DB in docker?","<django><postgresql><docker>","<p>I have a hunch that <a href=""http://docker.io"">docker</a> could greatly improve my webdev workflow - but I haven't quite managed to wrap my head around how to approach a project adding docker to the stack. </p>

<p>The basic software stack would look like this: </p>

<h3>Software</h3>

<ul>
<li><p>Docker image(s) providing custom LAMP stack</p>

<ul>
<li>Apache with several modules</li>
<li>MYSQL</li>
<li>PHP</li>
<li>Some CMS, e.g. <a href=""http://www.silverstripe.org"">Silverstripe</a></li>
</ul></li>
<li><p>GIT</p></li>
</ul>

<h2>Workflow</h2>

<p>I could imagine the workflow to look somewhat like the following: </p>

<h3>Development</h3>

<ol>
<li>Write a <code>Dockerfile</code> that defines a LAMP-container meeting the requirements stated above
<ul>
<li>REQ: The machine should start apache/mysql right after booting</li>
</ul></li>
<li>Build the docker image</li>
<li>Copy the files required to run the CMS into e.g. <code>~/dev/cmsdir</code>
<ul>
<li>Put <code>~/dev/cmsdir/</code> under version control</li>
</ul></li>
<li>Run the docker container, and somehow mount <code>~/dev/cmsdir</code> to <code>/var/www/</code> on the container</li>
<li>Populate the database </li>
<li>Do work in <code>/dev/cmsdir/</code></li>
<li>Commit &amp; shut down docker container</li>
</ol>

<h3>Deployment</h3>

<ol>
<li>Set up remote host (e.g. with ansible)</li>
<li>Push container image to remote host</li>
<li>Fetch <code>cmsdir</code>-project via git</li>
<li>Run the docker container, pull in the database and mount <code>cmsdir</code> into <code>/var/www</code></li>
</ol>

<p>Now, this looks all quite nice on paper, BUT I am not quite sure whether this would be the right approach at all.</p>

<h2>Questions:</h2>

<ol>
<li><p>While developing locally, how would I get the database to persist between reboots of the container instance? Or would I need to run sql-dump every time before spinning down the container?</p></li>
<li><p>Should I have separate container instances for the db and the apache server? Or would it be sufficient to have a single container for above use case?</p></li>
<li><p>If using separate containers for database and server, how could I automate spinning them up and down at the same time?</p></li>
<li><p>How would I actually mount <code>/dev/cmsdir/</code> into the containers <code>/var/www/</code>-directory? Should I utilize <a href=""http://docs.docker.io/en/latest/use/working_with_volumes/#volume-def"">data-volumes</a> for this?</p></li>
<li><p>Did I miss any pitfalls? Anything that could be simplified? </p></li>
</ol>
","<p>case 1 : django+postgresql (container)</p>

<p>case 2 : django(container)&lt;-connect->postgresql(container)</p>

<p>(these cases are on one server)</p>

<p>What are the advantages of separating ? or performance?</p>
"
"21201493","Couldn't require openssl in ruby","<ruby><openssl><rubygems>","51911430","Can't gem install with containerized and built-from-source ruby 2.5.1","<ruby><docker><ruby-2.5>","<p>I have openssl installed in my virtual machine ubuntu12.04lts.</p>

<p>When I run the gem command it gives error.</p>

<blockquote>
  <p>Error: while executing gem (Gem::Exception)<br>
        Unable to require openssl. install openSSL and rebuilt ruby (preferred) or use non HTTPs sources</p>
</blockquote>

<p>And I also test require openssl in irb mode.
it gives error.</p>

<blockquote>
  <p>Loaderror: cannot load such file --openssl<br>
      from /usr/local/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55 `require'</p>
</blockquote>

<p>I have openssl</p>

<pre><code>$openssl version
OpenSSL 1.0.1 14 mar 2012
</code></pre>

<p>How to fix the error?</p>
","<p>I can't seem to get gem working within a Docker container that's got a ruby 2.5.1 built from source. This is my <code>Dockerfile</code>:</p>

<pre><code>FROM ubuntu:16.04

RUN apt-get update
RUN apt-get install -y openssl
RUN apt-get install -y zlib1g-dev

#Install ruby
RUN apt-get install -y wget
RUN apt-get update
RUN apt-get install -y gcc
RUN wget https://cache.ruby-lang.org/pub/ruby/2.5/ruby-2.5.1.tar.gz
RUN tar -xf ruby-2.5.1.tar.gz
RUN apt-get install make
RUN cd ruby-2.5.1 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install
</code></pre>

<p>If I try to issue the command <code>gem install bundler</code> from within a container derived from the above <code>Dockerfile</code>, I get:</p>

<pre><code>ERROR:  While executing gem ... (Gem::Exception)
Unable to require openssl, install OpenSSL and rebuild Ruby (preferred) or use non-HTTPS sources
</code></pre>

<p>How could I fix this error?</p>

<ul>
<li><p>I've tried adding a <code>--with-openssl-dir=$(which openssl)</code> to the <code>./configure</code> instruction, but it was to no avail. </p></li>
<li><p>If I don't issue the <code>apt-get install -y zlib1g-dev</code> command, the installation still succeeds, but I get an error saying that the <code>zlib</code> package is missing.</p></li>
</ul>
"
"21889053","What is the runtime performance cost of a Docker container?","<performance><docker>","65626587","Container vs Host (Native) Performance","<docker><kubernetes><containers><google-kubernetes-engine><amazon-ecs>","<p>I'd like to comprehensively understand the run-time performance cost of a Docker container. I've found references to <a href=""https://stackoverflow.com/questions/21691540/how-to-optimize-performance-for-a-docker-container/21707838#21707838"">networking anecdotally being ~100µs slower</a>.</p>

<p>I've also found references to the run-time cost being ""negligible"" and ""close to zero"" but I'd like to know more precisely what those costs are. Ideally I'd like to know what Docker is abstracting with a performance cost and things that are abstracted without a performance cost. Networking, CPU, memory, etc.</p>

<p>Furthermore, if there are abstraction costs, are there ways to get around the abstraction cost. For example, perhaps I can mount a disk directly vs. virtually in Docker.</p>
","<ul>
<li><p>Since containers are lightweight os virtualization, can we get the same performance as native (host)?</p>
</li>
<li><p>What would be the difference in performance?</p>
</li>
</ul>
<p>Any leads are highly appreciated or if you have any analysis reports or any reference with host vs containers performance comparison will help.</p>
<p>IA</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65651130","Dockerized Django project not able to connect to host's postgres database","<python><django><postgresql><docker><postgresql-12>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>So I have a Django project which is running in Docker, which is trying to connect postgres which is running on host machine. But I am getting error</p>
<pre><code>web_1  | django.db.utils.OperationalError: could not connect to server: Connection refused
web_1  |    Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
web_1  |    TCP/IP connections on port 5432?
web_1  | could not connect to server: Cannot assign requested address
web_1  |    Is the server running on host &quot;localhost&quot; (::1) and accepting
web_1  |    TCP/IP connections on port 5432?
</code></pre>
<p>I know that we need to make postgres to listen for requests from other IP addresses.
I have already made changes in postgres settings.</p>
<p>Added few lines in following files.</p>
<ol>
<li><p>/etc/postgresql/12/main/postgresql.conf</p>
<pre><code>      listen_addresses = '*'
</code></pre>
</li>
<li><p>/etc/postgresql/12/main/pg_hba.conf</p>
<pre><code>     host  all  all  172.17.0.0/16  trust
</code></pre>
</li>
</ol>
<p>In Django project settings.py has</p>
<pre><code>DATABASES = {
    'default': {
         'ENGINE': 'django.db.backends.postgresql_psycopg2',
         'NAME': 'django_docker',
         'USER': '&lt;postgres_user&gt;',
         'PASSWORD': '&lt;password&gt;',
         'HOST': 'localhost',
         'PORT': '5432',
         'ATOMIC_REQUESTS': True
     }
}
</code></pre>
<p>I am not sure why I am getting this error.</p>
<p>I also tried to use <code>127.0.0.1</code> and <code>&lt;public_ip&gt;</code> of host in Django database <code>HOST</code> settings, but I still get same error.</p>
<p>All versions</p>
<pre><code>Django : 3.0.5
PostgreSQL : 12.5
Docker : 20.10.1
docker-compose : 1.25.0
</code></pre>
<p>I am guessing that I am missing very small thing here, but I'm not sure what it is.</p>
<p>Please let me know if someone has any solutions, suggestions for this. Thank you.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65489632","Sending a request from inside a docker container to a host spring server","<docker><rest><containers><devops><restapi>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I'm new to docker and I was wondering if it was possible for an application inside a container to send a request to a REST server located outside of the container (in my case, a Spring Boot server located on the host port 8080), and how I should do it ?</p>
<p>There would be several containers trying to do the same so, sharing the whole network with the &quot;net=host&quot; option wouldn't be possible, I guess ?</p>
<p>Thanks !</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65489723","How to get rid of the port when `host.docker.internal` to interact with a service running locally on my machine","<docker><networking><containers><devops>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>To make a service running in a container interact with another one running locally at let's say <code>localhost:3000</code>, we can use <code>host.docker.internal:3000</code> in the container code to make the required connection. But is there a way I can get rid of the port while using <code>host.docker.internal</code> so that it automatically routes to <code>localhost:3000</code>? That is, I just use <code>host.docker.internal</code> in my code instead of <code>host.docker.internal:3000</code>?</p>
<p>Clarification: I am not asing how to connect to localhost from inside the container. What I am asking is how to route <code>host.docker.internal</code> to the correct port without specifying it explicitly like: <code>host.docker.internal:3000</code>.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65576088","How can I configure my docker-compose for connect my 'app' running in docker containner to my 'database' running in physical machine","<docker><networking><docker-compose><gateway>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<pre><code>version: '3'
services:
   my_app:
     build: ./api
     environments:
       - DB_HOST=192.168.10.1 #Physical IP database server
</code></pre>
<blockquote>
<p>Docker Engine Server IP: 192.168.20.1</p>
</blockquote>
"
"25540711","docker postgres pgadmin local connection","<postgresql><docker><pgadmin>","65654441","PgAdmin Connecting to Postgres through Docker-compose results in ""Unable to connect to server : timeout expired""","<postgresql><docker><docker-compose><pgadmin><pgadmin-4>","<p>I have created an ubuntu image with nginx, php and postgres.</p>

<p>I want to connect the postgres database in my current image with <code>pgadmin</code> located on my local machine. </p>

<p>I have tried using docker inspector to try to use the image ip to make a connection with my local pgadmin but without much success. I've also tried configuring some ports on local to make connection work.</p>
","<p>Consider the Docker-compose :</p>
<pre><code>version: &quot;3&quot;
services:
  postgres_svc:
    image: postgres:12.0-alpine
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: myCoolPassword
      POSTGRES_DB: postgres      
    ports:
      - &quot;5432:5432&quot;


   ... More services
</code></pre>
<p>When I run <code>Docker-compose up</code> and run :</p>
<pre><code>docker inspect &lt;POSTGRES_ID&gt; | grep Gateway &gt;&gt;&gt; 192.168.16.1
</code></pre>
<p>or</p>
<pre><code>docker inspect &lt;POSTGRES_ID&gt;| grep IPAddress  &gt;&gt;&gt; 192.168.16.3
</code></pre>
<p>I get the IP/Gateway of the postgres container , and then in <strong>PGADMIN</strong> :</p>
<p><a href=""https://i.stack.imgur.com/XCe8r.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XCe8r.gif"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/MaCZy.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MaCZy.gif"" alt=""enter image description here"" /></a></p>
<p>And the same thing for the IP <code>192.168.16.3</code>.</p>
<p>How can we solve this connection problem ?</p>
"
"1911253","The infamous java.sql.SQLException: No suitable driver found","<postgresql><tomcat><jdbc><geoserver>","66849595","No suitable JDBC driver found for docker postgres container on Windows","<java><postgresql><docker><jdbc>","<p>I'm trying to add a database-enabled JSP to an existing Tomcat 5.5 application (GeoServer 2.0.0, if that helps).</p>

<p>The app itself talks to Postgres just fine, so I know that the database is up, user can access it, all that good stuff. What I'm trying to do is a database query in a JSP that I've added. I've used the config example in the <a href=""http://tomcat.apache.org/tomcat-5.5-doc/jndi-datasource-examples-howto.html"" rel=""noreferrer""> Tomcat datasource example</a> pretty much out of the box. The requisite taglibs are in the right place -- no errors occur if I just have the taglib refs, so it's finding those JARs. The postgres jdbc driver, postgresql-8.4.701.jdbc3.jar is in $CATALINA_HOME/common/lib.</p>

<p>Here's the top of the JSP:</p>

<pre><code>&lt;%@ taglib uri=""http://java.sun.com/jsp/jstl/sql"" prefix=""sql"" %&gt;
&lt;%@ taglib uri=""http://java.sun.com/jsp/jstl/core"" prefix=""c"" %&gt;

&lt;sql:query var=""rs"" dataSource=""jdbc/mmas""&gt;
  select current_validstart as ValidTime from runoff_forecast_valid_time
&lt;/sql:query&gt;
</code></pre>

<p>The relevant section from $CATALINA_HOME/conf/server.xml, inside the <code>&lt;Host&gt;</code> which is in turn within <code>&lt;Engine&gt;</code>:</p>

<pre><code>&lt;Context path=""/gs2"" allowLinking=""true""&gt;
  &lt;Resource name=""jdbc/mmas"" type=""javax.sql.Datasource""
      auth=""Container"" driverClassName=""org.postgresql.Driver""
      maxActive=""100"" maxIdle=""30"" maxWait=""10000""
      username=""mmas"" password=""very_secure_yess_precious!""
      url=""jdbc:postgresql//localhost:5432/mmas"" /&gt;
&lt;/Context&gt;
</code></pre>

<p>These lines are the last in the  tag in webapps/gs2/WEB-INF/web.xml:</p>

<pre><code>&lt;resource-ref&gt;
  &lt;description&gt;
     The database resource for the MMAS PostGIS database
  &lt;/description&gt;
  &lt;res-ref-name&gt;
     jdbc/mmas
  &lt;/res-ref-name&gt;
  &lt;res-type&gt;
     javax.sql.DataSource
  &lt;/res-type&gt;
  &lt;res-auth&gt;
     Container
  &lt;/res-auth&gt;
&lt;/resource-ref&gt;
</code></pre>

<p>Finally, the exception:</p>

<pre><code>   exception
    org.apache.jasper.JasperException: Unable to get connection, DataSource invalid: ""java.sql.SQLException: No suitable driver""
    [...wads of ensuing goo elided]
</code></pre>
","<p>I'm trying to connect to Postgres using JDBC.
The database is running in the Docker container.
I am using Windows Docker Desktop (Windows 10 pro) with WSL2.
I launch the Docker container using docker-compose.
Here is the content of docker-compose:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.8&quot;
services:
  db:
    image: postgres:13.2
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: yoi
    ports:
      - &quot;5432:5432&quot;
    volumes:
      - &quot;./postgres:/var/lib/postgresql/data&quot;
  web:
    image: openjdk:15
    depends_on:
      - db
    ports:
      - &quot;4567:4567&quot;
    volumes:
      - &quot;.:/share/yoi&quot;
    working_dir: /share/yoi
    command: ./gradlew run

</code></pre>
<p>As you can see, it's supposed to run the database first, and then run the java application second.
Here is the content of the java application:</p>
<pre class=""lang-java prettyprint-override""><code>package app;

import java.sql.DriverManager;
import java.sql.SQLException;

public final class Main {

    public static void main(final String... args) {
        System.out.println(&quot;START&quot;);
        try {
            DriverManager.getConnection(
                &quot;jdbc:postgresql://db:5432/yoi&quot;,
                &quot;postgres&quot;,
                &quot;postgres&quot;
            );
        } catch (final SQLException ex) {
            ex.printStackTrace();
        }
        System.out.println(&quot;END&quot;);
    }

}

</code></pre>
<p>I ran the command <code>docker-compose up</code> and got this result:</p>
<pre><code>$ docker-compose up
Creating network &quot;javatest_default&quot; with the default driver
Creating javatest_db_1 ... done
Creating javatest_web_1 ... done
Attaching to javatest_db_1, javatest_web_1
web_1  | Downloading https://services.gradle.org/distributions/gradle-6.7-bin.zip
db_1   |
db_1   | PostgreSQL Database directory appears to contain a database; Skipping initialization
db_1   |
db_1   | 2021-03-29 06:09:21.766 UTC [1] LOG:  starting PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
db_1   | 2021-03-29 06:09:21.766 UTC [1] LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 5432
db_1   | 2021-03-29 06:09:21.766 UTC [1] LOG:  listening on IPv6 address &quot;::&quot;, port 5432
db_1   | 2021-03-29 06:09:21.776 UTC [1] LOG:  listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
db_1   | 2021-03-29 06:09:21.809 UTC [27] LOG:  database system was shut down at 2021-03-29 06:08:02 UTC
db_1   | 2021-03-29 06:09:21.835 UTC [1] LOG:  database system is ready to accept connections
web_1  | .........10%..........20%..........30%..........40%..........50%.........60%..........70%..........80%..........90%..........100%
web_1  |
web_1  | Welcome to Gradle 6.7!
web_1  |
web_1  | Here are the highlights of this release:
web_1  |  - File system watching is ready for production use
web_1  |  - Declare the version of Java your build requires
web_1  |  - Java 15 support
web_1  |
web_1  | For more details see https://docs.gradle.org/6.7/release-notes.html
web_1  |
web_1  | Starting a Gradle Daemon (subsequent builds will be faster)
web_1  | &gt; Task :compileJava
web_1  | &gt; Task :processResources UP-TO-DATE
web_1  | &gt; Task :classes
web_1  |
web_1  | &gt; Task :run
web_1  | START
web_1  | java.sql.SQLException: No suitable driver found for jdbc:postgresql://db:5432/yoi
web_1  |        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:702)
web_1  |        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:228)
web_1  |        at app.Main.main(Main.java:11)
web_1  | END
web_1  |
web_1  | Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.
web_1  | Use '--warning-mode all' to show the individual deprecation warnings.
web_1  | See https://docs.gradle.org/6.7/userguide/command_line_interface.html#sec:command_line_warnings
web_1  |
web_1  | BUILD SUCCESSFUL in 27s
web_1  | 3 actionable tasks: 2 executed, 1 up-to-date
javatest_web_1 exited with code 0

</code></pre>
<p>I also tried <code>jdbc:postgresql://localhost:5432/yoi</code>, but the result was the same.</p>
<p>Does anyone have any idea about the cause and resolution?</p>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","65660287","Use features from different base dockers","<docker>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>Imagine that I want to build and run an application in docker. This application requires 2 things:</p>
<ol>
<li>ROS</li>
<li>NVIDIA</li>
</ol>
<p>Docker containers already exist that have the features that I need:</p>
<ol>
<li><code>ros:noetic-ros-base-focal</code></li>
<li><code>nvidia/opengl:1.0-glvnd-runtime-ubuntu20.04</code></li>
</ol>
<p>I want everything that both these dockers install using <code>apt-get</code>, is it possible without rewriting one of them to use the base of another?</p>
"
"2382764","Escaping in makefile","<makefile><g++><escaping><gnu-make>","54082698","In Makefile use environment variable defined in Docker container to run a command","<shell><docker><makefile>","<p>I'm trying to do this in a makefile and it fails horribly:</p>

<pre><code>M_ARCH := $(shell g++ -dumpmachine | awk '{split($1,a,""-"");print a[1]}')
</code></pre>

<p>do you know why? I guess it has to do with escaping, but what and where?</p>
","<p>I am trying to run a command from a docker-compose service, and I want to to use the environment variables defined in my container like this:</p>

<p>In my Makefile:</p>

<pre><code>sqlcmd: 
    docker-compose run sqlcmd sqlcmd -S sqlserver -U ""$SQL_HOST"" -P ""$SQL_PASSWORD""
</code></pre>

<p>Then I run <code>make sqlcmd</code>.</p>

<p>But it's printing out truncated words. What I want it to do is use the environment variables <code>$SQL_HOST</code> and <code>$SQL_PASSWORD</code>, which are defined in the container.</p>

<p>How do I get this to escape properly?</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","67004579","What is the purpose of the EXPOSE command in a Dockerfile?","<docker><dockerfile>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I am just getting started with Docker and I've been trying to read through some other people's Dockerfiles and I occasionally see the command <code>EXPOSE $PORT</code>. Now I noticed that some people add it and some people don't, some tutorials tell you to add that command while others don't even mention it.</p>
<p>I looked up the command in the <a href=""https://docs.docker.com/engine/reference/builder/#expose"" rel=""nofollow noreferrer"">Dockerfile reference</a> for the command and it almost sounds like it's there to just tell whoever is reading the file that this is the intended port that the container will be listening on?</p>
<p>In that case, does the value of the port you expose have any effect on what happens when the container runs?</p>
"
"23071214","Use environment variables in CMD","<environment-variables><docker>","60828973","docker run python script in CMD with environment variables","<python><python-2.7><docker><environment-variables><dockerfile>","<p>Can I use environment variables in my CMD stanza in a Dockerfile?</p>

<p>I want to do something like this:</p>

<pre><code>CMD [""myserver"", ""--arg=$ARG"", ""--memcache=$MEMCACHE_11211_TCP_ADDR:$MEMCACHE_11211_TCP_PORT""]
</code></pre>

<p>Where $MEMCACHE_11211_TCP_* would be set automatically by the inclusion of the --link parameter of my <code>docker run</code> command.  And $ARG would be configurable by the user at runtime, maybe by the ""-e"" parameter?</p>

<p>This doesn't seem to be working for me, it seems to be literally passing through the string ""$ARG"" for example.</p>
","<p>I'm trying to run a simple Dockerfile that runs a python script and takes some environment variable as an argument (using argparser):</p>

<pre><code>FROM python:2.7
COPY . /app
WORKDIR /app
RUN pip install argparse
ENV POOL ""pool_argument""
CMD [""python"", ""script.py"", ""--pool"", ""${POOL}""]
</code></pre>

<p>and my python script <code>script.py</code>:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--pool', required=True)
known_args, unknown_args = parser.parse_known_args()
print(""Args: {}"".format(known_args))
</code></pre>

<p>after building and running, I'm getting:</p>

<pre><code>Args: Namespace(pool='${POOL}')
</code></pre>

<p>I tried many variations of this, but none seemed to work.</p>
"
"29663459","Python app does not print anything when running detached in docker","<python><docker><dockerfile>","65435090","When the docker startup parameter is -d, the flask application can print on the docker console, but not when the startup parameter is -it","<python><docker><flask><dockerfile>","<p>I have a Python (2.7) app which is started in my dockerfile:</p>

<pre><code>CMD [""python"",""main.py""]
</code></pre>

<p><em>main.py</em> prints some strings when it is started and goes into a loop afterwards:</p>

<pre><code>print ""App started""
while True:
    time.sleep(1)
</code></pre>

<p>As long as I start the container with the -it flag, everything works as expected:</p>

<pre><code>$ docker run --name=myapp -it myappimage
&gt; App started
</code></pre>

<p>And I can see the same output via logs later:</p>

<pre><code>$ docker logs myapp
&gt; App started
</code></pre>

<p>If I try to run the same container with the -d flag, the container seems to start normally, but I can't see any output:</p>

<pre><code>$ docker run --name=myapp -d myappimage
&gt; b82db1120fee5f92c80000f30f6bdc84e068bafa32738ab7adb47e641b19b4d1
$ docker logs myapp
$ (empty)
</code></pre>

<p>But the container still seems to run;</p>

<pre><code>$ docker ps
Container Status ...
myapp     up 4 minutes ... 
</code></pre>

<p>Attach does not display anything either:</p>

<pre><code>$ docker attach --sig-proxy=false myapp
(working, no output)
</code></pre>

<p>Any ideas whats going wrong? Does ""print"" behave differently when ran in background?</p>

<p>Docker version:</p>

<pre><code>Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.2
Git commit (client): a8a31ef
OS/Arch (client): linux/arm
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.2
Git commit (server): a8a31ef
</code></pre>
","<p>I have a flask application that is very simple, without any operation, the code is as follows, just for testing:</p>
<pre><code>from flask import Flask
app1 = Flask(__name__)
@app1.route('/', methods=['POST'])
def send():
    print(1111111111111111)
    a = &quot;ok&quot;
    print(1111111111111111)
    return a
app1.run(debug=False,host='0.0.0.0',port=8060)
</code></pre>
<p>Then I built him into a image through <code>Dockerfile</code>:</p>
<pre><code>From python:3.6.12
COPY ./a.py /usr/
RUN pip3 install -i https://mirrors.aliyun.com/pypi/simple/ flask
CMD python3 /usr/a.py
</code></pre>
<p>My understanding is that when I pass <code>docker run -p12345:8060 -d image_name</code>, Then I access the interface through the following simple code, I think the interface should output two lines of &quot;111111111111&quot; logs, but it is not</p>
<pre><code>import requests

url=&quot;http://127.0.0.1:12345&quot;
data = {
    &quot;server_id&quot;: 1,
}
header = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;charset&quot;: &quot;utf-8&quot;
}
res = requests.post(url=url,json=data,headers=header)
print(res.text)
</code></pre>
<p><a href=""https://i.stack.imgur.com/m8QHG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m8QHG.png"" alt=""enter image description here"" /></a></p>
<p>We can see in the picture when the startup parameter is <code>-it</code>, there is print, but when the parameter is <code>-d</code>, I get the log through docker logs, but I don't see print. The CMD of my image is to start this python code, so the print in the code should be output on the docker console, so no matter I start docker through <code>-it</code> or <code>-d</code>, the docker logs command can get the output of print.</p>
<p>Isn't it?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66386178","how to enable containers from docker-compose ""bridge"" network to reach out to service running on Docker host?","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a <code>docker-compose.yml</code> file, that defines three services, using shared network. Network is defined only by name. <code>docker network list</code> shows that it's a <code>bridge</code> network. <code>docker inspect network</code> gives the IP addresses (<code>172.23.0.x</code>, with <code>172.23.0.1</code> defined as a gateway). My understanding is that this <code>gateway</code> is my Docker host (a Windows 10 machine).</p>
<p>When I execute <code>wget http://containerA-ip:PORT//</code> from <code>bash</code> running in containerB, I get response alright.</p>
<p>When I execute <code>wget http://gateway-ip:PORT//</code> from <code>bash</code> running in containerB, I get response alright, too. (port PORT is published as PORT:PORT, so I assume that's the reason it works).</p>
<p><strong>BUT</strong>. If I try to <code>wget http://gateway-ip:OTHER-PORT</code>, where OTHER-PORT is a port number of some process that is listening on my Windows 10 machine, I get <code>Connection refused</code>.</p>
<p>I wonder, is it FW configuration on my Windows machine, that's preventing this connection, or is it some Docker configuration/implementation detail that's preventing this kind of connection.</p>
<p>I was unable to find an answer to &quot;how to connect to Docker host services from within a container using <code>bridge</code> network&quot; anywhere in the documentation..</p>
<p>Thanks to anyone, who could assist.</p>
<p>UPD: I've been suggested to check which interfaces this service is listening on, here's the output of <code>netstat -na | grep 8088</code>:</p>
<pre><code>TCP    0.0.0.0:8088           0.0.0.0:0              LISTENING                                                                                                                      
TCP    [::]:8088              [::]:0                 LISTENING          
</code></pre>
<p>to verify it's the case, I've run <code>curl --noproxy &quot;*&quot; -X POST &quot;http://192.168.0.13:8088/&quot;</code> and it produced expected response</p>
"
"24309526","How to change the docker image installation directory?","<docker>","60296120","change location of image which is pulled in docker-compose","<docker><docker-compose>","<p>From what I can tell, docker images are installed to <code>/var/lib/docker</code> as they are pulled. Is there a way to change this location, such as to a mounted volume like <code>/mnt</code>?</p>
","<p>I have a docker-compose.yml which is pulling an image. When i am running the docker-compose file. It throws no space error. How can i specify the path where the image is downloaded. it getting downloaded it root which just has 15gb. I want it to be downloaded in /home because it has a lot of space left.</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60423847","docker container can not connect to local postgres","<postgresql><docker><flask><docker-run>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>Can't connect to Postgres from the docker container.
I do not want user docker-compose and create a Postgres container, already got Postgres app running. I think it is a bad idea to container postgres and better use system.</p>

<p>OSX, Postgres10, Flask</p>

<p>I already made</p>

<p><strong>postgresql.conf</strong></p>

<pre><code>listen_addresses = '*'  
port = 5432
</code></pre>

<p><strong>pg_hba.conf</strong></p>

<pre><code>host    all             all             0.0.0.0/0               trust
</code></pre>

<p><em>I Used trust for any result, but no effect.</em></p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM python:3.8-alpine
RUN apk update \
    &amp;&amp; apk add --virtual build-deps gcc musl-dev \
    &amp;&amp; apk add python3-dev \
    &amp;&amp; apk add postgresql-dev \
    &amp;&amp; apk add jpeg-dev zlib-dev libjpeg \
    &amp;&amp; pip install --upgrade pip
ENV PYTHONUNBUFFERED 1 \
    &amp;&amp; FLASK_APP app.py
#EXPOSE 5000 5432
WORKDIR /app
ADD . /app
RUN pip install -r requirements.txt
CMD [""./bin/run.sh""]
</code></pre>

<p><strong>./bin/run.sh</strong></p>

<pre><code>#!/bin/sh
#python run.py
source venv/bin/activate
set -e
flask db upgrade
exec gunicorn -b --workers 4 --access-logfile --error-logfile run:app :5000
</code></pre>

<p><strong>The ""docker run"" command I try to use:</strong></p>

<pre><code>docker run --rm -e SQLALCHEMY_DATABASE_URI=postgresql://postgres:postgres@0.0.0.0:5432/my_db --net=host -p 5000:5000 my_container:v0.1
</code></pre>

<p>the command leads to error</p>

<pre><code>...
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host ""0.0.0.0"" and accepting
        TCP/IP connections on port 5432?
</code></pre>

<p><strong>command connect me to Postgres</strong></p>

<pre><code>psql -U postgres -h 0.0.0.0
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","60531220","Expose laptop localhost port to Docker container","<docker><docker-compose><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a RESTful application that takes a callback URL as an argument running on port 8000. It does some work and calls the callback URL to make sure that it should continue work and calls if anything goes wrong. In my tests, I just spin up an HTTP server on localhost port 8009 to wait for responses.</p>

<p>When I run the application directly, it works fine because the localhost is the same. However, when I run the application locally in a container, it obviously doesn't work because of the network isolation of the container. Here's part of my <code>docker-compose.yml</code>:</p>

<pre><code>version: '3'
services:
  app:
    build: .
    ports:
      - ""8000:8000""
</code></pre>

<p>I tried to add <code>- ""8009:8009""</code> under <code>ports</code>, but that only goes one direction. I can call port 8000 to call the API, but I can't figure out how to allow the container to call my laptop's <code>http://localhost:8009</code>.</p>

<p>Any idea how to allow the container to call out to another port on my laptop, or if there's a better way to test the callback functionality? I could run the tests inside the container, but I like being able to run them separately.</p>
"
"28583665","How to use docker logs","<docker>","67001652","see logs in docker container with .sh script in detach mode","<docker><docker-compose>","<p>The question may be a bit of newbie.
I run <code>docker exec -it mycontainer bash</code> to enter into a daemon container(postgresSQL ),
and <code>echo</code> something.
now I exit it , and use <code>docker logs mycontainer</code> so as to see my echos.</p>

<p>According to</p>

<blockquote>
  <p>The docker logs command batch-retrieves logs present at the time of execution.
  The docker logs --follow command will continue streaming the new output from the container's STDOUT and STDERR.</p>
</blockquote>

<p>The <code>docker logs</code> listen <code>STDOUT</code> of the container, why I don't see my string just <em>echoed</em> inside it?</p>
","<p>I started my container with <code>.sh</code> script due this command:</p>
<pre><code>docker-compose exec -d eos-indexer-node ./etc/runnodeos.sh
</code></pre>
<p>but when i want to see logs with:</p>
<pre><code>docker logs &lt;container&gt;

docker-compose logs &lt;service&gt;
</code></pre>
<p>everything is empty.</p>
<p>How to fix it ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66550844","MySQL connection error with native database","<docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have containerized a simple PHP app. It needs to connect to the host's database (localhost) and do some operations. However, while running the app, I have the following error:
<code>Fatal error: Uncaught PDOException: SQLSTATE[HY000] [2002] No such file or directory ...</code>
My docker-compose file looks like this:</p>
<pre><code>version: &quot;3.9&quot;
services:
  app:
    build: .
    stdin_open: true
    tty: true
    network_mode: &quot;host&quot;
    restart: on-failure
</code></pre>
<p>Looks like dockerized app can't connect with localhost database. I added <code>network_mode: &quot;host&quot;</code>, but it didn't help.</p>
<p>I use Docker Desktop and Mac OS.</p>
"
"30740828","Commit data in a mysql container","<mysql><linux><database><docker><virtualization>","65482012","Docker commit does not actually commit","<mysql><docker><docker-compose><joomla><commit>","<p>I created a mysql container using the officially supported <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql image</a>. I ran the image mounting a folder that contains a sql dump, then I created a new database in the container and imported the .sql dump in it:</p>

<pre><code>sudo docker run --name mysql-psat1 -v /opt/Projets/P1/sqldumps:/mnt -e MYSQL_ROOT_PASSWORD=secret -d mysql:latest
sudo docker exec -it mysql-psat1 bash
&gt; mysql -uroot -psecret -e 'create database liferay_psat1;'
&gt; mysql -uroot -psecret liferay_psat1 &lt; /mnt/liferay_sql_dump.sql
</code></pre>

<p>Then I listed the running containers to get that container's id:</p>

<pre><code>sudo docker ps -a
</code></pre>

<p>Then, I commited the container (with the imported sql) as a new container image</p>

<pre><code>sudo docker commit -m ""Imported liferay sql dump"" &lt;id-of-the-container&gt; jihedamine/mysql-psat1:v1
</code></pre>

<p>However, when if I start a container using that new image, the mysql database doesn't contain the newly created database liferay_psat1.</p>

<pre><code>sudo docker run -ti jihedamine/mysql-psat1:v1 bash
&gt; mysql -uroot -psecret
# show databases;
</code></pre>

<p>What am I doing wrong?</p>

<p>Thanks for your help!</p>
","<p>(I am new to docker so patience please)
NOTE: I DO NOT USE VOLUMES so the answers in <a href=""https://stackoverflow.com/questions/30740828/commit-data-in-a-mysql-container"">this question</a> do not apply</p>
<ol>
<li>I am creating two containers from two docker images (jomla+apache+php and mysql) using the docker-compose file listed at the end. Everything works fine.</li>
<li>I can connect to the Joomla installation page I go through the initial configuration, connect to the database, all is well again. (I can browse to the home &amp; administration pages now). Even the installation folder is removed just fine (mandatory step in joomla). I can see the new files and new database created/modified fine within the containers.</li>
<li>I now commit the two running containers to two new images:  <em>docker commit CONTAINER_ID new_image_name</em></li>
<li>The new images are created fine</li>
<li>I remove the containers with <em>docker-compose down</em></li>
<li>I modify the docker-compose.yml file to use the newly committed images respectively for joomla and mysql</li>
<li>I issue <em>docker-compose up -d</em></li>
<li>And here comes the frustration: when I browse to the page, I am back to the Joomla installation page, the config files are gone, the database is empty and installation folder reappeared. In other words, nothing was committed in either image.
I even removed the older images to exclude any possibility of accidentally using them.</li>
</ol>
<p>Why is docker not committing these changes?</p>
<p>If I do manual changes (like <em>docker cp ...</em> or <em>docker exec ...</em> or even inside the container) those stick and are committed just fine.
I spent two full days on this, any help is appreciated.</p>
<p>docker-compose.yml:</p>
<pre><code>version: '3.8'

networks:
  frontend:
  backend:

services:

  mysql_db:
    container_name: mysql
    image: mysql_me:latest
    command: mysqld --innodb-buffer-pool-size=20M
    restart: on-failure
    environment:
      MYSQL_DATABASE: 'joomla'
      MYSQL_USER: 'joomla_user'
      MYSQL_PASSWORD: 'JPassword'
      MYSQL_ROOT_PASSWORD: 'RPassword'
      MYSQL_ROOT_HOST: '%'
    ports:
      - '3306:3306'
    expose:
      - '3306'
    networks:
      - backend

  joomla:
    container_name: joomla
    image: joomla_me:latest
    restart: always
    ports:
      - &quot;443:443&quot;
      - &quot;8080:80&quot;
    environment:
      JOOMLA_DB_HOST: 'mysql_db:3306'
      JOOMLA_DB_USER: 'joomla_user'
      JOOMLA_DB_PASSWORD: 'JPassword'
      JOOMLA_DB_NAME: 'joomla'
    links:
      - mysql_db:3306
    networks:
      - frontend
      - backend
    depends_on:
      - mysql_db
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66250872","Connecion to mysql on host machine from a docker container not possible","<mysql><docker><docker-compose>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am running a SQL Server on my host machine (not inside a docker container). I bound it to 127.0.0.1 and all applications can access it, which are running native on the host.</p>
<p>Now I have a nodejs app inside a docker container which needs to connect to the mysql Server, too.</p>
<p><code>sudo docker inspect</code> says my gateway is <code>172.18.0.1</code></p>
<p>I tried to use <code>172.18.0.1</code> or <code>host.docker.internal</code> as hostname inside the container to connect. But I get alway a host not reachable error. Even if I bind mysql to 0.0.0.0 I can't connect.</p>
<p>I am using docker on a linux machine:</p>
<pre><code>Server: Docker Engine - Community
 Engine:
  Version:          20.10.3
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       46229ca
  Built:            Fri Jan 29 14:31:32 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.4.3
  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b
 runc:
  Version:          1.0.0-rc92
  GitCommit:        ff819c7e9184c13b7c2607fe6c30ae19403a7aff
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
docker-compose version 1.25.0, build unknown
docker-py version: 4.1.0
CPython version: 3.8.5
OpenSSL version: OpenSSL 1.1.1f  31 Mar 2020
</code></pre>
<p>That is my docker-compose.yml:</p>
<pre><code>version: &quot;3&quot;
services:
    mailadmin-backend:
            image: mailadmin-backend
            ports:
                    - &quot;10600:8080&quot;
</code></pre>
"
"31324981","How to access host port from docker container","<docker><docker-container>","65648295","Connect to a local PostgreSQL database from a docker-compose service","<python><postgresql><docker><docker-compose>","<p>I have a docker container running jenkins. As part of the build process, I need to access a web server that is run locally on the host machine. Is there a way the host web server (which can be configured to run on a port) can be exposed to the jenkins container?</p>

<p>EDIT: I'm running docker natively on a Linux machine.</p>

<p>UPDATE:</p>

<p>In addition to @larsks answer below, to get the IP address of the Host IP from the host machine, I do the following:</p>

<pre><code>ip addr show docker0 | grep -Po 'inet \K[\d.]+'
</code></pre>
","<h3>Context</h3>
<p>I need to connect from inside a compose service to a PostgreSQL (10) database which is on my local host machine (Ubuntu 18.04).</p>
<h3>Description of the errors</h3>
<p>I have set <code>service &gt; app &gt; build &gt;</code> to <code>network: host</code> in my compose file:</p>
<pre><code>version: '3.8'
  app:
    image: myservice:0.0.1
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
      network: host
</code></pre>
<p>The Dockerfile is build <code>FROM python:3.9.1-slim-buster</code>.</p>
<p>Then, from inside my compose service (<code>docker exec -it composefilefolder_app_1 bash</code>) I try to connect to a PostgreSQL database on the host machine with <code>psycopg2</code> (after having started <code>python</code>):</p>
<pre><code>import psycopg2 

DBPARAMS = {
    'user': 'postgres',
    'password': 'password',
    'host': 'localhost',
    'port': '5432',
    'dbname': 'mydatabase'
}

conn = psycopg2.connect(**DBPARAMS)
</code></pre>
<p>but this returns:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
    Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
    TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
    Is the server running on host &quot;localhost&quot; (::1) and accepting
    TCP/IP connections on port 5432?
</code></pre>
<p>Then I tried to use my host machine IP (given by <code>ifconfig</code>):</p>
<pre><code>import psycopg2 

DBPARAMS = {
    'user': 'postgres',
    'password': 'password',
    'host': '192.168.x.1',
    'port': '5432',
    'dbname': 'mydatabase'
}

conn = psycopg2.connect(**DBPARAMS)
</code></pre>
<p>but now I got a time-out after one minute:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection timed out
    Is the server running on host &quot;192.168.x.1&quot; and accepting
    TCP/IP connections on port 5432?
</code></pre>
<p>All other parameters are correct as I can connect to this database by running the same command from the host machine directly.</p>
<h3>Question</h3>
<p>How can I connect to my host machine PostgreSQL database from my compose service?</p>
<h3>What else I've tried</h3>
<ol>
<li><a href=""https://stackoverflow.com/questions/51313852/docker-compose-cannot-connect-to-database"">This</a> thread seemed promising but it doesn't work either.</li>
<li>And <a href=""https://stackoverflow.com/questions/48350843/how-to-connect-from-docker-compose-to-host-postgresql"">this one</a> is for Mac users.</li>
<li>I also tried to set <code>list_adressess='*'</code> in my local machine <code>postgres.conf</code> file AND to set <code>host all all 0.0.0.0/0 md5</code> into the <code>pg_hba.conf</code> file without any changes. But if I can avoid to tweak my local PostgreSQL config that would be great.</li>
<li><a href=""https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container"">This</a> thread also did not help as it makes use of my local machine IP, for which the connection is timing-out as I said.</li>
</ol>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66271108","Docker - Connect containers to local database","<java><docker><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am in process to dockerize my local server, on that have databases, application servers and all additional configuration to deploy my applications. Currently, i have created 3 docker containers that run application servers like Tomcat or WildFly, and exposed <code>80</code>, <code>8130</code> and <code>8080</code> ports this deployed applications are in Java and read and write data from/to my local database. On the <code>persistence.xml</code> file for each app, i have configured the access via <code>localhost</code> in my actual configuration that works ok without docker.</p>
<p>I want to connect the apps on the containers to database that correspond on the host machine, how can i do that?</p>
<p>I have created the containers like this:</p>
<pre><code>docker run -d --name app_server1 -p 80:80 wildfly_website:1.0
docker run -d --name app_server2 -p 8130:8130 geoserver:1.0
</code></pre>
<p>This image illustrates what i want to do
<a href=""https://i.stack.imgur.com/B2Yhp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B2Yhp.png"" alt=""enter image description here"" /></a></p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","65553039","Force Docker Container to wait until Postgresql container is ready","<javascript><postgresql><docker><docker-compose><postgresql-9.1>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>Consider the Dockerfile :</p>
<p><strong>Dockerfile :</strong></p>
<pre><code>FROM node:14
COPY . /app
RUN npm i
RUN npm run build_database_migrations &lt;&lt;&lt;&lt;====== This is the problematic command

... more commands 
</code></pre>
<p>Which uses <code>sequelize-cli</code> :</p>
<p><strong>In package.json :</strong></p>
<pre><code>&quot;scripts&quot;: {
   &quot;build_database_migrations&quot;: &quot;sequelize db:migrate&quot;, 
  }
</code></pre>
<p>And the Docker-compose :</p>
<p><strong>Docker-compose.yaml</strong></p>
<pre><code>version: &quot;3&quot;
services:
  
  postgresql:
    image: postgres:latest
    environment:
      POSTGRES_USER: user1
      POSTGRES_PASSWORD: pass1
      POSTGRES_DB: somedb      
    ports:
      - &quot;5432:5432&quot;
 
  app:    
    build:
      context: ./employee
      dockerfile: ./Dockerfile
    ports:
      - &quot;9000:9000&quot;
    restart: on-failure
    depends_on:    
      - postgresql      
    links:
      - postgresql
</code></pre>
<p>Whenever I run the <code>Docker-compose up</code> the <code>app</code> never waits for <code>postgresql</code> to be up and running ,
which results in building the <code>app</code> container and running the command <code>RUN npm run build_database_migrations</code> before the Postgreqsql Container is listening on port <code>5432</code>.</p>
<p>How can we force the <code>app</code> Container to wait until <code>Postreqsql</code> container is ready and listening on port <code>5432</code> , so it would be able to run the migrations on a live Postgresql Container ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66976384","Accessing a service host from inside a container","<docker><docker-compose><host><spring-cloud-config>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a Spring Boot config server running in localhost on port 8888. I'd like to access this service from inside a docker container. I cannot put this service inside the same docker-compose and in the same network.</p>
<p>Service: http://localhost:8888/cpo-executor/dev <br/>
Using my ip: <a href=""http://192.168.0.6:8888/cpo-executor/dev"" rel=""nofollow noreferrer"">http://192.168.0.6:8888/cpo-executor/dev</a></p>
<p>I've checked this address on browser and it's working. When I try to access from inside a docker container I got an error:</p>
<pre><code>docker exec -it 7febe846f2ea /bin/bash
curl http://192.168.0.6:8888/cpo-executor/dev
</code></pre>
<p>Error:</p>
<pre><code>curl: (7) Failed to connect to 192.168.0.6 port 8888: Connection timed out
</code></pre>
<p>I've tried to start my containers putting &quot;network_mode: host&quot; in docker-compose but I ended up falling into another error, from one container not communicating with another.</p>
<pre><code>Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure
The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.

Caused by: java.net.UnknownHostException: mysql
</code></pre>
<p>How can I access a service from host from inside a docker container?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","65576744","Why the application doesn't start for the first time, but starts from the second time?","<spring-boot><docker>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I am new to Docker and I'm trying to dockerize my application. I'm using MySQL and Spring Boot.<br />
When I use <code>docker-compose up</code>, a few exception occurs:</p>
<pre class=""lang-sh prettyprint-override""><code>todoapp-container_1  | The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
todoapp-container_1  |  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_111]
todoapp-container_1  |  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_111]
todoapp-container_1  |  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_111]
</code></pre>
<p>and a little bit down:</p>
<pre class=""lang-sh prettyprint-override""><code>todoapp-container_1  | Caused by: java.net.ConnectException: Connection refused (Connection refused)
todoapp-container_1  |  at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_111]
todoapp-container_1  |  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_111]
todoapp-container_1  |  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_111]
</code></pre>
<p>Here is my docker-compose:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3'
services:
    mysql-container:
        image: mysql:8
        environment:
            - MYSQL_ROOT_PASSWORD=password
            - MYSQL_DATABASE=todo
            - MYSQL_PASSWORD=todo
            - MYSQL_USER=todo
    todoapp-container:
        image: todoapp-service
        ports:
            - 8080:8080
        build:
            context: .
            dockerfile: Dockerfile
        depends_on:
            - mysql-container
</code></pre>
<p>and my application.properties:</p>
<pre class=""lang-json prettyprint-override""><code>#SERVER
server.port=8080
#MYSQL
spring.datasource.url=jdbc:mysql://mysql-container:3306/todo?createDatabaseIfNotExist=true
spring.datasource.username=todo
spring.datasource.password=todo
spring.jpa.generate-ddl=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect
#JACKSON SERIALIZATION FOR NULL FIELDS
spring.jackson.default-property-inclusion=non_null
</code></pre>
<p>The thing is, if I run <code>docker-compose up</code> for the second time, it works flawless without any exceptions and can use the postman for requests. I assume there is a problem at the moment when the containers are starting, I think I have to make the todoapp-container to wait until the mysql-container starts but I am not really sure about it.</p>
"
"31448821","How to write data to host file system from Docker container","<docker>","66397825","upload file with java to root of linux from docker container","<java><docker><api><file>","<p>I have a Docker container which is running some code and creating some HTML reports. I want these reports to be published into a specific directory on the host machine, i.e. at <code>/usr/share/nginx/reports</code></p>

<p>The way I have gone about doing this is to mount this host directory as a data volume, i.e. <code>docker run -v /usr/share/nginx/reports --name my-container com.containers/my-container</code></p>

<p>However, when I ssh into the host machine, and check the contents of the directory <code>/usr/share/nginx/reports</code>, I don't see any of the report data there.</p>

<p>Am I doing something wrong?</p>

<p>The host machine is an Ubuntu server, and the Docker container is also Ubuntu, no boot2docker weirdness going on here.</p>
","<p>i want to save the files that i upload to the java spring boot applicatiob to the root of ubuntu 18.
my application in docker container and start with docker compose.
after upload, my files save to</p>
<p>var/lib/docker/overlay2/jdjdsbe737bdj7837bdjwhs73828ndjdjd......</p>
<p>but i like save to this :</p>
<p>~/attachment/my-files    // without hash names and with my chosen name in my application</p>
<p>my code:</p>
<pre><code>String UPLOADED_FOLDER = &quot;/root/attachments/&quot;;
Files.createDirectories(Paths.get(UPLOADED_FOLDER + FileOwnerType.valueOfIndex(fileOwnerType).getTitle()));
            bytes = file.getBytes();
            Path path = Paths.get(UPLOADED_FOLDER + FileOwnerType.valueOfIndex(fileOwnerType).getTitle() + &quot;/&quot; + fileName + &quot;.&quot; + fileExtension);
            Files.write(path, bytes);
</code></pre>
"
"33913020","Docker remove <none> TAG images","<docker>","65667913","docker image prune not removing intermediate images","<docker>","<pre><code>root@server:~# docker images -a        
REPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
&lt;none&gt;                  &lt;none&gt;              5e2dfc857e73        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              d053e988f23d        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              1d5d4a2d89eb        5 days ago          261.6 MB
&lt;none&gt;                  &lt;none&gt;              ea0d189fdb19        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              26c6175962b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              73d5cec4a0b3        5 days ago          100.5 MB
&lt;none&gt;                  &lt;none&gt;              e19590e1bac1        5 days ago          100.5 MB
</code></pre>

<p>I've tried the following:</p>

<pre><code>docker rmi $(docker images | grep ""^&lt;none&gt;"" | awk ""{print $3}"")
</code></pre>

<p>And the following:</p>

<pre><code>docker rmi $(docker images -f ""dangling=true"" -q)
</code></pre>

<p>Get the following error:</p>

<pre><code>docker: ""rmi"" requires a minimum of 1 argument.
See 'docker rmi --help'.

Usage:  docker rmi [OPTIONS] IMAGE [IMAGE...]

Remove one or more images
</code></pre>
","<p>i have a problem with docker images.
If i run <code>docker info</code> i get this output</p>
<p><a href=""https://i.stack.imgur.com/lyptp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lyptp.png"" alt=""enter image description here"" /></a></p>
<p>saying 22 images are available.
When i run <code>docker images -a</code> indeed 22 images are shown:</p>
<p><a href=""https://i.stack.imgur.com/aX88G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aX88G.png"" alt=""enter image description here"" /></a></p>
<p>All those image tagged with <code>None</code> i suppose that are intermediate images.
I try to run <code>docker image prune</code> but it says</p>
<blockquote>
<p>Total reclaimed space: 0B</p>
</blockquote>
<p>Why prune command isn't working? Thanks</p>
<p>Docker version: 19.03.5.
Host: centos 7</p>
<p>Grazie</p>
"
"25281467","Fatal error: Call to undefined function mysqli_connect()","<php><mysqli>","60415725","Call to undefined function mysqli_connect()","<php><docker><nginx>","<p>For 2 days now I'm trying to solve this, but unfortunately no result. Let me tell you my story about the problem. I've bulid an application on a site, and the application deals with the reviews. But, I'm trying to put it on another site, and I copyed the php files, the sql file from the old site, and moved them to the new site (they are on different FTP servers). When I'm trying to go to the pages from the application, I receive this FATAL ERROR:</p>

<blockquote>
  <p>Fatal error: Call to undefined function mysqli_connect()</p>
</blockquote>

<p>The code that I wrote to connect to the database is this (with hidden credentials):</p>

<pre><code>$con = mysqli_connect("""",""*the_name*"",""*the_pass*"",""*the_database*"");
if (mysqli_connect_errno()) {
    echo ""Failed to connect to MySQL: "" . mysqli_connect_error();
}
</code></pre>

<p>Why do I get the error? It works on the old server, and the code I think it's not the problem, because it works on localhost, and on the new server it doesn't. Can anyone help me?</p>
","<p>In this code must be an error due to empty arguments in the <code>mysqli_connect</code> function. But browser display different error. As I know <code>mysqli_connect</code> function installed by default. Where is the problem or my mistake? How I can fix it?</p>

<p>Dockerfile</p>

<pre><code>FROM php:fpm

# Update system core
RUN apt update -y &amp;&amp; apt upgrade -y

# Start PHP-FPM
CMD [""php-fpm""]
</code></pre>

<p>index.php</p>

<pre><code>&lt;?php mysqli_connect('', '', '', '', '', ''); ?&gt;
</code></pre>

<p>Error in browser:</p>

<pre><code>Fatal error: Uncaught Error: Call to undefined function mysqli_connect() in /var/www/index.php:3 Stack trace: #0 {main} thrown in /var/www/index.php on line 3
</code></pre>
"
"36354423","What is the best way to pass AWS credentials to a Docker container?","<amazon-web-services><docker><docker-compose>","66376115","Pass AWS Credentials FILE to Python Docker (debian based)","<python><amazon-web-services><docker><debian>","<p>I am running docker-container on Amazon EC2. Currently I have added AWS Credentials to Dockerfile. Could you please let me know the best way to do this?</p>
","<p>I know there are several similar questions but none of them have helped me. I want to run a python script (<em>migrations.py</em>) in a Docker container that needs the AWS credentials. The Dockerfile is the following:</p>
<pre><code>FROM python:3.8

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY migrations.py ./
COPY aws_credentials ~/.aws/credentials

CMD [ &quot;python3&quot;, &quot;./migrations.py&quot; ]
</code></pre>
<p>When I do:</p>
<pre><code>docker build -t yoyo . &amp;&amp; docker run yoyo
</code></pre>
<p>It shows the error</p>
<pre><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>All of the files (Dockerfile, requirements.txt, migrations.py and aws_credentials) are in the same place (the root of a folder).</p>
<p>I have tried the copy line like:</p>
<pre><code>COPY aws_credentials $HOME$/.aws/credentials
COPY aws_credentials ./.aws/credentials
</code></pre>
<p>I need AWS to access the credentials using the file credentials. How could I achieve it? Thank you very much</p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","66851327","Docker initialize host mounted volume","<docker><docker-volume>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I want to create a docker image for a software that need a persistent data volume which has to be initialized with some specific files and folders and I think you could reach that be creating a volume in the dockerfile after the creation of the files.</p>
<p>But my problem is, that I want to access the files direct from the host system, but it seems that there is no way to initialize the host folder like the named volume would be.</p>
<p>Does anybody know a way to achieve such an initialization?</p>
"
"35414479","Containerized Node server inaccessible with server.listen(port, '127.0.0.1')","<networking><docker><port>","65452006","""This page isn’t working localhost didn’t send any data."" when navigating to localhost port bound to docker container","<node.js><docker><docker-compose>","<p>I set up a simple Node server in Docker.</p>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:latest
RUN apt-get -y update
ADD example.js .
EXPOSE 1337   
CMD node example.js
</code></pre>
<p><strong>example.js</strong></p>
<pre class=""lang-js prettyprint-override""><code>var http = require('http');
http.createServer(function (req, res) {
  res.writeHead(200, {'Content-Type': 'text/plain'});
  res.end('Hello World\n'+new Date);
}).listen(1337, '127.0.0.1');
console.log('Server running at http://127.0.0.1:1337/');
</code></pre>
<p>Now build the image</p>
<pre><code>$ docker build -t node_server .
</code></pre>
<p>Now run in container</p>
<pre><code>$ docker run -p 1337:1337 -d node_server  
$ 5909e87302ab7520884060437e19ef543ffafc568419c04630abffe6ff731f70
</code></pre>
<p>Verify the container is running and ports are mapped:</p>
<pre><code>$ docker ps  

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
5909e87302ab        node_server         &quot;/bin/sh -c 'node exa&quot;   7 seconds ago       Up 6 seconds        0.0.0.0:1337-&gt;1337/tcp   grave_goldberg
</code></pre>
<p>Now let's attach to the container and verify the server is running inside:</p>
<pre><code>$ docker exec -it 5909e87302ab7520884060437e19ef543ffafc568419c04630abffe6ff731f70 /bin/bash 
</code></pre>
<p>And in the container command line type:</p>
<pre><code>root@5909e87302ab:/# curl http://localhost:1337
Hello World
Mon Feb 15 2016 16:28:38 GMT+0000 (UTC)
</code></pre>
<p>Looks good right?</p>
<p><strong>The problem</strong></p>
<p>When I execute the same curl command on the host (or navigate with my browser to http://localhost:1337) I see nothing.</p>
<p>Any idea why the port mapping between container and host doesn't work?</p>
<p>Things I already tried:</p>
<ul>
<li>Running with the <code>--expose 1337</code> flag</li>
</ul>
","<p>I am running docker desktop for windows version 3.0.0 and am attempting to setup a basic hello world example using docker and docker-compose.  when I run <code>docker-compose up</code> the containers build fine and log the appropriate <code>Server running at...</code> messages.</p>
<p>However when I navigate to <code>localhost:3000</code> I would expect to see 'Hello World' and am instead seeing a browser error reading &quot;This page isn’t working  localhost didn’t send any data.&quot;</p>
<p>when I run the server.js file directly in node, I get &quot;hello world&quot; at <code>localhost:3000</code> as expected.</p>
<p>Ideally I would be able to access all of my docker containers on local host ports 3000-3003.  relevant files are below.</p>
<p>Each of my dockerfiles contain the following</p>
<pre><code>FROM node:14
WORKDIR /usr/src/app
RUN npm install
COPY . .
EXPOSE 3000
CMD [ &quot;node&quot;, &quot;server.js&quot; ]
</code></pre>
<p>Below is the server.js file:</p>
<pre class=""lang-js prettyprint-override""><code>const http = require('http');

const hostname = '127.0.0.1';
const port = 3000;

const server = http.createServer((req, res) =&gt; {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, hostname, () =&gt; {
  console.log(`Server running at http://${hostname}:${port}/`);
});
</code></pre>
<p>My docker-compose file is below:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.9&quot;
services:
  analyze:
    build: services/analyze/
    volumes:
      - ./services/analyze:/analyze
    ports:
      - &quot;3000:3000&quot;
  extract:
    build: services/extract/
    volumes:
      - ./services/extract:/extract
    ports:
      - &quot;3001:3000&quot;
  search:
    build: services/search/
    volumes:
      - ./services/search:/search
    ports:
      - &quot;3002:3000&quot;
  reply:
    build: services/reply/
    volumes:
      - ./services/reply:/reply
    ports:
      - &quot;3003:3000&quot;
</code></pre>
"
"25869428","Classpath resource not found when running as jar","<java><spring-boot>","60533945","Spring boot Resource getFile() is not working","<java><spring><spring-boot><docker>","<p>Having this problem both in Spring Boot 1.1.5 and 1.1.6 - I'm loading a classpath resource using an @Value annotation, which works just fine when I run the application from within STS (3.6.0, Windows). However, when I run a mvn package and then try to run the jar, I get FileNotFound exceptions.</p>

<p>The resource, message.txt, is in src/main/resources. I've inspected the jar and verified that it contains the file ""message.txt"" at the top level (same level as application.properties).</p>

<p>Here's the application:</p>

<pre class=""lang-java prettyprint-override""><code>@Configuration
@ComponentScan
@EnableAutoConfiguration
public class Application implements CommandLineRunner {

    private static final Logger logger = Logger.getLogger(Application.class);

    @Value(""${message.file}"")
    private Resource messageResource;

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Override
    public void run(String... arg0) throws Exception {
        // both of these work when running as Spring boot app from STS, but
        // fail after mvn package, and then running as java -jar
        testResource(new ClassPathResource(""message.txt""));
        testResource(this.messageResource);
    }

    private void testResource(Resource resource) {
        try {
            resource.getFile();
            logger.debug(""Found the resource "" + resource.getFilename());
        } catch (IOException ex) {
            logger.error(ex.toString());
        }
    }
}
</code></pre>

<p>The exception:</p>

<pre><code>c:\Users\glyoder\Documents\workspace-sts-3.5.1.RELEASE\classpath-resource-proble
m\target&gt;java -jar demo-0.0.1-SNAPSHOT.jar

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v1.1.5.RELEASE)

2014-09-16 08:46:34.635  INFO 5976 --- [           main] demo.Application
                  : Starting Application on 8W59XV1 with PID 5976 (C:\Users\glyo
der\Documents\workspace-sts-3.5.1.RELEASE\classpath-resource-problem\target\demo
-0.0.1-SNAPSHOT.jar started by glyoder in c:\Users\glyoder\Documents\workspace-s
ts-3.5.1.RELEASE\classpath-resource-problem\target)
2014-09-16 08:46:34.640 DEBUG 5976 --- [           main] demo.Application
                  : Running with Spring Boot v1.1.5.RELEASE, Spring v4.0.6.RELEA
SE
2014-09-16 08:46:34.681  INFO 5976 --- [           main] s.c.a.AnnotationConfigA
pplicationContext : Refreshing org.springframework.context.annotation.Annotation
ConfigApplicationContext@1c77b086: startup date [Tue Sep 16 08:46:34 EDT 2014];
root of context hierarchy
2014-09-16 08:46:35.196  INFO 5976 --- [           main] o.s.j.e.a.AnnotationMBe
anExporter        : Registering beans for JMX exposure on startup
2014-09-16 08:46:35.210 ERROR 5976 --- [           main] demo.Application
                  : java.io.FileNotFoundException: class path resource [message.
txt] cannot be resolved to absolute file path because it does not reside in the
file system: jar:file:/C:/Users/glyoder/Documents/workspace-sts-3.5.1.RELEASE/cl
asspath-resource-problem/target/demo-0.0.1-SNAPSHOT.jar!/message.txt
2014-09-16 08:46:35.211 ERROR 5976 --- [           main] demo.Application
                  : java.io.FileNotFoundException: class path resource [message.
txt] cannot be resolved to absolute file path because it does not reside in the
file system: jar:file:/C:/Users/glyoder/Documents/workspace-sts-3.5.1.RELEASE/cl
asspath-resource-problem/target/demo-0.0.1-SNAPSHOT.jar!/message.txt
2014-09-16 08:46:35.215  INFO 5976 --- [           main] demo.Application
                  : Started Application in 0.965 seconds (JVM running for 1.435)

2014-09-16 08:46:35.217  INFO 5976 --- [       Thread-2] s.c.a.AnnotationConfigA
pplicationContext : Closing org.springframework.context.annotation.AnnotationCon
figApplicationContext@1c77b086: startup date [Tue Sep 16 08:46:34 EDT 2014]; roo
t of context hierarchy
2014-09-16 08:46:35.218  INFO 5976 --- [       Thread-2] o.s.j.e.a.AnnotationMBe
anExporter        : Unregistering JMX-exposed beans on shutdown
</code></pre>
","<p>In my spring boot project I am trying to load an xml file located in resources folder. If I try loading file with </p>

<pre><code>Resource resource= resourceLoader.getResource(""classpath:"" + filepath);
File tempFile = resource.getFile();
</code></pre>

<p>I get following error in docker container where as the file is present in BOOT-INF\classes folder in jar file (packing type is jar)</p>

<pre><code>Exception occur while generating mock sso response java.io.FileNotFoundException: class path resource [mock_sso_response.xml] cannot be resolved to absolute file path because it does not reside in the file system: jar:file:/polst-webapp-2.1.5-SNAPSHOT.jar!/BOOT-INF/classes!/mock_sso_response.xml
</code></pre>

<p>However using getInputStream works.</p>

<pre><code>Resource resource= resourceLoader.getResource(""classpath:"" + MOCK_SSO_RESPONSE_FILE);
InputStream inputStream = resource.getInputStream();
</code></pre>

<p>It would be really helpful if someone could share the reason behind this and also is there any better/alternate way to load xml files in spring boot. Any help would be much appreciated</p>
"
"25920029","Setting up MySQL and importing dump within Dockerfile","<mysql><docker>","60808493","How to run a .sql file inside a sql image in Dockerfile?","<mysql><docker><dockerfile>","<p>I'm trying to setup a Dockerfile for my LAMP project, but i'm having a few problems when starting MySQL. I have the folowing lines on my Dockerfile:</p>

<pre><code>VOLUME [""/etc/mysql"", ""/var/lib/mysql""]
ADD dump.sql /tmp/dump.sql
RUN /usr/bin/mysqld_safe &amp; sleep 5s
RUN mysql -u root -e ""CREATE DATABASE mydb""
RUN mysql -u root mydb &lt; /tmp/dump.sql
</code></pre>

<p>But I keep getting this error: </p>

<pre><code>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)
</code></pre>

<p>Any ideas on how to setup database creation and dump import during a Dockerfile build?</p>
","<p>I need to execute some script in a mysql image and need to create a new image out of it. </p>

<pre><code>FROM mysql:5.7.29
COPY ./multicore.sql /bin
ENV MYSQL_ROOT_PASSWORD=root
RUN /bin/bash -c ""mysql -uroot -proot \
    &amp;&amp; source &gt; /bin/sample.sql""
</code></pre>

<p>So when i execute this Dockerfile i'm getting the following error. The sample.sql run perfectly fine. The error i'm getting is </p>

<pre><code>Sending build context to Docker daemon  116.7kB
Step 1/4 : FROM mysql:5.7.29
 ---&gt; 84164b03fa2e
Step 2/4 : COPY ./multicore.sql /bin
 ---&gt; 54738bfd2ce2
Step 3/4 : ENV MYSQL_ROOT_PASSWORD=root
 ---&gt; Running in 0efc6d84aedb
Removing intermediate container 0efc6d84aedb
 ---&gt; aab5ba5fb5a4
Step 4/4 : RUN /bin/bash -c ""mysql -uroot -proot     &amp;&amp; source &gt; /bin/sample.sql""
 ---&gt; Running in 526d17218da3
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
The command '/bin/sh -c /bin/bash -c ""mysql -uroot -proot     &amp;&amp; source &gt; /bin/sample.sql""' returned a non-zero code: 1
</code></pre>
"
"46597290","Can't change the permissions of files/folders on a volume with Docker windows","<windows><docker><docker-for-windows>","66388158","Changing file permissions from a mounted file from inside the docker container (Windows 10)","<docker><file-permissions>","<p>I tried to change the permissions of files/folders on a volume with Docker windows.  But the permissions are not changed, unexpectedly.</p>

<p>Environment:
Host: Windows 10 Pro
Docker version 17.09.0-ce, build afdb6d4</p>

<p>Step to reproduce:</p>

<ol>
<li>Build a image with the Dockerfile below.</li>
<li>Run a container with a volume.</li>
<li>Change the permissions of files/folders.</li>
</ol>

<p>Dockerfile:</p>

<pre><code>FROM microsoft/windowsservercore
CMD [ ""powershell"" ]
</code></pre>

<p>Outputs:</p>

<pre><code>D:\data\docker\sample&gt;docker build -t sample .
Sending build context to Docker daemon  1.272GB
Step 1/2 : FROM microsoft/windowsservercore
 ---&gt; 2cddde20d95d
Step 2/2 : CMD powershell
 ---&gt; Running in dd207fe8b262
 ---&gt; e0203df155cd
Removing intermediate container dd207fe8b262
Successfully built e0203df155cd
Successfully tagged sample:latest

D:\data\docker\sample&gt;docker run -d --name sample --mount type=volume,source=sample_volume,target=C:/data sample ping -t localhost
5a21f41d63de321e912ec3b99010a062d2e04d5f99145c6cd8bf649d3fbbebf1

D:\data\docker\sample&gt;docker exec -i sample cmd
Microsoft Windows [Version 10.0.14393]
(c) 2016 Microsoft Corporation. All rights reserved.

C:\&gt;cd c:\data
cd c:\data

c:\data&gt;MKDIR foo
MKDIR foo

c:\data&gt;ECHO hoge &gt; foo\hoge.txt
ECHO hoge &gt; foo\hoge.txt

c:\data&gt;cacls foo
cacls foo
c:\data\foo NT AUTHORITY\SYSTEM:(OI)(CI)(ID)F
            BUILTIN\Administrators:(OI)(CI)(ID)F
            CREATOR OWNER:(OI)(CI)(IO)(ID)F
            BUILTIN\Users:(OI)(CI)(ID)R
            BUILTIN\Users:(CI)(ID)(special access:)
                                  FILE_WRITE_DATA
                                  FILE_APPEND_DATA
                                  FILE_WRITE_EA
                                  FILE_WRITE_ATTRIBUTES



c:\data&gt;cacls foo\hoge.txt
cacls foo\hoge.txt
c:\data\foo\hoge.txt NT AUTHORITY\SYSTEM:(ID)F
                     BUILTIN\Administrators:(ID)F
                     BUILTIN\Users:(ID)R


c:\data&gt;cacls foo /T /E /G everyone:F
cacls foo /T /E /G everyone:F
processed dir: c:\data\foo
processed file: c:\data\foo\hoge.txt

c:\data&gt;cacls foo
cacls foo
c:\data\foo NT AUTHORITY\SYSTEM:(OI)(CI)(ID)F
            BUILTIN\Administrators:(OI)(CI)(ID)F
            CREATOR OWNER:(OI)(CI)(IO)(ID)F
            BUILTIN\Users:(OI)(CI)(ID)R
            BUILTIN\Users:(CI)(ID)(special access:)
                                  FILE_WRITE_DATA
                                  FILE_APPEND_DATA
                                  FILE_WRITE_EA
                                  FILE_WRITE_ATTRIBUTES



c:\data&gt;cacls foo\hoge.txt
cacls foo\hoge.txt
c:\data\foo\hoge.txt NT AUTHORITY\SYSTEM:(ID)F
                     BUILTIN\Administrators:(ID)F
                     BUILTIN\Users:(ID)R
</code></pre>

<p>I found a document which says that the permissions of files/folders on volumes cannot be changed on Linux containers.  But I could not found documentation about Windows containers.  Does Windows containers support the permission changes of file/folders on a volume <strong>on Windows containers</strong>?</p>

<p>Link:</p>

<ul>
<li>Logs and troubleshooting | Docker Documentation <a href=""https://docs.docker.com/docker-for-windows/troubleshoot/#permissions-errors-on-data-directories-for-shared-volumes"" rel=""nofollow noreferrer"">https://docs.docker.com/docker-for-windows/troubleshoot/#permissions-errors-on-data-directories-for-shared-volumes</a></li>
</ul>
","<p>Is it possible to change the file permissions from a mounted file from inside the docker container?
And if yes how do I do that?
If I try it with chmod nothing happens.
I'm using a Debian container on windows 10.</p>
<p>Here's a picture of what I've tried.</p>
<p><a href=""https://i.stack.imgur.com/Uxi0V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uxi0V.png"" alt=""What I've tried"" /></a></p>
<p>Thanks for the help. :)</p>
"
"26734402","How to upgrade docker container after its image changed","<docker>","60492913","How to restart the ROS docker container with GUI enabled","<docker><user-interface><ros>","<p>Let's say I have pulled the official <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql:5.6.21 image</a>. </p>

<p>I have deployed this image by creating several docker containers.</p>

<p>These containers have been running for some time until MySQL 5.6.22 is released. The official image of mysql:5.6 gets updated with the new release, but my containers still run 5.6.21.</p>

<p>How do I propagate the changes in the image (i.e. upgrade MySQL distro) to all my existing containers? What is the proper Docker way of doing this?</p>
","<ul>
<li>How to <strong>restart</strong> the ROS docker container <strong>with</strong> GUI enabled?  (Points to note: restart &amp; with GUI!)</li>
</ul>

<p>Following from questions like <a href=""https://stackoverflow.com/questions/47099483/run-gui-programs-in-docker-in-ubuntu"">this</a> and <a href=""https://answers.ros.org/question/326807/docker-gazeb-cannot-connect-to-x-server/"" rel=""nofollow noreferrer"">this</a> and the docker tutorial for ROS <a href=""http://wiki.ros.org/docker/Tutorials/GUI"" rel=""nofollow noreferrer"">here</a> I managed to stumble on to how one can ""run"" a docker container with GUI enabled.</p>

<p>It goes something like this:</p>

<pre><code>$ xhost +local:docker
$ docker run -it --env=""DISPLAY"" --env=""QT_X11_NO_MITSHM=1"" --volume=""/tmp/.X11-unix:/tmp/.X11-unix:rw"" --volume=""$HOME/.Xauthority:/root/.Xauthority"" ros
</code></pre>

<p>Afterwards, I can run inside the container in a terminal the following commands:</p>

<pre><code># roscore
</code></pre>

<p>And then in a separate terminal:</p>

<pre><code>$ docker exec -it priceless_engelbart bash
# apt-get update
# apt-get install ros-melodic-rqt-gui
# rosrun rqt_gui rqt_gui
</code></pre>

<p>And a window appears, which shows that the GUI is working from inside the container.</p>

<p><a href=""https://i.stack.imgur.com/QG7S8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QG7S8.png"" alt=""enter image description here""></a></p>

<p><strong>Question:</strong> The problem is, I don't want to create a new container every time, which is what the <code>docker run</code> command does. I have done work in the first container. Why would I want to completely ignore that and spin up a new container and start from scratch?</p>

<p>To remedy the situation, I looked into <a href=""https://docs.docker.com/engine/reference/commandline/start/"" rel=""nofollow noreferrer"">docker start</a>, <a href=""https://docs.docker.com/engine/reference/commandline/restart/"" rel=""nofollow noreferrer"">docker restart</a>, <a href=""https://docs.docker.com/engine/reference/commandline/attach/"" rel=""nofollow noreferrer"">docker attach</a> and  <a href=""https://docs.docker.com/engine/reference/commandline/exec/"" rel=""nofollow noreferrer"">docker exec</a>, to see if there was anyway to pass those extra parameters that I pass to <code>docker run</code>, like <code>--env</code> and <code>--volume</code> to help make the GUI work inside the container here.</p>

<p>Unfortunately, those commands do NOT allow passing any such parameters.</p>

<p>I have not been able to find a solution till now for the situation.</p>
"
"38015537","Python - requests.exceptions.SSLError - dh key too small","<python><ssl><python-requests>","65579807","BDD SeleniumGRID ChromeDriver initiated Exception in Jenkins Pipeline","<python><python-3.x><docker><jenkins><jenkins-pipeline>","<p>I'm scraping some internal pages using Python and requests. I've turned off SSL verifications and warnings. </p>

<pre><code>requests.packages.urllib3.disable_warnings()
page = requests.get(url, verify=False)
</code></pre>

<p>On certain servers I receive an SSL error I can't get past.</p>

<pre><code>Traceback (most recent call last):
  File ""scraper.py"", line 6, in &lt;module&gt;
    page = requests.get(url, verify=False)
  File ""/cygdrive/c/Users/jfeocco/VirtualEnv/scraping/lib/python3.4/site-packages/requests/api.py"", line 71, in get
    return request('get', url, params=params, **kwargs)
  File ""/cygdrive/c/Users/jfeocco/VirtualEnv/scraping/lib/python3.4/site-packages/requests/api.py"", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/cygdrive/c/Users/jfeocco/VirtualEnv/scraping/lib/python3.4/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/cygdrive/c/Users/jfeocco/VirtualEnv/scraping/lib/python3.4/site-packages/requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
  File ""/cygdrive/c/Users/jfeocco/VirtualEnv/scraping/lib/python3.4/site-packages/requests/adapters.py"", line 477, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: [SSL: SSL_NEGATIVE_LENGTH] dh key too small (_ssl.c:600)
</code></pre>

<p>This happens both in/out of Cygwin, in Windows and OSX. My research hinted at outdated OpenSSL on the server. I'm looking for a fix client side ideally. </p>

<p>Edit:
I was able to resolve this by using a cipher set</p>

<pre><code>import requests

requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'
try:
    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST += 'HIGH:!DH:!aNULL'
except AttributeError:
    # no pyopenssl support used / needed / available
    pass

page = requests.get(url, verify=False)
</code></pre>
","<p>I am doing selenium GRID Automation in BDD/Cucumber with Python 3.8 which gives an error while running with Jenkins pipeline with Docker Hub container.</p>
<p>Answer for this Error:</p>
<blockquote>
<p>SSLError(SSLError(1, '[SSL: DH_KEY_TOO_SMALL] dh key too small (_ssl.c:1123)')))</p>
</blockquote>
<p>Reason for this error:</p>
<blockquote>
<p>Python 3.8 having validation check of SSL Cipher text with Linux Operation system in Jenkins Docker Hub Pipeline execution.</p>
</blockquote>
<p>To avoid this issue: [Solution]</p>
<blockquote>
<p>To resolve this issue, we need to paste below command on top of runnerfile or any PageFactory file with SSL flag OFF to validate.</p>
</blockquote>
<pre><code>import requests

requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'
</code></pre>
"
"48483541","Ubuntu container keep restarting","<ubuntu><docker><containers>","67018329","Why container based on ubuntu didn't run from docker-compose file, when that work for similar nginx container?","<docker><docker-compose>","<p>I'm using docker-compose to add a new ubuntu container but the container keeps restarting and I don't know why... any clue of what I can check ?</p>

<p>here my docker-compose service:</p>

<pre><code>  ubuntu:
    image: ubuntu
    container_name: ubuntu
    network_mode: host
    restart: unless-stopped
    volumes:
      - /mnt:/NAS:rw
    environment:
      - TZ=""Asia/Shanghai""
</code></pre>

<p>and here is the <em>docker ps</em> output:</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                          PORTS                                            NAMES
6c084528838c        ubuntu              ""/bin/bash""              6 minutes ago       Restarting (0) 18 seconds ago                                                    ubuntu
</code></pre>

<p>i'm using Docker 17.09 on Ubuntu server 17.04 and I'm running the container with this alias:</p>

<pre><code>alias dcrun='docker-compose -f /home/docker-compose.yml'

dcrun up -d ubuntu
</code></pre>

<p>Thank you</p>
","<p>I try to run docker container using docker-compose file instead of a long command line.</p>
<p>I want to run docker-compose file based on ubuntu:latest. Container created but can't run.</p>
<pre><code>version: &quot;3.9&quot;

services:

  ubuntu:
    image: ubuntu:latest
    container_name: nginx_from_scratch3
    ports:
     - &quot;80:80&quot;
</code></pre>
<p>But before I've tried add in my docker-compose file line</p>
<pre><code>command: bash
</code></pre>
<p>And noting change. I think what after running container continue to work. But that didn't happend.</p>
<p>But on the other side if I use nginx image all run perfectly.</p>
<pre><code>version: &quot;3.9&quot;

services:

nginx1:
  image: nginx
  container_name: nginx_from_scratch4
  ports:
    - &quot;80:80&quot;
</code></pre>
<p>Why docker-compose file for nginx image work, and doesn’t work for ubuntu image.</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","60344899","How to copy files when files are in different folder than the dockerfile?","<windows><docker>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I need to build a docker image and need to copy a file from the different folder which don't have docker file</p>

<p>Basically this is my dir structure</p>

<pre><code>C:\AGENT\_WORK\2
├───a
│   └───s
└───s
    └───Docker
            Dockerfile
</code></pre>

<p>so I need to copy all files from <code>C:\AGENT\_WORK\2\a\s</code> but my docker file is in  <code>C:\AGENT\_WORK\2\s\Docker\Dockerfile</code></p>

<p>I'm building the image by this command, I have given context as C:\agent_work`</p>

<pre><code>docker build -t testapp:latest -f C:\AGENT\_WORK\2\s\Docker\Dockerfile C:\agent\_work
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM mcr.microsoft.com/windows/nanoserver:1709 AS base
WORKDIR /app
ARG BINARY_SRC
EXPOSE 80
EXPOSE 443
ADD C:\AGENT\_WORK\2\a\s .
RUN ls
FROM base AS final
ENTRYPOINT [""dotnet"", ""TestApp.dll""]
</code></pre>

<p>Error:</p>

<blockquote>
  <p>ADD failed: CreateFile
  \?\C:\ProgramData\Docker\tmp\docker-builder076789264\agent_work: The
  system cannot find the path specified.</p>
</blockquote>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","65564520","Communication between 2 Docker Composes using Network results in ""SequelizeHostNotFoundError""","<javascript><postgresql><docker><docker-compose><dockerfile>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have 2 docker composes , one is Infrastructures and the second is Appplications.</p>
<p>I need a direct line from <strong>Appplications =&gt; Infrastructures</strong> , to run migrations on a Postresql container in <code>Appplications</code> docker ,and it (Infrastructures) must be listening before make requests from <code>Appplications</code>.</p>
<p><strong>Infrastructures :</strong></p>
<pre><code>version: &quot;3&quot;

networks:
  shared_network:
    driver: bridge

services:

  # PostgreSQL
  my_postgres:
    image: postgres:latest
    environment:
      .... // ENVS
    ports:
      - &quot;5432:5432&quot;
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:5432&quot;]
      interval: 30s
      timeout: 10s
      retries: 15
    networks:
      - shared_network
      
      
      ... more infrastructures
</code></pre>
<p><strong>Applications :</strong></p>
<pre><code>version: &quot;3&quot;

networks:
  shared_network:
    driver: bridge

services:
  # Employees Service
  emps:    
    build:
      context: ./emps_service
      dockerfile: ./Dockerfile
    ports:
      - &quot;9000:9000&quot;
    networks:
      - shared_network
      
      
      
      
      
      ... More apps
</code></pre>
<p>I'm using <strong>default.yaml</strong> for Sequelize :</p>
<pre><code>... more definitions 


sequelize:
  database: some_random_db
  host: my_postgres
  user: xxxx
  password: yyyy
  dialect: postgres  
  seederStorage: &quot;sequelize&quot;
</code></pre>
<p>and when I run <code>docker-compose up</code> , twice , first for the <strong>Infrastructures</strong> , and only once it's up another <code>docker-compose up</code> for the <strong>Applications</strong> , I get :</p>
<pre><code>**SequelizeHostNotFoundError: getaddrinfo ENOTFOUND my_postgres**
</code></pre>
<p>How can we fix this ?</p>
"
"50136694","What is the simplest way to share a multi-container app on docker hub?","<docker><docker-compose><docker-swarm><dockerhub>","66387086","Upload docker-compose.yml file to docker hub","<docker><docker-compose><dockerhub>","<p>I have a fully dockerised app with multiple services: frontend, backend, user-service, multiple databases etc...</p>

<p>I orchestrate everything using a <code>docker-compose.yml</code> file. I am now trying to share this app with someone who has no CS/coding knowledge so that all they will need to do is pull this app from docker hub and enter the command <code>docker-compose up</code> or something similar. </p>

<p>I'm looking into the best way to do this and I'm getting very lost with the docker lexicon of swarms, bundles, stacks, services etc...</p>

<p>What is the best practice for sharing a collection of images started with a docker-compose.yml file?</p>
","<p>I am trying to sign and upload a <strong>docker-compose.yml</strong> to <strong>docker-hub</strong>, I have been looking and only images can be uploaded so I have searched for how to create images from a docker-compose.yml but I can't find anything, I have tried the command: <code>docker-compose build</code> but it doesn't create a full image for me just from the php part</p>
<p>Here is the docker-compose.yml file just in case it doesn't work due to a bug</p>
<pre><code>version: '3'

networks:
    LEMP:

services:
    nginx:
        image: nginx:stable-alpine
        container_name: LEMP_nginx
        ports:
            - &quot;80:80&quot;
        volumes:
            - ./app:/app
            - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
        depends_on:
            - php
        networks:
            - LEMP

    php:
        # image: php:7-fpm-alpine
        build: ./php
        container_name: LEMP_php
        volumes:
            - ./app:/app
        ports:
            - &quot;9000:9000&quot;
        networks:
            - LEMP


    mysql:
        image: mysql:latest
        container_name: LEMP_mysql
#        command: --default-authentication-plugin=mysql_native_password
        volumes:
            - ./database:/docker-entrypoint-initdb.d
        ports:
            - &quot;3307:3306&quot;
        depends_on:
            - nginx
        environment:
            - MYSQL_ROOT_PASSWORD=admin123
        networks:
            - LEMP

    phpmyadmin:
        image: phpmyadmin/phpmyadmin
        container_name: LEMP_phpmyadmin
        ports:
            - &quot;8000:80&quot;
        environment:
            PMA_ARBITRARY: 1
        depends_on:
            - mysql
        networks:
            - LEMP
</code></pre>
<p>And this is my Dockerfile for PHP</p>
<pre><code>FROM php:7-fpm-alpine
RUN docker-php-ext-install mysqli pdo pdo_mysql
</code></pre>
"
"27500861","What's the proper way to ""go get"" a private repository?","<git><go>","60585483","How to properly build a private Go project?","<docker><go><github>","<p>I'm searching for the way to get <code>$ go get</code> work with private repository, after many google try.  </p>

<p>The first try:</p>

<pre><code>$ go get -v gitlab.com/secmask/awserver-go
Fetching https://gitlab.com/secmask/awserver-go?go-get=1
https fetch failed.
Fetching http://gitlab.com/secmask/awserver-go?go-get=1
Parsing meta tags from http://gitlab.com/secmask/awserver-go?go-get=1 (status code 200)
import ""gitlab.com/secmask/awserver-go"": parse http://gitlab.com/secmask/awserver-go?go-get=1: no go-import meta tags
package gitlab.com/secmask/awserver-go: unrecognized import path ""gitlab.com/secmask/awserver-go
</code></pre>

<p>Yep, it did not see the meta tags because I could not know how to provide login information.</p>

<p>The second try:</p>

<p>Follow <a href=""https://gist.github.com/shurcooL/6927554"" rel=""noreferrer"">https://gist.github.com/shurcooL/6927554</a>. Add config to .gitconfig.</p>

<pre><code>[url ""ssh://git@gitlab.com/""]
    insteadOf = https://gitlab.com/
$ go get -v gitlab.com/secmask/awserver-go --&gt; not work
$ go get -v gitlab.com/secmask/awserver-go.git --&gt; work but I got src/gitlab.com/secmask/awserer-go.git
</code></pre>

<p>Yes it work but with <code>.git</code> extension with my project name, I can rename it to original but do it everytime <code>$ go get</code> is not so good, is there an otherway?</p>
","<p>I have a Go project, which is a private repo on GitHub. I want to build this project from source. I've tried running <code>go get -d -v ./... &amp;&amp; go install -v ./...</code> to install all dependencies, but this is the error I got <code>fatal: could not read Username for 'https://github.com': terminal prompts disabled</code> which means that it tried to clone my project from GitHub rather than building it from source.</p>

<p>I can really use some help in properly building a Go project from source, with all the dependencies installed. I am using Docker and this is how my <code>Dockerfile</code> looks:</p>

<pre><code>#build stage
FROM golang:alpine AS builder
WORKDIR /go/src/app
COPY . .
RUN apk add --no-cache git
RUN go get -d -v ./...
RUN go install -v ./...

#final stage
FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /go/bin/app /app
ENTRYPOINT ./app
EXPOSE 8080
</code></pre>

<p>Only my project's code is private, all other dependencies I have are publically available e.g. <code>mux</code>. I have no problem if Go pulls those files from GitHub, but I want to compile my project's code from source.</p>
"
"54428608","Docker Node Alpine Image Build Fails on node-gyp","<node.js><docker><npm><node-gyp><alpine>","67022931","node-gyp error in docker build, how do i install Python in dockerfile while dockerizing my testcafe project?","<python><node.js><docker><npm><testcafe>","<p>I'm attempting to Dockerize a Vue.js application. I'm using the <code>node:10.15-alpine</code> Docker image as a base. The image build fails with the following error:</p>

<pre><code>gyp ERR! configure error
gyp ERR! stack Error: Can't find Python executable ""python"", you can set the PYTHON env variable.
gyp ERR! stack     at PythonFinder.failNoPython (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:484:19)
gyp ERR! stack     at PythonFinder.&lt;anonymous&gt; (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:406:16)
gyp ERR! stack     at F (/usr/local/lib/node_modules/npm/node_modules/which/which.js:68:16)
gyp ERR! stack     at E (/usr/local/lib/node_modules/npm/node_modules/which/which.js:80:29)
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/which/which.js:89:16
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/isexe/index.js:42:5
gyp ERR! stack     at /usr/local/lib/node_modules/npm/node_modules/isexe/mode.js:8:5
gyp ERR! stack     at FSReqWrap.oncomplete (fs.js:154:21)
gyp ERR! System Linux 4.9.125-linuxkit
gyp ERR! command ""/usr/local/bin/node"" ""/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js"" ""rebuild""
gyp ERR! cwd /app/node_modules/inotify
gyp ERR! node -v v10.15.0
gyp ERR! node-gyp -v v3.8.0
gyp ERR! not ok
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})

npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! inotify@1.4.2 install: `node-gyp rebuild`
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the inotify@1.4.2 install script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
</code></pre>

<p>The application runs on my Ubuntu machine. And I've searched for a resolution online.</p>

<p>I tried:</p>

<pre><code>FROM node:10.15-alpine
EXPOSE 8080
RUN mkdir -p /app/src
WORKDIR /app
COPY build/ config/ dist/ static/ .babelrc .postcssrc.js index.html /app/
COPY package*.json ./
RUN apk add --no-cache make gcc g++ python &amp;&amp; \
  npm install --production --silent &amp;&amp; \
  apk del make gcc g++ python
ADD src/ /app/src/
CMD [""npm"", ""start""]
</code></pre>

<p>This fails too. The error output is quite verbose and references C/C++ code.</p>

<p>Here's my current Dockerfile:</p>

<pre><code>FROM node:10.15-alpine
EXPOSE 8080
RUN mkdir -p /app/src
WORKDIR /app
COPY build/ config/ dist/ static/ .babelrc .postcssrc.js index.html /app/
COPY package*.json ./
RUN npm install
ADD src/ /app/src/
CMD [""npm"", ""start""]
</code></pre>

<p>Can anyone help me to resolve this issue with node-gyp? I'd like to be able to run the application from with a Docker container, but I need to get the image to build first.</p>

<h3>Update</h3>

<p>Since the build was working on my Ubuntu machine, I checked the <code>node</code> version. It was 8.12, so I switch to using the <code>node:8.12-alpine</code> image and the application now works with the following Dockerfile:</p>

<pre><code>FROM node:8.12-alpine
RUN apk add g++ make python
EXPOSE 8080
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN npm install
CMD [""npm"", ""start""]
</code></pre>
","<p>Hello i am new to docker and i am dockerizing my testcafe project. I get node- gyp error while building the image</p>
<p>gyp ERR! find Python
gyp ERR! find Python Python is not set from command line or npm configuration
gyp ERR! find Python Python is not set from environment variable PYTHON
gyp ERR! find Python checking if &quot;python&quot; can be used
gyp ERR! find Python - &quot;python&quot; is not in PATH or produced an error
gyp ERR! find Python checking if &quot;python2&quot; can be used
gyp ERR! find Python - &quot;python2&quot; is not in PATH or produced an error
gyp ERR! find Python checking if &quot;python3&quot; can be used
gyp ERR! find Python - &quot;python3&quot; is not in PATH or produced an error
gyp ERR! find Python
gyp ERR! find Python **********************************************************
gyp ERR! find Python You need to install the latest version of Python.
gyp ERR! find Python Node-gyp should be able to find and use Python. If not,
gyp ERR! find Python you can try one of the following options:
gyp ERR! find Python - Use the switch --python=&quot;/path/to/pythonexecutable&quot;
gyp ERR! find Python   (accepted by both node-gyp and npm)
gyp ERR! find Python - Set the environment variable PYTHON
gyp ERR! find Python - Set the npm configuration variable python:
gyp ERR! find Python   npm config set python &quot;/path/to/pythonexecutable&quot;
gyp ERR! find Python For more information consult the documentation at:
gyp ERR! find Python <a href=""https://github.com/nodejs/node-gyp#installation"" rel=""nofollow noreferrer"">https://github.com/nodejs/node-gyp#installation</a>
gyp ERR! find Python **********************************************************</p>
<p>Here is my Dockerfile:</p>
<pre><code>FROM testcafe/testcafe
USER root
WORKDIR /app
COPY .npmrc .npmrc
COPY package*.json ./
RUN npm install
COPY . .
USER user
</code></pre>
"
"40651561","Symfony can changing the 'secret' parameter break anything?","<php><symfony><security><parameters>","51947898","Can I safely change the APP_SECRET variable in Symfony 4?","<php><symfony><security><docker>","<p>In the <strong>parameters.yml</strong> file there is a parameter named <strong>secret</strong> which defaults to <code>ThisTokenIsNotSoSecretChangeIt</code> but it should be changed to something else.</p>

<p>What happens if the value of this parameter is changed in production? Can it break anything? </p>
","<p>Working with docker I was thinking about the possibility of generating a random hash for the APP_SECRET environment variable every time the container is upgraded (since I am not using files on volumes to store the environment variables).</p>
<p>That way the configuration of a Symfony application would be easier since one can omit the APP_SECRET hash on the docker host. Also this could be seen as more secure since people tend to skip or forget to configure secret variables during production deployment.</p>
<ul>
<li>What could be possible outcomes or side effects of changing the APP_SECRET variable while the application is running?</li>
<li>What could be possible outcomes or side effects of changing the APP_SECRET variable while the application is being upgraded?</li>
</ul>
"
"29556879","Is it possible change date in docker container?","<tomcat><docker>","60818445","How to change the date of a docker container without using libfaketime?","<database><docker><db2><containers>","<p>I have a container with a running program inside tomcat. I need to change date only in this container and test my program behaviour. I have time sensitive logic, and sometimes need to see what happens in a few days or months later.
Is it possible in docker? I read that if I change date in container, date will get changed on the host system. But it is a bad idea for me. I need to have a few instances of this application on one server and have possibilities of setting up different time for each instance.</p>

<p>But when I try to change date inside the container I get the error:</p>

<pre><code>sudo date 04101812
date: cannot set date: Operation not permitted
Fri Apr 10 18:12:00 UTC 2015
</code></pre>
","<p>I am trying to launch a container at runtime with a different date :</p>

<p><code>docker run -it db container /bash/bin date +%T -s ""20190322 10:00:00""</code></p>

<p>This doesn't changes also the date of the system.</p>
"
"41093812","How to get Docker containers to talk to each other while running on my local host?","<docker>","65495914","Multiple containers in docker works on same IP","<docker><docker-compose>","<p>I have a Webapp running completely locally on my MacBook.</p>

<p>The Webapp has a Front End (Angular/Javascript) and a Back End (Python/Django) which implements a RESTful API.</p>

<p>I have Dockerized the Back End so that it is completely self-contained in a Docker Container and exposes port 8000. I map this port locally to 4026.</p>

<p>Now I need to Dockerize the Front End. But if I have these two docker containers running on my localhost, how can I get the FE to send HTTP requests to the BE? The FE container won't know anything that exists outside of it. Right?</p>

<p>This is how I run the FE:</p>

<pre><code>$ http-server
Starting up http-server, serving ./
Available on:
  http://127.0.0.1:8080
  http://192.168.1.16:8080
Hit CTRL-C to stop the server
</code></pre>

<p>Please provide references explaining how I can achieve this.</p>
","<p>I want my Docker containers to work on the same IP. Is it possible? I want them to have the same IP address so that they can link to each other through it.</p>
"
"58398715","Docker-Compose can't connect to MySQL","<mysql><django><python-3.x><docker><docker-compose>","66408689","Dockerize Django project with MySQL container","<mysql><django><docker><docker-compose>","<p>I'm trying to connect Django project with MySQL using docker.</p>

<p>I have the problem when upping docker-compose. It says the next error:</p>

<blockquote>
  <p>super(Connection, self).<strong>init</strong>(*args, **kwargs2)
  django.db.utils.OperationalError: (2002, ""Can't connect to MySQL server on 'db' (115)"")</p>
</blockquote>

<p>I'm using port 3307, should i create first new database schema in my local? Or how can I do it because my default port is 3306 but if i try to use it, it says that is busy.</p>

<p>My code here: </p>

<p>Dockerfile</p>

<pre><code>
FROM python:3.7
ENV PYTHONUNBUFFERED 1
ENV LANG=C.UTF-8
RUN mkdir /code
WORKDIR /code
ADD requirements.txt /code/
RUN apt-get update
RUN pip install -r requirements.txt
ADD . /code/


</code></pre>

<p>Docker-compose</p>

<pre><code>
    version: '2'

services:
  app:
    container_name: container_app
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    command: bash -c ""python3 manage.py migrate &amp;&amp; python manage.py shell &lt; backend/scripts/setup.py &amp;&amp; python3 manage.py runserver 0.0.0.0:8000""
    links:
      - db
    depends_on:
      - db
    ports:
      - ""8000:8000""
  db:
    container_name: container_database
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_HOST: 'host.docker.internal'
      MYSQL_DATABASE: 'container_develop'
      MYSQL_USER: 'root'
      MYSQL_PASSWORD: 'password'
      MYSQL_ROOT_PASSWORD: 'password'
    ports:
      - ""3307:3307""

</code></pre>

<p>settings.py:</p>

<pre><code>
DATABASES = {
    'default' : {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'database_develop',
        'USER': 'root',
        'PASSWORD': 'password',
        'HOST': 'db',
        'PORT': 3307,
        'CHARSET': 'utf8',
        'COLLATION': 'utf8_bin',
        'OPTIONS': {
            'use_unicode' : True,
            'init_command': 'SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED',
        },
    }
}

</code></pre>

<p>The thing is that I do not have the database anywhere so I'm using it locally, do I have to upload the db to a server and then use the port that server provides to me?
Is there any way to use it locally from docker? Or installing it to docker? So I can share that docker to a friend and he can use the same DB?</p>
","<p>I have a very simple Django project (v.2.2) with Python (v.3.6.9), Docker (v.20.10.3), and docker-compose (v. 1.28.4) running on Ubuntu 18.04.</p>
<p>I'm trying to dockerize Django project with MySQL container. I did all the migrations and have the .sql file of my database. The project works fine without Docker, however, when I run <em>docker-compose up</em> I got the following error:</p>
<pre><code>django.db.utils.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
</code></pre>
<p>Here's my full configuration: <a href=""https://github.com/aziele/docker-django-mysql"" rel=""nofollow noreferrer"">https://github.com/aziele/docker-django-mysql</a></p>
<p>Briefly, this is my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.9
ENV PYTHONUNBUFFERED 1
COPY ./requirements.txt /requirements.txt
RUN pip install -r /requirements.txt
WORKDIR /app
</code></pre>
<p>And these my <code>requirements.txt</code>:</p>
<pre><code>Django==2.2
mysqlclient==2.0.3
django-mysql
</code></pre>
<p>The <code>docker-compose.yml</code> file:</p>
<pre><code>version: '3'
services:
  db:
    image: mysql:5.7.33
    restart: unless-stopped
    environment:
      MYSQL_DATABASE: 'projectdb1'
      MYSQL_USER: 'user'
      MYSQL_PASSWORD: 'password'
      MYSQL_ROOT_PASSWORD: 'password'
    volumes:
      - ./db/projectdb1.sql:/docker-entrypoint-initdb.d/projectdb1.sql
    ports:
      - '3307:3306'

  django:
    build:
      context: .
      dockerfile: ./Dockerfile-dev
    volumes:
      - ./src:/app
    command: python manage.py runserver 0.0.0.0:8000
    ports:
     - 8080:8000
    restart: always
    depends_on:
      - db
</code></pre>
<p>Also I have change the database configurations in <code>settings.py</code> for this:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'projectdb1',
        'USER': 'root',
        'PASSWORD': 'password',
        'HOST': 'db',
        'PORT': '3307',   
    }
}
</code></pre>
"
"58423571","Why do I have to delete docker containers?","<docker><docker-container>","66856373","Why does Docker not remove stopped containers?","<docker>","<p>What artifacts do stopped docker containers leave on the host file system that makes it necessary to run the command ""docker container prune"". I thought docker containers where simply running versions of images that are suppose to be wiped out after they exit? In what scenarios are artifacts left behind if it is not always the case?</p>
","<p>I am new to Docker and just getting started. I pulled a basic ubuntu image and started a few containers with it and stopped them. When I run the command to list all the docker containers (even the stopped ones) I get an output like this:</p>
<pre><code>&gt; docker container ls -a
   
CONTAINER ID   IMAGE           COMMAND       CREATED          STATUS                      PORTS     NAMES
099c42011f24   ubuntu:latest   &quot;/bin/bash&quot;   6 seconds ago    Exited (0) 6 seconds ago              sleepy_mccarthy
dde61c10d522   ubuntu:latest   &quot;/bin/bash&quot;   8 seconds ago    Exited (0) 7 seconds ago              determined_rosalind
cd1a6fa35741   ubuntu:latest   &quot;/bin/bash&quot;   9 seconds ago    Exited (0) 8 seconds ago              unruffled_lichterman
ff926b6eba23   ubuntu:latest   &quot;/bin/bash&quot;   10 seconds ago   Exited (0) 10 seconds ago             cool_rosalind
8bd50c2c4729   ubuntu:latest   &quot;/bin/bash&quot;   12 seconds ago   Exited (0) 11 seconds ago             cranky_darwin
</code></pre>
<p>My question is, is there a reason why docker does not delete the stopped containers by default?</p>
"
"30022983","can't build matplotlib (png package issue)","<python><linux><matplotlib><fedora>","60229248","Matplotlib wont get installed when building docker container","<python><docker><matplotlib>","<p>try to build matplotlib on fedora-18, build fails with</p>

<pre><code>...
 png: no  [pkg-config information for 'libpng' could not
                        be found.]
...
                        * The following required packages can not be built:
                        * png
</code></pre>

<p>What should I do/check to build png package ?</p>
","<p>Trying to build a docker container with a python script. It breaks when trying to install matplotlib.  At the required dependencies, it says <code>png: no  [pkg-config information for 'libpng' could not be found.]</code>, however, I can't find a package named <code>libpng</code> anywhere for python. This is the error msg:</p>

<pre><code>Collecting matplotlib==2.2.2
  Downloading matplotlib-2.2.2.tar.gz (37.3 MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-9zxcndql/matplotlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-9zxcndql/matplotlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-install-9zxcndql/matplotlib/pip-egg-info
         cwd: /tmp/pip-install-9zxcndql/matplotlib/
    Complete output (55 lines):
    ============================================================================
    Edit setup.cfg to change the build options

 BUILDING MATPLOTLIB
                matplotlib: yes [2.2.2]
                    python: yes [3.6.6 (default, Sep 12 2018, 02:15:29)  [GCC
                            6.4.0]]
                  platform: yes [linux]

REQUIRED DEPENDENCIES AND EXTENSIONS
                 numpy: yes [not found. pip may install it below.]
      install_requires: yes [handled by setuptools]
                libagg: yes [pkg-config information for 'libagg' could not
                        be found. Using local copy.]
              freetype: no  [The C/C++ header for freetype2 (ft2build.h)
                        could not be found.  You may need to install the
                        development package.]
                   png: no  [pkg-config information for 'libpng' could not
                        be found.]
                 qhull: yes [pkg-config information for 'libqhull' could not
                        be found. Using local copy.]
</code></pre>
"
"61891405","Is it okay to switch between WSL versions whenever I need to?","<windows-subsystem-for-linux><wsl-2>","67007201","Will my files get deleted if I upgrade from wsl to wsl 2?","<docker><windows-subsystem-for-linux><wsl-2>","<p>I just recently installed Windows Subsystem for Linux 2. My only current Distro is Ubuntu and I'm using it for everything I do. I know that WSL 1 is better to handle projects that interact with both Windows and Linux, while WSL 2 is better for projects specific to Linux only.</p>

<p><br></p>

<p>I might want to switch between the versions depending on what I'm working on, and I would like to if it's safe to continuously change between versions?</p>

<p><br></p>

<p>Another alternative I found was doing something like:</p>

<pre><code>wsl --export Ubuntu ubuntu.tar
wsl --import Ubuntu2 ./Ubuntu2 ubuntu.tar
</code></pre>

<p>and then setting the version of the second one with <code>wsl --set-version Ubuntu2 2</code>.</p>
","<p>I am upgrading to wsl 2 for using docker with it, wanted to know if it can affect my files stored previously.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60326180","How to view Dash app running in docker container","<docker><flask><docker-compose><hyphen>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<ol>
<li>I Execute <code>docker-compose up</code></li>
<li>The Dash app seems to work fine (see the output)</li>
<li>Dash app cannot be accessed through the local browser <code>localhost:8050</code></li>
</ol>

<p><strong><em>Dash file:</em></strong></p>

<pre><code>app = dash.Dash(__name__)

//...

if __name__ == '__main__':
    app.run_server(debug=True, host='0.0.0.0', port=8050)
</code></pre>

<p><strong><em>docker-compose file:</em></strong></p>

<pre><code>version: ""3.7""
services:
  dashapp:
    image: my_dash_image
    container_name: my-dash
    ports:
      - 8050:8050
    volumes:
      - /tmp/files:/usr/src/app
</code></pre>

<p><strong><em>Output execution:</em></strong></p>

<pre><code>my-dash | Hello!
my-dash | Running on http://127.0.0.1:8050/
my-dash | Debugger PIN: 576-910-730
my-dash |  * Serving Flask app ""global_overview_indicators"" (lazy loading)
my-dash |  * Environment: production
my-dash |    WARNING: This is a development server. Do not use it in a production deployment.
my-dash |    Use a production WSGI server instead.
my-dash |  * Debug mode: on
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60299144","Not able to access running server on docker in uvicorn","<python><docker><fastapi><uvicorn>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have already ran a uvicorn server in docker, its running fine too but i am not able to see it</p>

<pre><code>FROM python
RUN pip install fastapi uvicorn mongoengine
EXPOSE 8002
COPY ./app /app
ENV PYTHONPATH ""${PYTONPATH}:/app""
VOLUME /app
RUN cd app
CMD [""uvicorn"", ""main:app"", ""--host"", ""127.0.0.1"", ""--port"", ""8002""]
</code></pre>

<p>This ran successfully and i am seeing this success logs as well</p>

<pre><code>D:\***\****\****&gt;docker logs a1b
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)
</code></pre>

<p>On visiting </p>

<ul>
<li><a href=""http://127.0.0.1:8002/"" rel=""nofollow noreferrer"">http://127.0.0.1:8002/</a></li>
</ul>

<p>its saying
<strong>This page isn’t working</strong></p>

<p>How should i access this then..</p>

<p>On <code>docker ps -a</code>:</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS                                 NAMES
832bf82a0fb1        edmaskbackend6      ""uvicorn main:app ...""   5 seconds ago       Up 4 seconds                   127.0.0.1:32772-&gt;8002/tcp             infallible_cray
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60273477","docker not able to start flask application","<docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>The flask application is started but I am not able to view anything from browser.</p>

<pre><code>root@1ec89cb3dad1:/app# python main.py
 * Serving Flask app ""main"" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 231-587-342
</code></pre>

<p>The Dockerfile looks like this:</p>

<pre><code>FROM ubuntu
RUN apt-get update -y &amp;&amp; \
    apt-get install -y python-pip python-dev  libhunspell-dev
WORKDIR /app
COPY ./requirements.txt /app/requirements.txt
RUN pip install -r requirements.txt
COPY . /app
ENTRYPOINT [ ""python"" ]
CMD [ ""main.py"" ]
</code></pre>

<p>The files are here...
<a href=""https://github.com/shantanuo/Spell-Checker/tree/master"" rel=""nofollow noreferrer"">https://github.com/shantanuo/Spell-Checker/tree/master</a></p>

<p>I am not sure if I should tag docker or flask!</p>

<hr>

<p>Update:</p>

<p>I used these commands to build and run the image...</p>

<pre><code>git clone https://github.com/shantanuo/Spell-Checker.git

cd Spell-Checker/

docker build -t shantanuo/flask .

docker run -p 5000:5000 shantanuo/flask
</code></pre>
"
"63754742","Authentication failure while trying to save to mongodb","<mongodb>","66354195","Cannot connect to MongoDB when I specify database name","<mongodb><docker><security>","<p>I have following code to save to a local running mongo instance:</p>
<pre><code>MongoCredential credential = MongoCredential.createCredential(&quot;myuser&quot;, &quot;mydatabase&quot;, &quot;mypassword&quot;.toCharArray());

MongoClient mongo = MongoClients.create(MongoClientSettings.builder()
                    .applyToClusterSettings(builder -&gt; builder.hosts(Arrays.asList(new 
ServerAddress(&quot;localhost&quot;, 27017))))
                    .credential(credential)
                    .build());
MongoDatabase database = mongo.getDatabase(&quot;mydatabase&quot;);
MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;mycollection&quot;);
collection.insertOne(document);
</code></pre>
<p>I have created a user for usernmae/password used  in code above using db.createUser() command in mongo.exe shell and these are same credentials I provided while installing mongodb.</p>
<pre><code>db.createUser(
{   user: &quot;myuser&quot;,
    pwd: &quot;mypassword&quot;,

    roles:[{role: &quot;userAdminAnyDatabase&quot; , db:&quot;admin&quot;}]})
</code></pre>
<p>But code fails with:</p>
<pre><code>Exception in thread &quot;main&quot; com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=SCRAM-SHA-1, userName='myuser', source='mydatabase', password=&lt;hidden&gt;, mechanismProperties={}}
</code></pre>
<p>What am I missing here?</p>
","<p>I run MongoDB docker container on my local computer and when I try to connect to MongoDB container without specifying database name, it works fine.<br>
With following command, I managed to connect to MongoDB.</p>
<pre><code>$ mongo &quot;mongodb://username@127.0.0.1:27017&quot;
</code></pre>
<p>But when I try to connect with database name, it will return following error.</p>
<pre><code>$ mongo &quot;mongodb://username@127.0.0.1:27017/dbname&quot;

MongoDB shell version v4.2.0
Enter password: 
connecting to: mongodb://127.0.0.1:27017/dbname?compressors=disabled&amp;gssapiServiceName=mongodb
2021-02-25T00:26:09.511+0900 E  QUERY    [js] Error: Authentication failed. :
connect@src/mongo/shell/mongo.js:341:17
@(connect):2:6
2021-02-25T00:26:09.515+0900 F  -        [main] exception: connect failed
2021-02-25T00:26:09.515+0900 E  -        [main] exiting with code 1
</code></pre>
<p>I thought it might be because database doesn't exist yet, so I tried with database which already exist but same result.</p>
<p>And this is my local environment.<br>
MacOS version 11.2.1<br>
MongoDB shell version v4.2.0<br>
mongo official docker image version 4.0.4<br></p>
"
"41379083","Sourcing a file before executing commands in Ansible","<ansible><ansible-playbook>","51781081","How run an shell alias docker command from Ansible?","<docker><ansible>","<p>I am trying to install node js version using nvm using below Ansible yml file.</p>

<p>I get error like source ""source /home/centos/.nvm/nvm.sh"" file not found. But if I do the same by logging into the machine using ssh then it works fine.</p>

<pre><code>- name: Install nvm
  git: repo=https://github.com/creationix/nvm.git dest=~/.nvm version={{ nvm.version }}
  tags: nvm

- name: Source nvm in ~/.profile
  lineinfile: &gt;
    dest=~/.profile
    line=""source ~/.nvm/nvm.sh""
    create=yes
  tags: nvm

- name: Install node {{ nvm.node_version }}
  command: ""{{ item }}""
  with_items:
     - ""source /home/centos/.nvm/nvm.sh""
     - nvm install {{ nvm.node_version }}
  tags: nvm
</code></pre>

<p>Error:</p>

<pre><code>failed: [172.29.4.71] (item=source /home/centos/.nvm/nvm.sh) =&gt; {""cmd"": ""source /home/centos/.nvm/nvm.sh"", ""failed"": true, ""item"": ""source /home/centos/.nvm/nvm.sh"", ""msg"": ""[Errno 2] No such file or directory"", ""rc"": 2}

failed: [172.29.4.71] (item=nvm install 6.2.0) =&gt; {""cmd"": ""nvm install 6.2.0"", ""failed"": true, ""item"": ""nvm install 6.2.0"", ""msg"": ""[Errno 2] No such file or directory"", ""rc"": 2}
</code></pre>
","<p>one customer has a playbook which runs local actions, in this case, a sqlplus command against RDS endpoint. </p>

<pre><code> shell: ""echo exit|sqlplus64 '{{db_pnyx_username}}/{{db_pnyx_password}}@//{{ host }}:{{ port }}/{{db_pnyx_service_name}}' @vhosts.sql""
</code></pre>

<p>As I'm running Ubuntu I got curious how to avoid to install so many dependencies of packages due to sqlplus package so I decided to try in a container way.</p>

<pre><code> - name : Execute SQL 
    shell: ""echo exit|docker run --rm  --net=host -v $PWD:/tmp rubendob/sqlplus:dockerplus '{{db_pnyx_username}}/{{db_pnyx_password}}@//{{ host }}:{{ port }}/{{db_pnyx_service_name}}' @/tmp/vhosts.sql""
    register: result
</code></pre>

<p>And it worked!</p>

<p>but I also wanted to run the docker with the alias shortcut from my .profile file as it is shown below:</p>

<pre><code>alias sqlplus64='docker run --rm --net=host -v $PWD:/tmp rubendob/sqlplus:dockerplus'
</code></pre>

<p>Ansible always returns an error:</p>

<pre><code>""stderr"": ""/bin/sh: 1: sqlplus64: not found"", ""stderr_lines"": [""/bin/sh: 1: sqlplus64: not found""], ""stdout"": """", ""stdout_lines"": []}
</code></pre>

<p>Is there any way to achieve this according to best practices? I bet I'm doing lot of bad stuff here :(</p>
"
"65869296","Installing pip is not working in python < 3.6","<python><pip>","66372117","Python Pip broken with sys.stderr.write(f“ERROR: {exc}”) centos:7","<python><docker><pip><centos><sympy>","<p>I am starting to make a app using bitbucket CI and i am using the following steps to deploy the application and the steps to install pip is failing.</p>
<pre><code> script:
    - apt-get update
    - apt-get install -y python-dev
    - curl -O https://bootstrap.pypa.io/get-pip.py
    - python get-pip.py
    ... and a few more steps
</code></pre>
<p>Dont know why but <code>python get-pip.py</code> step fails with the following error.</p>
<pre><code>Traceback (most recent call last):
  File &quot;get-pip.py&quot;, line 24226, in &lt;module&gt;
    main()
  File &quot;get-pip.py&quot;, line 199, in main
    bootstrap(tmpdir=tmpdir)
  File &quot;get-pip.py&quot;, line 82, in bootstrap
    from pip._internal.cli.main import main as pip_entry_point
  File &quot;/tmp/tmpUgc5ng/pip.zip/pip/_internal/cli/main.py&quot;, line 60
    sys.stderr.write(f&quot;ERROR: {exc}&quot;)
                                   ^

SyntaxError: invalid syntax

</code></pre>
<p>This worked fine upto yesterday. Not sure why this is not working now.</p>
<p>I thought it may be because of windows but i checked in my local machine running linux but these steps but they worked fine.</p>
","<p>upgrade Pip I notice that my Python is broken. After that all pip commands end with SyntaxError</p>
<p>Below is my docker commands:</p>
<pre><code>FROM centos:7
ENV LANG en_US.UTF-8
RUN yum clean all &amp;&amp; yum -y update &amp;&amp;  yum install epel-release -y &amp;&amp; yum install python-pip R 
libcurl-devel openssl-devel -y
RUN pip install --upgrade pip
RUN pip install sympy
WORKDIR /root/
COPY ./ agro-green/
WORKDIR /root/agro-green/
RUN Rscript build/install.R
</code></pre>
<p>Error as below:</p>
<pre><code>Step 4/9 : RUN pip install --upgrade pip
Using cache
9f731589b8b7
Step 5/9 : RUN pip install sympy
Running in c60ca73cc452
[91mTraceback (most recent call last):
File &quot;/usr/bin/pip&quot;, line 9, in &lt;module&gt;
load_entry_point('pip==21.0.1', 'console_scripts', 'pip')()
File &quot;/usr/lib/python2.7/site-packages/pkg_resources.py&quot;, line 378, in load_entry_point
return get_distribution(dist).load_entry_point(group, name)
File &quot;/usr/lib/python2.7/site-packages/pkg_resources.py&quot;, line 2566, in load_entry_point
return ep.load()
File &quot;/usr/lib/python2.7/site-packages/pkg_resources.py&quot;, line 2260, in load
entry = __import__(self.module_name, globals(),globals(), ['__name__'])
File &quot;/usr/lib/python2.7/site-packages/pip/_internal/cli/ma`enter code here`in.py&quot;, line 60
sys.stderr.write(f&quot;ERROR: {exc}&quot;)`enter code here`
                               ^`enter code here`
 SyntaxError: invalid syntax
 [0mThe command '/bin/sh -c pip install sympy' returned a non-zero code: 1
 Build step 'Execute shell' marked build as failure
 Finished: FAILURE` enter code here`
</code></pre>
"
"42158596","Can Windows Containers be hosted on linux?","<linux><docker><docker-for-windows><windows-container><docker-desktop>","65371364","Is Docker Platform Independent?","<docker><containers>","<p>Is it possible to run <strong>Windows Containers</strong> on <strong>Linux</strong>? The scenario is based on an app written in the <strong>.NET</strong> (old net) and the <strong>Linux user</strong> that wants to run this with Docker to provide a <code>net462</code> written API on the <code>localhost</code>.</p>

<p>I am using beta version from <a href=""https://docs.docker.com/docker-for-windows/"" rel=""noreferrer"">Docker Desktop for Windows</a></p>

<p>If no, then why can Windows run Linux containers and not vice-versa?</p>

<p>EDIT:</p>

<p>As some time has passed and this question is a popular one. I'd like to add one note here that the workaround is to use the new netstandard. It allowed me to pack <code>4.6.2</code> framework into new library.</p>
","<p>I've always this question and no where I found exact answer:</p>
<p><strong>Is Docker Platform Independent?</strong></p>
<p>I mean, when I generate image in Windows, can I run that image on Linux or MacOS? Or when I generate image on Ubuntu, can I run that image on Windows?</p>
<p>If yes, how?
If no, what's the point of saying, docker images can be ran anywhere without worrying about any dependencies?</p>
<p>Please help me with this question</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","60577240","Dockerized Python Flask REST API displays ""The page isn't working","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I am trying to run Flask in Docker. The containers seem to be running but the browser can't connect.</p>

<p><code>docker-compose.yml</code>:</p>

<pre><code>version: '3'

services: 
  api:
    build: .
    ports:
      - ""5000:5000""
    volumes:
      - ./src:/usr/src
</code></pre>

<p><code>Dockerfile</code>:</p>

<pre><code>FROM python:3.8.2
COPY ./src /usr/src
WORKDIR /usr/src
RUN pip install -r requirements.txt
ENTRYPOINT [""python""]
CMD [""run.py""]
</code></pre>

<pre><code>from flask import Flask

def create_app(config_filename):
    app = Flask(__name__)
    app.config.from_object(config_filename)

    from app import api_bp
    app.register_blueprint(api_bp, url_prefix='/api')

    return app

if __name__ == ""__main__"":
    app = create_app(""config"")
    app.run(debug=True)
</code></pre>

<p>When I try to go to <code>http://localhost:5000/api/Hello</code>, I get:</p>

<p><a href=""https://i.stack.imgur.com/jzGmT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jzGmT.png"" alt=""chrome page isn&#39;t working error""></a></p>
"
"42494853","standard_init_linux.go:178: exec user process caused ""exec format error""","<python><linux><bash><docker>","65391847","standard_init_linux.go:211: exec user process caused ""exec format error"" when build tiktokapi:latest","<python><linux><docker><api>","<p>docker started throwing this error:</p>

<blockquote>
  <p>standard_init_linux.go:178: exec user process caused ""exec format error""</p>
</blockquote>

<p>whenever I run a specific docker container with CMD or ENTRYPOINT, with no regard to any changes to the file other then removing CMD or ENTRYPOINT. here is the docker file I have been working with which worked perfectly until about an hour ago:</p>

<pre><code>FROM buildpack-deps:jessie

ENV PATH /usr/local/bin:$PATH

ENV LANG C.UTF-8

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        tcl \
        tk \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV GPG_KEY 0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D
ENV PYTHON_VERSION 3.6.0

ENV PYTHON_PIP_VERSION 9.0.1

RUN set -ex \
    &amp;&amp; buildDeps=' \
        tcl-dev \
        tk-dev \
    ' \
    &amp;&amp; apt-get update &amp;&amp; apt-get install -y $buildDeps --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/* \
    \
    &amp;&amp; wget -O python.tar.xz ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz"" \
    &amp;&amp; wget -O python.tar.xz.asc ""https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc"" \
    &amp;&amp; export GNUPGHOME=""$(mktemp -d)"" \
    &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys ""$GPG_KEY"" \
    &amp;&amp; gpg --batch --verify python.tar.xz.asc python.tar.xz \
    &amp;&amp; rm -r ""$GNUPGHOME"" python.tar.xz.asc \
    &amp;&amp; mkdir -p /usr/src/python \
    &amp;&amp; tar -xJC /usr/src/python --strip-components=1 -f python.tar.xz \
    &amp;&amp; rm python.tar.xz \
    \
    &amp;&amp; cd /usr/src/python \
    &amp;&amp; ./configure \
        --enable-loadable-sqlite-extensions \
        --enable-shared \
    &amp;&amp; make -j$(nproc) \
    &amp;&amp; make install \
    &amp;&amp; ldconfig \
    \
    &amp;&amp; if [ ! -e /usr/local/bin/pip3 ]; then : \
        &amp;&amp; wget -O /tmp/get-pip.py 'https://bootstrap.pypa.io/get-pip.py' \
        &amp;&amp; python3 /tmp/get-pip.py ""pip==$PYTHON_PIP_VERSION"" \
        &amp;&amp; rm /tmp/get-pip.py \
    ; fi \
    &amp;&amp; pip3 install --no-cache-dir --upgrade --force-reinstall ""pip==$PYTHON_PIP_VERSION"" \
    &amp;&amp; [ ""$(pip list |tac|tac| awk -F '[ ()]+' '$1 == ""pip"" { print $2; exit }')"" = ""$PYTHON_PIP_VERSION"" ] \
    \
    &amp;&amp; find /usr/local -depth \
        \( \
            \( -type d -a -name test -o -name tests \) \
            -o \
            \( -type f -a -name '*.pyc' -o -name '*.pyo' \) \
        \) -exec rm -rf '{}' + \
    &amp;&amp; apt-get purge -y --auto-remove $buildDeps \
    &amp;&amp; rm -rf /usr/src/python ~/.cache

RUN cd /usr/local/bin \
    &amp;&amp; { [ -e easy_install ] || ln -s easy_install-* easy_install; } \
    &amp;&amp; ln -s idle3 idle \
    &amp;&amp; ln -s pydoc3 pydoc \
    &amp;&amp; ln -s python3 python \
    &amp;&amp; ln -s python3-config python-config

RUN pip install uwsgi

RUN mkdir /config

RUN mkdir /logs

ENV HOME /var/www

WORKDIR /config

ADD conf/requirements.txt /config

RUN pip install -r /config/requirements.txt

ADD conf/wsgi.py /config

ADD conf/wsgi.ini /config

ADD conf/__init__.py /config

ADD start.sh /bin/start.sh

RUN chmod +x /bin/start.sh

EXPOSE 8000

ENTRYPOINT [""start.sh"", ""uwsgi"", ""--ini"", ""wsgi.ini""]
</code></pre>
","<p>trying to build</p>
<blockquote>
<p>docker build . -t tiktokapi:latest and get this error message:</p>
</blockquote>
<blockquote>
<p>Step 2/4 : RUN apt-get update &amp;&amp; apt-get install -y python3-pip
---&gt; Running in e46c478aadab
standard_init_linux.go:211: exec user process caused &quot;exec format error&quot;</p>
</blockquote>
<p>I am new to doker so how can I fix this.</p>
"
"42848279","How to mount volume from container to host in Docker?","<docker>","65447118","How to map a directory from docker container to host?","<docker>","<p>I have a question regarding the whole data volume process in Docker. Basically here are two Dockerfiles and their respective run commands:</p>

<p><strong>Dockerfile 1 -</strong></p>

<pre><code># Transmission over Debian
#
# Version 2.92

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install transmission-daemon transmission-common transmission-cli \
    &amp;&amp; mkdir -p /transmission/config /transmission/watch /transmission/download

ENTRYPOINT [""transmission-daemon"", ""--foreground""]
CMD [""--config-dir"", ""/transmission/config"", ""--watch-dir"", ""/transmission/watch"", ""--download-dir"", ""/transmission/download"", ""--allowed"", ""*"", ""--no-blocklist"", ""--no-auth"", ""--no-dht"", ""--no-lpd"", ""--encryption-preferred""]
</code></pre>

<p><strong>Command 1 -</strong></p>

<pre><code>docker run --name transmission -d -p 9091:9091 -v C:\path\to\config:/transmission/config -v C:\path\to\watch:/transmission/watch -v C:\path\to\download:/transmission/download transmission  
</code></pre>

<p><strong>Dockerfile 2 -</strong></p>

<pre><code># Nginx over Debian
#
# Version 1.10.3

FROM debian:testing

RUN apt-get update \
    &amp;&amp; apt-get -y install nano \
    &amp;&amp; apt-get -y install nginx

EXPOSE 80 443

CMD [""nginx"", ""-g"", ""daemon off;""]
</code></pre>

<p><strong>Command 2 -</strong></p>

<pre><code>docker run --name nginx -d -p 80:80 -v C:\path\to\config:/etc/nginx -v C:\path\to\html:/var/www/html nginx
</code></pre>

<p>So, the weird thing is that the first dockerfile and command works as intended. Where the docker daemon mounts a directory from the container to the host. So, I am able to edit the configuration files as I please and they will be persisted to the container on a restart.</p>

<p>However, as for the second dockerfile and command it doesn't seem to be working. I know if you go to the Docker Volume documentation it says that volume mounts are only intended to go one-way, from host-to-container, but how come the Transmission container works as intended, while the Nginx container doesn't?</p>

<p>P:S - I'm running <em>Microsoft Windows 10 Pro Build 14393</em> as my host and <em>Version 17.03.0-ce-win1 (10300) Channel: beta</em> as my Docker version.</p>

<p>Edit - Just to clarify. I'm trying to get the files from inside the Nginx container to the host. The first container (Transmission) works in that regard, by using a data volume. However, for the second container (Nginx), it doesn't want to copy the files in the mounted directory from inside the container to the host. Everything else is working though, it does successfully start.</p>
","<p>I have a docker container which has <code>/opt/boost/</code> directory. Is there any way that I can map it
to <code>/opt/boost</code> on my host machine and use its content?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","60276241","How to start container depent on other container's services in Docker Compose?","<docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p><li> My Docker Compose file have 4 service is <strong>web</strong>, <strong>mongodb</strong>, <strong>redis</strong>, and <strong>rabbitmq</strong>. The <strong>web</strong> service can work only if three other service work(container started and service in container also started).
<li> I had try to use <code>depends_on</code> code on docker-compose.yaml but it only check the other container status, not their service. So it still not work.</p>

<p>So is there any way to start container form other container's service status??</p>
"
"66992375","Kafka consumer.poll hangs with bitnami container","<python><docker><apache-kafka><kafka-consumer-api>","67021894","Kafka docker container doesn't respond to requests in network","<python><docker><apache-kafka><kafka-python>","<p>I have the latest bitnami kafka container installed on a remote server.</p>
<pre><code>[2021-04-07 18:05:38,263] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)
[2021-04-07 18:05:40,137] INFO Kafka version: 2.7.0 (org.apache.kafka.common.utils.AppInfoParser)
</code></pre>
<p>My kafka is configured so that I can have external connections.</p>
<pre><code>kafka:
image: 'bitnami/kafka:latest'
container_name: kafka
ports:
  - '9092:9092'
  - '9093:9093'
environment:
  - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  - ALLOW_PLAINTEXT_LISTENER=yes
  - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
  - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
  - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
  - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
</code></pre>
<p>ping and telnet to the ip address both work.</p>
<p>I am able to run a producer and send data in python.</p>
<pre><code>import kafka
from time import sleep
from json import dumps
from kafka import KafkaProducer
from kafka import KafkaConsumer

#Producer---------------------------------------------------------------
producer = KafkaProducer(bootstrap_servers=['192.xxx.xx.xx:9093'],
                     value_serializer=lambda x: 
                     dumps(x).encode('utf-8'))

producer.send('TestTopic1', value='MyTest')
</code></pre>
<p>But, I am unable to consume the data. The script hangs at consumer.poll and never changes lines.</p>
<pre><code>import kafka
from time import sleep
from json import dumps
from kafka import KafkaConsumer

# Consumer---------------------------------------------------------------
consumer = KafkaConsumer(
    'TestTopic1',
    bootstrap_servers=['192.xxx.xx.xx:9093'],
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    group_id='testgroup',
    value_deserializer=lambda x : loads(x.decode('utf-8')))

#I've tried both with group_id to None or with a group_id.

print('BEFORE subscribe: ')
consumer.subscribe(['TestTopic1'])

print('BEFORE poll: ')
# HANGS HERE!! Never gets to the print after
consumer.poll(timeout_ms=500)

print('AFTER POLL: ')
consumer.seek_to_beginning()

print('partitions of the topic: ', consumer.partitions_for_topic('TestTopic1'))

for msg in consumer:
    print(type(msg))
</code></pre>
<p>In the Kafka logs, I see the Topic getting created as well as other lines that I'm not quite sure what they mean.</p>
<pre><code>[2021-04-07 18:05:40,234] INFO [broker-1001-to-controller-send-thread]: Recorded new controller, from now on will use broker 1001 (kafka.server.BrokerToControllerRequestThread)
[2021-04-07 18:06:37,509] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment Map(23 -&gt; ArrayBuffer(1001), 32 -&gt; ArrayBuffer(1001), 41 -&gt; ArrayBuffer(1001), 17 -&gt; ArrayBuffer(1001), 8 -&gt; ArrayBuffer(1001), 35 -&gt; ArrayBuffer(1001), 44 -&gt; ArrayBuffer(1001), 26 -&gt; ArrayBuffer(1001), 11 -&gt; ArrayBuffer(1001), 29 -&gt; ArrayBuffer(1001), 38 -&gt; ArrayBuffer(1001), 47 -&gt; ArrayBuffer(1001), 20 -&gt; ArrayBuffer(1001), 2 -&gt; ArrayBuffer(1001), 5 -&gt; ArrayBuffer(1001), 14 -&gt; ArrayBuffer(1001), 46 -&gt; ArrayBuffer(1001), 49 -&gt; ArrayBuffer(1001), 40 -&gt; ArrayBuffer(1001), 13 -&gt; ArrayBuffer(1001), 4 -&gt; ArrayBuffer(1001), 22 -&gt; ArrayBuffer(1001), 31 -&gt; ArrayBuffer(1001), 16 -&gt; ArrayBuffer(1001), 7 -&gt; ArrayBuffer(1001), 43 -&gt; ArrayBuffer(1001), 25 -&gt; ArrayBuffer(1001), 34 -&gt; ArrayBuffer(1001), 10 -&gt; ArrayBuffer(1001), 37 -&gt; ArrayBuffer(1001), 1 -&gt; ArrayBuffer(1001), 19 -&gt; ArrayBuffer(1001), 28 -&gt; ArrayBuffer(1001), 45 -&gt; ArrayBuffer(1001), 27 -&gt; ArrayBuffer(1001), 36 -&gt; ArrayBuffer(1001), 18 -&gt; ArrayBuffer(1001), 9 -&gt; ArrayBuffer(1001), 21 -&gt; ArrayBuffer(1001), 48 -&gt; ArrayBuffer(1001), 3 -&gt; ArrayBuffer(1001), 12 -&gt; ArrayBuffer(1001), 30 -&gt; ArrayBuffer(1001), 39 -&gt; ArrayBuffer(1001), 15 -&gt; ArrayBuffer(1001), 42 -&gt; ArrayBuffer(1001), 24 -&gt; ArrayBuffer(1001), 6 -&gt; ArrayBuffer(1001), 33 -&gt; ArrayBuffer(1001), 0 -&gt; ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[2021-04-07 18:06:37,534] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2021-04-07 18:06:37,547] INFO Creating topic TestTopic1 with configuration {} and initial partition assignment Map(0 -&gt; ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[2021-04-07 18:06:37,557] INFO [KafkaApi-1001] Auto creation of topic TestTopic1 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2021-04-07 18:06:37,906] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2021-04-07 18:06:37,979] INFO [Log partition=__consumer_offsets-0, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2021-04-07 18:06:37,991] INFO Created log for partition __consumer_offsets-0 in /bitnami/kafka/data/__consumer_offsets-0 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; compact, flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 104857600, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:37,992] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2021-04-07 18:06:37,994] INFO [Partition __consumer_offsets-0 broker=1001] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,011] INFO [Log partition=__consumer_offsets-29, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
</code></pre>
<p>Then I have another series of this type of messages in my log.</p>
<pre><code>[2021-04-07 18:06:38,563] INFO Created log for partition __consumer_offsets-13 in /bitnami/kafka/data/__consumer_offsets-13 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; compact, flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 104857600, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:38,563] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2021-04-07 18:06:38,563] INFO [Partition __consumer_offsets-13 broker=1001] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,577] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,579] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,579] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
....
[2021-04-07 18:06:38,589] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 12 milliseconds, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,596] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 17 milliseconds, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2021-04-07 18:06:38,597] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 18 milliseconds, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
....
[2021-04-07 18:06:38,638] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(TestTopic1-0) (kafka.server.ReplicaFetcherManager)
[2021-04-07 18:06:38,643] INFO [Log partition=TestTopic1-0, dir=/bitnami/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2021-04-07 18:06:38,644] INFO Created log for partition TestTopic1-0 in /bitnami/kafka/data/TestTopic1-0 with properties {compression.type -&gt; producer, min.insync.replicas -&gt; 1, message.downconversion.enable -&gt; true, segment.jitter.ms -&gt; 0, cleanup.policy -&gt; [delete], flush.ms -&gt; 9223372036854775807, retention.ms -&gt; 604800000, segment.bytes -&gt; 1073741824, flush.messages -&gt; 9223372036854775807, message.format.version -&gt; 2.7-IV2, max.compaction.lag.ms -&gt; 9223372036854775807, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1048588, min.compaction.lag.ms -&gt; 0, message.timestamp.type -&gt; CreateTime, preallocate -&gt; false, index.interval.bytes -&gt; 4096, min.cleanable.dirty.ratio -&gt; 0.5, unclean.leader.election.enable -&gt; false, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, segment.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760}. (kafka.log.LogManager)
[2021-04-07 18:06:38,647] INFO [Partition TestTopic1-0 broker=1001] No checkpointed highwatermark is found for partition TestTopic1-0 (kafka.cluster.Partition)
[2021-04-07 18:06:38,647] INFO [Partition TestTopic1-0 broker=1001] Log loaded for partition TestTopic1-0 with initial high watermark 0 (kafka.cluster.Partition)
</code></pre>
<p>I don't see anything related to the consumer in here.</p>
<p>Note that this is only a dev server. We're supposed to use that as a proof of concept to see if Kafka works for us and see if we'll use it in prod.</p>
<p>Any help would be appreciated as we'd really like to be able to make it work and use it in production.</p>
","<p>I have set up a kafka docker container with ports mapped to the host windows machine.</p>
<p>compose-file:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.0.1
    hostname: zookeeper
    container_name: zookeeper
    restart: always
    ports:
      - &quot;2181:2181&quot;
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-server:6.0.1
    hostname: broker
    container_name: broker
    restart: always
    depends_on:
      - zookeeper
    ports:
      - &quot;9092:9092&quot;
      - &quot;9101:9101&quot;
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

  control-center:
    image: confluentinc/cp-enterprise-control-center:6.0.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - broker
    ports:
      - &quot;9021:9021&quot;
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021
</code></pre>
<p>I have also a simple producer and consumer scripts written in python:</p>
<pre><code>from kafka import KafkaProducer

topic = 'kontext-kafka'
bootstrap_servers = '192.168.1.103:9092'
producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

# Generate 100 messages
for _ in range(10):
    msg = f'Kontext kafka msg: {_}'
    future = producer.send(topic, msg.encode('utf-8'))
    print(f'Sending msg: {msg}')
    result = future.get(timeout=60)

metrics = producer.metrics()
print(metrics)
</code></pre>
<p>consumer:</p>
<pre><code>from kafka import KafkaConsumer

topic = 'kontext-kafka'
bootstrap_servers = '192.168.1.103:9092'
consumer = KafkaConsumer(
    topic, bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')
for msg in consumer:
    print(msg)
</code></pre>
<p>These scripts work on the host machine, but when I try to run these scripts on another computer, they can’t send or receive messages even when the broker is connected. In about 10 sec I receive this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;//main.py&quot;, line 12, in &lt;module&gt;
    result = future.get(timeout=60)
  File &quot;//venv/lib/python3.8/site-packages/kafka/producer/future.py&quot;, line 65, in get
    raise self.exception # pylint: disable-msg=raising-bad-type
kafka.errors.KafkaTimeoutError: KafkaTimeoutError: Batch for TopicPartition(topic='kontext-kafka', partition=0) containing 1 record(s) expired: 30 seconds have passed since batch creation plus linger time
</code></pre>
<p>I still can connect the controller-interface container via browser and &quot;netstat -ab&quot; command shows port 9092 as &quot;LISTENING&quot;.</p>
<p>How should I correctly configure my container in the host machine, so I could send and receive messages in my network?</p>
<p>version of myclient:<br />
kafka-python==2.0.2</p>
<p>host os:<br />
win 10</p>
<p>desktop in my network:<br />
Ubuntu 20.04.2</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","66552576","django and mysql not connect together in docker","<docker><docker-compose><dockerfile>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>i'm new in docker and very like to learn that and for this i created a django app that use mysql for save users but when run docker-compose up it's show me below error:</p>
<pre><code>WARNING: Found orphan containers (uservalidation_admin_1) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.
Starting uservalidation_db_1 ... done
Recreating uservalidation_web_1 ... done
Attaching to uservalidation_db_1, uservalidation_web_1
db_1   | 2021-03-09 16:22:33+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.
db_1   | 2021-03-09 16:22:33+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
db_1   | 2021-03-09 16:22:33+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.
db_1   | 2021-03-09T16:22:33.865176Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.23) starting as process 1
db_1   | 2021-03-09T16:22:34.262041Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
db_1   | 2021-03-09T16:22:36.803850Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
db_1   | 2021-03-09T16:22:36.807125Z 1 [ERROR] [MY-011096] [Server] No data dictionary version number found.
db_1   | 2021-03-09T16:22:36.807436Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
db_1   | 2021-03-09T16:22:36.807683Z 0 [ERROR] [MY-010119] [Server] Aborting
db_1   | 2021-03-09T16:22:37.332256Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.23)  MySQL Community Server - GPL.
uservalidation_db_1 exited with code 1
db_1   | 2021-03-09 16:22:48+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.
db_1   | 2021-03-09 16:22:49+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
db_1   | 2021-03-09 16:22:49+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.
db_1   | 2021-03-09T16:22:49.552755Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.23) starting as process 1
db_1   | 2021-03-09T16:22:49.595687Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
db_1   | 2021-03-09T16:22:50.785069Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
db_1   | 2021-03-09T16:22:50.838098Z 1 [ERROR] [MY-011096] [Server] No data dictionary version number found.
db_1   | 2021-03-09T16:22:50.838509Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
db_1   | 2021-03-09T16:22:50.838816Z 0 [ERROR] [MY-010119] [Server] Aborting
db_1   | 2021-03-09T16:22:51.381748Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.23)  MySQL Community Server - GPL.
web_1  | Watching for file changes with StatReloader
web_1  | Performing system checks...
web_1  | 
web_1  | System check identified no issues (0 silenced).
web_1  | Exception in thread django-main-thread:
web_1  | Traceback (most recent call last):
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
web_1  |     self.connection = self.get_new_connection(conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/mysql/base.py&quot;, line 234, in get_new_connection
web_1  |     return Database.connect(**conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/__init__.py&quot;, line 130, in Connect
web_1  |     return Connection(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/connections.py&quot;, line 185, in __init__
web_1  |     super().__init__(*args, **kwargs2)
web_1  | MySQLdb._exceptions.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
web_1  | 
web_1  | The above exception was the direct cause of the following exception:
web_1  | 
web_1  | Traceback (most recent call last):
web_1  |   File &quot;/usr/local/lib/python3.9/threading.py&quot;, line 954, in _bootstrap_inner
web_1  |     self.run()
web_1  |   File &quot;/usr/local/lib/python3.9/threading.py&quot;, line 892, in run
web_1  |     self._target(*self._args, **self._kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
web_1  |     fn(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/commands/runserver.py&quot;, line 121, in inner_run
web_1  |     self.check_migrations()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 459, in check_migrations
web_1  |     executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/executor.py&quot;, line 18, in __init__
web_1  |     self.loader = MigrationLoader(self.connection)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 53, in __init__
web_1  |     self.build_graph()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 216, in build_graph
web_1  |     self.applied_migrations = recorder.applied_migrations()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 77, in applied_migrations
web_1  |     if self.has_table():
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 55, in has_table
web_1  |     with self.connection.cursor() as cursor:
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 259, in cursor
web_1  |     return self._cursor()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 235, in _cursor
web_1  |     self.ensure_connection()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/utils.py&quot;, line 90, in __exit__
web_1  |     raise dj_exc_value.with_traceback(traceback) from exc_value
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
web_1  |     self.connection = self.get_new_connection(conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/mysql/base.py&quot;, line 234, in get_new_connection
web_1  |     return Database.connect(**conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/__init__.py&quot;, line 130, in Connect
web_1  |     return Connection(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/connections.py&quot;, line 185, in __init__
web_1  |     super().__init__(*args, **kwargs2)
web_1  | django.db.utils.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
db_1   | 2021-03-09 16:22:55+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
db_1   | 2021-03-09 16:22:55+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.
db_1   | 2021-03-09T16:22:55.963940Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.23) starting as process 1
db_1   | 2021-03-09T16:22:55.986407Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
</code></pre>
<p><strong>docker-compose</strong>:</p>
<pre><code>version: &quot;3.9&quot;

services: 
  db:
    image: mysql
    ports: 
      - &quot;3306:3306&quot;
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    environment:
      - MYSQL_DATABASE=userdatavalidation
      - MYSQL_USER=root
      - MYSQL_PASSWORD=mjs
      - MYSQL_ROOT_PASSWORD=mjs
      - MYSQL_HOST=localhost
      - MYSQL_TCP_PORT=3306

    volumes: 
      -  /UserValidation/my_mysql/datadir:/var/lib/mysql
      -  /UserValidation/my_mysql/mysqld:/var/run/mysqld
    

  web:
    build: .
    command: python manage.py runserver 0.0.0.0:8000
    volumes: 
      - ./:/UserValidation
      - /UserValidation/my_mysql/mysqld:/run/mysqld
    ports: 
      - &quot;8000:8000&quot;
    depends_on: 
      - db
</code></pre>
<p><strong>settings.py</strong>:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'userdatavalidation',
        'USER': 'root',
        'PASSWORD': 'mjs',
        'HOST': 'db',
        'PORT': '3306',
        'default-character-set': 'utf8',
        'OPTIONS': {
            'sql_mode': 'traditional',
        }
    }
}
</code></pre>
<p><strong>Dockerfile</strong>:</p>
<pre><code>FROM python:3
ENV PYTHONUNBUFFERED 1
WORKDIR /UserValidation
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
</code></pre>
<p>in above Dockerfile i use python:3 and then create a directoy in docker and copy everything in my directory to them, with pip install -r requirements.txt in install all dependencies.
but for install mysql-client in python i install :
<em>sudo apt-get install python3-dev default-libmysqlclient-dev build-essential</em>
then install mysql-client</p>
<p><strong>Show me the above error in Django ERROR</strong>:</p>
<pre><code>web_1  | Watching for file changes with StatReloader
web_1  | Performing system checks...
web_1  | 
db_1   | 2021-03-10T07:48:06.773044Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
db_1   | 2021-03-10T07:48:06.776334Z 1 [ERROR] [MY-011096] [Server] No data dictionary version number found.
db_1   | 2021-03-10T07:48:06.776700Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
db_1   | 2021-03-10T07:48:06.776988Z 0 [ERROR] [MY-010119] [Server] Aborting
web_1  | System check identified no issues (0 silenced).
web_1  | Exception in thread django-main-thread:
web_1  | Traceback (most recent call last):
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
web_1  |     self.connection = self.get_new_connection(conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/mysql/base.py&quot;, line 234, in get_new_connection
web_1  |     return Database.connect(**conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/__init__.py&quot;, line 130, in Connect
web_1  |     return Connection(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/connections.py&quot;, line 185, in __init__
web_1  |     super().__init__(*args, **kwargs2)
web_1  | MySQLdb._exceptions.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
web_1  | 
web_1  | The above exception was the direct cause of the following exception:
web_1  | 
web_1  | Traceback (most recent call last):
web_1  |   File &quot;/usr/local/lib/python3.9/threading.py&quot;, line 954, in _bootstrap_inner
web_1  |     self.run()
web_1  |   File &quot;/usr/local/lib/python3.9/threading.py&quot;, line 892, in run
web_1  |     self._target(*self._args, **self._kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
web_1  |     fn(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/commands/runserver.py&quot;, line 121, in inner_run
web_1  |     self.check_migrations()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 459, in check_migrations
web_1  |     executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/executor.py&quot;, line 18, in __init__
web_1  |     self.loader = MigrationLoader(self.connection)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 53, in __init__
web_1  |     self.build_graph()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 216, in build_graph
web_1  |     self.applied_migrations = recorder.applied_migrations()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 77, in applied_migrations
web_1  |     if self.has_table():
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 55, in has_table
web_1  |     with self.connection.cursor() as cursor:
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 259, in cursor
web_1  |     return self._cursor()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 235, in _cursor
web_1  |     self.ensure_connection()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/utils.py&quot;, line 90, in __exit__
web_1  |     raise dj_exc_value.with_traceback(traceback) from exc_value
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
web_1  |     self.connect()
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
web_1  |     self.connection = self.get_new_connection(conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
web_1  |     return func(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/mysql/base.py&quot;, line 234, in get_new_connection
web_1  |     return Database.connect(**conn_params)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/__init__.py&quot;, line 130, in Connect
web_1  |     return Connection(*args, **kwargs)
web_1  |   File &quot;/usr/local/lib/python3.9/site-packages/MySQLdb/connections.py&quot;, line 185, in __init__
web_1  |     super().__init__(*args, **kwargs2)
web_1  | django.db.utils.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db' (115)&quot;)
db_1   | 2021-03-10T07:48:07.315233Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.23)  MySQL Community Server - GPL.
uservalidation_db_1 exited with code 1
</code></pre>
<p><strong>django.db.utils.OperationalError: (2002, &quot;Can't connect to MySQL server on 'db'</strong></p>
<p><strong>I really do not know what to do anymore :(</strong></p>
<p><em>Thank you for your help</em></p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","65658723","localhost rejects requests to a simple Docker-run node app","<node.js><docker><express><dockerfile><docker-for-windows>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I am experimenting with Docker (actually Docker Toolbox on Windows 8.1) and a very simple node/express app, and I am getting refused requests from localhost.</p>
<p><strong>server.js</strong></p>
<pre><code>const express = require('express');
const app = express();

app.get('/', (req, res) =&gt; {
  res.send('hello!');
});

app.listen(3500, () =&gt; console.log('Running on 3500'))
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM node:latest
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3500
CMD [&quot;node&quot;, &quot;server.js&quot;]
</code></pre>
<p>I followed <a href=""https://docs.docker.com/get-started/nodejs/run-containers/"" rel=""nofollow noreferrer"">Docker's guide</a> here.</p>
<p>I am then running the following commands:</p>
<ul>
<li><code>docker build -t dockertest .</code></li>
<li><code>docker run dockertest -d -p 3500:3500</code></li>
</ul>
<p>And try to access http://localhost:3500 in the browser - <strong>connection refused!</strong></p>
<p>Trying <code>curl --request GET --url http://localhost:3500</code> - <strong>connection refused!</strong></p>
<p>The same app run directly un-Dockered works just fine.
What can be the reason for such a simple project to fail?
How can I troubleshoot it further?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","66806511","Docker start 2 containers specific order","<docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I'm experimenting with 2 containers. container_a depends on container_b, they both have a restart policy of unless-stopped. When the host machine is rebooted, the containers are both started, but in the wrong order.</p>
<p>I searched the internet and found that I need a docker-compose.yaml. I didn't understand how to solve this &quot;easy&quot; problem with a docker-compose.yaml and where to place the docker-compose.yaml to make it work when the hosts boots</p>
"
"43099116","Error ""The input device is not a TTY""","<docker><jenkins><jenkins-pipeline>","65678122","The input device is not a TTY when I'm using docker?","<docker><dockerfile><bitbucket-pipelines><tty>","<p>I am running the following command from my <code>Jenkinsfile</code>. However, I get the error <em>""The input device is not a TTY""</em>.</p>

<pre><code>docker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh
</code></pre>

<p>Is there a way to run the script from the <code>Jenkinsfile</code> without doing interactive mode?</p>

<p>I basically have a file called <code>script.sh</code> that I would like to run inside the Docker container.</p>
","<p>I'm currently running the following command:</p>
<pre><code>docker run -it -v $PWD:/e2e -w /e2e cypress/included:6.2.1 

</code></pre>
<p>Error Message:</p>
<pre><code>+ docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1
the input device is not a TTY
</code></pre>
<p>I'm pulling the cypress container from the <a href=""https://github.com/cypress-io/cypress-docker-images/tree/master/included/6.2.1"" rel=""nofollow noreferrer"">Cypress Github Account</a></p>
<p>My bitbucket-pipelines.yaml file:</p>
<pre><code>
image: atlassian/default-image:2

pipelines:
  default:
    - step:
        services:
          - docker
        script:
          - docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1

</code></pre>
<p>My dockerfile:</p>
<pre><code>
FROM cypress/browsers:node12.18.3-chrome87-ff82


ENV CI=1


ENV QT_X11_NO_MITSHM=1
ENV _X11_NO_MITSHM=1
ENV _MITSHM=0

# should be root user
RUN echo &quot;whoami: $(whoami)&quot;
RUN npm config -g set user $(whoami)

# command &quot;id&quot; should print:
# uid=0(root) gid=0(root) groups=0(root)
# which means the current user is root
RUN id

# point Cypress at the /root/cache no matter what user account is used
# see https://on.cypress.io/caching
ENV CYPRESS_CACHE_FOLDER=/root/.cache/Cypress
RUN npm install -g &quot;cypress@6.2.1&quot;
RUN cypress verify

# Cypress cache and installed version
# should be in the root user's home folder
RUN cypress cache path
RUN cypress cache list
RUN cypress info
RUN cypress version

# give every user read access to the &quot;/root&quot; folder where the binary is cached
# we really only need to worry about the top folder, fortunately
RUN ls -la /root
RUN chmod 755 /root

# always grab the latest NPM and Yarn
# otherwise the base image might have old versions
RUN npm i -g yarn@latest npm@latest

# should print Cypress version
# plus Electron and bundled Node versions
RUN cypress version
RUN echo  &quot; node version:    $(node -v) \n&quot; \
  &quot;npm version:     $(npm -v) \n&quot; \
  &quot;yarn version:    $(yarn -v) \n&quot; \
  &quot;debian version:  $(cat /etc/debian_version) \n&quot; \
  &quot;user:            $(whoami) \n&quot; \
  &quot;chrome:          $(google-chrome --version || true) \n&quot; \
  &quot;firefox:         $(firefox --version || true) \n&quot;

ENTRYPOINT [&quot;cypress&quot;, &quot;run&quot;]

</code></pre>
<p>If I run this command:</p>
<pre><code>docker run -it -v %cd%:/e2e -w /e2e cypress/included:6.2.1
</code></pre>
<p>It will tell me that it cannot find my son file, and I don't why. Could someone help me a little bit?</p>
<p>What should I have to do? &amp; Why I'm getting this issue, I guess I don't understand what is TTY means?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","66782563","Force docker to wait for a specific container during docker-compose restart","<docker><docker-compose>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I have container A and B, when I do <code>docker-compose restart</code> I want it to start container A first and then container B. I specified the <code>depends_on</code> directive, but it seems to be ignored (I see it starting up container B first).</p>
<p>Running version 3.4 of the YML file which has 2 services (A, B).</p>
<p>Thank you</p>
"
"43911793","Cannot connect to Go GRPC server running in local Docker container","<docker><go><grpc>","65539581","docker doesn't forward tcp connection to golang","<docker><go>","<p>I have a go grpc service. I'm developing on a mac, sierra. When running a grpc client against the service locally, all is well, but when running same client against same service in the docker container I get this error:</p>

<pre><code>transport: http2Client.notifyError got notified that the client transport was broken EOF.
FATA[0000] rpc error: code = Internal desc = transport is closing
</code></pre>

<p>this is my docker file:</p>

<pre><code>FROM golang:1.7.5

RUN mkdir -p /go/src/github.com/foo/bar
WORKDIR /go/src/github.com/foo/bar

COPY . /go/src/github.com/foo/bar
# ONBUILD RUN go-wrapper download
RUN go install

ENTRYPOINT /go/bin/bar

EXPOSE 51672
</code></pre>

<p>my command to build the image:</p>

<pre><code>docker build -t bar .
</code></pre>

<p>my command to launch the docker container:</p>

<pre><code>docker run -p 51672:51672 --name bar-container bar
</code></pre>

<h2>Other info:</h2>

<ul>
<li>client program runs fine from within the docker container</li>
<li>connecting to a regular rest endpoint works fine (http2, grpc related?)</li>
<li><p>running the <code>lsof</code> command in OS X yields these results</p>

<pre><code>$lsof -i | grep 51672
com.docke   984 oldDave   21u  IPv4 0x72779547e3a32c89      0t0  TCP *:51672 (LISTEN)
com.docke   984 oldDave   22u  IPv6 0x72779547cc0fd161      0t0  TCP localhost:51672 (LISTEN)
</code></pre></li>
<li><p>here's a snippet of my server code:</p>

<pre><code>server := &amp;Server{}
endpoint := ""localhost:51672""
lis, err := net.Listen(""tcp"", endpoint)
if err != nil {
    log.Fatalf(""failed to listen: %v"", err)
}

s := grpc.NewServer(grpc.Creds(creds))

pb.RegisterExpServiceServer(s, server)

// Register reflection service on gRPC server.
reflection.Register(s)

log.Info(""Starting Exp server: "", endpoint)

if err := s.Serve(lis); err != nil {
    log.Fatalf(""failed to serve: %v"", err)
}
</code></pre></li>
</ul>
","<p>I am new to both golang and docker.</p>
<p>I have this golang program.</p>
<pre class=""lang-golang prettyprint-override""><code>package main

import (
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;net&quot;
)

const (
    CONN_HOST = &quot;localhost&quot;
    CONN_PORT = &quot;3333&quot;
    CONN_TYPE = &quot;tcp&quot;
)

func main() {
    l, err := net.Listen(CONN_TYPE, CONN_HOST+ &quot;:&quot;+CONN_PORT)
    if err != nil {
        fmt.Println(&quot;Error listening: &quot;, err.Error())
        os.Exit(1)
    }
    defer l.Close()
    fmt.Println(&quot;Listening on &quot; + CONN_HOST + &quot;：&quot; + CONN_PORT)
    for {
        fmt.Println(&quot;about to accpet.&quot;)
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&quot;Error accepting: &quot;, err.Error())
            os.Exit(1)
        }
        fmt.Println(&quot;Accepted connection.&quot;)
        go handleRequest(conn)
    }
}

func handleRequest(conn net.Conn) {
    buf := make([]byte, 1024)
    _, err := conn.Read(buf)
    if err != nil {
        fmt.Println(&quot;Error reading: &quot;, err.Error())
    }
    fmt.Println(&quot;received &quot; + string(buf))
    conn.Write([]byte(&quot;echo from server: &quot; + os.Getenv(&quot;ServerName&quot;)))
    conn.Close()
}
</code></pre>
<p>It's a standard echo server. I tested it locally.</p>
<pre class=""lang-sh prettyprint-override""><code>echo &quot;test&quot; | netcat -v localhost 3333
</code></pre>
<p>It works fine. By fine I mean, it receives the echo from server, and server prints message out in the console.</p>
<p>Then I dockerized the golang program. My dockerfile goes like this</p>
<pre><code>FROM golang:alpine
ENV ServerName=test
WORKDIR /build
COPY . .
RUN go mod download
RUN go build -o main .
WORKDIR /dist
RUN cp /build/main .
EXPOSE 3333/tcp
CMD [&quot;/dist/main&quot;]
</code></pre>
<p>I built the image and run it with the following command.</p>
<pre class=""lang-sh prettyprint-override""><code>docker run -p 3333:3333/tcp --env ServerName=test echo-server
</code></pre>
<p>It starts up fine. But somehow, when I run the same commands</p>
<pre class=""lang-sh prettyprint-override""><code>echo &quot;test&quot; | netcat -v localhost 3333
</code></pre>
<p>It doesn't work. It shows</p>
<pre><code>Connection to localhost 3333 port [tcp/*] succeeded!
</code></pre>
<p>But that's it, no echo from the server, the server doesn't print message either.</p>
<p>To make sure the server runs ok, I went into the container and run the same commands. Again, it shows the echo message, and prints the message.</p>
<p>It seems the docker doesn't forward tcp connection into the container. I can't seem to figure out why this happens. Is it because I didn't write the dockefile correctly? Or is it because something is wrong with the environment?</p>
<p>P.S. My system is windows 10. Docker is the latest version of docker desktop, with WSL2 integration enabled. Please help me, thanks.</p>
"
"31776546","Why does Runtime.exec(String) work for some but not all commands?","<java><runtime.exec>","60436724","Unable to execute docker command through java","<java><arrays><docker>","<p>When I try to run <code>Runtime.exec(String)</code>, certain commands work, while other commands are executed but fail or do different things than in my terminal. Here is a self-contained test case that demonstrates the effect:</p>

<pre><code>public class ExecTest {
  static void exec(String cmd) throws Exception {
    Process p = Runtime.getRuntime().exec(cmd);

    int i;
    while( (i=p.getInputStream().read()) != -1) {
      System.out.write(i);
    }
    while( (i=p.getErrorStream().read()) != -1) {
      System.err.write(i);
    }
  }

  public static void main(String[] args) throws Exception {
    System.out.print(""Runtime.exec: "");
    String cmd = new java.util.Scanner(System.in).nextLine();
    exec(cmd);
  }
}
</code></pre>

<p>The example works great if I replace the command with <code>echo hello world</code>, but for other commands -- especially those involving filenames with spaces like here -- I get errors even though the command is clearly being executed:</p>

<pre><code>myshell$ javac ExecTest.java &amp;&amp; java ExecTest
Runtime.exec: ls -l 'My File.txt'
ls: cannot access 'My: No such file or directory
ls: cannot access File.txt': No such file or directory
</code></pre>

<p>meanwhile, copy-pasting to my shell:</p>

<pre><code>myshell$ ls -l 'My File.txt'
-rw-r--r-- 1 me me 4 Aug  2 11:44 My File.txt
</code></pre>

<p>Why is there a difference? When does it work and when does it fail? How do I make it work for all commands?</p>
","<p>I want to execute this command through java </p>

<p>""<strong>docker ps -q | xargs docker inspect --format '{{.HostConfig.NetworkMode}} {{ .Config.Image }} {{ .NetworkSettings.IPAddress }}</strong>'""</p>

<p>when I try to execute this command through java, I'm unable to get any output. 
I'm not getting any error or exception.</p>

<p>Below the code</p>

<pre><code>String command =""docker ps -q | xargs docker inspect --format '{{.HostConfig.NetworkMode}} {{ .Config.Image }} {{ .NetworkSettings.IPAddress }}'"";

process = Runtime.getRuntime().exec(command);
bufferedReaderObj = new BufferedReader(new InputStreamReader(process.getInputStream()));
                    while ((sLine = bufferedReaderObj.readLine()) != null) {
}
</code></pre>

<p>Please help me, I think it is because of curly braces in command.</p>
"
"45788253","Mounting development docker container directory on host","<docker>","65322794","How create a reverse bind volume with docker-compose?","<docker><docker-compose><containers><devops>","<p>I am using docker for software development, as I can bundle all my dependencies (compilers, libraries, ...) within a nice contained environment, without polluting the host.</p>

<p>The way I usually do things (which I guess is pretty common): I have a directory on the host that only contains the source code, which is mounted into a development container using a docker volume, where my software gets built and executed. Thanks to volumes being in sync, any changes in the source is reflected within the container.</p>

<p>Here is the pitfall: when using a code editor, software dependencies are considered broken as they are not accessible from the host. Therefore linting, etc... does not work.</p>

<p>I would like to be able to mount, let's say <code>/usr/local/include</code> from the container onto the host so that, be correctly configuring my editor, I can fix all the warnings.</p>

<p>I guess docker volume is not the solution here, because it would override the contained file system...</p>

<p>Also, I'm using Windows (no choice here) therefore my flow is:</p>

<p><code>Windows &gt; Samba &gt; Linux Host &gt; Docker &gt; Container</code></p>

<p>and I'd prefer not switching IDE (VS Code).</p>

<p>Any ideas? Thank you!</p>
","<p>maybe it's a stupid question but, i would like to know how can i bring the data,folders and code FROM my container in MY desktop to work with my IDE with docker-compose, is it possible?</p>
<p>Thank you in advance to everyone!</p>
"
"46711990","Error: pg_config executable not found when installing psycopg2 on Alpine in Docker","<python><postgresql><docker><psycopg2><alpine>","65468602","Problem creating a Docker container for Flask web application","<python><python-3.x><postgresql><docker><psycopg2>","<p>I'm trying to build a Flask app using Postgres with Docker. I'd like to connect to an AWS RDS instance of Postgres, but use Docker for my Flask app. However, when trying to set up <code>psycopg2</code> it runs into an error because it can't find <code>pg_config</code>. Here's the error:</p>
<pre><code>Building api
Step 1/5 : FROM python:3.6.3-alpine3.6
 ---&gt; 84c98ca3b5c5
Step 2/5 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 407c158f5ee4
Step 3/5 : COPY . .
 ---&gt; 966df18d329e
Step 4/5 : RUN pip install -r requirements.txt
 ---&gt; Running in 284cc97aeb63
Collecting aniso8601==1.3.0 (from -r requirements.txt (line 1))
  Downloading aniso8601-1.3.0.tar.gz (57kB)
Collecting click==6.7 (from -r requirements.txt (line 2))
  Downloading click-6.7-py2.py3-none-any.whl (71kB)
Collecting Flask==0.12.2 (from -r requirements.txt (line 3))
  Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting Flask-RESTful==0.3.6 (from -r requirements.txt (line 4))
  Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whl
Collecting Flask-SQLAlchemy==2.3.2 (from -r requirements.txt (line 5))
  Downloading Flask_SQLAlchemy-2.3.2-py2.py3-none-any.whl
Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting Jinja2==2.9.6 (from -r requirements.txt (line 7))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))
  Downloading MarkupSafe-1.0.tar.gz
Collecting psycopg2==2.7.3.1 (from -r requirements.txt (line 9))
  Downloading psycopg2-2.7.3.1.tar.gz (425kB)
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info/psycopg2.egg-info
    writing pip-egg-info/psycopg2.egg-info/PKG-INFO
    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
    Error: pg_config executable not found.

    Please add the directory containing pg_config to the PATH
    or specify the full executable path with the option:

        python setup.py build_ext --pg-config /path/to/pg_config build ...

    or with the pg_config option in 'setup.cfg'.

    ----------------------------------------
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-01lf5grh/psycopg2/
ERROR: Service 'api' failed to build: The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p>Here's my <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.6.3-alpine3.6

WORKDIR /usr/src/app

COPY . .

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p><strong>Many others seem to have a similar issue locally, but none of them involve using Docker. This seems like a Docker issue because I can set up a local virtual environment and the setup works just fine since I have Postgres installed locally and it's able to find my local <code>pg_config</code>.</strong></p>
<p>It appears that during the container build/setup, Docker is trying to find <code>pg_config</code> within the container. Is there a way to install a <code>pg_config</code> in the container, even though I won't be using a containerized instance of Postgres, but rather the instance on RDS?</p>
<p>Any and all suggestions on how to get around this are welcomed.</p>
","<p>I'm trying to build a Docker container for my Python web application that uses Flask and PostgreSQL.I work with Windows 10 and use Python 3.7.3, PostgreSQL 13 and PyCharm. Here is my Dockerfile:</p>
<pre><code>FROM python:3.7

WORKDIR /app

COPY requirements.txt ./
RUN pip install --trusted-host pypi.python.org -r requirements.txt

RUN export PYTHONPATH='${PYTHONPATH}:/app'

COPY . .

CMD [&quot;python&quot;, &quot;./run.py&quot;]
</code></pre>
<p>And requierements.txt file:</p>
<pre><code>click==7.1.2
Flask==1.1.2
Flask-SQLAlchemy==2.4.4
itsdangerous==1.1.0
Jinja2==2.11.2
MarkupSafe==1.1.1
psycopg2==2.8.6
SQLAlchemy==1.3.20
Werkzeug==1.0.1
</code></pre>
<p>All files look normal, but when I try to build a container, I get an error:</p>
<pre><code>D:\PyCharms\Module 6\app&gt;docker build -t doc .
[+] Building 91.6s (8/10)
 =&gt; [internal] load build definition from Dockerfile                                                                                                                                                             3.6s
 =&gt; =&gt; transferring dockerfile: 256B                                                                                                                                                                             1.6s
 =&gt; [internal] load .dockerignore                                                                                                                                                                                1.4s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                                                  0.1s
 =&gt; [internal] load metadata for docker.io/library/python:3.7                                                                                                                                                    9.4s
 =&gt; [1/6] FROM docker.io/library/python:3.7@sha256:bdd950df83006ce9e7f7f9cb878a2ca72e945f3cbd37faef07509c8510d22ba8                                                                                              0.3s
 =&gt; [internal] load build context                                                                                                                                                                                7.1s
 =&gt; =&gt; transferring context: 172.02kB                                                                                                                                                                            5.2s
 =&gt; CACHED [2/6] WORKDIR /app                                                                                                                                                                                    0.0s
 =&gt; CACHED [3/6] COPY requirements.txt ./                                                                                                                                                                        0.0s
 =&gt; ERROR [4/6] RUN pip install --trusted-host pypi.python.org -r requirements.txt                                                                                                                              70.7s
------
 &gt; [4/6] RUN pip install --trusted-host pypi.python.org -r requirements.txt:
#8 55.94 Collecting click==7.1.2
#8 56.54   Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
#8 56.90 Collecting Flask==1.1.2
#8 57.01   Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
#8 57.74 Collecting Flask-SQLAlchemy==2.4.4
#8 57.89   Downloading Flask_SQLAlchemy-2.4.4-py2.py3-none-any.whl (17 kB)
#8 58.55 Collecting itsdangerous==1.1.0
#8 58.66   Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
#8 58.96 Collecting Jinja2==2.11.2
#8 59.07   Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
#8 59.55 Collecting MarkupSafe==1.1.1
#8 59.65   Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)
#8 59.88 Collecting psycopg2==2.8.6
#8 60.00   Downloading psycopg2-2.8.6.tar.gz (383 kB)
#8 64.98     ERROR: Command errored out with exit status 1:
#8 64.98      command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-8yce09_u
#8 64.98          cwd: /tmp/pip-install-0g810mp0/psycopg2_ac2d50477f704274a8d0a974f85ef1b7/
#8 64.98     Complete output (7 lines):
#8 64.98     running egg_info
#8 64.98     creating /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info
#8 64.98     writing /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/PKG-INFO
#8 64.98     writing dependency_links to /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/dependency_links.txt
#8 64.98     writing top-level names to /tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/top_level.txt
#8 64.98     writing manifest file '/tmp/pip-pip-egg-info-8yce09_u/psycopg2.egg-info/SOURCES.txt'
#8 64.98     Error: b'strict.pm did not return a true value at /usr/bin/pg_config line 12.\n'
#8 64.98     ----------------------------------------
#8 64.99 ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
------
executor failed running [/bin/sh -c pip install --trusted-host pypi.python.org -r requirements.txt]: exit code: 1
</code></pre>
<p>Looks like there is some problem with installing psycopg(concretely, some trouble with pg_config file),
but I didn't find any useful information about that kind of error. Only one tip I found is that &quot;strict.pm&quot; is some kind of Perl file, that should end with &quot;1;&quot; line or something like that.
So my questions are:</p>
<ol>
<li>What is &quot;strict.pm&quot; file and what is connection between it and pg_config and psycopg?</li>
<li>How I can get rid of this kind of error?</li>
</ol>
<p>(P.S. I'm complete newbie in Docker and Postgre stuff and this is my first question, so sorry for possible mistakes)</p>
"
"33799885","How to stop all containers when one container stops with docker-compose?","<docker><docker-compose>","60671912","Docker-compose - wait for a container to exit","<docker><docker-compose><dockerfile>","<p>Up until recently, when one was doing <code>docker-compose up</code> for a bunch of containers and one of the started containers stopped, all of the containers were stopped. This is not the case anymore since <a href=""https://github.com/docker/compose/issues/741"" rel=""noreferrer"">https://github.com/docker/compose/issues/741</a> and this is a really annoying for us: We use docker-compose to run selenium tests which means starting application server, starting selenium hub + nodes, starting tests driver, then exiting when tests driver stops.</p>

<p>Is there a way to get back old behaviour?</p>
","<p>I have a docker-compose setup, where I have 4 services.  One of these services runs tests, and it then exits after the tests finish.  I would like the other services to listen to this service exiting, so that they can exit themselves automatically. How can I achieve this?</p>
"
"52904847","How do I copy variables between stages of multi stage Docker build?","<dockerfile><docker-build><docker-multi-stage-build>","65399483","docker - using global variable in Dockerfile that persists between intermediate images? (without --build-arg)","<docker><dockerfile>","<p>I've only seen examples of using COPY to copy files between stages of a multi stage Dockerfile, but is there a way to simply copy an ENV variable? My use case is to start out with a git image to just to get the commit hash that will be part of the build. The image I'm later building with hasn't got git.</p>

<p>I realise I could just pipe out the git hash to a file and use COPY but I'm just wondering if there's a cleaner way?</p>
","<p>Is there a way to have variable that persists between docker images, particularly between intermediate and main image which are all specified in Dockerfile?</p>
<p>I know I can use <code>--build-arg</code>, but I need variable that makes more sense to be saved via source control.</p>
<p>Particularly I would use variable to specify revision date. For a example:</p>
<pre><code>ARG DATE_REV=20201220
</code></pre>
<p>And this variable would be used in my intermediate image and then in main image, because same date is used to download specific packages per specified date.</p>
<p>I tried using <code>.env</code> file, <code>environment</code> in compose, defining arg/env inside Dockerfile, but in all cases either Dockerfile does not get that variable at all (I guess <code>.env</code> file for example is not meant for Dockerfile?) or it only persists in image that it was defined.</p>
<p>So it would be kind of silly if I would need to redefine same variable between images.</p>
"
"35406213","How to copy docker volume to local?","<docker><boot2docker><docker-volume>","66243623","How to switch from docker named volumes to path based volumes without losing the container data?","<docker><persistent-volumes>","<p>I create a docker volume ""hello"", and it contains some data . </p>

<p>How can i copy it to local ?</p>

<p><strong>First :</strong> </p>

<pre><code>kerydeMacBook-Pro:~ hu$ docker volume create --name hello
hello
</code></pre>

<p><strong>checking :</strong></p>

<pre><code>kerydeMacBook-Pro:~ hu$ docker volume ls
DRIVER              VOLUME NAME
local               hello
</code></pre>

<p><strong>volume ""hello"" inspect</strong></p>

<pre><code>kerydeMacBook-Pro:~ hu$  docker volume inspect hello
[
    {
        ""Name"": ""hello"",
        ""Driver"": ""local"",
        ""Mountpoint"": ""/mnt/sda1/var/lib/docker/volumes/hello/_data""
    }
]
</code></pre>

<p><strong>How can i copy volume ""hello"" to local ?</strong></p>

<blockquote>
  <p>I try :</p>
</blockquote>

<pre><code>kerydeMacBook-Pro:~ hu$  docker cp hello:/mnt/sda1/var/lib/docker/volumes/hello/_data /Users/hu/Desktop/12
Error response from daemon: no such id: hello
</code></pre>

<p><strong>It not works as expect !</strong></p>

<p>Who can help me ?</p>
","<p>Im runing several docker containers on my raspberry pi. All containers are using named volumes to store persistent data. But since I often need to edit config files etc from the docker volumes I prefer to use path based volumes instead of named volumes that are managed by docker.</p>
<p>I first tought that I just could copy all the content from</p>
<pre><code>/var/lib/docker/volumes/
</code></pre>
<p>to a folder on my home directory and remove all containers and rerun them with the new path based volumes.
But unfortunately this seems not to work. For example if I rerun portainer with the new path based volume (which is just the folder that I copied from /var/lib/docker/volumes/ ) I need to create a new user etc. as if portainer could not use the copied data. I already used chown to add permissions for the current user.</p>
<p>Hope someone can help.</p>
"
"53902507","unknown error: session deleted because of page crash from unknown error: cannot determine loading status from tab crashed with ChromeDriver Selenium","<python><selenium><google-chrome><selenium-webdriver><selenium-chromedriver>","65492402","Chrome Selenium Crashing with Docker","<python><docker><selenium><webdriver><selenium-chromedriver>","<p>I'm using InstaPy which use Python and Selenium. I start the script per Cron and from time to time it crashes. So it'r really irregular, sometimes it runs well through. I'v posted on GitHub Repo as well already but didn't get an answer there, so i'm asking here now if someone has an idea why.</p>

<p>It's a digital ocean ubuntu server and i'm using it on headless mode. The driver version are visible on the log. here are error messages:</p>

<pre><code>ERROR [2018-12-10 09:53:54] [user]  Error occurred while deleting cookies from web browser!
b'Message: invalid session id\n  (Driver info: chromedriver=2.44.609551 (5d576e9a44fe4c5b6a07e568f1ebc753f1214634),platform=Linux 4.15.0-42-generic x86_64)\n'
Traceback (most recent call last):
  File ""/root/InstaPy/instapy/util.py"", line 1410, in smart_run
    yield
  File ""./my_config.py"", line 43, in &lt;module&gt;
    session.follow_user_followers(['xxxx','xxxx','xxxx','xxxx'], amount=100, randomize=True, interact=True)
  File ""/root/InstaPy/instapy/instapy.py"", line 2907, in follow_user_followers
    self.logfolder)
  File ""/root/InstaPy/instapy/unfollow_util.py"", line 883, in get_given_user_followers
    channel, jumps, logger, logfolder)
  File ""/root/InstaPy/instapy/unfollow_util.py"", line 722, in get_users_through_dialog
    person_list = dialog_username_extractor(buttons)
  File ""/root/InstaPy/instapy/unfollow_util.py"", line 747, in dialog_username_extractor
    person_list.append(person.find_element_by_xpath(""../../../*"")
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webelement.py"", line 351, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webelement.py"", line 659, in find_element
    {""using"": by, ""value"": value})['value']
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webelement.py"", line 633, in _execute
    return self._parent.execute(command, params)
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: headless chrome=70.0.3538.110)
  (Driver info: chromedriver=2.44.609551 (5d576e9a44fe4c5b6a07e568f1ebc753f1214634),platform=Linux 4.15.0-42-generic x86_64)

During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
  File ""/root/InstaPy/instapy/instapy.py"", line 3845, in end
    self.browser.delete_all_cookies()
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 878, in delete_all_cookies
    self.execute(Command.DELETE_ALL_COOKIES)
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: chrome not reachable
  (Session info: headless chrome=71.0.3578.80)
  (Driver info: chromedriver=2.44.609551 (5d576e9a44fe4c5b6a07e568f1ebc753f1214634),platform=Linux 4.15.0-42-generic x86_64)
</code></pre>

<p>Any idea what the reason could be and how to solve it?</p>
","<p>I've been looking everywhere for an answer with no luck. I've got a Python script that works fine locally running Selenium with a Chrome Driver, which I'm now testing in Docker.</p>
<p>The script is running fine until the webdriver kicks in, at which point I get the error message <em>&quot;unknown error: session deleted because of page crash&quot;</em>.</p>
<p>I've seen some suggestions that extending the memory allowance could fix this but I've had no luck. To be honest my knowldege of the docker-compose file is quite poor so any suggestions on what I could change with this file would be helpful too (see below).</p>
<pre><code>version: '3.8'

services:
  lambda:
    build: .
    environment:
      - PYTHONPATH=/var/task/src:/var/task/lib
      - PATH=/var/task/bin
      - AWS_BUCKET_NAME=REMOVED
      - AWS_ACCESS_KEY_ID=REMOVED
      - AWS_SECRET_ACCESS_KEY=REMOVED
      - AWS_REGION=ap-southeast-2
    volumes:
      - ./src/:/var/task/src/
</code></pre>
"
"35931579","How can I install lxml in docker","<python><docker><lxml><dockerfile>","66779632","Docker | Problem with installing lxml on Python 3.8","<docker><dockerfile><lxml><alpine>","<p>I want to deploy my python project in docker, I wrote <code>lxml&gt;=3.5.0</code> in the requirments.txt as the project needs lxml. Here is my dockfile:</p>

<pre><code>FROM gliderlabs/alpine:3.3
RUN set -x \
    &amp;&amp; buildDeps='\
        python-dev \
        py-pip \
        build-base \
    ' \
    &amp;&amp; apk --update add python py-lxml $buildDeps \
    &amp;&amp; rm -rf /var/cache/apk/* \
    &amp;&amp; mkdir -p /app
ENV INSTALL_PATH /app
WORKDIR $INSTALL_PATH
COPY requirements-docker.txt ./
RUN pip install -r requirements.txt
COPY . .
RUN apk del --purge $buildDeps
ENTRYPOINT [""celery"", ""-A"", ""tasks"", ""worker"", ""-l"", ""info"", ""-B""]
</code></pre>

<p>I got this when I deploy it to docker:</p>

<pre><code>*********************************************************************************
Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
*********************************************************************************
error: command 'gcc' failed with exit status 1
----------------------------------------
Rolling back uninstall of lxml
</code></pre>

<p>I though it was because 'python-dev' and 'python-lxml', then I edited the dockfile like this:</p>

<pre><code>WORKDIR $INSTALL_PATH
COPY requirements-docker.txt ./
RUN apt-get build-dev python-lxml
RUN pip install -r requirements.txt
</code></pre>

<p>It did not work, and I got another error:</p>

<pre><code>---&gt; Running in 73201a0dcd59
/bin/sh: apt-get: not found
</code></pre>

<p>How can I install lxml correctly in docker?</p>
","<p>I'm building a docker container that uses <a href=""https://hub.docker.com/r/tiangolo/uwsgi-nginx-flask"" rel=""nofollow noreferrer"">this</a> image. (uwsgi-nginx-flask) This is based on an Alpine Linux image.</p>
<p>The latest python version available in this container is python 3.8.
I developed my flask application on Python 3.9. One of the python package dependencies is <code>lxml</code>. When I tried installing this, it throws the error given below.</p>
<pre><code>  cc -I/usr/include/libxml2 -c /tmp/xmlXPathInitnq3gdzvk.c -o tmp/xmlXPathInitnq3gdzvk.o
  unable to execute 'cc': No such file or directory
  *********************************************************************************
  Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
  *********************************************************************************
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for lxml

unable to execute 'cc': No such file or directory
*********************************************************************************
Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?
*********************************************************************************
error: command 'gcc' failed with exit status 1
</code></pre>
<p>This is my Dockerfile</p>
<pre><code>FROM tiangolo/uwsgi-nginx-flask:python3.8-alpine
RUN apk add libxml2-dev 
RUN apk --update add bash nano
RUN apk --update add --no-cache python3
RUN python --version
RUN mkdir -p /var/www/app/
RUN pip3 install --no-cache --upgrade pip setuptools
WORKDIR /var/www/app/
COPY . /var/www/app/
ENV STATIC_URL /static
ENV STATIC_PATH /var/www/app/static
RUN pip3 install -r requirements.txt
</code></pre>
<p>You can see that I even tried to install <code>libxml2</code> on the alpine image as it is considered a dependency. But that doesn't work.</p>
"
"36283908","Re-using environment variables in docker-compose.yml","<docker><docker-compose><environment-variables>","60420088","Pass same variables to multiple services in docker-compose without duplicating instructions?","<docker><docker-compose><dockerfile>","<p>Is it possible to re-use environment variables that are shared among multiple containers?</p>
<p>The idea is to avoid duplication, as illustrated in this example:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '2'

services:

  db:
    image: example/db
    ports:
      - &quot;8443:8443&quot; 
    container_name: db
    hostname: db
    environment:
      - USER_NAME = admin 
      - USER_PASSWORD = admin 

svc:
  image: example/svc
  depends_on:
    - db
  ports:
    - &quot;9443:9443&quot;
  container_name: svc
  hostname: svc
  environment:
    - DB_URL = https://db:8443
    - DB_USER_NAME = admin
    - DB_USER_PASSWORD = admin 
</code></pre>
","<p>I have two services in my docker-compose file. Both of them need the same set of environment variables. Is there a better way to do this than duplicating instructions?</p>

<blockquote>
  <p>PS: I have stored my variables in the ""<code>.env</code>"" file, which resides in the same directory as the docker-compose file.</p>
</blockquote>

<p><strong>example docker-compose file:</strong></p>

<pre><code>version: '3'

services: 
     service_1: 
           build: './s_1/'
           environment: 
                - variable_1 = ${variable_1}
                - variable_2 = ${variable_2}
                - variable_x = ${variable_y}
     service_2:
           build: './s_2/'
           environment: 
                - variable_1 = ${variable_1}
                - variable_2 = ${variable_2}
                - variable_x = ${variable_y}
</code></pre>
"
"36996046","How to prevent Dockerfile caching git clone","<git><docker><dockerfile>","66566713","How to rebuild docker image when remote repository changes","<git><docker><dockerfile>","<p>I have a Dockerfile trying to package and deploy a web app to a container. The code of app fetches from git repository during Docker image building.
Here's the Dockerfile snapshot:</p>

<pre><code>........
RUN git clone --depth=1 git-repository-url $GIT_HOME/
RUN mvn package -Dmaven.test.skip
........
</code></pre>

<p>I want the docker do not cache the step of <code>RUN git clone --depth=1 git-repository-url $GIT_HOME/</code> so that the on-going updated on the the repository can be reflected on the Docker image building. Is it possible to a achieve that?</p>
","<p>I build a docker image containing the Foo package, which is built from its master version pulled from its git repo:</p>
<pre><code>RUN git clone git://my.git.server/Foo.git
RUN (configure, build and install Foo)
</code></pre>
<p>Every time the image is built I'd like the Docker build cache to be invalidated if a new commit has been pushed into the Foo repository, in order to always build the latest version. On the contrary, if the master version is the same as the last time the image was built then the build cache should be used to avoid an unnecessary rebuild.</p>
<p>In its current form, my Dockerfile never needs to be changed so the build cache is never invalidated. Given my poor skills with Docker I can't figure out a possible modification to achieve my desired behavior, so I'd need help with this. Thanks in advance.</p>
<p>Edit: as suggested in the linked question the desired behavior can be achieved with a trick: modifying the Dockerfile as</p>
<pre><code>ARG FOO_MASTER_COMMIT
RUN git clone git://my.git.server/Foo.git
RUN (configure, build and install Foo)
</code></pre>
<p>and then launching the build as:</p>
<pre><code>git clone git://my.git.server/Foo.git
DOCKER_BUILDKIT=1 docker build --build-arg FOO_MASTER_COMMIT=$(git -C Foo rev-parse HEAD) -t myimage:ver .
</code></pre>
<p>does the job.</p>
"
"37439887","How to access the metadata of a docker container from a script running inside the container?","<bash><docker><dockerfile>","66256352","How to get docker image labels in runtime","<docker>","<p>I am trying to understand whether it is possible to read the metadata (Labels, in particular) properties of a container using a bash script.</p>

<p>For instance, if there is a Dockerfile like:</p>

<pre><code>FROM busybox
LABEL abc = abc_value1
</code></pre>

<p>And, if I build and run an image based on the file above, like so:</p>

<pre><code>docker build . -t image1
docker run -ti image1 /bin/bash
</code></pre>

<p>Is there any way to access the value of the ""abc"" label inside the bash shell?  If so, how?</p>
","<p>How to get docker image labels in runtime?</p>
<p>Or get <code>docker inspect image</code> info when the image instantiation is running .</p>
<p>thanks!</p>
"
"3315730","What is the difference between the operating system and the kernel?","<operating-system><kernel>","54293233","Is the OS and kernel different things?","<linux><docker><linux-kernel><operating-system>","<p>I do not understand the difference between operating system and kernel. Can someone please explain it?</p>
","<p>In various docker posts there seems to differentiate between an OS and the kernel.
I always thought these are interchangeable terms.<br>
Is there a difference between a host OS an application is running and the kernel?</p>
"
"54269442","Why does docker create empty node_modules and how to avoid it?","<node.js><docker>","65395132","Use of anonymous volume to prevent Docker container to create folder locally not working","<docker><docker-compose><docker-volume>","<p>There are some <a href=""https://stackoverflow.com/questions/39651908/why-node-modules-is-empty-after-docker-build"">similar questions</a> but they haven't answered why docker creates the empty <code>node_modules</code> directory in the repo even though the dockerfile is setup to hold node_modules in the container?</p>

<p>I'm interested to know why directory is created on the host empty, give that yarn already installs packages inside the container within <code>node_modules</code> and how to avoid it.</p>

<pre><code>## Dockerfile

FROM node:8.11.4-alpine
RUN apk update &amp;&amp; apk add yarn
RUN yarn global add nodemon
WORKDIR /usr/app

COPY package.json yarn.lock /usr/app/
RUN yarn

EXPOSE 3000

</code></pre>

<pre><code>## docker-compose.yml

version: ""3.2""

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    command: nodemon index.js
    volumes:
      - .:/usr/app
      - /usr/app/node_modules
    ports:
      - ""3000:3000""

</code></pre>
","<p>As I read in this tutorial <a href=""https://mherman.org/blog/dockerizing-a-react-app/"" rel=""nofollow noreferrer"">https://mherman.org/blog/dockerizing-a-react-app/</a> it's possible to mount an anonymous volume into a subpath of a mounted volume in order to prevent the changes in that subpath to be reflected back to the actual volume. Is that correct? Because it's not working for me.</p>
<p>I'm trying to dockerize a react app by mounting the <code>src</code> folder into the container and building everything their. But I don't want the <code>node_modules</code> folder that is created during the build step to be reflected back in the mounted <code>src</code>.</p>
<p>My <code>Dockerfile.dev</code> looks like this:</p>
<pre><code>FROM node:15.4.0-alpine3.12

# set working directory
ARG APP_PATH=/app
WORKDIR $APP_PATH

# add `/app/node_modules/.bin` to $PATH
ENV PATH $APP_PATH/node_modules/.bin:$PATH

# install app dependencies
COPY package*.json ./
RUN npm install --silent

# add app
COPY . ./

# start app
EXPOSE 3000
CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;start&quot; ]
</code></pre>
<p>This is my <code>docker-compose.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.8&quot;
services:
    frontend:
        build: 
            context: ./frontend
            dockerfile: ../docker/Dockerfile.dev
        image: frontend_react
        ports: 
            - 4000:3000
        volumes:
            - ./frontend:/app
            - /app/node_modules
        environment: 
            - NODE_ENV=development
</code></pre>
<p>How can I prevent that the <code>node_modules</code> folder shows up on my local machine?
Should I even prevent the <code>node_modules</code> folder from being reflected back to my machine?</p>
"
"4760215","Running shell command and capturing the output","<python><shell><subprocess>","54156050","Output of os.system in Python and use it to create a text node in a xml file","<xml><linux><python-3.x><bash><docker>","<p>I want to write a function that will execute a shell command and return its output <strong>as a string</strong>, no matter, is it an error or success message. I just want to get the same result that I would have gotten with the command line.</p>

<p>What would be a code example that would do such a thing?</p>

<p>For example:</p>

<pre><code>def run_command(cmd):
    # ??????

print run_command('mysqladmin create test -uroot -pmysqladmin12')
# Should output something like:
# mysqladmin: CREATE DATABASE failed; error: 'Can't create database 'test'; database exists'
</code></pre>
","<p>I want to exec the command <code>docker ps -q</code> using python <code>os.system</code> function and then get its output to use it to create xml text node.</p>

<p>I tried <code>xml.createTextNode(os.system(""docker ps -q"")</code>:</p>

<pre class=""lang-python prettyprint-override""><code>  6 from xml.dom import minidom
  7 import os
  8
  9 xml = minidom.Document()
 10
 11 rootElem = xml.createElement('containers')
 12
 13 dataElem = xml.createElement('data')
 14
 15 idElem = xml.createElement('id')
 16 idElem.appendChild(xml.createTextNode(os.system(""docker ps -q"")))
</code></pre>

<p>But it gives me this error:</p>

<pre class=""lang-none prettyprint-override""><code> File ""scriptCreateXML.py"", line 16, in &lt;module&gt;
    idElem.appendChild(xml.createTextNode(os.system(""docker ps -q"")))
  File ""/usr/lib/python3.6/xml/dom/minidom.py"", line 1658, in createTextNode
    raise TypeError(""node contents must be a string"")
TypeError: node contents must be a string
</code></pre>

<p>I expect the output of this </p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" ?&gt;
&lt;containers&gt;
    &lt;data&gt;
        &lt;id&gt;some id&lt;/id&gt;
    &lt;/data&gt;
&lt;/containers&gt;
</code></pre>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","60456516","How can I automatically start cron in a Docker container that is running PHP?","<php><linux><docker><cron>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>I'm trying to run some PHP scripts automatically and want to have cron running once the container starts. Below is my current Dockerfile but upon running the <code>docker-compose up -d</code> command, the PHP container crashes immediately and I get the errors <code>Cannot start service composer: Cannot link to a non running container</code> and <code>Cannot start service apache: Cannot link to a non running container</code> which makes sense, because the PHP container already crashed. I've also tried <code>CMD [""cron"", ""-f""]</code> in place of <code>CMD cron start &amp;&amp; tail -f /var/log/cron.log</code> and all the containers run, and the test job is executed, but I get a 
503 service unavailable error when trying to access the app. From what I've read this is because cron is running in the foreground and blocking PHP.</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM php:7.4.2-fpm-buster

RUN apt-get update &amp;&amp; apt-get install -y \
        freetds-bin \
        freetds-dev \
        freetds-common \
        libct4 \
        libsybdb5 \
        tdsodbc \
        libfreetype6-dev \
        libjpeg62-turbo-dev \
        libmcrypt-dev \
        libpng-dev \
        libldap2-dev \
        libpq-dev \
        zlib1g-dev \
        libc-client-dev \
        libzip-dev \
        zip \
        cron

RUN ln -s /usr/lib/x86_64-linux-gnu/libsybdb.a /usr/lib/
RUN ln -s /usr/lib/x86_64-linux-gnu/libpq-fe.h /usr/lib/

RUN docker-php-ext-install pdo pdo_mysql pdo_dblib pdo_pgsql gd zip

RUN apt-get install unixodbc unixodbc-dev -y \
 &amp;&amp; docker-php-ext-configure pdo_odbc --with-pdo-odbc=unixODBC,/usr \
 &amp;&amp; docker-php-ext-install pdo_odbc

COPY ./freetds.conf /etc/freetds/freetds.conf
COPY ./odbc.ini /etc/odbc.ini
COPY ./odbcinst.ini /etc/odbcinst.ini
COPY ./php.ini /usr/local/etc/php

# Install Composer
RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer

RUN mkdir /var/www/html/files &amp;&amp; chown -R www-data:www-data /var/www/html/files &amp;&amp; chmod -R 755 /var/www/html/files
RUN mkdir /var/www/html/exports &amp;&amp; chown -R www-data:www-data /var/www/html/exports &amp;&amp; chmod -R 755 /var/www/html/exports
RUN mkdir /var/www/html/scripts &amp;&amp; chown -R www-data:www-data /var/www/html/scripts &amp;&amp; chmod -R 755 /var/www/html/scripts

# copying cronjobs, a file I created with a test cronjob to /etc
COPY ./cronjobs /etc
# using the cronjobs file I created as the crontab
RUN crontab /etc/cronjobs

CMD cron start &amp;&amp; tail -f /var/log/cron.log

EXPOSE 9000
</code></pre>

<p><strong>docker-compose.yml</strong> (some info redacted ... the xxx parts)</p>

<pre><code>version: ""3.7""
services:
  php:
    build: './php/'
    links:
      - 'mysql'
    network_mode: ""bridge""
    ports:
      - ""xxxx:9000""
    volumes:
      - ./www/:/var/www/html/
      - files:/var/www/html/files
  apache:
    build: './apache/'
    depends_on:
      - php
    links:
      - 'php'
    network_mode: ""bridge""
    ports:
      - ""xxxx:80""
    volumes:
      - ./www/:/var/www/html/
      - files:/var/www/html/files
  mysql:
    image: mysql:latest
    command: --init-file /docker-entrypoint-initdb.d/init.sql
    environment:
      MYSQL_ROOT_PASSWORD: 'xxx'
      MYSQL_USER: 'xxx'
      MYSQL_PASS: 'xxx'
    ports:
      - ""xxxx:3306""
    network_mode: ""bridge""
    volumes:
      - data:/var/lib/mysql
      - ./mysql/init:/docker-entrypoint-initdb.d
  phpmyadmin:
    depends_on: 
      - mysql
    environment:
      PMA_HOST: mysql
      PMA_PORT: 3306
    image: phpmyadmin/phpmyadmin
    links:
      - mysql
    network_mode: ""bridge""
    ports:
      - ""xxxx:80""
  composer:
    container_name: composer
    depends_on:
      - php
    links:
      - 'php'
    image: composer
    network_mode: ""bridge""
    volumes:
      - ./www:/var/www/html/
    command: composer install --ignore-platform-reqs &amp;&amp; composer require phpoffice/phpspreadsheet
networks:
  default:
    external:
      name: bridge
volumes:
  data:
  files:
</code></pre>

<p><strong>cronjobs file</strong></p>

<pre><code># Edit this file to introduce tasks to be run by cron.
#
# Each task to run has to be defined through a single line
# indicating with different fields when the task will be run
# and what command to run for the task
#
# To define the time you can provide concrete values for
# minute (m), hour (h), day of month (dom), month (mon),
# and day of week (dow) or use '*' in these fields (for 'any').
#
# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
#
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
#
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
#
# For more information see the manual pages of crontab(5) and cron(8)
#
# m h  dom mon dow   command

42 22 * * * /usr/local/bin/php /var/www/html/scripts/test.php
</code></pre>
"
"10664244","Django: How to manage development and production settings?","<python><django>","54307396","Django separate settings files in Docker","<python><django><docker><docker-compose>","<p>I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.</p>

<p>It would be great to know the following:</p>

<ul>
<li>How best to deal with development and production settings. </li>
<li>How to keep apps such as django-debug-toolbar only in a development environment.</li>
<li>Any other tips and best practices for development and deployment settings.</li>
</ul>
","<p>Following <a href=""https://medium.com/@ayarshabeer/django-best-practice-settings-file-for-multiple-environments-6d71c6966ee2"" rel=""nofollow noreferrer"">multiple</a> <a href=""https://blog.apptension.com/2017/11/09/django-settings-for-multiple-environments/"" rel=""nofollow noreferrer"">articles</a> <a href=""https://simpleisbetterthancomplex.com/tips/2017/07/03/django-tip-20-working-with-multiple-settings-modules.html"" rel=""nofollow noreferrer"">around</a> the web, I've split my Django 1.11 <code>settings.py</code> file into multiple files, <code>base.py</code>, <code>local.py</code>, <code>staging.py</code>, <code>production.py</code>, in a <code>settings</code> folder.</p>

<p>The Django app runs in a Docker container.</p>

<p><strong>docker-compose.yml</strong>:</p>

<pre><code>django:
    build:
      context: .
    env_file: .env
    command: [""/wait-for-it.sh"", ""mariadb:3306"", ""--"", ""/gunicorn.sh""]
</code></pre>

<p>In the <code>Dockerfile</code> I export the <code>DJANGO_SETTINGS_MODULE</code> as the <code>local</code> settings file (which in turn imports the <code>base.py</code> settings file).</p>

<p><strong>Dockerfile</strong>:</p>

<pre><code>FROM python:3.6.4
RUN mkdir /app
ADD requirements/*.txt app/requirements/
RUN pip install -r mira/requirements/dev.txt
ADD . /app
RUN export DJANGO_SETTINGS_MODULE=app.settings.local
RUN export DJANGO_SECRET_KEY=dummy-secret-key
COPY ./docker/web/entrypoint.sh ./docker/web/gunicorn.sh ./docker/web/wait-for-it.sh ./docker/web/logging.conf ./scripts/setup.sh /
RUN chmod +x /entrypoint.sh /gunicorn.sh /wait-for-it.sh /setup.sh
WORKDIR ./app
</code></pre>

<p><strong>settings/base.py</strong>:</p>

<pre><code>SECRET_KEY = os.environ['DJANGO_SECRET_KEY']
</code></pre>

<p><strong>.env</strong>:</p>

<pre><code>DJANGO_SECRET_KEY=FOOBAR
</code></pre>

<p>When I try to up the container, I get this error:</p>

<blockquote>
  <p>The SECRET_KEY setting must not be empty.</p>
</blockquote>
"
"55034727","What does . mean in docker? Does it mean the current working directory of the image or the local machine?","<docker>","65343923","COPY . . in Dockerfile doing?","<visual-studio><docker><.net-core>","<p>I am confused about whether <code>.</code> means that it's a shortened abbreviation of the current directory of the image or if it's the current working directory on the local machine. Or is it the same meaning of <code>.</code> in most console commands like essentially selecting all in the current directory.</p>

<pre><code>COPY somecode.java .

#copy the rest of the code

COPY . .
</code></pre>

<p>The <code>.</code> also seems to mean find the docker file in the current directory. </p>

<pre><code>docker build -t image-tag .
</code></pre>
","<p>I used Visual Studio to add Dockerfile to my Dotnet Core Web API project.</p>
<pre><code>FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build
WORKDIR /src
COPY App.WebAPI/App.WebAPI.csproj App.WebAPI/
COPY App.Data/App.Data.csproj App.Data/
RUN dotnet restore &quot;App.WebAPI/App.WebAPI.csproj&quot;
COPY . .
WORKDIR &quot;/src/App.WebAPI&quot;
RUN dotnet build &quot;App.WebAPI.csproj&quot; -c Release -o /app/build

FROM build AS publish
RUN dotnet publish &quot;App.WebAPI.csproj&quot; -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [&quot;dotnet&quot;, &quot;App.WebAPI.dll&quot;]
</code></pre>
<p>What does the following line actually do?</p>
<pre><code>COPY . .
</code></pre>
"
"37526509","How to install pdo driver in php docker image?","<php><mysql><pdo><docker>","60384378","PDO could not find driver ( DOCKER )","<php><mysql><docker><pdo>","<p>I am using as a base the <a href=""https://hub.docker.com/_/php/"" rel=""noreferrer"">php docker container</a> with the tag:</p>



<pre class=""lang-none prettyprint-override""><code>php:5.6-apache
</code></pre>

<p>I linked it with a basic <code>mysql:5.6</code> image which I can reach at the host <code>mysql</code>. I created a DB, and filled a table with basic values.</p>

<p>Yet trying to access my app, I get:</p>

<pre class=""lang-none prettyprint-override""><code>Fatal error: Uncaught exception 'PDOException' with message
could not find driver' in /var/www/html/index.php:30 
Stack trace: #0 [internal function]: 
PDO-&gt;__construct('mysql:host=mysq...', 'root', 'root', Array) 
#1 [internal function]: Phalcon\Db\Adapter\Pdo-&gt;connect(Array)
#2 /var/www/html/index.php(30): Phalcon\Db\Adapter\Pdo-__construct(Array)
#3 [internal function]: {closure}()
#4 [internal function]: Phalcon\Di\Service-&gt;resolve(NULL, Object(Phalcon\Di\FactoryDefault))
#5 [internal function]: Phalcon\Di-&gt;get('db', NULL)
#6 [internal function]: Phalcon\Di-&gt;getShared('db')
#7 [internal function]: Phalcon\Mvc\Model\Manager-&gt;_getConnection(Object(Reviews), NULL)
#8 [internal function]: Phalcon\Mvc\Model\Manager-&gt;getReadConnection(Object(Reviews))
#9 [internal function]: Phalcon\Mvc\Model-&gt;getReadConnection()
#10 [internal function]: Phalcon\Mvc\Model\MetaData\Strategy\Introspection-&gt;getMetaData(Object(Reviews), Object(Phalcon\Di\FactoryDefault))
#11 [internal function]: Phalcon\Mvc\Model\MetaData-&gt;_initialize(Object(Rev in /var/www/html/index.php on line 30
</code></pre>

<p>Hence, I thought that the php container was lacking the <code>php-mysql</code> component I installed via:</p>

<pre class=""lang-none prettyprint-override""><code>apt-get install php5-mysql
</code></pre>

<p>I also added a mysql.ini at:</p>

<pre class=""lang-none prettyprint-override""><code>cat /usr/local/etc/php/conf.d/mysql.ini
; configuration for php MySQL module
; priority=20
extension=pdo_mysql.so
</code></pre>

<p>If I <code>echo phpinfo();die</code> it tells me that:</p>

<pre class=""lang-none prettyprint-override""><code>Additional .ini files parsed:
    /usr/local/etc/php/conf.d/mysql.ini,
    /usr/local/etc/php/conf.d/phalcon.ini
</code></pre>

<p>Yet still, the error persists.</p>

<p>Furthermore, when running:</p>

<pre class=""lang-none prettyprint-override""><code>php -i|grep PDO
</code></pre>

<p>I get:</p>

<pre class=""lang-none prettyprint-override""><code>PHP Warning:  PHP Startup: Unable to load dynamic library '/usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so' - /usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so: cannot open shared object file: No such file or directory in Unknown on line 0
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite
PDO Driver for SQLite 3.x =&gt; enabled
</code></pre>

<p>so it seems the mysql extension isn't even activated.</p>

<p>What am I doing wrong?</p>
","<p>I have the following docker.yml file</p>

<pre><code>version: ""3""
services:
  wwww:
    build: .
    ports:
      - ""8888:80""
    volumes:
      - ""./src:/var/www/html/""
    networks:
      - default
  db:
    image: mysql
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example

  adminer:
    image: adminer
    restart: always
    ports:
      - 8060:8080
</code></pre>

<p>and the docker file for php is </p>

<pre><code>FROM php:7.2-apache
RUN docker-php-ext-install mysqli pdo pdo_mysql
</code></pre>

<p>I checked with <code>phpinfo()</code> 
and the pdo driver is enabled for sqlite.</p>

<p>Even when I do docker-compose I get the following warning 
<code>warning: pdo (pdo.so) is already loaded!</code></p>

<p>However, I still get an exception that pdo is not found.</p>

<p>Is there anything that I am missing ?</p>

<p>The exact error is </p>

<pre><code>Fatal error: Uncaught PDOException: could not find driver in /var/www/html/index.php:5 

Stack trace: 
#0 /var/www/html/index.php(5): PDO-&gt;__construct('mysql:host=127....', 'root', 'example') 
#1 {main} thrown in /var/www/html/index.php on line 5
</code></pre>
"
"11616835","'\r': command not found - .bashrc / .bash_profile","<bash><shell><cygwin><newline>","54088193","Execute a Shell-Script from Dockerfile","<bash><docker>","<p>I have windows, using Cygwin, trying to set <code>JAVA_HOME</code> permanently through my <code>.bashrc</code> file. </p>

<p><strong>.bashrc:</strong></p>

<pre><code>export PATH=""$JAVA_HOME/bin:$PATH""  
export JAVA_HOME=$JAVA_HOME:""/cygdrive/c/Program Files (x86)/Java/jdk1.7.0_05""
</code></pre>

<p><strong>.bash_profile:</strong></p>

<pre><code>if [ -f ~/.bashrc ]; then
   source ~/.bashrc
fi
</code></pre>

<p><strong>running cygwin:</strong></p>

<pre><code>-bash: $'\377\376if': command not found
-bash: $'then\r': command not found
: No such file or directorysu//.bashrc
-bash: /cygdrive/c/Users/jhsu//.bash_profile: line 3: syntax error near unexpected token `fi'
-bash: /cygdrive/c/Users/jhsu//.bash_profile: line 3: `fi'
</code></pre>

<p>I am not sure if I took the commands from a tutorial that was meant for another system or if I am missing a step.  Or whitespace is causing my commands not to run properly. </p>

<p>I've looked at multiple similar questions but I haven't found one where the question has my error exactly. </p>

<hr>

<p>My home path:</p>

<pre><code>$ echo $HOME
/cygdrive/c/Users/jhsu
$ echo ~
/cygdrive/c/Users/jhsu/
</code></pre>

<p>So I believe the files should be placed in the correct spot.</p>
","<p>I have a Shellscript that starts a python command (taken from <a href=""https://unix.stackexchange.com/a/413051"">HERE</a>)</p>
<pre><code>#!/bin/sh

COMMAND='cd /sc2ai/agent &amp;&amp; python3 myscript.py'
LOGFILE=restart.txt

writelog() {
  now=`date`
  echo &quot;$now $*&quot; &gt;&gt; $LOGFILE
}

writelog &quot;Starting&quot;
while true ; do
  $COMMAND
  writelog &quot;Exited with status $?&quot;
  writelog &quot;Restarting&quot;
done
</code></pre>
<p>I would like this script to be executed immediately after the Container is loaded and I am doing it like this: (the very last lines of my <code>Dockerfile</code>)</p>
<pre><code># Run Apache
CMD apachectl -D FOREGROUND

# Start the Agent
RUN chmod +x agent.sh
ENTRYPOINT [&quot;/bin/bash&quot;, &quot;./agent.sh&quot;]
</code></pre>
<p>The builds, but upon starting (<code>docker-compose up -d --build &amp;&amp; winpty docker-compose run sc2 bash</code>) it creates this error:</p>
<blockquote>
<p>./agent.sh: line 2: $'\r': command not found</p>
<p>./agent.sh: line 5: $'\r': command not found</p>
<p>./agent.sh: line 6: syntax error near unexpected token `$'{\r''</p>
<p>'/agent.sh: line 6: `writelog() {</p>
</blockquote>
<p>What am I doing wrong?</p>
"
"55164223","Access mysql running on localhost from minikube","<mysql><kubernetes><database-connection><minikube>","65332296","Accessing a local postgres database from a rails app running on a Minikube cluster","<ruby-on-rails><postgresql><docker><kubernetes><minikube>","<p>I am running some services in minikube and trying to connect to mysql running on localhost(127.0.0.1) on 3306 port. </p>

<p>I read <a href=""https://stackoverflow.com/a/43477742/9958058"">this</a> and trying to create <code>service</code> and <code>Endpoints</code>. However, when I specify <code>127.0.0.1</code> as IP, it throws error as below:</p>

<p><code>The Endpoints ""mysql-service"" is invalid: subsets[0].addresses[0].ip: Invalid value: ""127.0.0.1"": may not be in the loopback range (127.0.0.0/8)</code></p>

<p>my deployment is like below:</p>

<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  ports:
  - protocol: TCP
    port: 1443
    targetPort: mysql

---
apiVersion: v1
kind: Endpoints
metadata:
  name: mysql-service
subsets:
  - addresses:
    - ip: 127.0.0.1
    ports:
    - name: mysql
      port: 3306
</code></pre>

<p>Please assist me to understand how can I connect to <code>mysql</code> db from <code>minikube</code>.</p>

<p>I have also tried replacing <code>127.0.0.1</code> with public IP of my computer(Don't know why though) and connection was timed out.</p>

<p>Any help or guide towards right direction is appreciated.</p>
","<p>I have a rails app running on a Minikube cluster locally. However this app is unable to talk to the postgres database that is outside this Minikube cluster. (Just like my production environment would be)</p>
<p>I run in to a <code>could not connect to server: Connection refused is the server running on host &quot;127.0.0.1&quot; and accepting TCP/IIP connections on port 5432</code></p>
<p>I have added the following:</p>
<pre><code>kind: Service
apiVersion: v1
metadata:
  name: postgres
  namespace: default
spec:
  type: ExternalName
  externalName: host.docker.internal
  ports:
    - name: port
      port: 5432
</code></pre>
<p>Does my rails app need any special configuration?
is there another way to do this? What host/port do I need to specify for my rails app?
Thanks!</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","65459273","Laravel Sail / Docker is too slow on Mac: optimization possible?","<laravel><performance><docker><local><laravel-sail>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>Just installed Laravel Sail in an existing project. I really love the easy setup, but it seems way too slow for development. I'm running the latests Docker (20.10) on a Macbook Pro 2019. I gave Docker 6 cores and 8GB of RAM in the resource-settings.
The response times I'm getting are 700-800ms, sometimes even higher, for simple API calls. Coming from Laravel Valet this feels like a huge step back.</p>
"
"12919081","gcc/g++: ""No such file or directory""","<c++><c><gcc><g++><c++-faq>","54746772","Docker build is throwing error ""fatal error: json/json.h: No such file or directory FROM ubuntu:14.04""","<c++><docker><dockerfile>","<p><code>g++</code> gives me errors of the form:</p>

<pre><code>foo.cc:&lt;line&gt;:&lt;column&gt;: fatal error: &lt;bar&gt;: No such file or directory
compilation terminated.
</code></pre>

<p>It is the same when compiling C-programs with <code>gcc</code>.</p>

<p>Why is that?</p>

<hr>

<p><sup><strong>Please note:</strong> This question has been asked many times before, but each time it was specific to the askers situation. This question's purpose is <strong>to have a question that others can be closed as duplicates of</strong>, once and for all; a <em>FAQ</em>.</sup></p>
","<p>The application is working for VM but failing when we are deploying using docker file.We have installed json cpp package, still it throws the error mentioned in the title.</p>

<p>Tried :In docker file:  </p>

<pre><code>RUN apt-get -y install libcairomm-1.0* libjpeg-dev libgif-dev libjsoncpp-dev libprotobuf-dev protobuf-compiler
</code></pre>

<p>where libjsoncpp would have sorted the problem, but it is throwing error No such file or directory.</p>
"
"63455621","COPY failed: stat /var/lib/docker/tmp/docker-xxx : no such file or directory","<docker><dockerfile>","65676514","wrong path in copy command in dockerfile in azure pipeline","<docker><azure-pipelines>","<p>I have a github actions workflow to build a docker image:</p>
<pre><code>name: Backend-Demo Docker Image CI
on:
  push:
    branches: [ master ]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Login to Azure Container Registry
        run: echo ${{ secrets.REGISTRY_PASSWORD }} | docker login ${{ secrets.LOGIN_SERVER_URL }} -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin
      - name: Get the version
        id: vars
        run: echo ::set-output name=tag::$(echo ${GITHUB_REF:10})
      - name: Build the tagged Docker image
        run: docker build . --file backend/Dockerfile --tag backend-demo/spring-boot:v1.0
</code></pre>
<p>The Dockerfile is:</p>
<pre><code>FROM openjdk:14-alpine
MAINTAINER example.com
RUN mkdir -p /opt/demo-0.0.1/lib
# Setting application source code working directory
WORKDIR /opt/demo-0.0.1/
COPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar
# ADD target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/

RUN sh -c 'touch demo-0.0.1-SNAPSHOT.jar'
ENTRYPOINT [&quot;java&quot;]
CMD [&quot;-jar&quot;, &quot;/opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar&quot;]
</code></pre>
<p>But when I execute this workflow I got this error at the <code>COPY</code> instruction:</p>
<pre><code>Step 5/8 : COPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar
COPY failed: stat /var/lib/docker/tmp/docker-builder851513197/target/demo-0.0.1-SNAPSHOT.jar: no such file or directory
##[error]Process completed with exit code 1.
</code></pre>
<p>I have been checking and it looks a typical error when the file we have the <code>Dockerfile</code> in a different directory like my instruction:</p>
<p><code>docker build . --file backend/Dockerfile --tag backend-demo/spring-boot:v1.0</code></p>
<p>I also don't have <code>.dockerignore</code> file and my Dockerfile is called <code>Dockerfile</code> precisely.</p>
<p>The <code>target/demo-0.0.1-SNAPSHOT.jar</code> file I am trying to copy is present in my github repository
Not sure what could be happening with the context, but probably <a href=""https://github.com/docker/for-linux/issues/90#issuecomment-509810005"" rel=""noreferrer"">this answer</a> could be a good hint?</p>
","<p>I use azure devops pipeline to build jar and put it to docker image.<br />
I use maven to build jar and put jar and dependencies to folder /componentA/target/container<br />
Pom file - /componemtA/pom.xml<br />
Docker file - /componentA/docker/Dockerfile</p>
<p>My pipeline yuml file is</p>
<pre><code>......
stages:
 - stage: BUILD
    jobs:
      - job: BUILD_JAR_AND_IMAGE
        steps:
          - task: MavenAuthenticate@0
....
          - task: Maven@3
......
          - task: Docker@2
             inputs:
              command: login
              containerRegistry: xxxx
  
          - task: Docker@2
            displayName: Build an image
            inputs:
              containerRegistry: 'xxx'
              repository: '$(image_name)'
              command: 'build'
              Dockerfile: '$(component_dir)/docker/Dockerfile'
              tags: '$(Build.BuildNumber)'
 
</code></pre>
<p>My docker file is:</p>
<pre><code>.....
COPY /componentA/target/container /opt
....
</code></pre>
<p>And I get error<br />
2021-01-12T00:04:43.9376526Z Step 2/19 : COPY /componentA/target/container /opt<br />
2021-01-12T00:04:43.9431565Z COPY failed: stat /var/lib/docker/tmp/docker-builder630817503/componentA/target/container: no such file or directory<br />
2021-01-12T00:04:43.9501516Z ##[error]COPY failed: stat /var/lib/docker/tmp/docker-builder630817503/besReportService/target/container: no such file or directory<br />
2021-01-12T00:04:43.9717162Z ##[error]The process '/usr/bin/docker' failed with exit code 1</p>
<p>copy command starts in <strong>var/lib/docker/tmp/docker-builder630817503</strong> instead of <strong>/home/vsts/work/1/s</strong></p>
<p>How to fix this problem ?</p>
"
"64204333","Service elasticsearch is not visible when run tests","<elasticsearch><github-actions>","65628275","Why is the port not being mapped correctly?","<django><postgresql><docker><github-actions>","<pre class=""lang-yml prettyprint-override""><code>name: Rspec
on: [push]
jobs:
  build:
    runs-on: [self-hosted, linux]
    services:
      elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
        env:
          discovery.type: single-node
        options: &gt;-
          --health-cmd &quot;curl http://localhost:9200/_cluster/health&quot;
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      redis:
        image: redis
        options: --entrypoint redis-server
    steps:
      - uses: actions/checkout@v2
      - name: running tests
        run: |
          sleep 60
          curl -X GET http://elasticsearch:9200/
</code></pre>
<p>I am running tests self hosted, I see on host with <code>docker ps</code>  the containers (redis and elasticsearch) when they up to test.</p>
<p>I access a container of redis, install a <code>curl</code> and run <code>curl -X GET http://elasticsearch:9200/</code> and i see a response ok before 60 sec (wait time to service up)</p>
<p>On step <strong>running tests</strong> I got error message &quot;Could not resolve host: elasticsearch&quot;</p>
<p>So, inside service/container redis I see a host elasticsearch but on step <strong>running tests</strong> no. What I can do?</p>
","<p>I have a Github Action to run some tests in Python. Those tests need a postgressql service, so I create it and map the necessary port. To save time installing some dependencies, I use a docker container to run the action, with the following Dockerfile:</p>
<pre><code>FROM ubuntu:16.04

RUN apt update &amp;&amp; \
    apt install -y gdal-bin python2.7 python-pip git &amp;&amp; \
    pip install --upgrade pip==9.0.1
EXPOSE 5432
WORKDIR /usr/src/app
</code></pre>
<p>And for the Github Action, I have the following workflow file:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: Run python tests

on: [push]

jobs:
  test:
    runs-on: ubuntu-16.04
    container:
      image: ghcr.io/myorg/python-unit-tests
      credentials:
        username: ${{ github.repository_owner }}
        password: ${{ secrets.BOT_GITHUB_REGISTRY_TOKEN }}

    strategy:
      matrix:
        group: [1]

    services:
      postgres:
        image: cheewai/postgis:postgres-10.1
        env:
          PG_USER: postgres
          PG_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test
        ports:
          - 5432:5432
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5

    steps:
      - uses: actions/checkout@v2
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements_dev.txt

      - name: Prepare database
        run: |
          python manage.py migrate --noinput --settings=&lt;somesettings&gt;

      - name: Running tests (page ${{ matrix.group }})
        run: echo 'Run tests¡
</code></pre>
<p>With this configuration, it should be working nicely, but when I run it I get the following error:</p>
<pre><code>django.db.utils.OperationalError: could not connect to server: Connection refused
    Is the server running on host &quot;127.0.0.1&quot; and accepting
    TCP/IP connections on port 5432?
</code></pre>
<p>I added the <code>EXPOSE 5432</code> line because I thought it would fix it, but it does not work without it either. Any idea what's going on?</p>
"
"16047306","How is Docker different from a virtual machine?","<docker><containers><virtual-machine><virtualization>","53972099","Why is the Ubuntu docker image not a VM","<docker><containers><virtual-machine><virtualization><dockerhub>","<p>I keep rereading <a href=""https://docs.docker.com/"" rel=""noreferrer"">the Docker documentation</a> to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?</p>

<p>Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?</p>
","<p>I get the big difference between VMs and containers. But that has me confused about how an Ubuntu container can even exist. It feels contradictory to me since Ubuntu is an OS.</p>

<p><a href=""https://hub.docker.com/_/ubuntu"" rel=""noreferrer"">https://hub.docker.com/_/ubuntu</a></p>

<p>Isn't this an entire guest OS? So what makes this a container over a VM? Or is the line between container and VM blurred?</p>

<p>I've tried googling this but the only results I find are the classic VM vs container answers which isn't really what I'm asking I don't think.</p>

<p>Edit - I've updated to try further clarify my question.</p>
"
"39404280","Make container accessible only from localhost","<docker><portforwarding>","66232627","How to secure my docker containers on a server?","<docker><nginx><docker-compose><devops><iptables>","<p>I have Docker engine installed on Debian Jessie and I am running there container with nginx in it. My ""run"" command looks like this:</p>

<pre><code>docker run -p 1234:80 -d -v /var/www/:/usr/share/nginx/html nginx:1.9
</code></pre>

<p>It works fine, problem is that now content of this container is accessible via <code>http://{server_ip}:1234</code>. I want to run multiple containers (domains) on this server so I want to setup reverse proxies for them. </p>

<p>How can I make sure that container will be only accessible via reverse proxy and not directly from <code>IP:port</code>? Eg.:</p>

<pre><code>http://{server_ip}:1234  # not found, connection refused, etc...
http://localhost:1234  # works fine
</code></pre>

<p><strong>//EDIT:</strong> Just to be clear - I am not asking how to setup reverse proxy, but how to run Docker container to be accessible only from localhost.</p>
","<p>Hello everyone :) I am working on a DigitalOcean server, on which I am running docker containers.
My docker containers have their port exposed to the outside world and I would like to only expose the port 3001 to the outside world, and expose others port only on my server.</p>
<p>Indeed I have a postgres container on port 5432 and a nodeJS container on port 3000, and they are accessible from &lt;server_ip&gt;:5432 and &lt;server_ip&gt;:3000 so it's really not safe!
I would like to do like on this :
<a href=""https://i.stack.imgur.com/UaGem.png"" rel=""nofollow noreferrer"">scheme</a></p>
<p>I tried with adding iptables rules in the chain DOCKER-USER but I didn't success.... I saw that  could use a reverse proxy like NGINX to change IP of requests but I didn't well understood and I am really not sure it's gonna work.</p>
<p>I don't really know how to secure my application, so if you know some ways to help me to solve my problem ? Thanks by advance !</p>
"
"16296753","Can you run GUI applications in a Docker container?","<x11><sandbox><docker><vnc>","66666539","How do I show pygame in cocalc docker? I am trying to run pygame on an x11 file, but it keeps saying ""No Available Video Device.""","<python><docker><pygame><remote-access><remote-debugging>","<p>How can you run GUI applications in a <a href=""http://www.docker.io"" rel=""noreferrer"">Docker</a> container?</p>

<p>Are there any images that set up <code>vncserver</code> or something so that you can - for example - add an extra speedbump sandbox around say Firefox?</p>
","<p>I opened an x11 file in cocalc docker and ran a pygame python file in the x11 terminal. However, it keeps saying &quot;pygame.error: No available video device.&quot; I know that the reason this error might have occurred is because the pygame file does not know where to display the graphics, and I also know that the $DISPLAY variable is related to where to display. However, I don't know what to set $DISPLAY to, or if I am even supposed to do this. How can I show pygame graphics on cocalc and make sure the pygame file finds the x11 display?</p>
"
"40366192","Kubernetes how to make Deployment to update image","<docker><kubernetes>","66555295","Forcing Kubernetes to redeploy a deployment YAML if docker image updates?","<docker><kubernetes><gitlab><gitlab-ci>","<p>I do have deployment with single pod, with my custom docker image like:</p>

<pre><code>containers:
  - name: mycontainer
    image: myimage:latest
</code></pre>

<p>During development I want to push new latest version and make Deployment updated.
Can't find how to do that, without explicitly defining tag/version and increment it for each build, and do</p>

<pre><code>kubectl set image deployment/my-deployment mycontainer=myimage:1.9.1
</code></pre>
","<p>I have this workflow where I write some code and a docker image is deployed under <code>latest</code>. Currently, it deploys to my container registry and then I run this <code>kubectl apply file.yaml</code> after the container deploys, but K8s doesn't seem to recognize that it needs to re-pull and rollout a new deployment with the newly pulled image.</p>
<p>How can I basically feed in the YAML spec of my deployments and just rollout restart the deployments?</p>
<p>Alternatively, is there a better approach? I unconditionally am rolling out deployment restarts on all my deployments this way.</p>
"
"42016126","CORS issue - No 'Access-Control-Allow-Origin' header is present on the requested resource","<jquery><spring><tomcat><spring-security><cors>","60275484","Spring Security CORS issue when passing Authorization header","<angularjs><spring-boot><docker><spring-security><cors>","<p>I have created two web applications - client and service apps.<br>The interaction between client and service apps goes fine when they are deployed in same Tomcat instance.<br>
But when the apps are deployed into seperate Tomcat instances (different machines), I get the below error when request to sent service app.</p>

<pre class=""lang-none prettyprint-override""><code>Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. 
Origin 'http://localhost:8080' is therefore not allowed access. The response had HTTP status code 401
</code></pre>

<p>My Client application uses JQuery, HTML5 and Bootstrap.</p>

<p>AJAX call is made to service as shown below:</p>

<pre><code>var auth = ""Basic "" + btoa({usname} + "":"" + {password});
var service_url = {serviceAppDomainName}/services;

if($(""#registrationForm"").valid()){
    var formData = JSON.stringify(getFormData(registrationForm));
    $.ajax({
        url: service_url+action,
        dataType: 'json',
        async: false,
        type: 'POST',
        headers:{
            ""Authorization"":auth
        },
        contentType: 'application/json',
        data: formData,
        success: function(data){
            //success code
        },
        error: function( jqXhr, textStatus, errorThrown ){
            alert( errorThrown );
        });
}
</code></pre>

<p>My service application uses Spring MVC, Spring Data JPA and Spring Security.</p>

<p>I have included <code>CorsConfiguration</code> class as shown below:</p>

<p><code>CORSConfig.java</code>:</p>

<pre class=""lang-java prettyprint-override""><code>@Configuration
@EnableWebMvc
public class CORSConfig extends WebMvcConfigurerAdapter  {
    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping(""*"");
    }
}
</code></pre>

<p><code>SecurityConfig.java</code>:</p>

<pre><code>@Configuration
@EnableGlobalMethodSecurity(prePostEnabled = true)
@EnableWebSecurity
@ComponentScan(basePackages = ""com.services"", scopedProxy = ScopedProxyMode.INTERFACES)
public class SecurityConfig extends WebSecurityConfigurerAdapter {

    @Autowired
    @Qualifier(""authenticationService"")
    private UserDetailsService userDetailsService;

    @Bean
    @Override
    public AuthenticationManager authenticationManagerBean() throws Exception {
        return super.authenticationManagerBean();
    }

    @Override
    protected void configure(AuthenticationManagerBuilder auth) throws Exception {
        auth.userDetailsService(userDetailsService);
        auth.authenticationProvider(authenticationProvider());
    }

    @Override
    protected void configure(HttpSecurity http) throws Exception {
       http
                .authorizeRequests()
                .antMatchers(""/login"").permitAll()
                .anyRequest().fullyAuthenticated();
        http.httpBasic();
        http.sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS);
        http.csrf().disable();
    }

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

    @Bean
    public DaoAuthenticationProvider authenticationProvider() {
        DaoAuthenticationProvider authenticationProvider = new DaoAuthenticationProvider();
        authenticationProvider.setUserDetailsService(userDetailsService);
        authenticationProvider.setPasswordEncoder(passwordEncoder());
        return authenticationProvider;
    }
}
</code></pre>

<p>Spring Security dependencies:</p>

<pre class=""lang-xml prettyprint-override""><code> &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt;
            &lt;version&gt;3.2.3.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt;
            &lt;version&gt;3.2.3.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I am using <strong>Apache Tomcat</strong> server for deployment.</p>
","<p>I had wasted lot's of time fixing this issue, but none of the existing solutions work in my case.
Let me explain you the server set up I have.
I have 2 docker containers, one for angular app (nginx - url - <a href=""http://localhost:8080"" rel=""nofollow noreferrer"">http://localhost:8080</a>) and spring boot application (tomcat - url - <a href=""http://localhost:8081"" rel=""nofollow noreferrer"">http://localhost:8081</a>).
This app is using Oauth2 jdbcToken authentication for API request.</p>

<p>This app is a simple user registration app.</p>

<p>I could register a new user since the register url is not secured and not passing any Authorization header.
But once when user logs in the CORS issue kick in. below I have listed the errors.</p>

<blockquote>
  <p>Access to XMLHttpRequest at '<a href=""http://localhost:8081/v1/api/group/find/shib"" rel=""nofollow noreferrer"">http://localhost:8081/v1/api/group/find/shib</a>' from origin '<a href=""http://localhost:8080"" rel=""nofollow noreferrer"">http://localhost:8080</a>' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: It does not have HTTP ok status.</p>
</blockquote>

<p>Let me tell you what I had done to resolve this.
In angular HTTP request I have added the following headers.</p>

<pre><code>'Authorization' : 'Bearer '+this.token(),
'Access-Control-Allow-Methods' : '*',
'Access-Control-Allow-Origin' : ""*"",
'Access-Control-Allow-Headers' : 'Content-Type, Accept, X-Requested-With, remember-me, Authorization',
""Access-Control-Expose-Headers"" : ""Content-Type, Accept, X-Requested-With, remember-me, Authorization""
</code></pre>

<p>In spring boot I added @CrossOrigin / @CrossOrigin(""http:localhost:8080"") on Rest controllers added with a CORSFilter</p>

<pre><code>@Override
public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException {


    HttpServletRequest request = (HttpServletRequest) req;
    HttpServletResponse response = (HttpServletResponse) res;

    response.setHeader(""Access-Control-Allow-Origin"", ""*"");
    response.setHeader(""Access-Control-Allow-Credentials"", ""true"");
    response.setHeader(""Access-Control-Allow-Methods"", ""POST, GET, OPTIONS, DELETE"");
    response.setHeader(""Access-Control-Max-Age"", ""3600"");
    response.setHeader(""Access-Control-Allow-Headers"", ""Content-Type, Accept, X-Requested-With, remember-me, Authorization"");
    chain.doFilter(req, res);
}
</code></pre>

<p>After all the trial and error, I am still getting the same error</p>
"
"18786209","What is the relationship between the docker host OS and the container base image OS?","<docker>","54872834","It's said Docker is running on Host OS, but we still build docker file from a base image, say Ubuntu? Isn't that running OS inside docker container?","<docker>","<p>I'm not certain that I'm asking the right question... but while I have been reading everything docker that I can get my hands on I see that I can install Docker on Ubuntu 12.04 (for example) and then I can install a Fedora container or a different version of ubuntu? (there is an example where the user installed busybox in the container.)</p>

<p>And of course I could be completely wrong.</p>

<p>But it would be my expectation that there was a ephemeral connection between the base system and the container.</p>

<p>restated: what is the relationship between the host OS and the container base image's OS?</p>
","<p>For example I have used docker container which is based on Ubuntu, running on my MacOS.</p>

<p>Doesn't that mean the process inside docker container actually runs on a different Operating System than my host one? I am really confused by this.</p>

<p>Coz people say docker is light coz the application directly runs on the host OS, but in the above case isn't the application still running on the OS (ubuntu) inside the docker? And the docker file is built on top of Ubuntu base image, how is that different from a VM then?</p>
"
"42385977","accessing a docker container from another container","<docker>","66502654","Docker container talks to docker container in the same local host?","<docker>","<p>i created two docker containers based on two different images. one of db and another for webserver. both containers are running on my mac osx. </p>

<p>i can access db container from host machine and same way can access webserver from host machine. </p>

<p>however, how do i access db connection from webserver? </p>

<p>the way i started db container is</p>

<pre><code>docker run --name oracle-db -p 1521:1521 -p 5501:5500 oracle/database:12.1.0.2-ee
</code></pre>

<p>I started wls container as</p>

<pre><code>docker run --name oracle-wls -p 7001:7001 wls-image:latest
</code></pre>

<p>I can access db on host by connecting to </p>

<pre><code>sqlplus scott/welcome1@//localhost:1521/ORCLCDB
</code></pre>

<p>I can access wls on host as</p>

<pre><code>http://localhost:7001/console
</code></pre>
","<p>I am in a confusion right now. I try many things I can find on the web, but, none solved it. I have Win10 and Docker desktop installed using WSL 2 to host Linux containers. I use the following command to start the Jenkins website.</p>
<pre><code>docker run --name jenkins-master-c -d -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkins/jenkins:2.282-alpine
</code></pre>
<p>This works fine. I can access the website using <code>http://localhost:8080/</code></p>
<p>The problem is, I try to curl http://localhost:8080 from another alpine docker container, but, I am not getting the web page back, it said connection refused. I tried my own tiny web service on my Windows machine without docker. Same thing. I can access the web service using web browser on Windows 10. However, if I get inside a container, I couldn't access the web service on the localhost.</p>
<p>I know I am missing some thing really basic, because the web doesn't seem to have this topic. I am just on my own computer without anything fancy. Thus, I just want to use localhost. The web said the default is supposed to use bridge which the container should talk to each other easily, but, it is not working for me. What am I missing. Maybe I shouldn't type <code>localhost</code>? But, what else should I do?</p>
<p>thank you</p>
<p>Edit: just want to explain what I did to get my problem solved. The creating network <code>--network my-network-name</code> was what I originally did, which failed because the way I curl the webpage is wrong. I did <code>--name jenkins-master-c</code> only to make it easy locate my container on the <code>docker ps</code>. But, as pointed out in my question, I suspected the <code>localhost</code> is wrong, which is confirmed by the solution. Instead of using localhost, I do <code>curl http://jenkins-master-c:8080</code> which worked. Thanks</p>
"
"42866013","Docker Toolbox - Localhost not working","<windows><docker><docker-toolbox>","60723357","Run docker container with official postgres image, failed to access the container","<postgresql><image><docker><containers><psql>","<p>So I'm using Docker Toolbox because I don't have Hyper-V on my machine since it's not Windows 10 pro. Everything seems to work fine, but when I try to go on my browser <code>0.0.0.0:80</code> it always returns me: This site can’t be reached</p>

<p>But when I run the command: <code>docker container ps</code> I get the following: <code>0.0.0.0:80-&gt;80/tcp</code> meaning that this address should work. I searched across stackoverflow and github issues. Now I'm stuck. </p>

<p>Am I missing something? </p>

<p>Thanks,
Mark</p>

<p>EDIT:</p>

<p>Using <code>docker-machine ip default</code> returns me <code>192.168.99.100</code>. I run that on port 80. I still get the same result except that the address becomes the container id: <code>https://fd677edg12</code></p>

<p>I run that command on cmd to find my ipv4: <code>cmd /k ipconfig /all</code>. Put the result with the port and it returns the same thing: <code>https://fd677edg12</code></p>
","<p>I wanted to run postgres database in docker container.
I pulled the newest image of postgres:</p>

<pre><code>docker pull postgres
</code></pre>

<p>And started container out of it:</p>

<pre><code>docker run --rm --name pg-docker -e POSTGRES_PASSWORD=docker -d -p 5432:5432 postgres
</code></pre>

<p>After that I tried to access the postgres:</p>

<pre><code>psql -h localhost -U postgres -d postgres
</code></pre>

<p>And connection failed:</p>

<pre><code>psql: error: could not connect to server: could not connect to server: Connection refused (0x0000274D/10061) Is the server running on host ""localhost"" (::1) and accepting TCP/IP connections on port 5432?

could not connect to server: Connection refused (0x0000274D/10061) Is the server running on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on port 5432?
</code></pre>

<p>I don't understand why?</p>

<p>[EDIT]
I tried it on Windows 10 Pro with Docker Toolbox and it fails, however on Ubuntu 18.04 works fine.</p>
"
"43296019","Docker build gives ""unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile""","<docker><dockerfile>","66239870","i want build docker file","<java><spring-boot><docker>","<p>I have a Dockerfile that is supposed to build an Ubuntu image. But whenever I run</p>
<pre><code>docker build -t ubuntu-test:latest ./Dockerfile
</code></pre>
<p>it shows the following error on the console</p>
<blockquote>
<p>unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile</p>
</blockquote>
<p>I'm on Mac OsX. I tried to <code>sudo</code> as well. Nothing works.</p>
","<p>when run this command</p>
<pre><code>sudo docker build -t jhipster /home/abed/project/microServies/StudentApp/src/main/docker/dockerfile1

sudo docker build -t jhipster /home/abed/project/microServies/StudentApp/src/main/docker/dockerfile2
</code></pre>
<p>see this error:</p>
<pre><code>unable to prepare context: context must be a directory: /home/abed/project/microServies/StudentApp/src/main/docker/dokcerfile1

unable to prepare context: context must be a directory: /home/abed/project/microServies/StudentApp/src/main/docker/dokcerfile2
</code></pre>
<p>i don't know this message what is do you meaning</p>
<p>and i have many docker file i must build and run but i cant run any file</p>
"
"24151129","Network calls fail during image build on corporate network","<dns><docker>","55000152","Error while running fabcar sample in javascript","<docker><npm><hyperledger-fabric>","<p>I'm having a problem building Docker images on my corporate network. I'm just getting started with Docker, so I have the following Dockerfile for a hello-world type app: </p>

<pre><code># DOCKER-VERSION 0.3.4
FROM    centos:6.4
# Enable EPEL for Node.js
RUN     rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm
# Install Node.js and npm
RUN     yum install -y npm
# Bundle app source
ADD . /src
# Install app dependencies
RUN cd /src; npm install
EXPOSE  8080
CMD [""node"", ""/src/index.js""]
</code></pre>

<p>This works fine when I build it on my laptop at home, on my own wireless network. It pulls down the requisite dependencies and builds the image correctly.</p>

<p>However, when I'm on my corporate network at work, this same docker build fails when trying to pull down the RPM from download.fedoraproject.org, with this error message:</p>

<blockquote>
  <p>Step 2 : RUN     rpm -Uvh <a href=""http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm"" rel=""noreferrer"">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a>
   ---> Running in e0c26afe9ed5
  curl: (5) Couldn't resolve proxy 'some.proxy.address'
  error: skipping <a href=""http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm"" rel=""noreferrer"">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a> - transfer failed</p>
</blockquote>

<p>On my corporate network, I can access that URL just fine from my laptop. But once Docker is trying to build the container, all of a sudden it can't resolve at all. This behavior is the same for a variety of external resources (apt-get, etc.): They all can resolve just fine on my laptop on the corporate network, but Docker can't resolve them.</p>

<p>I don't have the network know-how to figure out what's going on here. Does anyone know why this strange behaviour would be occurring when building Docker containers?</p>
","<p>I am trying to run fabcar sample using javascript chaincode,it fails with the following error while instantiating the chaincode</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>error: could not assemble transaction, err proposal response was not successful, error code 500, msg error starting container: error starting container: Failed to generate platform-specific docker build: Error returned from build: 1 ""npm ERR! code EAI_AGAIN
npm ERR! errno EAI_AGAIN
npm ERR! request to https://registry.npmjs.org/fabric-shim failed, reason: getaddrinfo EAI_AGAIN registry.npmjs.org:443</code></pre>
</div>
</div>
</p>

<p>please help me regarding this...
It is happenning for all fabric samples whenever i try to run in javascript/typescript
thanks in advance</p>
"
"24151129","Network calls fail during image build on corporate network","<dns><docker>","54860647","Getting Error while building my docker file","<node.js><docker><docker-compose><dockerfile>","<p>I'm having a problem building Docker images on my corporate network. I'm just getting started with Docker, so I have the following Dockerfile for a hello-world type app: </p>

<pre><code># DOCKER-VERSION 0.3.4
FROM    centos:6.4
# Enable EPEL for Node.js
RUN     rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm
# Install Node.js and npm
RUN     yum install -y npm
# Bundle app source
ADD . /src
# Install app dependencies
RUN cd /src; npm install
EXPOSE  8080
CMD [""node"", ""/src/index.js""]
</code></pre>

<p>This works fine when I build it on my laptop at home, on my own wireless network. It pulls down the requisite dependencies and builds the image correctly.</p>

<p>However, when I'm on my corporate network at work, this same docker build fails when trying to pull down the RPM from download.fedoraproject.org, with this error message:</p>

<blockquote>
  <p>Step 2 : RUN     rpm -Uvh <a href=""http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm"" rel=""noreferrer"">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a>
   ---> Running in e0c26afe9ed5
  curl: (5) Couldn't resolve proxy 'some.proxy.address'
  error: skipping <a href=""http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm"" rel=""noreferrer"">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a> - transfer failed</p>
</blockquote>

<p>On my corporate network, I can access that URL just fine from my laptop. But once Docker is trying to build the container, all of a sudden it can't resolve at all. This behavior is the same for a variety of external resources (apt-get, etc.): They all can resolve just fine on my laptop on the corporate network, but Docker can't resolve them.</p>

<p>I don't have the network know-how to figure out what's going on here. Does anyone know why this strange behaviour would be occurring when building Docker containers?</p>
","<p>I'm getting while trying to build my docker file</p>

<p>Dockerfile.dev</p>

<pre><code>FROM node:alpine

WORKDIR '/app'


COPY package.json .
RUN npm install

COPY . .

CMD [""npm"",""run"",""start""]
</code></pre>

<p>Command Im using to build.</p>

<pre><code>docker build -f Dockerfile.dev .
</code></pre>

<p>ERROR</p>

<p>ERRO[0000] Can't add file /root/frontend/1.31.1-main.sock to tar: archive/tar: sockets not supported
        ERRO[0000] Can't add file /root/frontend/1.31.1-shared.sock to tar: archive/tar: sockets not supported
        Sending build context to Docker daemon    248MB
            npm ERR! code EAI_AGAIN
        npm ERR! errno EAI_AGAIN
        npm ERR! request to <a href=""https://registry.npmjs.org/react-dom"" rel=""nofollow noreferrer"">https://registry.npmjs.org/react-dom</a> failed, reason: getaddrinfo EAI_AGAIN registry.npmjs.org registry.npmjs.org:443</p>

<pre><code>    npm ERR! A complete log of this run can be found in:
    npm ERR!     /root/.npm/_logs/2019-02-25T07_39_03_442Z-debug.log
    The command '/bin/sh -c npm install' returned a non-zero code: 1
    [root@kubeboss frontend]#
  [root@kubeboss frontend]#
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66672038","Docker container can't connect to the host machine database","<mysql><docker><drupal-6>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I tried to deploy a drupal 6 docker container that works with the host machine database (for testing purpose before deploying it to a remote database) but it just doesn't want to work despite following the different advices I found. ( particularly in this question <a href=""https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach"">From inside of a Docker container, how do I connect to the localhost of the machine?</a> when it talks about host mode, so even if the question looks alike it doesn't feel like it is a duplicate )</p>
<p>Dockerfile</p>
<pre><code>FROM php:7.3-apache

COPY . /var/www/html

EXPOSE 80

RUN chmod o+w /var/www/html/sites/default/settings.php &amp;&amp;\
    chmod o+w /var/www/html/sites/default/files &amp;&amp;\
    apt-get update &amp;&amp;\
    apt-get install -y libfreetype6-dev libjpeg62-turbo-dev libpng-dev &amp;&amp;\
    docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ &amp;&amp;\
    docker-php-ext-install mysqli gd mbstring pdo pdo_mysql &amp;&amp;\
    docker-php-ext-enable mysqli
</code></pre>
<p>docker-compose.yml</p>
<pre><code>version: '3.8'

services:
    website:
        build: 
            context: ./drupal-6-lts
            network: host
        volumes:
            - ./drupal-6-lts:/var/www/html
        ports:
            - 8080:80
</code></pre>
<p>For the database connection I specified these settings :</p>
<p>Driver : mysqli
Host : 127.0.0.1:8889 (the mysql port is 8889, I also tried to replace 127.0.0.1 by localhost)
And the right username, password and database name.</p>
<p>The drupal installation does work when I launch it with MAMP only, but it seems that it can't do the link to the db when in the container despite the <code>network: host</code> statement in docker-compose.yml</p>
<p>When I try Do Trung Duc solution I get this error when executing docker compose build :</p>
<blockquote>
<p>services.network_mode must be a mapping</p>
</blockquote>
"
"43535394","Docker-compose not passing environment variable to container","<docker><docker-compose>","66548556","Docker-compose is not reading env variable COMPOSE_FILE","<python><django><linux><docker><devops>","<p>I am using Docker 17.04.0-ce, build 4845c56 with docker-compose 1.12.0, build b31ff33 on Ubuntu 16.04.2 LTS. I simply want to pass an environment variable and display it from my script running in a container. I am doing this according to the documentation <a href=""https://docs.docker.com/compose/compose-file/#environment"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#environment</a> . The problem is that the variable is not passed to the container.</p>

<p>My docker-compose.yml file:</p>

<pre><code>env-file-test:
  build: .
  dockerfile: Dockerfile
  environment:
    - DEMO_VAR
</code></pre>

<p>My Dockerfile:</p>

<pre><code>FROM alpine
COPY docker-start.sh /
CMD [""/docker-start.sh""]
</code></pre>

<p>And the <code>docker-start.sh</code> file:</p>

<pre><code>#!/bin/sh
echo ""DEMO_VAR Var Passed in: $DEMO_VAR""
</code></pre>

<p>I try to set the variable in my current terminal session and pass it to the container:</p>

<pre><code>$ export DEMO_VAR=aabbdd
$ echo $DEMO_VAR
aabbdd
$ sudo docker-compose up
Starting envfiletest_env-file-test_1
Attaching to envfiletest_env-file-test_1
env-file-test_1  | DEMO_VAR Var Passed in: 
envfiletest_env-file-test_1 exited with code 0
</code></pre>

<p>So you can see that the variable DEMO_VAR is empty!</p>

<p>I also tried using variables in docker-compose.yml like this: <code>DEMO_VAR=${DEMO_VAR}</code> but then when I run <code>sudo docker-compose up</code>, I get a warning: ""WARNING: The DEMO_VAR variable is not set. Defaulting to a blank string."".</p>

<p>What am I doing wrong? What should I do to pass the variable to the container?</p>
","<p>I am using django cookiecutter configuration with Docker and I can not make docker-compose read $COMPOSE_FILE env variable.</p>
<p>I did this</p>
<pre><code>export COMPOSE_FILE=local.yml
</code></pre>
<p>And this is the outcome when a I run either docker-compose up or build:</p>
<pre><code>Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml
</code></pre>
"
"25170527","Get dockerfile / docker commands from docker image","<docker>","54068554","Can I create dockerfile from existing image?","<docker>","<p>Is it possible to get back the docker commands which were run to produce a given docker image? Since each line of a docker file should map to a single layer, it seems this would be possible, but I don't see anything in the docs.</p>
","<p>I'm new to docker.I followed some instruction to install a server with docker. Will it be possible for me to write a dockerfile for that image?</p>
"
"43856554","Connect to a PostgreSQL database on a Docker container","<python><django><postgresql><docker>","66817505","Why do I have connection issues between docker and postgresql?","<python><postgresql><docker><docker-compose>","<p>I want to run an application with a PostgreSQL database and a REST API powered by Django on separate Docker containers. So far the API has been running on Docker connecting to a SQLite database, but I'm having trouble now that I want to connect to a PostgreSQL database instead.</p>

<p>Docker's <code>docker-compose.yml</code> file:</p>

<pre><code>version: '2'
services:
    postgres:
        image: postgres
    api:
        build: .
        command: bash -c ""python manage.py migrate &amp;&amp; python manage.py runserver 0.0.0.0:1337""
        volumes:
            - .:/usr/src/app
        ports:
            - ""1337:1337""
        depends_on:
            - postgres
</code></pre>

<p>Django's <code>settings.py</code> (using the default settings which the base <code>postgres</code> image works with according to the documentation):</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'wgomanager',
        'USER': 'postgres',
        'PASSWORD': 'mysecretpassword',
        'HOST': 'localhost',
        'PORT': '5432',
    }
}
</code></pre>

<p>When I launch the application with <code>docker-compose up</code> eventually this error is thrown:</p>

<pre><code>api_1       |     connection = Database.connect(**conn_params)
api_1       |   File ""/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py"", line 130, in connect
api_1       |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
api_1       | django.db.utils.OperationalError: could not connect to server: Connection refused
api_1       |   Is the server running on host ""localhost"" (::1) and accepting
api_1       |   TCP/IP connections on port 5432?
api_1       | could not connect to server: Connection refused
api_1       |   Is the server running on host ""localhost"" (127.0.0.1) and accepting
api_1       |   TCP/IP connections on port 5432?
api_1       |
orchestrator_api_1 exited with code 1
</code></pre>

<p>What am I doing wrong?</p>
","<p>I am fairly new to docker and I do not know what's causing the issue to not run my python script on docker..
here is how i am creating my docker-compose.yml</p>
<pre><code>version: &quot;3.6&quot;
services:
  app :
    build: ./app/
  db:
    build: ./database/
</code></pre>
<p>Here is the error :</p>
<pre><code>File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
app_1  |     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
app_1  | sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
app_1  |        Is the server running on host &quot;127.0.0.1&quot; and accepting
app_1  |        TCP/IP connections on port 5432?

</code></pre>
<p>upon running docker-compose ps:</p>
<pre><code>             Name                            Command               State     Ports  
------------------------------------------------------------------------------------
542132_app_final_db_1   docker-entrypoint.sh postgres    Up       5432/tcp
app_1               python abc ...               Exit 1
</code></pre>
<p>How do I solve it? Please help. I am fairly new to Docker/Docker-compose. Thanks!</p>
"
"48633244","Access Angular app running inside Docker container","<angular><docker><dockerfile><redhat><redhat-containers>","60606780","Can't open Angular application in the browser, that is running from docker container","<angular><docker><dockerfile><docker-for-windows>","<p>I've created a docker image to test an Angular app but I cannot connect from host to running app inside docker container.</p>

<p>The image was created using a Dockerfile with:
EXPOSE 4200 8080 80</p>

<p>I run the docker container with command:
docker run -ti -p 4200:4200 angulardev /bin/bash</p>

<p>Inside container I create the Angular application and start it using:
ng serve</p>

<p>From container, if I open localhost:4200 I see the application but I cannot access it from host OS (RHEL7)</p>

<p>What is wrong? The Angular app starts on port 4200 which is exposed and mapped to host 4200.</p>

<p>Thanks.</p>
","<p>I'm pretty new to Docker and I'm trying to create a docker image that clones a github repo with a very simple Angular application and runs it in the container. Then I want to access this running Angular application in my browser.</p>

<p>This is my Dockerfile:</p>

<pre><code>FROM node:latest

RUN apt-get update &amp;&amp; apt-get install -y git

WORKDIR /usr/src/app

RUN git clone https://github.com/path-to-repo.git

WORKDIR /usr/src/app/docker-angular

RUN npm install -g @angular/cli

RUN npm install

CMD ng serve --port 5000
</code></pre>

<p>I build the image and I'm running the container with the following command</p>

<pre><code>docker container run -d -p 5000:5000 --name [container-name] [image-name]
</code></pre>

<p>When i check the logs i see that the application is running successfully in the container, but when i try to access it in my browser on localhost:5000 it's not there.
This is the output when I run docker container ls:</p>

<pre><code>806a71aa5972        angular-app         ""docker-entrypoint.s…""   24 minutes ago      Up 24 minutes       0.0.0.0:5000-&gt;5000/tcp   sample-angular 
</code></pre>

<p>Can I access the application that is running in the container from the browser and how should I do that?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","66978740","when i run docker command in ubuntu20.04 LTE on virtual box , the following error came","<docker><ubuntu-20.04>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<pre><code>docker run -it -v $(pwd):/openLANE_flow -v $PDK_ROOT:$PDK_ROOT -e PDK_ROOT=$PDK_ROOT -u $(id -u $USER):$(id -g $USER) openlane:rc6 
docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create: dial unix /var/run/docker.sock: connect: permission denied.
See 'docker run –help'.
</code></pre>
"
"25305788","How to avoid reinstalling packages when building Docker image for Python projects?","<python><docker>","66651022","Docker - Python: Posibiility to use locally installed pip packages","<python><docker><dockerfile>","<p>My Dockerfile is something like</p>
<pre><code>FROM my/base

ADD . /srv
RUN pip install -r requirements.txt
RUN python setup.py install

ENTRYPOINT [&quot;run_server&quot;]
</code></pre>
<p>Every time I build a new image, dependencies have to be reinstalled, which could be very slow in my region.</p>
<p>One way I think of to <code>cache</code> packages that have been installed is to override the <code>my/base</code> image with newer images like this:</p>
<pre><code>docker build -t new_image_1 .
docker tag new_image_1 my/base
</code></pre>
<p>So next time I build with this Dockerfile, my/base already has some packages installed.</p>
<p>But this solution has two problems:</p>
<ol>
<li>It is not always possible to override a base image</li>
<li>The base image grow bigger and bigger as newer images are layered on it</li>
</ol>
<p>So what better solution could I use to solve this problem?</p>
<h2>EDIT:</h2>
<p>Some information about the docker on my machine:</p>
<pre><code>☁  test  docker version
Client version: 1.1.2
Client API version: 1.13
Go version (client): go1.2.1
Git commit (client): d84a070
Server version: 1.1.2
Server API version: 1.13
Go version (server): go1.2.1
Git commit (server): d84a070
☁  test  docker info
Containers: 0
Images: 56
Storage Driver: aufs
 Root Dir: /var/lib/docker/aufs
 Dirs: 56
Execution Driver: native-0.2
Kernel Version: 3.13.0-29-generic
WARNING: No swap limit support
</code></pre>
","<p>Hey all im currently building an web application based on flask/mariadb/nginx and so on.
Im currently wondering if you guys know of a way to use the python packages i have installed on my local dev machine.</p>
<p>Currently all is done via the regular requirements.txt in my Dockerfile.</p>
<p>Here is my Compose file:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>version: '3'
services:
  backendserver:
    build:
      context: ./backendServer
      #dockerfile: ./backendServer/Dockerfile
    ports:
     - ""5000:5000""
    networks:
      - db_network

  db:
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: DB_PW
      MYSQL_USER: DB_USER
      MYSQL_DATABASE: DB_NAME
    volumes:
      - ./shared-files:/var/lib/mysql
    networks:
      - db_network
    ports:
      - ""3306:3306""
  pabs_ss:
    restart: always
    env_file: .env
    build: .

    volumes:
      - ./shared-files:/shared-files
    ports:
      - ""5005:5005""
    networks:
      - db_network
      - web_network
  nginx:
    restart: always
    image: ""nginx:latest""
    ports:
      - ""85:85""
    volumes:
      - ./nginx:/etc/nginx/conf.d
    networks:
      - web_network
    depends_on: 
      - pabs_ss
networks:
  db_network:
    driver: bridge
  web_network:
    driver: bridge</code></pre>
</div>
</div>
</p>
<p>Here is my Dockerfile:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>FROM python:3.6

ENV FLASK_APP run.py

COPY run.py gunicorn-cfg.py requirements.txt config.py .env ./
COPY app app
#COPY shared-files/db.sqlite3 /

RUN pip install -r requirements.txt


EXPOSE 5005 8080
CMD [""gunicorn"", ""--config"", ""gunicorn-cfg.py"", ""run:app""]</code></pre>
</div>
</div>
</p>
<p>As you can see im currently using the &quot;normal&quot; way of doing, though im growing quite tired of basically downloading everything everytime.</p>
<p>So any idea of how i can use the python packages from my local machine in the container?</p>
<p>Thanks and advance and so long :)</p>
"
"27192770","How to change the resources allocated to a container at run time?","<resources><docker>","54822362","Is it possible to increase the memory limit of a docker container after it is started?","<docker><memory>","<p>I am aware that I can limit the resources allocated to a container while provisioning using docker with the <code>-c</code> and <code>-m</code> flags for CPU and memory.</p>

<p>However, is there a way I can change these allocated resources to containers dynamically (after they have been provisioned) and without redeploying the same container with changed resources?</p>
","<p>To set up memory limit for a docker container we can use the  <code>-m</code> flag, so I started a container with a 2GB of memory limit using the <code>-m</code> flag, now I want to know if it is possible to increase the memory limit of this running container ?</p>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","54480830","Can I combine these two docker images?","<.net><docker>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I'm new to docker so this might be the wrong way to achieve this. </p>

<p>When I create a new .net project in visual studio and say I want it to run on docker, it creates this docker file. </p>

<pre><code>#Depending on the operating system of the host machines(s) that will build 

or run the containers, the image specified in the FROM statement may need to be changed.
#For more information, please see https://aka.ms/containercompat

FROM microsoft/dotnet:2.1-aspnetcore-runtime-nanoserver-1803 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM microsoft/dotnet:2.1-sdk-nanoserver-1803 AS build
WORKDIR /src
COPY [""WebApplication1/WebApplication1.csproj"", ""WebApplication1/""]
RUN dotnet restore ""WebApplication1/WebApplication1.csproj""
COPY . .
WORKDIR ""/src/WebApplication1""
RUN dotnet build ""WebApplication1.csproj"" -c Release -o /app

FROM build AS publish
RUN dotnet publish ""WebApplication1.csproj"" -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""WebApplication1.dll""]
</code></pre>

<p>I want to use Azure table storage in my application, which has this image already created</p>

<p>microsoft/azure-storage-emulator as tableStorage</p>

<p>Can I add </p>

<pre><code>FROM microsoft/azure-storage-emulator as tableStorage
</code></pre>

<p>in the same Dockerfile and use the Table Storage emulator on the same continer image somehow? Clearly they seem to be combining a bunch of images, but I cannot figure out how to combine the table storage image. </p>

<p>Thanks.</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","66595778","Run docker container on localhost","<docker><flask><kubernetes><deployment>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>Below is my DockerFile:</p>
<pre><code>FROM python:3.8.5
COPY . /usr/app/
EXPOSE 5500
WORKDIR /usr/app/
RUN pip install -r requirements.txt
CMD python app.py
</code></pre>
<p>I am running this container using the command &quot;<code>docker run -p 5500:5500 api_name</code>&quot;, after which I get the below message in cmd:
C:\Users\win10\Documents\ML files&gt;docker run -p 5500:5500 api_name</p>
<ul>
<li>Serving Flask app &quot;app&quot; (lazy loading)</li>
<li>Environment: production
WARNING: This is a development server. Do not use it in a production deployment.
Use a production WSGI server instead.</li>
<li>Debug mode: on</li>
<li>Running on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</li>
<li>Restarting with stat</li>
<li>Debugger is active!</li>
<li>Debugger PIN: xxx-xxx-xxx</li>
</ul>
<p>I tried accessing <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a> or <a href=""http://127.0.0.1:5500/"" rel=""nofollow noreferrer"">http://127.0.0.1:5500/</a> but &quot;This site can’t be reached&quot;.
PS: I am using docker desktop for windows</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","54776600","Unable to connect to flask while running on docker container","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I am trying to move a simple flask application to docker, but I can't see to figure out why the application is not accessible from the browser.</p>

<h3>docker-compose file</h3>

<pre><code>version: '3'
services:
  web:
    build: .
    ports:
     - ""5000:5000""
    volumes:
      - .:/app
    environment:
      FLASK_APP: api
      FLASK_ENV: development

  redis:
    image: ""redis:alpine""
</code></pre>

<h3>dockerfile</h3>

<pre><code>FROM python:3.7

ADD . /app
WORKDIR /app

RUN pip install -r requirements.txt
CMD [""flask"", ""run""]

</code></pre>

<h3>__init__py:</h3>

<pre><code>def create_app(test_config=None):
  app = Flask(__name__, instance_relative_config=True)
  ...

  return app
</code></pre>

<h3>result from docker-compose up --build</h3>

<pre><code>web_1    |  * Serving Flask app ""api"" (lazy loading)
web_1    |  * Environment: development
web_1    |  * Debug mode: on
web_1    |  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
web_1    |  * Restarting with stat
web_1    |  * Debugger is active!
web_1    |  * Debugger PIN: 277-205-758

</code></pre>

<p>But when I got to the browser:</p>

<p><code>The connection was reset</code></p>

<p><a href=""https://i.stack.imgur.com/RgXN9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RgXN9.png"" alt=""enter image description here""></a></p>

<p>Any ideas why? I can't see what I am missing because the server is running but it does reset the connection right away...</p>

<h3>Edit:</h3>

<ul>
<li>I did try to access it via localhost, 0.0.0.0, also using the container IP address</li>
</ul>
"
"30554702","Can't connect to Flask web service, connection refused","<python><flask>","54680759","nginx proxy_pass connection refused for flask development server","<docker><nginx><flask><docker-compose>","<p>I'm trying to run a simple web server on a Raspberry Pi with Flask. When I run my Flask app, it says:</p>

<blockquote>
  <p>running on <a href=""http://127.0.0.1:5000/"" rel=""noreferrer"">http://127.0.0.1:5000/</a></p>
</blockquote>

<p>But when I enter this address on my laptop's in Chrome, I get </p>

<blockquote>
  <p>ERR_CONNECTION_REFUSED</p>
</blockquote>

<p>I can open 127.0.0.1:5000 on the Raspberry Pi's browser. What do I need to do to connect from another computer?</p>
","<p><strong>EDIT</strong>: I needed to specify the host as 0.0.0.0 in app.run, which fixed my issue.</p>

<p>I've got a web-app that I'm trying to setup to use docker and docker-compose.  The backend is a flask api, front end is react, with nginx serving as a reverse-proxy, each piece running in its own container.  I'm still developing it, so I'm trying to using the flask development server to host the flask app, and the node to host the react app, with the flask container serving on port 5000, and node serving on port 3000, as is pretty standard.  </p>

<p>It builds successfully with <code>docker-compose -f docker-compose-dev.yml up --build</code> and each container starts up.  I can open up localhost in my browser, which loads the react app just fine.  However, when I try to login, I get a bad gateway error, which from the searching I've done, means that nginx can't find a listener where it expects one.</p>

<p>I've tried changing the proxy_pass value in the <code>/api</code> location, but that hasn't worked.  Do I need to specify the IP manually?  I've tried changing the ports in the <code>flask-api</code> section of my <code>docker-compose</code> file.  But that hasn't worked, either.</p>

<p>Where am I going wrong?</p>

<p>Here's my nginx configuration: </p>

<pre><code>server {

  listen 80;

  location / {
    proxy_pass http://client:3000;
    proxy_redirect    default;
    proxy_set_header  Host $host;
    proxy_set_header  X-Real-IP $remote_addr;
    proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header  X-Forwarded-Host $server_name;
  }

  location /api {
    proxy_pass        http://flask-api:5000;
    proxy_redirect    default;
    proxy_set_header  Host $host;
    proxy_set_header  X-Real-IP $remote_addr;
    proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header  X-Forwarded-Host $server_name;
  }

}
</code></pre>

<p>And here's the compose file:</p>

<pre><code>version: '3'
services:

  flask-api:
    image: flask-translate
    build:
      context: ./flask_back_end/
      dockerfile: Dockerfile-dev
    volumes:
      - ""./flask_back_end/:/app""
    ports:
      - 5000:5000

  client:
    image: react-translate
    build:
      context: ./react-app/
      dockerfile: Dockerfile-dev
    ports:
      - 3000:3000
    depends_on:
      - flask-api
    volumes:
      - './env.json:/usr/src/app/src/env.json'

  nginx:
    image: nginx-translate
    build:
      context: ./nginx/
      dockerfile: Dockerfile-dev
    ports:
      - 80:80
    depends_on:
      - flask-api
      - client
</code></pre>

<p>And when I start up the containers, the flask server lets me know that it's running on <code>http://127.0.0.1:5000/</code></p>

<p>If I attempt to login nginx spits out the error</p>

<p><code>[error] 6#6: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: ""POST /api/user/login HTTP/1.1"", upstream: ""http://172.18.0.2:5000/api/user/login"", host: ""localhost"", referrer: ""http://localhost/""</code></p>

<p>rather than the expected forwarding of the connection to the flask development api server.</p>

<p>From this error message I think that the client (Which is 172.18.01) is sending a post request to localhost/api/user/login, which is attempting to forward it to 172.18.02:5000/api/user/login, but there is no listener there, so the connection is refused.</p>
"
"31843792","Can not install package within docker debian:jessie","<docker><dockerfile>","66696674","Installing libsndfile1 on docker container","<docker><libsndfile><soundfile>","<p>I am tryint to install git within debian based container</p>

<p>postgres image is based on debian:jessie</p>

<p>dockerfile</p>

<pre><code>FROM postgres:9.4


RUN apt-get -qq update

RUN apt-get install git-core
RUN apt-get install osm2pgsql
</code></pre>

<p>Both git and osm2pgsql can not be located </p>

<p>error</p>

<pre><code>E: Unable to locate package git-core
</code></pre>

<p>What have I missed ?</p>
","<p>I'm trying to install soundfile over pip install on my docker container. Sadly i need to install <code>libsndfile1</code> manually over <code>apt get</code> by myself. This fails somehow and i don't really get why and does anyone know how to install it.</p>
<p>I'm running docker desktop on Win10 - but container will finally run on a Linux machine.</p>
<pre><code> &gt; [ 7/11] RUN apt-get install libsndfile1:
#11 0.618 Reading package lists...
#11 1.814 Building dependency tree...
#11 2.219 Reading state information...
#11 2.829 The following additional packages will be installed:
#11 2.830   libflac8 libogg0 libvorbis0a libvorbisenc2
#11 2.942 The following NEW packages will be installed:
#11 2.944   libflac8 libogg0 libsndfile1 libvorbis0a libvorbisenc2
#11 2.956 0 upgraded, 5 newly installed, 0 to remove and 3 not upgraded.
#11 2.956 Need to get 669 kB of archives.
#11 2.956 After this operation, 2136 kB of additional disk space will be used.
#11 2.956 Do you want to continue? [Y/n] Abort.
------
executor failed running [/bin/sh -c apt-get install libsndfile1]: exit code: 1
</code></pre>
<p>Anyone know something?</p>
"
"33462123","Connecting webpack-dev-server inside a Docker container from the host","<docker><webpack>","54869185","""The connection was reset"" after starting my server","<node.js><docker><webpack><docker-compose><webpack-dev-server>","<p>I'm running a webpack-dev-server application inside a Docker container (node:4.2.1). If I try to connect to the server port from within the container - it works fine. However, trying to connect it from the host computer results in reset connection (the port is published, of course). How can I fix it?</p>
","<p>I have set up Webpack Dev Server which works fine when I start it (<code>yarn run webpack-dev-server</code>): I can visit <a href=""http://localhost:3000"" rel=""nofollow noreferrer"">http://localhost:3000</a> and see my app.</p>

<p>However, I've also set up Docker compose to run this server, among others, in a Docker container. When I start the Docker container, I can call <code>wget http://localhost:3000</code> in the container just fine and receive a successful response. However, when I visit <a href=""http://localhost:3000"" rel=""nofollow noreferrer"">http://localhost:3000</a> in my browser, I see ""The connection was reset"".</p>

<p>My <code>docker-compose.yml</code> does include the following line:</p>

<pre><code>ports:
  - ""3000:3000""
</code></pre>

<p>Is there anything else I forgot to do that is needed to successfully access the server from outside the Docker container?</p>
"
"34559557","How to enable authentication on MongoDB through Docker?","<mongodb><docker>","54764184","How to connect securely to mongo docker container with mongoose through a node container for production?","<node.js><mongodb><docker>","<p>I want to spin-up a docker for <code>mongodb:latest</code> but allow only certain user(s) to access certain db(s) (i.e. enable <code>--auth</code>). No one else should access mongodb whatsoever! How should I do this as part of the docker initiation?</p>

<p>BTW, <code>data directory</code> sits on the host by utilising the following command during initiation: <code>-v /my/own/datadir:/data/db</code>.</p>
","<p>Here is my docker-compose.production.yml.</p>

<pre><code>version: ""3.7""
services:
##############################
# Back-End Container
##############################
  backend:
    container_name: mern_backend
    init: true
    environment:
      - MONGO_URI=mongodb://root_username:super_secret_pass@mongo_db:27017/mern_db?authSource=admin
#     - MONGO_URI=mongodb://mongo_db:27017/mern_db
    restart: always
    depends_on:
      - db
    networks:
      - server
##############################
# MongoDB Container
##############################
  db:
    container_name: mongo_db
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root_username
      MONGO_INITDB_ROOT_PASSWORD: super_secret_pass
    volumes:
      - dbdata:/data/db/
    networks:
      - server
networks:
  server:
volumes:
  dbdata:
</code></pre>

<p>In my app.js from the node.js/express container, I do the following.</p>

<pre><code>// Database Setup
mongoose.connect((process.env.MONGO_URI), { useNewUrlParser: true });

// Mongoose Setup
const db = mongoose.connection;
db.on('error', console.error.bind( console, 'Connection Error:' ))
  .once('open', function() { console.log('Database Connected!') });
</code></pre>

<p>Sometimes I get this error:</p>

<pre><code>Connection Error: admin { MongoNetworkError: failed to connect to server [mern_db:27017] on first connect [MongoNetworkError: connect ECONNREFUSED 172.27.0.2:27017]
Pool.&lt;anonymous&gt; (/home/node/node_modules/mongodb-core/lib/topologies/server.js:564:11)
Pool.emit (events.js:197:13)
Connection.&lt;anonymous&gt; (/home/node/node_modules/mongodb-core/lib/connection/pool.js:317:12)
Object.onceWrapper (events.js:285:13)
Connection.emit (events.js:197:13)
Socket.&lt;anonymous&gt; (/home/node/node_modules/mongodb-core/lib/connection/connection.js:246:50)
Object.onceWrapper (events.js:285:13)
Socket.emit (events.js:197:13)
emitErrorNT (internal/streams/destroy.js:82:8)
emitErrorAndCloseNT (internal/streams/destroy.js:50:3)
processTicksAndRejections (internal/process/next_tick.js:76:17)
name: 'MongoNetworkError',
errorLabels: [ 'TransientTransactionError' ],
[Symbol(mongoErrorContextSymbol)]: {} }
</code></pre>

<p>Sometimes I get this error:</p>

<pre><code>Connection Error: admin { MongoError: Authentication failed.
/home/node/node_modules/mongodb-core/lib/connection/pool.js:581:63
authenticateStragglers (/home/node/node_modules/mongodb-core/lib/connection/pool.js:504:16)
Connection.messageHandler (/home/node/node_modules/mongodb-core/lib/connection/pool.js:540:5)
emitMessageHandler (/home/node/node_modules/mongodb-core/lib/connection/connection.js:310:10)
Socket.&lt;anonymous&gt; (/home/node/node_modules/mongodb-core/lib/connection/connection.js:453:17)
Socket.emit (events.js:197:13)
addChunk (_stream_readable.js:288:12)
readableAddChunk (_stream_readable.js:269:11)
Socket.Readable.push (_stream_readable.js:224:10)
TCP.onStreamRead (internal/stream_base_commons.js:145:17)
ok: 0,
errmsg: 'Authentication failed.',
code: 18,
codeName: 'AuthenticationFailed',
name: 'MongoError',
[Symbol(mongoErrorContextSymbol)]: {} }
</code></pre>

<p>I followed <a href=""https://medium.com/@MaxouMask/secured-mongodb-container-6b602ef67885"" rel=""nofollow noreferrer"">this tutorial</a>, as well, to no avail. I tried reading up on the <a href=""https://hub.docker.com/_/mongo"" rel=""nofollow noreferrer"">mongo image documentation</a>, but I don't understand how it all works, to be honest. I recently learned of the fact that OS X can't map the data directory to the host, but the data volume still doesn't work properly, I think.</p>

<hr>

<p>I did <code>docker-compose up</code> and it comes on without errors sometimes, and others with errors. I do <code>docker system prune</code>, tried removing the volumes line from the docker-compose.yml file, but nothing worked so far. I used both MONGO_URI environment variables and each gives me the one of the errors.</p>
"
"37253068","Programmatically get the name of the pod that a container belongs to in Kubernetes?","<java><kubernetes><fabric8>","54891525","Get pod name and ip from Springboot application","<java><spring-boot><docker><kubernetes><azure-aks>","<p>Is there a way to programmatically get the name of the pod that a container belongs to in Kubernetes? If so how? I'm using fabric8's java client but curl or something similar will be fine as well.</p>

<p>Note that I don't want to find the pod using a specific label since then (I assume) I may not always find the right pod if it's scaled with a replication controller.</p>
","<p>I have a SpringBoot application, dockerized, and deployed in a kubernetes cluster.
There is any way to log the pod name and pod ip from the springboot application inside the container?</p>

<p>Thanks in advance.</p>
"
"38769390","Dollars in Makefile environment variables","<bash><shell><makefile><environment-variables>","66915521","in a Makefile obtain the Current UID","<docker><makefile>","<p>Is it possible to ""disable"" variable expansion in my <code>Makefile</code> for a certain section?</p>

<p>Here's an example of the issue I'm having:</p>

<pre class=""lang-makefile prettyprint-override""><code>print_command:
    @echo '$(COMMAND)'
</code></pre>

<p>And here's the output I'm getting:</p>

<pre class=""lang-shell prettyprint-override""><code>$ export COMMAND='My favourite shell is $SHELL'
$ make print_command
My favourite shell is HELL
$ make print_command COMMAND='Welcome to $SHELL'
Welcome to HELL
</code></pre>

<p>And what I would like to get:</p>

<pre class=""lang-shell prettyprint-override""><code>$ export COMMAND='My favourite shell is $SHELL'
$ make print_command
My favourite shell is $SHELL
$ make print_command COMMAND='Welcome to $SHELL'
Welcome to $SHELL
</code></pre>

<p>Is it possible to do this without using a double dollar like so:</p>

<pre class=""lang-shell prettyprint-override""><code>$ export COMMAND='My favourite shell is $$SHELL'
$ make print_command
My favourite shell is $SHELL
$ make print_command COMMAND='Welcome to $$SHELL'
Welcome to $SHELL
</code></pre>

<p>In it's simplest form I'm looking to forward the exact contents of the variable <code>COMMAND</code> without <code>make</code> mangling it.</p>
","<p>I'm trying to update my makefile instruction, but I don't understand how to do it.</p>
<p>I want to execute this docker instruction:</p>
<pre><code>start: ## Start the environment
docker-compose stop
docker-compose up -d --remove-orphans
</code></pre>
<p>with this prefix:</p>
<pre><code>CURRENT_UID=$(id -u):$(id -g)
</code></pre>
<p>like this:</p>
<pre><code>start: ## Start the environment
CURRENT_UID=$(id -u):$(id -g) docker-compose stop
CURRENT_UID=$(id -u):$(id -g) docker-compose up -d --remove-orphans
</code></pre>
<p>If I add direct that string and I try to run the makefile in the console appear this:</p>
<pre><code>CURRENT_UID=: docker-compose stop
CURRENT_UID=: docker-compose up -d --remove-orphans
</code></pre>
<p>The <code>$()</code> did not work, so the result is just <code>:</code>
What do I need to do?</p>
<p>I tried to follow this example but it doesn't work
<a href=""https://stackoverflow.com/questions/38769390/dollars-in-makefile-environment-variables"">Dollars in Makefile environment variables</a></p>
"
"41427756","Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379","<node.js><macos><docker><redis><docker-image>","54728863","I am running a redis(docker image) on local nodejs application","<node.js><docker><redis>","<p>I'm trying to allow communication between my nodeJs docker image with my redis docker image (Mac OS X environment):</p>

<p>nodeJs Dockerfile:</p>

<pre><code>FROM node:4.7.0-slim
EXPOSE 8100
COPY . /nodeExpressDB
CMD [""node"", ""nodeExpressDB/bin/www""]
</code></pre>

<p>redis Dockerfile:</p>

<pre><code>FROM ubuntu:14.04.3
EXPOSE 6379
RUN apt-get update &amp;&amp; apt-get install -y redis-server
</code></pre>

<p>nodeJs code which is trying to connect to redis is:</p>

<pre><code>var redis = require('redis');
var client = redis.createClient();
</code></pre>

<p>docker build steps:</p>

<pre><code>docker build -t redis-docker .
docker build -t node-docker .
</code></pre>

<p>docker run images steps flow: </p>

<pre><code>docker run -p 6379:6379 redis-docker
docker run -p 8100:8100 node-docker
</code></pre>

<p>ERROR:</p>

<pre><code>Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379
    at Object.exports._errnoException (util.js:907:11)
    at exports._exceptionWithHostPort (util.js:930:20)
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1078:14)
</code></pre>

<p>What should I do inorder to connect to Redis from node-docker?</p>
","<p>i'm trying to use redis connection on local node js application but it throwing an error ---</p>

<p>events.js:183
    throw er; // Unhandled 'error' event
    ^</p>

<p>Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED 127.0.0.1:6379
  at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)</p>

<p><a href=""https://i.stack.imgur.com/K1vQY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K1vQY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/5JfXK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5JfXK.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/VcBud.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VcBud.png"" alt=""running none docker image""></a></p>

<p><a href=""https://i.stack.imgur.com/RicZx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RicZx.png"" alt=""running docker image""></a></p>
"
"42552034","Bind container port to host inside Dockerfile","<docker><binding><port><dockerfile>","54918072","docker: how to set port in dockerfile","<bash><docker><dockerfile>","<p>Normally when binding port, I would do <code>docker run -p hostport:dockerport ...</code>, but can I specify the port binding inside my <code>Dockerfile</code> ?</p>

<p>I'm starting a server that listens on a port. The server takes the port through cmd line arguments. It would be great if I don't have to repeat the port in two places (in <code>docker run</code> command and in <code>Dockerfile</code>)</p>
","<p>When I call the <code>docker run</code> command in my terminal. The server starts up fine and is accessible, but when I try to add the port in the dockerfile. It does not work. </p>

<p>Is there a way I can set the port in the dockerfile explicitly? Thanks for any help.</p>

<p>This works:</p>

<pre><code>docker run -d -p 5555:4444 -v /dv/sm:/dv/sm sa:latest
</code></pre>

<p>I remove the -p flag and try to ""pass"" it in via the docker file, but it does not work (error: This site can’t be reached)</p>

<p>Not working:</p>

<pre><code>docker run -d -v /dv/sm:/dv/sm sa:latest
</code></pre>

<p>I've tried - 
Docker file:</p>

<pre><code>FROM WorkingTestImage as MyImage
ENTRYPOINT ""/opt/bin/entry_point.sh""
CMD [""-p"",""5555:4444""]

FROM WorkingTestImage as MyImage
ENTRYPOINT [""/opt/bin/entry_point.sh"",""-p"",""5555:4444""]
CMD [""-p"",""5555:4444""]

FROM WorkingTestImage as MyImage
ENTRYPOINT [""/opt/bin/entry_point.sh"",""-p"",""5555:4444""]
</code></pre>
"
"43383276","How does Docker run a Linux kernel under macOS host?","<linux><macos><docker><linux-kernel>","66618003","Why is it possible to run Linux containers on docker in MacOS","<linux><macos><docker><operating-system><containers>","<p>I installed Docker on my macOS Sierra as follows. Note I don't have VirtualBox installed.</p>

<pre><code>brew cask uninstall virtualbox
brew cask install docker
</code></pre>

<p>My macOS details.</p>

<pre><code>$ uname -a
Darwin m-C02QG7TRG8WN.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

$ docker version
Client:
 Version:      17.03.1-ce
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   c6d412e
 Built:        Tue Mar 28 00:40:02 2017
 OS/Arch:      darwin/amd64

Server:
 Version:      17.03.1-ce
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   c6d412e
 Built:        Fri Mar 24 00:00:50 2017
 OS/Arch:      linux/amd64
 Experimental: true
</code></pre>

<p>Once I run Docker from launchpad, I am able to run Docker containers.</p>

<pre><code>$ docker run -it ubuntu
root@2351d4222a4e:/# uname -a
Linux 2351d4222a4e 4.9.13-moby #1 SMP Sat Mar 25 02:48:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<p>My question is how does Docker manage to run a Linux kernel within macOS? I thought Docker would at least require boot2docker or some other such Linux kernel running so that it can create the Ubuntu's filesystem with the help of it. But the above output seems to indicate that it is not so. Where does the Linux kernel come from then?</p>
","<p>Please correct me if there are any issues in my understanding of Docker and operating systems.</p>
<p>From what I understand, Docker uses the underlying kernel of the OS of the machine running the Docker engine. As a result you are not able to run Windows containers on a Linux machine as it requires Windows kernel. My question is how am I able to run CentOS, Ubuntu containers on my MacOS machine, given that MacOS does not use a Linux kernel even though they are both Unix based.</p>
"
"44678725","Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?","<linux><ubuntu><docker><ubuntu-14.04>","66604589","How can i fix the docker daemon error in termux?","<android><docker><error-handling><daemon><termux>","<p>I have applied every solution available on internet but still I cannot run Docker.</p>

<p>I want to use Scrapy Splash on my server.</p>

<p>Here is <code>history</code> of commands I ran.</p>

<pre><code>docker run -p 8050:8050 scrapinghub/splash
sudo docker run -p 8050:8050 scrapinghub/splash
sudo usermod -aG docker $(whoami)
sudo docker run -p 8050:8050 scrapinghub/splash
newgrp docker
sudo docker run -p 8050:8050 scrapinghub/splash
reboot
sudo docker run -p 8050:8050 scrapinghub/splash
docker run -p 8050:8050 scrapinghub/splash
</code></pre>

<p>You can see I tried to restart my server as well, but it didnt help.</p>

<p>see output of <code>ps -aux | grep docker</code></p>

<pre><code>root@mani:/var/www/html# ps aux | grep docker
root      8524  0.0  0.8 127904 13964 ?        Ssl  17:21   0:00 /usr/bin/dockerd --raw-logs
root      8534  0.0  0.3  90588  5012 ?        Ssl  17:21   0:00 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc
root      8543  0.0  0.0   8812   764 pts/1    S+   17:21   0:00 grep --color=auto docker
root     16356  0.0  0.0  17200   964 pts/1    S    17:14   0:00 newgrp docker
root     20080  0.0  0.0  17200   964 pts/1    S    17:06   0:00 newgrp docker
root     30221  0.0  0.0  17200   964 pts/1    S    17:09   0:00 newgrp docker
</code></pre>
","<p>I installed docker on termux and when i tried to pull centos it gives me this error</p>
<p>Using default tag: latest
Cannot connect to the Docker daemon at unix:///data/docker/run/docker.sock. Is the docker daemon running?</p>
<p>How can i fix this ?</p>
"
"44678725","Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?","<linux><ubuntu><docker><ubuntu-14.04>","66650896","Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See 'docker run --help'","<linux><docker>","<p>I have applied every solution available on internet but still I cannot run Docker.</p>

<p>I want to use Scrapy Splash on my server.</p>

<p>Here is <code>history</code> of commands I ran.</p>

<pre><code>docker run -p 8050:8050 scrapinghub/splash
sudo docker run -p 8050:8050 scrapinghub/splash
sudo usermod -aG docker $(whoami)
sudo docker run -p 8050:8050 scrapinghub/splash
newgrp docker
sudo docker run -p 8050:8050 scrapinghub/splash
reboot
sudo docker run -p 8050:8050 scrapinghub/splash
docker run -p 8050:8050 scrapinghub/splash
</code></pre>

<p>You can see I tried to restart my server as well, but it didnt help.</p>

<p>see output of <code>ps -aux | grep docker</code></p>

<pre><code>root@mani:/var/www/html# ps aux | grep docker
root      8524  0.0  0.8 127904 13964 ?        Ssl  17:21   0:00 /usr/bin/dockerd --raw-logs
root      8534  0.0  0.3  90588  5012 ?        Ssl  17:21   0:00 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc
root      8543  0.0  0.0   8812   764 pts/1    S+   17:21   0:00 grep --color=auto docker
root     16356  0.0  0.0  17200   964 pts/1    S    17:14   0:00 newgrp docker
root     20080  0.0  0.0  17200   964 pts/1    S    17:06   0:00 newgrp docker
root     30221  0.0  0.0  17200   964 pts/1    S    17:09   0:00 newgrp docker
</code></pre>
","<p>I started to learn docker recently. After I install the docker,and I input <code>sudo docker run hello-world</code> to check does it install successfully, it shows as the title. By the way,when I input <code>docker version</code>, it shows the version that I installed.</p>
"
"44791060","run jenkins pipeline agent with sudo","<docker><jenkins><jenkins-pipeline>","54576235","How to access docker from my jenkins image running on my local Kubernetes instance","<docker><jenkins><kubernetes>","<p>I have an Jenkins Server running in an docker container and have access to docker an the host system, so far it is working well. Now I want to set up a pipeline testing an script inside an docker container.</p>

<p><strong>Jenkinsfile:</strong></p>

<pre><code>pipeline {
    agent { docker 'nginx:1.11' }
    stages {
        stage('build') {
            steps {
                sh 'nginx -t'
            }
        }
    }
}
</code></pre>

<p><strong>Error Message:</strong></p>

<pre><code>&gt; + docker pull nginx:1.11
&gt; 
&gt; Warning: failed to get default registry endpoint from daemon (Got
&gt; permission denied while trying to connect to the Docker daemon socket
&gt; at unix:///var/run/docker.sock: Get
&gt; http://%2Fvar%2Frun%2Fdocker.sock/v1.29/info: dial unix
&gt; /var/run/docker.sock: connect: permission denied). Using system
&gt; default: https://index.docker.io/v1/
&gt; 
&gt; Got permission denied while trying to connect to the Docker daemon
&gt; socket at unix:///var/run/docker.sock: Post
&gt; http://%2Fvar%2Frun%2Fdocker.sock/v1.29/images/create?fromImage=nginx&amp;tag=1.11:
&gt; dial unix /var/run/docker.sock: connect: permission denied
&gt; 
&gt; script returned exit code 1
</code></pre>

<p>My problem is that jenkins needs to run the docker command with sudo, but how to say the agent running the command with sudo?</p>
","<p>I have Kubernetes running on my Macintosh computer and I'm trying to run a Jenkins image which has the ability to connect to the internal docker system and then spin up more docker images in order to be able to run jobs on them. So far I have not gotten it to work, and I was hoping that someone might be able to help.</p>

<p>The version of docker that I'm running is:</p>

<ol>
<li>Docker Desktop 2.0.0.0-mac81 (29211) Stable</li>
<li>Engine 10.09.0</li>
<li><p>Kubernetes v1.10.3</p>

<ul>
<li>I went from installing docker from a tar file to actually using apt-get</li>
<li>I have also tried adding the Jenkins user to the docker group which is created when docker is installed via apt-get</li>
</ul></li>
</ol>

<p>My Docker File</p>

<pre><code>FROM jenkins/jenkins:lts

USER root

RUN apt-get update \
    &amp;&amp; apt-get install -y \
    maven \
    vim \
    libunwind8 \
    gettext \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg2 \
    software-properties-common

RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -
RUN add-apt-repository \
    ""deb [arch=amd64] https://download.docker.com/linux/debian \
    $(lsb_release -cs) \
    stable""
RUN apt-get update -qq \
    &amp;&amp; apt-get install docker-ce -y

USER jenkins
</code></pre>

<p>and my deployment.yml</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: jenkins
  namespace: deployment-tools
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
        - name: jenkins
          image: my/jenkins
          imagePullPolicy: Never
          env:
            - name: JAVA_OPTS
              value: -Djenkins.install.runSetupWizard=false
          ports:
            - name: http-port
              containerPort: 8080
            - name: jnlp-port
              containerPort: 50000
          volumeMounts:
            - name: jenkins-home
              mountPath: /var/jenkins_home
            - name: docker-socket
              mountPath: /var/run/docker.sock
      securityContext:
      # Specify fsGroup for pod, so that the persistent volume is writable for the non-privileged uid/gid 1000
        runAsUser: 1000
        fsGroup: 1000
      volumes:
        - name: jenkins-home
          hostPath:
            path: /Users/myUser/links/code/jenkins/filesystem
            type: Directory
        - name: docker-socket
          hostPath:
            path: /var/run/docker.sock
            type: File
</code></pre>

<p>I'm currently trying to access the subsystem by mounting my local filesystem docker socket, but it gives me this error:</p>

<pre><code>myUser@mymac jenkins (master) $ kubectl exec -it jenkins-78689b8786-rjqf6 --namespace=deployment-tools -- /bin/bash
jenkins@jenkins-78689b8786-rjqf6:/$ docker images list
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/images/json?filters=%7B%22reference%22%3A%7B%22list%22%3Atrue%7D%7D: dial unix /var/run/docker.sock: connect: permission denied
</code></pre>

<p>and I was expecting it to not throw an error and return a list of the docker images that I've installed on my computer.</p>
"
"44850565","Docker not found when building docker image using Docker Jenkins container pipeline","<docker><jenkins><dockerfile><jenkins-pipeline>","66928999","Docker and Jenkins integration","<docker><jenkins><kubernetes><digital-ocean><jenkins-docker>","<p>I have a Jenkins running as a docker container, now I want to build a Docker image using pipeline, but Jenkins container always tells Docker not found.</p>

<pre><code>[simple-tdd-pipeline] Running shell script
+ docker build -t simple-tdd .
/var/jenkins_home/workspace/simple-tdd-pipeline@tmp/durable-
ebc35179/script.sh: 2: /var/jenkins_home/workspace/simple-tdd-
pipeline@tmp/durable-ebc35179/script.sh: docker: not found
</code></pre>

<p>Here is how I run my Jenkins image:</p>

<pre><code>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v 
/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock 
jenkins
</code></pre>

<p>And the DockerFile of Jenkins image is:
<a href=""https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile"" rel=""noreferrer"">https://github.com/jenkinsci/docker/blob/9f29488b77c2005bbbc5c936d47e697689f8ef6e/Dockerfile</a></p>
","<p>I've added the BitBucket server integration plugin (<a href=""https://plugins.jenkins.io/atlassian-bitbucket-server-integration/"" rel=""nofollow noreferrer"">https://plugins.jenkins.io/atlassian-bitbucket-server-integration/</a>) and can connect to the BitBucket cloud repo from Jenkins:</p>
<p><a href=""https://i.stack.imgur.com/0cUEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0cUEI.png"" alt=""enter image description here"" /></a></p>
<p>But I receive an error when I try to build:</p>
<pre><code>/var/jenkins_home/workspace/bb_add-jenkins-file@tmp/durable-c49dbeca/script.sh: 1: /var/jenkins_home/workspace/bb_add-jenkins-file@tmp/durable-c49dbeca/script.sh: docker: not found
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
[Bitbucket] Notifying commit build result
[Bitbucket] Build result notified
ERROR: script returned exit code 127
Finished: FAILURE
</code></pre>
<p>So it seems I need to install Docker on the Jenkins instance ?</p>
<p><a href=""https://plugins.jenkins.io/docker-build-publish/"" rel=""nofollow noreferrer"">https://plugins.jenkins.io/docker-build-publish/</a></p>
<p>I'm following this tutorial to configure Docker with Jenkins: <a href=""https://medium.com/@karthi.net/docker-tutorial-build-docker-images-using-jenkins-d2880e65b74"" rel=""nofollow noreferrer"">https://medium.com/@karthi.net/docker-tutorial-build-docker-images-using-jenkins-d2880e65b74</a></p>
<p>and have reached this step:</p>
<p><a href=""https://i.stack.imgur.com/Cxtsi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cxtsi.png"" alt=""enter image description here"" /></a></p>
<p>On my own Jenkins Docker setup page I have :</p>
<p><a href=""https://i.stack.imgur.com/p5saw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p5saw.png"" alt=""enter image description here"" /></a></p>
<p>I'm unsure what Docker URL should be used? Do I need to provision a new container instance within the Kubernetes cluster and run docker within this new instance? This new Docker instance is then the <code>Docker Host URI</code> field?</p>
"
"44945815","How to split a string into command line arguments like the shell in python?","<python><argparse>","66917185","Python Docker SDK long command to container","<python><docker><ffmpeg><quoting>","<p>I have command line arguments in a string and I need to split it to feed to <code>argparse.ArgumentParser.parse_args</code>.</p>

<p>I see that <a href=""https://docs.python.org/3/library/argparse.html"" rel=""noreferrer"">the documentation</a> uses <code>string.split()</code> plentifully. However in complex cases, this does not work, such as</p>

<pre><code>--foo ""spaces in brakets""  --bar escaped\ spaces
</code></pre>

<p>Is there a functionality to do that in python?</p>

<p>(<em>A similar question for java was asked <a href=""https://stackoverflow.com/questions/29162368/how-to-split-string-to-arguments-like-shell"">here</a></em>).</p>
","<p>I need to send a fairly long command using python docker SDK</p>
<pre><code>ffmpeg -f lavfi -re -i smptebars=duration=60:size=1280x720:rate=30 -f lavfi -re -i sine=frequency=1000:duration=60:sample_rate=44100 -pix_fmt yuv420p -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 -profile:v baseline -preset veryfast -f mpegts &quot;srt://127.0.0.1:4200?pkt_size=1316&quot;
</code></pre>
<p>I have an example where you ned to wrap each element in double quotes.</p>
<pre><code>container1 = client.containers.run(&quot;alpine&quot;, [&quot;echo&quot;, &quot;hello&quot;, &quot;world&quot;],name='autotest', detach=True)
</code></pre>
<p>2 Questions</p>
<p>Is there an elegant way to do this?
If not how do I handle the bits that already have quotes?</p>
"
"49947694","WebDriverException: 'geckodriver' executable needs to be in PATH even though it is","<macos><selenium><firefox><selenium-webdriver><geckodriver>","60514243","executable is in path but is not found by selenium in docker frolvlad/alpine-python3","<python><linux><docker><selenium><selenium-webdriver>","<p>Trying to get firefox to run using selenium in spyder. My current code is</p>

<pre><code>from selenium import webdriver
import os
os.environ[""PATH""] += "":/usr/local/bin/geckodriver""
browser = webdriver.Firefox()
</code></pre>

<p>and I still get this error:</p>

<pre><code>runfile('/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder/test.py', 
wdir='/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder')
Traceback (most recent call last):

  File ""&lt;ipython-input-1-3f3f96ccf515&gt;"", line 1, in &lt;module&gt;

runfile('/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder/test.py', 
wdir='/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder')

  File ""/Applications/anaconda3/lib/python3.6/site- 
  packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile
  execfile(filename, namespace)

  File ""/Applications/anaconda3/lib/python3.6/site- 
  packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder/test.py"", 
line 12, in &lt;module&gt;
  browser = webdriver.Firefox()

  File ""/Applications/anaconda3/lib/python3.6/site- 
packages/selenium/webdriver/firefox/webdriver.py"", line 152, in 
__init__
self.service.start()

  File ""/Applications/anaconda3/lib/python3.6/site- 
packages/selenium/webdriver/common/service.py"", line 83, in start
os.path.basename(self.path), self.start_error_message)

WebDriverException: 'geckodriver' executable needs to be in PATH. 
</code></pre>

<p>Even though gekodriver is in that folder. </p>

<p><a href=""https://i.stack.imgur.com/1mlk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1mlk4.png"" alt=""proof that gekodriver is in the right place""></a></p>

<p>I have also tried exporting the path to ~./bash_profile which looks like this right now.</p>

<pre><code>Last login: Fri Apr 20 10:57:16 on ttys000
dhcp-54-85:~ mherl$ nano ./bash_profile
dhcp-54-85:~ mherl$ nano .bash_profile
  GNU nano 2.0.6             File: .bash_profile                      
Modified  


# Setting PATH for Python 3.6
# The original version is saved in .bash_profile.pysave
PATH=""/Library/Frameworks/Python.framework/Versions/3.6/bin:${PATH}""
export PATH

# Setting PATH for Python 3.6
# The original version is saved in .bash_profile.pysave
PATH=""/Library/Frameworks/Python.framework/Versions/3.6/bin:${PATH}""
export PATH

# added by Anaconda3 5.1.0 installer
export PATH=""/Applications/anaconda3/bin:$PATH""

#added by mherl to show path to gekodriver
export PATH=$PATH:/usr/local/bin/gekodriver
</code></pre>

<p>I also have the current paths set in spyder:</p>

<pre><code>/usr/local/bin
/Users/mherl/Dropbox/AnacondaProjects/MWS/MWSSpyder
</code></pre>

<p>of which gekodriver is in </p>

<pre><code>/usr/local/bin
</code></pre>

<p>I have looked everywhere and most people say it should run automatically if it's in /usr/local/bin but it still can't seem to find it even when I explicitly state the path.</p>

<p>This is a mac running High Sierra if that's important.</p>
","<p>I'm trying to run selenium in an alpine docker. I allready managed it <code>FROM python:3</code> but it would be nice to save that disk space.</p>

<p>I get the error<br>
<code>selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.</code><br>
and <code>FileNotFoundError: [Errno 2] No such file or directory: 'geckodriver'</code><br>
Googling this only gives <em>""Put geckodriver in your $PATH""</em> and <em>""Add folder path of geckodriver to $PATH with PATH=$PATH:/usr/src/app""</em></p>

<p>But <code>geckodriver</code> is in $PATH and it still results in the same error.  </p>

<p>I also tried adding the executable as argument to the <code>driver = webdriver.Firefox()</code> but it changed nothing.<br>
<em>WebDriverException: 'geckodriver' executable needs to be in PATH even though it is (1 answer)<br>
selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH with GeckoDriver Selenium Firefox (2 answers)<br>
Python 3.5 - “Geckodriver executable needs to be in PATH” (4 answers)</em></p>

<p>Dockerfile:</p>

<pre><code>FROM frolvlad/alpine-python3
WORKDIR /usr/src/app
COPY . .
RUN tar -xzf geckodriver-v0.26.tar.gz &amp;&amp; cp geckodriver /bin

RUN pip install --no-cache-dir -r requirements.txt
RUN apk add --no-cache firefox-esr xvfb
ENV DISPLAY=:1.0

ENTRYPOINT [""./entrypoint.sh""]
</code></pre>

<p>entrypoint.sh</p>

<pre class=""lang-sh prettyprint-override""><code>Xvfb :1 -screen 0 1024x768x16 &amp;&gt; xvfb.log &amp;
python ./seleniumTest.py
</code></pre>

<p>output:</p>

<pre class=""lang-sh prettyprint-override""><code>/usr/src/app # echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/src/app:/usr/src/app
/usr/src/app # which geckodriver
/bin/geckodriver
/usr/src/app # type geckodriver
geckodriver is /bin/geckodriver
/usr/src/app # ls -l /bin/geckodriver
-rwxr-xr-x    1 root     root       7008696 Mar  3 18:29 /bin/geckodriver
/usr/src/app # ./entrypoint.sh
xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/selenium/webdriver/common/service.py"", line 72, in start
    self.process = subprocess.Popen(cmd, env=self.env,
  File ""/usr/lib/python3.8/subprocess.py"", line 854, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/usr/lib/python3.8/subprocess.py"", line 1702, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'geckodriver'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./seleniumTest.py"", line 8, in &lt;module&gt;
    driver= webdriver.Firefox()
  File ""/usr/lib/python3.8/site-packages/selenium/webdriver/firefox/webdriver.py"", line 164, in __init__
    self.service.start()
  File ""/usr/lib/python3.8/site-packages/selenium/webdriver/common/service.py"", line 81, in start
    raise WebDriverException(
selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.
</code></pre>

<p>seleniumTest.py:</p>

<pre class=""lang-py prettyprint-override""><code>from pyvirtualdisplay import Display
from selenium import webdriver

display = Display(visible=0, size=(1024, 768))
display.start()

url = 'https://stackoverflow.com/questions/15018372/how-to-take-partial-screenshot-with-selenium-webdriver-in-python'
driver= webdriver.Firefox()

driver.get(url)

element = driver.find_element_by_css_selector('#question-header')
image = element.screenshot('screenshot.png')
</code></pre>
"
"45461017","Connect to host mongodb from docker container","<node.js><mongodb><docker><networking><containers>","54356430","docker container cannot connect to localhost mongodb","<node.js><mongodb><docker><meteor>","<p>So I want to connect to my mongodb running on my host machine (DO droplet, Ubuntu 16.04). It is running on the default <code>27017</code> port on localhost.</p>

<p>I then use <a href=""https://github.com/zodern/meteor-up"" rel=""noreferrer"">mup</a> to deploy my Meteor app on my DO droplet, which is using docker to run my Meteor app inside a container. So far so good.
A standard <code>mongodb://...</code> connection url is used to connect the app to the mongodb.
Now I have the following problem:</p>

<p><code>mongodb://...@localhost:27017...</code> obviously does not work inside the docker container, as <code>localhost</code> is not the host's localhost.</p>

<p>I already read many stackoverflow posts on this, I already tried using:</p>

<ul>
<li><code>--network=""host""</code> - did not work as it said <code>0.0.0.0:80</code> is already in use or something like that (nginx proxy)</li>
<li><code>--add-host=""local:&lt;MY-DROPLET-INTERNET-IP&gt;""</code> and connect via <code>mongodb://...@local:27017...</code>: also not working as I can access my mongodb only from localhost, not from the public IP</li>
</ul>

<p>This has to be a common problem!</p>

<p><strong>tl;dr</strong> - What is the proper way to expose the hosts <code>localhost</code> inside a docker container so I can connect to services running on the host? (including their ports, e.g. 27017).</p>

<p>I hope someone can help!</p>
","<p>A. I have a container that includes the following<br>
       1. <code>NodeJS</code> version 8.11.4<br>
       2. <code>Rocketchat</code> meteor app   </p>

<p>B. This is my <code>Dockerfile</code>  </p>

<pre><code>FROM node:8.11.4
ADD . /app
RUN npm install -g node-gyp
RUN set -x \
  &amp;&amp; cd /app/programs/server/ \
  &amp;&amp; npm install \
  &amp;&amp; npm cache clear --force
WORKDIR /app/
ENV PORT=3000 \
    ROOT_URL=http://localhost:3000
EXPOSE 3000
CMD [""node"", ""main.js""]

</code></pre>

<p>C. This command is executed well</p>

<pre><code>docker build -t memo:1.0 .

</code></pre>

<p>When I try to run the container, it encounters the following error in containers log</p>

<pre><code>{""log"":""MongoNetworkError: failed to connect to server [localhost:27017] on first connect [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]\n"",""stream"":""stderr"",""time"":""2019-01-24T21:56:42.222722362Z""}

</code></pre>

<p>So container can not be executed.
The <code>mongodb</code> is running and I've added 0.0.0.0 to <code>bindIp</code> in the <code>mongod.conf</code> file.</p>

<pre><code># network interfaces
net:
  port: 27017
  bindIp: 127.0.0.1,0.0.0.0  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.
</code></pre>

<p>My <code>mongodb</code> is installed in host(outside the container)<br>
The problem was not resolved and my container status is Exited</p>

<p>I put the IP instead of the localhost,but it encounters the following error</p>

<pre><code>{""log"":""MongoNetworkError: failed to connect to server [192.168.0.198:27017] on first connect [MongoNetworkError: connect EHOSTUNREACH

</code></pre>
"
"48162574","How to circumvent ""apt-key output should not be parsed""?","<gnupg><apt>","53935949","apt-key fails recently inside docker","<docker><ubuntu><debian><gnupg><apt-key>","<p>I'm automating my Docker installation. Something like this:</p>

<pre><code>if apt-key fingerprint 0EBFCD88 | grep ""Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88"" &gt; /dev/null
then
  # proceed
fi
</code></pre>

<p>This worked fine in older versions of <code>apt-key</code>, but recent versions have two issues:</p>

<ol>
<li>A different output format: I can hack around that</li>
<li><p>A warning:</p>

<pre><code>Warning: apt-key output should not be parsed (stdout is not a terminal)
</code></pre></li>
</ol>

<p>Clearly, I can hack around this as well, just redirect <code>stderr</code> to <code>/dev/null</code>. It just made me curious:</p>

<p><strong>How do these fine folks suggest I verify my key fingerprints?</strong> Or am I getting this fundamentally wrong by wanting to automate it, does that defeat the point? (I think not, since I still manually lifted the expected fingerprint from the website, but feel free to tell me otherwise...)</p>
","<p>Lately, I'm getting this error when building Ubuntu/Debian based containers:</p>

<pre><code>Warning: apt-key output should not be parsed (stdout is not a terminal)
Executing: /tmp/apt-key-gpghome.jZsMrv3GZH/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF
mgpg: cannot open '/dev/tty': No such device or address
</code></pre>

<p>When I'm calling the apt-key commands such as these:</p>

<pre><code>apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF
</code></pre>

<p>I could use apt with an untrusted option so I wouldn't need the key at all, but that sounds as a dirty solution.</p>

<p>When using gpg directly I probably could use the --no-tty or --batch options to fix it, but what is the somewhat clean way to do it with apt-key command which is a wrapper for the pgp?</p>
"
"48620107","Trouble communicating between two docker containers","<java><mysql><spring><docker><spring-boot>","54726032","Spring-Boot API container can't connect to MySQL container on same user-defined Docker network","<mysql><spring-boot><docker><docker-network>","<p>I’m new to docker and I’m trying to connect my spring boot app running into my <strong>boot-example</strong> docker container to a mysql server running into my <strong>mymysql</strong> docker container on port 6603, both running on the same phisical machine.
The fact is: if I connect my spring-boot app to my <strong>mymysql</strong> docker container in order to communicate with the database, <strong>I get no errors and everything works fine.</strong></p>

<p>When <strong>I move my spring boot application</strong> into my <strong>boot-example</strong> container and try to communicate (through Hibernate) to my <strong>mymysql</strong> container, then I get this error:</p>

<pre><code>2018-02-05 09:58:38.912 ERROR 1 --- [           main] o.a.tomcat.jdbc.pool.ConnectionPool      : Unable to create initial connections of pool.

com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_111]
</code></pre>

<p>My spring boot application.properties are:</p>

<pre><code>server.port=8083
spring.jpa.hibernate.ddl-auto=create-drop
spring.datasource.url=jdbc:mysql://localhost:6603/mydockerdb
spring.datasource.username=root
spring.datasource.password=mypassword
</code></pre>

<p>It works fine until my spring boot app runs in a docker container on port 8082, (after the docker image is correctly built):</p>

<pre><code>docker run -it -p 8082:8083 boot-example 
</code></pre>
","<p>I am running 2 containers.  The first is a Java Spring-Boot REST API/service.  The second is a MySQL database.  I am trying to perform a simple GET request from the running API container to the running MySQL container that should return all of the records from a table in the MySQL db container.</p>

<p>When both containers are running I try to make a simple GET request to <code>http://localhost:15505/users/all</code> or <code>http://172.18.0.3:15505/users/all</code> (via browser or Postman), I am getting an error:</p>

<pre><code>{
    ""timestamp"": ""2019-02-16T16:51:39.127+0000"",
    ""status"": 500,
    ""error"": ""Internal Server Error"",
    ""message"": ""Could not open JPA EntityManager for transaction; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection"",
    ""path"": ""/users/all""
}
</code></pre>

<p>When I inspect the container logs for the API via <code>docker logs &lt;container-id&gt;</code> I see a more detailed error message indicating issues connecting to the MySQL DB container: </p>

<p><code>com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure</code></p>

<p><strong>What I have done to troubleshoot</strong></p>

<p>I am able to run the API from IntelliJ locally, run the MySQL container, perform the GET request, and receive the expected response (i.e. all records from the users table).</p>

<p>I am able to connect to the running MySQL container via MySQL Workbench.  I can perform any DB actions from this client (i.e. INSERT/SELECT/DELETE etc).</p>

<p>I created a Docker user-defined network <code>local-network</code>.  When I created my containers using <code>docker run</code>, I added both to this network.  When I inspect the network I see both containers on that network.</p>

<p>I have used <code>docker exec &lt;container-id&gt; bash</code> for each container.  I was able to successfully perform a <code>ping</code> in both directions - by container ID and by container name.</p>

<p><strong>From Spring-Boot API</strong> - <code>application.properties</code></p>

<pre><code>server.port=15505

spring.datasource.url=jdbc:mysql://localhost:3306/snc_users?useSSL=false
spring.datasource.username=root
spring.datasource.password=pass123

spring.jpa.hibernate.ddl-auto=none
spring.jpa.hibernate.naming-strategy = org.hibernate.cfg.ImprovedNamingStrategy
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5Dialect
</code></pre>

<p><strong>Containers collection from <code>docker network inspect local-network</code></strong></p>

<pre><code>""Containers"": {
            ""61a9c53af62c26bedbf0ed8ffaeed2b97e61f7cff1e9644b71ffa2f45877c2e6"": {
                ""Name"": ""snc-users-api-local"",
                ""EndpointID"": ""3dc0a3abd3571cedf7d2250d35868b604531a75abb2091bfde1a9a444889d7eb"",
                ""MacAddress"": ""02:42:ac:12:00:03"",
                ""IPv4Address"": ""172.18.0.3/16"",
                ""IPv6Address"": """"
            },
            ""efda04231e97a46c9d16c3825d6b370b475f315387c92be2ca86f87e80533786"": {
                ""Name"": ""snc-dev-db"",
                ""EndpointID"": ""151bdc6c7a8a5a85aba15bfcb5c522cfc390b9b54ac57295f6ac989e08363ad8"",
                ""MacAddress"": ""02:42:ac:12:00:02"",
                ""IPv4Address"": ""172.18.0.2/16"",
                ""IPv6Address"": """"
            }
        },
</code></pre>

<p><strong>Docker Commands Used To Create &amp; Start Containers</strong></p>

<ul>
<li><p><strong>MySQL Container</strong>
<code>docker run -p 3306:3306 -d --name snc-dev-db -e MYSQL_ROOT_PASSWORD=pass123 --network local-network mysql:5.6.41</code></p></li>
<li><p><strong>Spring-Boot API Container</strong>
<code>docker run -p 15505:15505 -d --name snc-users-api-local --network local-network  5386a58f10c2</code>
NOTE: 5386a58f10c2 is the ID for my API image</p></li>
</ul>

<p>I expect to be able to be able to hit the route <code>http://localhost:15505/users/all</code> or <code>http://172.18.0.3:15505/users/all</code> and receive a 200 response with all of the records in my users table.</p>
"
"50278632","What does localhost means inside a Docker container?","<linux><docker>","60362455","Why two containers on the same docker machine can't communicate through localhost?","<windows><docker><docker-for-windows><docker-toolbox>","<p>Say, if I use this command inside a docker container.</p>

<pre><code>/opt/lampp/bin/mysql -h localhost -u root -pThePassword
</code></pre>

<p>What would the localhost here refer to? The host machine's IP or the docker container's own IP? </p>
","<p>Getting started with Docker I do not understand why two containers on the same docker machine can't communicate through <strong>localhost</strong>.</p>

<p>I read about this SO post already : <a href=""https://stackoverflow.com/questions/29143903/how-to-connect-two-docker-containers-through-localhost"">how-to-connect-two-docker-containers-through-localhost</a> but could not get a clear answer.</p>

<p>My use case, I am on Windows Home using Docker Toolbox (virtualbox) having the current docker machine :</p>

<pre><code>NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER     ERRORS
default   *        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.5
</code></pre>

<p>I am launching 3 services, one <strong>react</strong> application, one <strong>spring boot</strong> application, and one <strong>postgreSQL</strong> database. </p>

<p>In my understanding each one will be hosted on <strong>192.168.99.100</strong> on the port I specified (respectively 3000, 8080, 5432). So each one in their context should have <strong>localhost</strong> refering to <strong>192.168.99.100</strong>. </p>

<p>I know it is not working like that because of <strong>CORS</strong> issues that I only get when launching the app environment with docker.</p>

<p>Why is that ? Can someone explain ?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","66710947","configuring docker to put files","<docker>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I was just getting started with docker, and I run this:</p>
<pre><code>docker pull redis  
</code></pre>
<p>and I get a permission denied error. It turns out, docker writes to <code>/var/*</code> directories, which requires permission to write. and so many other docker commands also require something like:</p>
<pre><code>sudo docker ***    
</code></pre>
<p>Now, I don't really like the notion of add root privileges to every docker command.(It might be because I just don't know docker much yet, but that's true with every program). Is this a requirement by docker?</p>
<p>If it is not required, then how do I configure it so that it is much like other programs, that only ask me privileges when they need to, all the pulling, running commands would just write to my normal directories or run from them, not from a system directory.</p>
<p>EDIT: my concern was, if docker was allowed access to system files, meaning, it has some embedded scipt that had a potential harm to the computer, and it executed when I ran the docker. Since, I give it root privileges, the script could do anything. Would adding it to the user group instead of sudo fix that?</p>
"
"50768317","""docker pull"" certificate signed by unknown authority","<docker><ssl><curl>","60837639","certificate signed by unknown authority when docker pull","<docker><ssl><docker-compose>","<p>I was trying to pull a docker image from a docker registry but hit the following issue:</p>

<pre><code>$ docker pull &lt;docker registry&gt;/&lt;image name&gt;/&lt;tag&gt; 
Error response from daemon: Get &lt;docker registry&gt;/v1/_ping: x509: certificate signed by unknown authority
</code></pre>

<p>I tried with ""curl"" and get a similar error message:</p>

<pre><code> curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
</code></pre>

<p>So I downloaded the CA certificate and imported to the server (RedHat Linux 7) with the following commands:</p>

<pre><code>cp root_cert.cer /etc/pki/ca-trust/source/anchors/
update-ca-trust
</code></pre>

<p>After the root cert is imported, I can see <code>curl</code> is working fine as it won't complain the cert error, however if I use <code>docker pull</code> I still have the same issue. Is <code>docker</code> using different ca-cert location than <code>curl</code>? How do I fix the issue with <code>docker pull</code> in this situation?</p>
","<p>I set the docker private registry with self cirtification</p>

<p>For now, it works with <code>curl</code></p>

<pre><code>curl https://docker.mysite.jp:5000/v2/ --cacert /etc/docker/certs.d/docker.mysite.jp\:5000/ca.crt
</code></pre>

<p>then I am testing docker pull.</p>

<p>It shows error.</p>

<p><code>certificate signed by unknown authority</code></p>

<pre><code>$docker image push docker.mysite.jp:5000/myapp/nginx:latest
The push refers to repository [docker.mysite.jp:5000/myapp/nginx]
Get https://docker.mysite.jp:5000/v2/: x509: certificate signed by unknown authority
</code></pre>

<p>I think it means docker repository dosen't accept self-certificate ?? Is there any work around? or the problem of my localhost(mac OSX)??</p>
"
"49787780","Docker RUN ls shows cached files","<node.js><docker>","54310062","How to avoid the Docker build cache for a single command/RUN?","<docker><dockerfile>","<p>I am trying to setup a docker container and am using <code>RUN ls</code> to help debug. However, when I run <code>RUN ls</code>, docker prints out the following:</p>

<pre><code> ---&gt; Using cache
 ---&gt; 96223b1b0748
</code></pre>

<p>I am expecting it to log out files in the folder. Does anyone know what might be happening?</p>

<p>Here is my full Dockerfile:</p>

<pre><code>FROM node:latest 

WORKDIR /app

COPY app .

RUN ls
</code></pre>

<p>Thanks in advance!</p>
","<p>I'm using <code>RUN ls /some/directory</code> for ""printf-debugging"" of a <code>Dockerfile</code>. After the command has been run once and no command has changed before, the result is cached and no output is provided.</p>

<p>I can change the command with a part which is never executed, e.g. append <code>|| echo some changing number]</code> which is annoying for every run or turn off the build cache which affects more <code>RUN</code>s than necessary and thus slows down the development of the <code>Dockerfile</code>.</p>

<p>Is there a way to disable the cache for just one <code>RUN</code> without any of the described downsides?</p>
"
"50956952","Is there any way by which we can list out all the dependencies or libraries installed in running docker container?","<docker>","60600339","View installed packages in a docker container","<docker>","<p>Is there any way by which we can list out all the dependencies or libraries installed in running <strong>docker container</strong>?</p>
","<p>There are containers raised in Docker. Containers run applications such as nginx, apache, etc.</p>

<p>How can I list the installed versions of those applications in each container?</p>
"
"50229798","How to define docker commit message in Dockerfile","<docker><dockerfile>","66642983","How do I add comments to layers in docker images?","<docker><dockerfile>","<p>Using <code>Dockerfile</code> (<code>docker build</code>) is an alternative for doing <code>docker commit</code> by hand. By using <code>docker commit</code>, there is an option named <code>--message</code>, which can be used to define commit messages. Commit messages are displayed in <code>docker history</code> in a dedicated column called <code>COMMENT</code>. My question is: how to define docker commit message in a <code>Dockerfile</code>?</p>
","<p>The docker history command shows the various layers and has this nice column named &quot;COMMENT&quot; but I can not seem to find a way to make a comment for the layers.</p>
<p>It would be great to have some way to get a comment into the layers such that they can be annotated for users to read the history.  (Nothing big, but a few words would be nice)</p>
<p>What I do now for the &quot;RUN&quot; layers is to have the first thing be an <code>echo &quot;MyThing&quot; &amp;&amp; \</code> such that &quot;MyThing&quot; effectively is the comment but in the &quot;CREATED BY&quot; field.</p>
"
"51011552","MongoDB on with Docker ""failed to connect to server [localhost:27017] on first connect ""","<node.js><mongodb><docker><docker-compose>","66562704","Node js and mongodb cannot connect docker-compose","<node.js><mongodb><docker>","<p>I am using mongoDB with and NodeJS backend. The Problem is I am getting the following error</p>

<blockquote>
  <p>node:16) UnhandledPromiseRejectionWarning: MongoNetworkError: failed
  to connect to server [localhost:27017] on first connect
  [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]</p>
</blockquote>

<p>This is my docker-compose</p>

<pre><code>version: '3.4'

services:
  db:
    image: mongo:latest
    ports:
      - '27017:27017'

  rest-api-node:
    build: .
    ports:
      - '5000:5000'
    links:
      - db
    restart: on-failure
</code></pre>

<p>I have tried with <code>depends_on</code> as well , was not working.</p>

<p>On backend I am mongoose as a middleware to communicate with DB. this is the part of my <code>index.js</code></p>

<pre><code>mongoose.Promise = global.Promise
mongoose.connect('mongodb://localhost/demo')
app.listen(port, () =&gt; console.log(""live""))
</code></pre>

<p>I have tried using promise as well , no change though. Please Help me out.
Thanks in advance </p>

<p>complete error log</p>

<blockquote>
  <p>at Pool.
  (/app/node_modules/mongodb-core/lib/topologies/server.js:505:11)
  rest-api-node_1  |     at Pool.emit (events.js:180:13) rest-api-node_1
  |     at Connection.
  (/app/node_modules/mongodb-core/lib/connection/pool.js:329:12)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Connection.emit (events.js:180:13)
  rest-api-node_1  |     at Socket.
  (/app/node_modules/mongodb-core/lib/connection/connection.js:245:50)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Socket.emit (events.js:180:13)
  rest-api-node_1  |     at emitErrorNT
  (internal/streams/destroy.js:64:8) rest-api-node_1  |     at
  process._tickCallback (internal/process/next_tick.js:178:19)
  rest-api-node_1  |   name: 'MongoNetworkError', rest-api-node_1  |<br>
  message: 'failed to connect to server [localhost:27017] on first
  connect [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]' }</p>
</blockquote>
","<p>Both these services work individually, Yet when I go to the docker endpoint through the container the app crashes as it fails to connect to my database. Can anyone see why?</p>
<p>After running docker-compose i have inspected each container individually and they work fine but just not together. Unsure as to why as for others it seems to work</p>
<p>Docker compose</p>
<pre><code>version: '3.3'
services:
  db:
    container_name: db
    image: mongo
    # volumes:
    #   - ./db:/data/db
    ports:
      - '27017:27017'
  # frontend:
  #   build:
  #     context: './frontend/'
  #     dockerfile: Dockerfile
  #   container_name: reactfront
  #   # depends_on: [server]
  #   ports:
  #     - '3000:3000'
  #   restart: always
  server:
    build:
      context: ./backend/
      dockerfile: Dockerfile
    container_name: nodeserver
    ports:
      - '4000:4000'
    restart: always
    depends_on: [db]
    links:
      - db
volumes:
    db_data: {}
</code></pre>
<p>Node js app</p>
<pre><code>const express = require('express')
const app = express()
const port = 4000
var cors = require('cors')
const { MongoClient } = require(&quot;mongodb&quot;);
const uri = 'mongodb://localhost:27017'
const client = new MongoClient(uri);
app.use(cors())
app.get('/', (req, res) =&gt; {
  res.send('Hello Worldd!')
})
app.get('/docker', async (req, res) =&gt; {
  let myres;
  client.connect().then(async (db, b) =&gt; {
    myres = await db.db().admin().listDatabases()
    console.log(await db.db().admin().listDatabases());
    res.json(myres)
  })
})
app.listen(port, () =&gt; {
  console.log(`Example app listening at http://localhost:${port}`)
})

</code></pre>
"
"50310402","Cannot connect to exposed port of container started with docker-compose on Windows","<docker><docker-compose>","66670606","Unable to access Dockerized Angular application in container","<node.js><angular><docker><dockerfile>","<p>I'm having trouble accessing apps that should expose ports to the host via docker-compose. Here is a reproducible example:</p>

<p>I create a new angular app using the angular CLI:</p>

<pre><code>ng new angular-docker
</code></pre>

<p>Then I create a Dockerfile with the following contents in that directory:</p>

<pre><code>FROM node:8

RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app
COPY . /usr/src/app

RUN npm install

EXPOSE 4200

CMD [""npm"", ""start""]
</code></pre>

<p>Next I create a docker-compose.yaml file in the same directory:</p>

<pre><code>version: '3'

services:
  angular:
    build: .
    container_name: angular-docker
    volumes:
      - ./src:/usr/src/app/src
    ports:
      - ""4200:4200""
</code></pre>

<p>Then I run:</p>

<pre><code>docker-compose up
</code></pre>

<p>I wait until I get the following line in the docker-compose output</p>

<pre><code>angular-docker | ** Angular Live Development Server is listening on localhost: 4200, open your browser on http://localhost:4200/ **
</code></pre>

<p>From <code>docker inspect angular-docker</code> I see that the port-forwarding rule is in place:</p>

<pre><code>...
""Ports"": {
            ""4200/tcp"": [
                {
                    ""HostIp"": ""0.0.0.0"",
                    ""HostPort"": ""4200""
                }
            ]
        },
...
</code></pre>

<p>Now, when I try to go to <a href=""http://localhost:4200"" rel=""nofollow noreferrer"">http://localhost:4200</a> in Chrome, I get ERR_EMPTY_RESPONSE.</p>

<p>However, when I use <code>docker exec -it angular-docker bash</code> to bash into the container, I get a response, when I <code>curl localhost:4200</code>.</p>

<p><strong>My environment:</strong></p>

<ul>
<li>OS: Windows 10 Pro Version 10.0.16299 Build 16299</li>
<li>Docker: 18.03.1-ce-win65 (17513)</li>
</ul>

<p>I figured that this might be a firewall issue. So for debugging, I closed the firewall application (I'm using Kaspersky Internet Security), with no luck.</p>

<p>Why can I not access the container from the exposed port?</p>

<p>EDIT:</p>

<p><code>netstat -an | findstr "":4200""</code> returns the following:</p>

<pre><code>Proto  Local Address          Foreign Address        State
TCP    0.0.0.0:4200           0.0.0.0:0              LISTENING
TCP    [::1]:4200             [::]:0                 LISTENING
</code></pre>
","<p><strong>Dockerfile</strong>:<br></p>
<pre><code>FROM node:current-alpine3.10

RUN mkdir -p /dist/angular

WORKDIR /dist/angular

COPY package.json .

RUN npm install --legacy-peer-deps

COPY . .

EXPOSE 8500

CMD [&quot;npm&quot;,&quot;run&quot;,&quot;start:stage-ena-sso&quot;]
</code></pre>
<p><strong>package.json</strong>:</p>
<pre><code>...
  &quot;scripts&quot;: {
    &quot;start:stage-ena-sso&quot;: &quot;ng serve -o -c=stage-ena-sso --port=8500 --baseHref=/&quot;
  }...
</code></pre>
<p><strong>Folder structure</strong>:<br>
<a href=""https://i.stack.imgur.com/rLLr6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rLLr6.png"" alt=""enter image description here"" /></a></p>
<p><strong>Command used to build the Docker image</strong>:<br></p>
<pre><code>docker build . -t ssoadminuiapp
</code></pre>
<p><strong>Command used to run the Docker image</strong>:<br></p>
<pre><code>docker run --rm -it  -p 8500:8500/tcp ssoadminuiapp:latest
</code></pre>
<p><strong>Check if container is running</strong>:<br></p>
<pre><code>CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                    NAMES
8959e5180eba        ssoadminuiapp:latest   &quot;docker-entrypoint.s…&quot;   6 minutes ago       Up 6 minutes        0.0.0.0:8500-&gt;8500/tcp   recursing_fermat
</code></pre>
<p>But accessing <strong>localhost:8500</strong> doesnt seem to work:<br>
<a href=""https://i.stack.imgur.com/31lN2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31lN2.png"" alt=""enter image description here"" /></a></p>
<p>I'm really new to Docker, so any useful beginner-friendly tips/infos would be very appreciated.</p>
<p>Edit #1, this is the result after running docker run command:<br>
<a href=""https://i.stack.imgur.com/htQNp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/htQNp.png"" alt=""enter image description here"" /></a></p>
"
"51247609","What is the difference between docker run and docker container run","<docker>","60798379","Difference between docker container commit and docker commit command","<docker><docker-container><docker-command>","<p>can anyone help me in the understanding difference between <strong>docker run</strong> &amp; <strong>docker container run</strong>?</p>

<p>when i do <strong>docker run --help</strong> &amp; <strong>docker container run --help</strong> from docker cmd line. I see the following </p>

<p><strong>Run a command in a new container</strong>.</p>

<p>Is there any difference in how they run the container internally or both are same doing same work?</p>

<p>As per <a href=""https://forums.docker.com/t/docker-run-and-docker-container-run/30526"" rel=""noreferrer"">https://forums.docker.com/t/docker-run-and-docker-container-run/30526</a>. <strong>docker run</strong> is still the old one, which will be deprecated soon but same is not confirmed.</p>
","<p>What is the difference between the following two commands:</p>

<pre><code>docker container commit 

docker commit
</code></pre>

<p>I read the documentation for both the commands but couldn't found any difference between the two commands(Also both the commands contain the same options).</p>

<p><a href=""https://docs.docker.com/engine/reference/commandline/commit/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/commandline/commit/</a>
<a href=""https://docs.docker.com/engine/reference/commandline/container_commit/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/commandline/container_commit/</a></p>

<p>The help page of both the commands show the same output on Docker 19.03</p>

<pre><code>Usage:  docker container commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]

Create a new image from a container's changes

Options:
  -a, --author string    Author (e.g., ""John Hannibal Smith &lt;hannibal@a-team.com&gt;"")
  -c, --change list      Apply Dockerfile instruction to the created image
  -m, --message string   Commit message
  -p, --pause            Pause container during commit (default true)
</code></pre>
"
"51500288","unable to install pyspark","<python><pyspark>","66304992","Error as:-ModuleNotFoundError: No module named ‘pyspark’ While running Pyspark in docker","<python><docker><apache-spark><pyspark><dockerfile>","<p>I am trying to install pyspark as this:</p>

<pre><code>python setup.py install
</code></pre>

<p>I get this error:</p>

<pre><code>Could not import pypandoc - required to package PySpark
</code></pre>

<p>pypandoc is installed already</p>

<p>Any ideas how can I install pyspark?</p>
","<p>Getting the error as:</p>
<blockquote>
<p>Traceback (most recent call last):   File “/opt/application/main.py”,
line 6, in 
from pyspark import SparkConf, SparkContext ModuleNotFoundError: No module named ‘pyspark’</p>
</blockquote>
<p>While running pyspark in docker.</p>
<p>And my dockerfile is as follows:</p>
<pre><code>FROM centos
ENV DAEMON_RUN=true
ENV SPARK_VERSION=2.4.7
ENV HADOOP_VERSION=2.7
WORKDIR /opt/application
RUN yum -y install python36
RUN yum -y install wget
ENV PYSPARK_PYTHON python3.6
ENV PYSPARK_DRIVER_PYTHON python3.6
RUN ln -s /usr/bin/python3.6 /usr/local/bin/python
RUN wget https://bootstrap.pypa.io/get-pip.py
RUN python get-pip.py
RUN pip3.6 install numpy
RUN pip3.6 install pandas
RUN wget --no-verbose http://apache.mirror.iphh.net/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz &amp;&amp; tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
      &amp;&amp; mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark \
      &amp;&amp; rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
ENV SPARK_HOME=/usr/local/bin/spark
RUN yum -y install java-1.8.0-openjdk
ENV JAVA_HOME /usr/lib/jvm/jre
COPY main.py .
RUN chmod +x /opt/application/main.py
CMD [&quot;/opt/application/main.py&quot;]
</code></pre>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","66235533","How to prevent others from seeing code inside my container?","<docker><security><containers>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I don't want the other party to see the code in my container when the other party pulls the image of my container registered on the docker hub.</p>
<p>Or is there a way to make my container inaccessible with the command of <code>docker exec</code>??</p>
<p>Is there any way to prevent this?</p>
"
"53198798","Remote debugging: No connection to Wildfly 14 on OpenJDK 11 at port 8787","<java><debugging><wildfly><remote-debugging><java-11>","54596012","Debug JEE application in a Docker container","<java><docker><debugging><docker-compose>","<p>I'm trying to connect my debugger to Wildlfy running on Open JDK 11.</p>

<p>Despite Wildfly says: </p>

<blockquote>
  <p><code>Listening for transport dt_socket at address: 8787</code> </p>
</blockquote>

<p>My IDE (IntelliJ IDEA CE 2018.1) claims that it doesn't get any connection:</p>

<blockquote>
  <p><code>Unable to open debugger port (localhost:8787): java.io.IOException ""handshake failed - connection prematurally closed""</code>.</p>
</blockquote>

<p>I'm starting Wildfly via <code>standalone.sh --debug</code> resulting in the following <code>JAVA_OPTS</code>:</p>

<pre><code>-server
-Xms64m
-Xmx512m
-XX:MetaspaceSize=96M
-XX:MaxMetaspaceSize=256m
-Djava.net.preferIPv4Stack=true
-Djboss.modules.system.pkgs=org.jboss.byteman
-Djava.awt.headless=true
-agentlib:jdwp=transport=dt_socket,address=8787,server=y,suspend=n
--add-exports=java.base/sun.nio.ch=ALL-UNNAMED
--add-exports=jdk.unsupported/sun.misc=ALL-UNNAMED
--add-exports=jdk.unsupported/sun.reflect=ALL-UNNAMED
--add-modules=java.se
</code></pre>

<p>Did something change in Java 9/10/11? Remote debugging with the exact same setup works fine when using Oracle JDK 8.</p>

<p>Using telnet I can confirm, that port <code>8787</code> is indeed not reachable.</p>

<p><strong>Update</strong> after reading @ehsavoie's comment: <code>netstat -ln</code> on the server running Wildfly shows:</p>

<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State   
tcp        0      0 127.0.0.1:8787          0.0.0.0:*               LISTEN 
</code></pre>

<p>So apparently with OpenJDK 11 the debug port is now bound to localhost by default.</p>
","<p>I want to debug my containerized application running on Wildfly. But it doesn't work. :(</p>

<p>Dockerfile for my application:</p>

<pre><code>FROM jboss/wildfly
ADD standalone-custom.xml /opt/jboss/wildfly/standalone/configuration/ 
EXPOSE 9990 
EXPOSE 8787
CMD [""/opt/jboss/wildfly/bin/standalone.sh"", ""-c"", ""standalone-custom.xml"", ""-b"", ""0.0.0.0"", ""-bmanagement"", ""0.0.0.0"", ""--debug"", ""8787""]
</code></pre>

<p>docker-compose.yml:</p>

<pre><code>version: '3'
...

services:
...

  myappservice:
    image: myappimage
    ports:
      - ""8080:8080""
      - ""8787:8787""
      - ""9990:9990""
    ...
</code></pre>

<p>When I try to attach a debugger with: </p>

<pre><code>jdb -attach localhost:8787
</code></pre>

<p>I receive the following:</p>

<blockquote>
  <p>java.io.IOException: handshake failed - connection prematurally closed
    at
  jdk.jdi/com.sun.tools.jdi.SocketTransportService.handshake(SocketTransportService.java:142)
    at
  jdk.jdi/com.sun.tools.jdi.SocketTransportService.attach(SocketTransportService.java:255)
    at
  jdk.jdi/com.sun.tools.jdi.GenericAttachingConnector.attach(GenericAttachingConnector.java:119)
    at
  jdk.jdi/com.sun.tools.jdi.SocketAttachingConnector.attach(SocketAttachingConnector.java:83)
    at
  jdk.jdi/com.sun.tools.example.debug.tty.VMConnection.attachTarget(VMConnection.java:519)
    at
  jdk.jdi/com.sun.tools.example.debug.tty.VMConnection.open(VMConnection.java:328)
    at jdk.jdi/com.sun.tools.example.debug.tty.Env.init(Env.java:63)    at
  jdk.jdi/com.sun.tools.example.debug.tty.TTY.main(TTY.java:1082)</p>
  
  <p>Fatal error: Unable to attach to target VM.</p>
</blockquote>

<p>I would be grateful for any suggestions.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60816086","KafkaProducer not reading messages","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have a dockerized Spark app for simple streaming. The listener generates random numbers and sends them to Kafka using this code:</p>

<pre><code>producer = KafkaProducer(bootstrap_servers=kafka_brokers, api_version=(0, 10, 1))
while True:
    data = //generate a json with a single number
    producer.send(topic_name, str.encode(json.dumps(data)))
</code></pre>

<p>Then I try to read this data using the consumer as such:</p>

<pre><code>consumer = KafkaConsumer(topic_name, bootstrap_servers=['192.168.99.100:9092'])
for message in consumer:
    record = json.loads(message.value)
    list.append(record['field'])
</code></pre>

<p>When I run the code it never gets past the 'for message in consumer' part. I checked within Kafka and the messages are all there but I cannot access them through Python.</p>

<p>Edit: I am using the <a href=""https://github.com/bitnami/bitnami-docker-spark"" rel=""nofollow noreferrer"">bitnami spark</a> containers and <a href=""https://github.com/simplesteph/kafka-stack-docker-compose/blob/master/zk-single-kafka-single.yml"" rel=""nofollow noreferrer"">this</a> settings for kafka and zookeeper.</p>

<p>I just have two separate files, one for the producer and one for the consumer. I run the producer file which sends to Kafka then I spark-submit the consumer file which should just print a list of the numbers received. For this I simply do spark-submit --master spark://spark:7077 consumer.py</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60894615","""kafka.errors.NoBrokersAvailable: NoBrokersAvailable"" problem","<python><docker><apache-kafka><spring-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i investigate too much topic in stackoverflow but i couldn't find solution for me. I have a docker-compose file and i am trying to set true parameters for configuration. Other than that i have a django api that manages web requests. </p>

<p>My goal is when a user visits someone elses ""post"" i want to send a message to kafka ""x user visited y users 'z' titled post"". Finished all my configurations. In endpoint i configured a producer that sends message to kafka. After request to endpoint it gives me that error in title. How can i solve that ? </p>

<p>django view.py;</p>

<pre><code> def get(self,request,post_id,format=None):
    """"""
    Gets a post
    """"""
    post = self.get_post(post_id)
    serializer = PostSerializer(post)

    send_data(post.owner, request.user, post, datetime.now().strftime(""%d/%m/%Y %H:%M:%S"") )

    return Response(serializer.data)
</code></pre>

<p>send data func;</p>

<pre><code>def send_data(visitor_user:User, owner_user:User, visitedPost:Post, timestamp:date):
  from kafka.errors import KafkaError
  from kafka import KafkaProducer
  producer = KafkaProducer( bootstrap_servers=['kafka:9092'] )


  visitor = visitor_user.first_name + "" "" + visitor_user.last_name + ""(username:"" + visitor_user.username + "") ""
  owner = owner_user.first_name + owner_user.last_name + ""(username:"" + owner_user.username + "") ""
  post = ""'"" + visitedPost.title + ""' titled post.""
  message = timestamp + ""-&gt;"" + visitor + ""read"" + owner + "" 's "" + post

  result = producer.send( 'visitation-log', message.encode() )
</code></pre>

<p>docker-compose.yml;</p>

<pre><code>version : ""3""

services:
    db:
       image: postgres
       environment:
        - ""POSTGRES_HOST_AUTH_METHOD=trust""
    dj: 
       container_name: dja 
       build: django # django's Dockerfile path
       command: python manage.py runserver 0.0.0.0:80
       volumes:
       - ./django:/code

       ports:
        - ""80:80""
       depends_on:
        - db

    ng:
        container_name: ngtrip
        build: angular
        ports:
            - ""8080:80""

    zookeeper:
        image: wurstmeister/zookeeper
        container_name: zookeeper
        restart: always
        ports:
            - 2181:2181

    kafka:
        image: wurstmeister/kafka
        container_name: kafka
        hostname: localhost
        restart: always
        ports:
            - 9092:9092
        depends_on:
            - zookeeper
        links:
            - zookeeper:zookeeper
        environment:
            KAFKA_CREATE_TOPICS: ""visitation-log:1:3,Topic2:1:1:compact""
            KAFKA_LISTENERS: PLAINTEXT://localhost:9092
            KAFKA_ADVERTISED_HOST: localhost
            KAFKA_ADVERTISED_POST: 9092
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
</code></pre>

<p><strong>UPDATE</strong></p>

<p>I organized my kafka environment configs like belove thanks to help of cricket and it is now producing messages from django with ""kafka:9092"" . </p>

<pre><code>kafka:
    image: wurstmeister/kafka
    command: [start-kafka.sh]
    container_name: kafka
    restart: always
    ports:
        - ""29092:9092""
    environment:
        KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
        KAFKA_ADVERTISED_LISTENERS: INSIDE://${container_ip}:9092,OUTSIDE://${outside_host_ip}:29092
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: ""true""
</code></pre>

<p>${} fields are empty string for now because i didn't attach anything to them. But i must ask another thing. My consumer is located outside of docker. Its in my local. When i set ConsumerConfig bootstrap.server config to ""kafka:29092"" or anything else it doesn't matter when i run the app it stills checking localhost:9092. How can i solve that?</p>

<p><strong>FINAL UPDATE</strong></p>

<p>It seems it is a total rookie mistake. I changed image to confluent kafka image and about that packaging problem i used spotify's docker maven plugin and created a image. Final docker-compose file;</p>

<pre><code>zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
        ZOOKEEPER_CLIENT_PORT: 2181
        ZOOKEEPER_TICK_TIME: 2000

kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
        - zookeeper
    ports:
        - 9092:9092
    environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: ""true""
consumer:
    image: kafkaconsumer:latest
    ports:
        - ""8070:8070""
    depends_on:
      - kafka
    environment:
      SPRING_KAFKA_BOOTSTRAPSERVERS: kafka:29092
    restart: always
</code></pre>

<p>i can access to kafka in docker with kafka:29092 and outside the kafka i use localhost:9092 with these configuration. Consumer image is builted by docker-maven plugin.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60755094","Running clickhouse and Kafka in docker","<docker><apache-kafka><clickhouse>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am trying to integrate Kafka with clickhouse. I want to send in some data from my REST API to kafka, and pass it to clickhouse which would be my DB. So far, I was able to get Kafka to working indivually and pass data from one end to other. But when I tried to use it in docker, I am having some issues. Well, I can see the data being consumed at the kafka end, using a kafka tool. But my clickhouse apparently is not getting connected. This is the error, I keep getting when I run my docker:</p>

<pre><code>clickhouse-server_1  | %3|1584611439.583|FAIL|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 1ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 1ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-1| [thrd:localhost:9092/bootstrap]: 1/1 brokers are down
clickhouse-server_1  | %3|1584611439.583|FAIL|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
clickhouse-server_1  | %3|1584611439.583|ERROR|ClickHouse 20.3.3.6#consumer-2| [thrd:localhost:9092/bootstrap]: 1/1 brokers are down
</code></pre>

<p>I have also added the log file below:</p>

<pre><code>2020.03.19 10:16:11.637331 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:8123 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.637658 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:9000 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.637854 [ 1 ] {} &lt;Error&gt; Application: Listen [::]:9009 failed: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = DNS error: EAI: -9 (version 20.3.3.6 (official build)). If it is an IPv6 or IPv4 address and your host has disabled IPv6 or IPv4, then consider to specify not disabled IPv4 or IPv6 address to listen in &lt;listen_host&gt; element of configuration file. Example for disabled IPv6: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; . Example for disabled IPv4: &lt;listen_host&gt;::&lt;/listen_host&gt;
2020.03.19 10:16:11.678748 [ 74 ] {} &lt;Error&gt; void DB::DDLWorker::runMainThread(): Code: 999, e.displayText() = Coordination::Exception: All connection tries failed while connecting to ZooKeeper. Addresses: 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused (version 20.3.3.6 (official build)), 192.168.144.3:2181
 (Connection loss), Stack trace (when copying this message, always include the lines below):

0. Poco::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0x102d352c in /usr/bin/clickhouse
1. DB::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0x8f2d989 in /usr/bin/clickhouse
2. Coordination::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int) @ 0xdd300e4 in /usr/bin/clickhouse
3. Coordination::Exception::Exception(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int) @ 0xdd3070e in /usr/bin/clickhouse
4. ? @ 0xdd638b2 in /usr/bin/clickhouse
5. Coordination::ZooKeeper::ZooKeeper(std::__1::vector&lt;Poco::Net::SocketAddress, std::__1::allocator&lt;Poco::Net::SocketAddress&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, Poco::Timespan, Poco::Timespan, Poco::Timespan) @ 0xdd6099b in /usr/bin/clickhouse
6. zkutil::ZooKeeper::init(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) @ 0xdd3fd53 in /usr/bin/clickhouse
7. zkutil::ZooKeeper::ZooKeeper(Poco::Util::AbstractConfiguration const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) @ 0xdd405bd in /usr/bin/clickhouse
8. DB::Context::getZooKeeper() const @ 0xce5fb97 in /usr/bin/clickhouse
9. DB::DDLWorker::getAndSetZooKeeper() @ 0xce84c53 in /usr/bin/clickhouse
10. DB::DDLWorker::runMainThread() @ 0xce8eae3 in /usr/bin/clickhouse
11. ThreadFromGlobalPool::ThreadFromGlobalPool&lt;void (DB::DDLWorker::*)(), DB::DDLWorker*&gt;(void (DB::DDLWorker::*&amp;&amp;)(), DB::DDLWorker*&amp;&amp;)::'lambda'()::operator()() const @ 0xce96031 in /usr/bin/clickhouse
12. ThreadPoolImpl&lt;std::__1::thread&gt;::worker(std::__1::__list_iterator&lt;std::__1::thread, void*&gt;) @ 0x8f50b07 in /usr/bin/clickhouse
13. ? @ 0x8f4f00f in /usr/bin/clickhouse
14. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so
15. __clone @ 0x12188f in /lib/x86_64-linux-gnu/libc-2.27.so
 (version 20.3.3.6 (official build))
</code></pre>

<p>Here are some of the configuration I am using:</p>

<pre><code>CREATE TABLE IF NOT EXISTS filters (
  name String,
  value String
) ENGINE = Kafka SETTINGS
            kafka_broker_list = 'kafka://localhost:9092',
            kafka_topic_list = 'testtopic',
            kafka_group_name = 'test',
            kafka_format = 'JSONEachRow',
            kafka_num_consumers = 2
</code></pre>

<p>My kafka bootstrapping part:</p>

<pre><code>producer = KafkaProducer(bootstrap_servers=['kafka://localhost:9092'], value_serializer=lambda m: json.dumps(m).encode('ascii'))
</code></pre>

<p>And my docker config part:</p>

<pre><code>version: ""3.6""

services:
  clickhouse-server:
    image: yandex/clickhouse-server
    volumes:
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse/zookeeper-servers.xml:/etc/clickhouse-server/conf.d/zookeeper-servers.xml
      - ./shared/ch-data/clickhouse:/var/lib/clickhouse
    depends_on:
      - kafka
    links:
      - kafka
    ports:
      - 9000:9000

  clickhouse-client:
    image: yandex/clickhouse-client
    entrypoint:
      - /bin/sleep
    command:
      - infinity

  kafka:
    image: wurstmeister/kafka:2.11-1.0.2
    volumes:
      - ./shared/ch-data/kafka:/data
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: 192.168.1.86
    links:
     - zookeeper
    ports:
     - 9092:9092
     - 9094:9094

  zookeeper:
    image: zookeeper
    volumes:
      - ./shared/ch-data/zookeeper:/data
    ports:
      - 2181:2181
</code></pre>

<p>I am particularly confused on the bootstrap_Servers and KAFKA_ADVERTISED_LISTENERS part. So far as I know, I have provided my host IP as the KAFKA_ADVERTISED_LISTENER, from what I understood from <a href=""https://www.confluent.io/blog/kafka-listeners-explained/"" rel=""nofollow noreferrer"">here</a> </p>
"
"53451103","Java using much more memory than heap size (or size correctly Docker memory limit)","<java><linux><docker><memory><jvm>","54713829","Java memory usage much higher than heap+nonheap","<java><spring-boot><docker><memory><heap-memory>","<p>For my application, the memory used by the Java process is much more than the heap size.</p>

<p>The system where the containers are running starts to have memory problem because the container is taking much more memory than the heap size.</p>

<p>The heap size is set to 128 MB (<code>-Xmx128m -Xms128m</code>) while the container takes up to 1GB of memory. Under normal condition, it needs 500MB. If the docker container has a limit below (e.g. <code>mem_limit=mem_limit=400MB</code>) the process gets killed by the out of memory killer of the OS.</p>

<p><strong>Could you explain why the Java process is using much more memory than the heap? How to size correctly the Docker memory limit? Is there a way to reduce the off-heap memory footprint of the Java process?</strong></p>

<hr>

<p>I gather some details about the issue using command from <a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">Native memory tracking in JVM</a>.</p>

<p>From the host system, I get the memory used by the container.</p>

<pre><code>$ docker stats --no-stream 9afcb62a26c8
CONTAINER ID        NAME                                                                                        CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
9afcb62a26c8        xx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.0acbb46bb6fe3ae1b1c99aff3a6073bb7b7ecf85   0.93%               461MiB / 9.744GiB   4.62%               286MB / 7.92MB      157MB / 2.66GB      57
</code></pre>

<p>From inside the container, I get the memory used by the process.</p>

<pre><code>$ ps -p 71 -o pcpu,rss,size,vsize
%CPU   RSS  SIZE    VSZ
11.2 486040 580860 3814600
</code></pre>

<hr>

<pre><code>$ jcmd 71 VM.native_memory
71:

Native Memory Tracking:

Total: reserved=1631932KB, committed=367400KB
-                 Java Heap (reserved=131072KB, committed=131072KB)
                            (mmap: reserved=131072KB, committed=131072KB) 

-                     Class (reserved=1120142KB, committed=79830KB)
                            (classes #15267)
                            (  instance classes #14230, array classes #1037)
                            (malloc=1934KB #32977) 
                            (mmap: reserved=1118208KB, committed=77896KB) 
                            (  Metadata:   )
                            (    reserved=69632KB, committed=68272KB)
                            (    used=66725KB)
                            (    free=1547KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=9624KB)
                            (    used=8939KB)
                            (    free=685KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=24786KB, committed=5294KB)
                            (thread #56)
                            (stack: reserved=24500KB, committed=5008KB)
                            (malloc=198KB #293) 
                            (arena=88KB #110)

-                      Code (reserved=250635KB, committed=45907KB)
                            (malloc=2947KB #13459) 
                            (mmap: reserved=247688KB, committed=42960KB) 

-                        GC (reserved=48091KB, committed=48091KB)
                            (malloc=10439KB #18634) 
                            (mmap: reserved=37652KB, committed=37652KB) 

-                  Compiler (reserved=358KB, committed=358KB)
                            (malloc=249KB #1450) 
                            (arena=109KB #5)

-                  Internal (reserved=1165KB, committed=1165KB)
                            (malloc=1125KB #3363) 
                            (mmap: reserved=40KB, committed=40KB) 

-                     Other (reserved=16696KB, committed=16696KB)
                            (malloc=16696KB #35) 

-                    Symbol (reserved=15277KB, committed=15277KB)
                            (malloc=13543KB #180850) 
                            (arena=1734KB #1)

-    Native Memory Tracking (reserved=4436KB, committed=4436KB)
                            (malloc=378KB #5359) 
                            (tracking overhead=4058KB)

-        Shared class space (reserved=17144KB, committed=17144KB)
                            (mmap: reserved=17144KB, committed=17144KB) 

-               Arena Chunk (reserved=1850KB, committed=1850KB)
                            (malloc=1850KB) 

-                   Logging (reserved=4KB, committed=4KB)
                            (malloc=4KB #179) 

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #512) 

-                    Module (reserved=258KB, committed=258KB)
                            (malloc=258KB #2356) 

$ cat /proc/71/smaps | grep Rss | cut -d: -f2 | tr -d "" "" | cut -f1 -dk | sort -n | awk '{ sum += $1 } END { print sum }'
491080
</code></pre>

<p>The application is a web server using Jetty/Jersey/CDI bundled inside a fat far of 36 MB.</p>

<p>The following version of OS and Java are used (inside the container). The Docker image is based on <code>openjdk:11-jre-slim</code>.</p>

<pre><code>$ java -version
openjdk version ""11"" 2018-09-25
OpenJDK Runtime Environment (build 11+28-Debian-1)
OpenJDK 64-Bit Server VM (build 11+28-Debian-1, mixed mode, sharing)
$ uname -a
Linux service1 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux
</code></pre>

<p><a href=""https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58"" rel=""noreferrer"">https://gist.github.com/prasanthj/48e7063cac88eb396bc9961fb3149b58</a></p>
","<p>I have a Java application (oracle JDK 8u191) with SpringBoot version 2.1.1.RELEASE and I'm experiencing a weird memory usage (don't thinks this is related to spring).</p>

<p>The sum of the HEAP and NONHEAP values provided by the application is much lower the real memory usage reported by Docker.</p>

<p>I know that Java has others memory areas than HEAP and NONHEAP, but I don't know how to measure them and if the current behavior is normal.</p>

<p>Curently my applications is reporting the following (after 3 days running in production):</p>

<pre><code>           Init       Used       Committed       Max       
HEAP:      250M       79M        250M            800M      
NONHEAP:   2M         192M       201M            0M     
</code></pre>

<p>The actuator endpoint <code>/metrics</code> for springBoot reports the same memory usage.</p>

<p>But the docker stats commands reports this (inside the container <code>smem</code> reports the same value for the java process):</p>

<pre><code>MEM USAGE / LIMIT
750.2MiB / 1.172GiB
</code></pre>

<p>The difference between the heap+nonheap and the current used memory are <code>299,2mb</code>.</p>

<p>I was not expecting this 300mb the first time we put this service in production it caused a lot of OOM until we found the rigth configuration.</p>

<p>This ""unregistered memory usage"" seems to only go up and never shrink, even after GC.</p>

<p>If I restart my aplication the difference between the heap+nonheap withe the real usage is about 100mb.</p>

<p>My Dockerfile starts the application this way:</p>

<p><code>ENTRYPOINT [""java"", ""-Xms250m"", ""-Xmx800m"", ""-XX:+UseG1GC"", ""-Djava.security.egd=file:/dev/./urandom"", ""-jar"", ""/app.jar""]</code></p>

<p>I also have a scheduled rotine that runs <code>System.gc()</code> every hour to shrink back the commited memory because this service have small spikes over the day and I want unused memory to be free for another services.</p>

<p>I want to know if this behavior is normal. And how can I measure what is using so much memory? visualvm and jconsole only show the heap and nonheap. Seems there is no metrics in springBoot actuator to measure this too.</p>

<p>How much memory should I left free in my container for this type of usage? I couldn't find any helpful article about it.</p>

<p><strong>Edit1:</strong></p>

<p>After 7 days, the difference between (heap+nonheap)-total became 550mb, leaving my container without any memory left.</p>

<p><strong>Edit2:</strong></p>

<p>Things starting making sense after digging into this articles:</p>

<p><a href=""https://stackoverflow.com/questions/47591343/java-process-memory-usage-deviation-from-jcmd-committed-memory?rq=1"">Java process memory usage deviation from jcmd committed memory</a></p>

<p><a href=""https://stackoverflow.com/questions/44284911/where-do-these-java-native-memory-allocated-from"">Where do these java native memory allocated from?</a></p>

<p><a href=""https://stackoverflow.com/questions/39684464/java-process-memory-usage-jcmd-vs-pmap"">Java process memory usage (jcmd vs pmap)</a></p>

<p>And indeed we are using a third party library with reports of memory leaks: <code>compile(""axis:axis-wsdl4j:1.5.1"")</code></p>

<p>I'll try upgrade to Axis2 and will post the results here.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60791807","cant read\write from kafka topic","<python><docker><apache-kafka><kafka-producer-api><kafka-python>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have Kafka running on a container on my desktop.
I can connect to it just fine using a tool called ""Kafka tool"" where I can see my topics for example.</p>

<p>I'm having issues reading and writing to/from a Kafka topic. 
what's annoying me is that it won't give me an error message, it's behaving like if the topic doesn't have any messages on it, but it does, I can see using the tool, even added two messages manually.</p>

<p>the topic exists and has two messages on it (which I added manually using this UI)</p>

<p>problem:
the code that sends messages to the topic runs fine, but the messages don't make it to Kafka
the code that reads messages from the topic doesn't read anything. It sits there like if there are no messages to be read.
Also, I can use the same consume to list the topics (which indicates the connection was successful) </p>

<p>The kafka version is 2.4. 
Any idea what the problem may be?
I have tried ""bootstrap_servers=['localhost:9092', 'kafka-server:9092']"" but it also didnt work</p>

<p>Thanks</p>

<p><a href=""https://i.stack.imgur.com/e5gZP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e5gZP.png"" alt=""enter image description here""></a></p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60792143","Kafka failing to connect broker on docker","<node.js><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have set up a Kafka cluster with docker-compose file where i specify the brokers like this:</p>

<pre><code>kafka1:
    image: confluentinc/cp-kafka:latest
    hostname: kafka1
    ports:
      - ""19092:19092""
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:12181,zookeeper-2:12181,zookeeper-3:12181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:19092
</code></pre>

<p>Now when i try to create topic with js script and i refer to the brokers like follows(for the sake of this post i only attach code about 1 broker):</p>

<pre><code>try
    {
        const kafka = new Kafka({
            clientId: 'myapp',
            brokers: ['kafka1:19092','kafka2:29092','kafka3:39092']
        })
</code></pre>

<p>I get this error:</p>

<pre><code>{""level"":""ERROR"",""timestamp"":""2020-03-21T19:06:13.653Z"",""logger"":""kafkajs"",""message"":""[Connection] Connection error: getaddrinfo ENOTFOUND kafka1"",""broker"":""kafka1:19092"",""clientId"":""myapp"",""stack"":""Error: getaddrinfo ENOTFOUND kafka1\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:66:26)""}
</code></pre>

<p>I feel like i have exhausted internet trying to troubleshoot this. Anyone have any suggestions how to connect to the broker in the docker?</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","60647636","KafkaStreamApi in Docker worked, but don't consume record","<docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>i use a kafka-wurstmeister-image on docker with a bridge-network, it works fine, i can use local producer/consumer to in-&amp;output data from kafka-topic. 
then i try to push my local producer/consumer/streamapi to docker-repository. After running i've got:<br>
<code>org.apache.kafka.common.errors.TimeoutException: Topic source-validator-topic not present in metadata after 60000 ms.</code><br>
i think there must be sth wrong with my configuration.</p>

<p>following is my step:
1. output java producer from eclipse.<br>
2. generate pom.xml for maven-build, and build.<br>
3. generate Dockerfile to create image, and create.<br>
3. push image to docker-repository.<br>
4. generate docker-compose.yml.<br>
5. run in CLi: docker-compose up  </p>

<p>here is my pom.xml file:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;StreamForBC&lt;/groupId&gt;
  &lt;artifactId&gt;StreamForBC&lt;/artifactId&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;blockchain-docker&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;
  &lt;properties&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;
  &lt;dependencies&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;3.8.1&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;


   &lt;dependency&gt;
       &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
       &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
       &lt;version&gt;1.7.5&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
       &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
       &lt;version&gt;1.7.5&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;log4j&lt;/groupId&gt;
       &lt;artifactId&gt;log4j&lt;/artifactId&gt;
       &lt;version&gt;1.2.17&lt;/version&gt;
   &lt;/dependency&gt;



    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka_2.13&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-log4j-appender&lt;/artifactId&gt;
        &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;commons-io&lt;/groupId&gt;
        &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
        &lt;version&gt;2.6&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;


    &lt;build&gt;
        &lt;finalName&gt;blockchain-software&lt;/finalName&gt;
        &lt;plugins&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;mainClass&gt;StreamForBC.ProducerTest&lt;/mainClass&gt;
                            &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;!-- copy dependencies / jars to ${project.build.directory}/lib/ --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;outputDirectory&gt;
                                ${project.build.directory}/lib/
                            &lt;/outputDirectory&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>here is my Dockerfile:</p>

<pre><code>FROM openjdk
ARG JAR_FILE=blockchain-software.jar
ARG JAR_LIB_FILE=lib/
RUN mkdir app
WORKDIR /app
COPY blockchain-software.jar .
ADD ${JAR_LIB_FILE} lib/
EXPOSE 9092:9092
CMD [""java"",""-cp"",""blockchain-software.jar"",""StreamForBC.ProducerTest""]
</code></pre>

<p>here is my docker-compose.yml:</p>

<pre><code>version: '2'
services:
  validate:
    image: billkuku/bc-validate:v1
    ports:
      - ""4880:4880""
      - ""4822:22""
    volumes:
      - './data:/home/blockchain/validate/data'

  testproducer:
    image: billkuku/producer:v1
    ports:
      - ""9093:9093""
      - ""4823:23""
    volumes:
      - './data:/home/blockchain/producer/data'


  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""

  kafka:
    image: wurstmeister/kafka
    ports:
      - ""9092:9092""
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_MESSAGE_MAX_BYTES: 2000000000
      KAFKA_CREATE_TOPICS: ""source-topic:1:1,source-validator-topic:1:1,block-validator-topic:1:1,block-verifier-topic:1:1""
    depends_on:
      - zookeeper
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>
"
"53467499","JavaFX, OS X, Graphics Device initialization failed for : es2, sw","<java><javafx><java-11><javafx-11><openjfx>","54161264","Build openjdk 11 javafx application in docker","<java><maven><docker><javafx>","<p>I'm trying to create a JavaFX project in IntelliJ IDEA, but I get errors every time.</p>

<p>After completing tutorial <a href=""https://openjfx.io/openjfx-docs/#IDE-Intellij"" rel=""noreferrer"">https://openjfx.io/openjfx-docs/#IDE-Intellij</a>, I've got this:</p>

<pre><code>Graphics Device initialization failed for :  es2, sw
Error initializing QuantumRenderer: no suitable pipeline found
java.lang.RuntimeException: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer.getInstance(QuantumRenderer.java:280)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumToolkit.init(QuantumToolkit.java:222)
    at javafx.graphics/com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:260)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:409)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.init(QuantumRenderer.java:94)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.run(QuantumRenderer.java:124)
    at java.base/java.lang.Thread.run(Thread.java:834)
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: No toolkit found
    at javafx.graphics/com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:272)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:409)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    ... 5 more

Process finished with exit code 1
</code></pre>

<p>After adding -Dprism.verbose=true to VM options in IntelliJ IDEA (so my VM options look like this:</p>

<pre><code>--module-path /myPath/javafx-sdk-11.0.1/lib --add-modules=javafx.controls,javafx.fxml -Dprism.verbose=true
</code></pre>

<p>And then I get this output:</p>

<pre><code>Prism pipeline init order: es2 sw 
Using Double Precision Marlin Rasterizer
Using dirty region optimizations
Not using texture mask for primitives
Not forcing power of 2 sizes for textures
Using hardware CLAMP_TO_ZERO mode
Opting in for HiDPI pixel scaling
Prism pipeline name = com.sun.prism.es2.ES2Pipeline
Loading ES2 native library ... prism_es2
GraphicsPipeline.createPipeline failed for com.sun.prism.es2.ES2Pipeline
java.lang.UnsatisfiedLinkError: no prism_es2 in java.library.path: [/Users/Lisa/javafx-sdk-11.0.1/lib]
    at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2660)
    at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:829)
    at java.base/java.lang.System.loadLibrary(System.java:1867)
    at javafx.graphics/com.sun.glass.utils.NativeLibLoader.loadLibraryInternal(NativeLibLoader.java:150)
    at javafx.graphics/com.sun.glass.utils.NativeLibLoader.loadLibrary(NativeLibLoader.java:52)
    at javafx.graphics/com.sun.prism.es2.ES2Pipeline.lambda$static$0(ES2Pipeline.java:68)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at javafx.graphics/com.sun.prism.es2.ES2Pipeline.&lt;clinit&gt;(ES2Pipeline.java:50)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:315)
    at javafx.graphics/com.sun.prism.GraphicsPipeline.createPipeline(GraphicsPipeline.java:187)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.init(QuantumRenderer.java:91)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.run(QuantumRenderer.java:124)
    at java.base/java.lang.Thread.run(Thread.java:834)
*** Fallback to Prism SW pipeline
Prism pipeline name = com.sun.prism.sw.SWPipeline
GraphicsPipeline.createPipeline failed for com.sun.prism.sw.SWPipeline
java.lang.UnsatisfiedLinkError: no prism_sw in java.library.path: [/Users/Lisa/javafx-sdk-11.0.1/lib]
    at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2660)
    at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:829)
    at java.base/java.lang.System.loadLibrary(System.java:1867)
    at javafx.graphics/com.sun.glass.utils.NativeLibLoader.loadLibraryInternal(NativeLibLoader.java:150)
    at javafx.graphics/com.sun.glass.utils.NativeLibLoader.loadLibrary(NativeLibLoader.java:52)
    at javafx.graphics/com.sun.prism.sw.SWPipeline.lambda$static$0(SWPipeline.java:42)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at javafx.graphics/com.sun.prism.sw.SWPipeline.&lt;clinit&gt;(SWPipeline.java:41)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:315)
    at javafx.graphics/com.sun.prism.GraphicsPipeline.createPipeline(GraphicsPipeline.java:187)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.init(QuantumRenderer.java:91)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.run(QuantumRenderer.java:124)
    at java.base/java.lang.Thread.run(Thread.java:834)
Graphics Device initialization failed for :  es2, sw
Error initializing QuantumRenderer: no suitable pipeline found
java.lang.RuntimeException: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer.getInstance(QuantumRenderer.java:280)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumToolkit.init(QuantumToolkit.java:222)
    at javafx.graphics/com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:260)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:409)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.init(QuantumRenderer.java:94)
    at javafx.graphics/com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.run(QuantumRenderer.java:124)
    at java.base/java.lang.Thread.run(Thread.java:834)
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1051)
Caused by: java.lang.RuntimeException: No toolkit found
    at javafx.graphics/com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:272)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
    at javafx.graphics/com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:409)
    at javafx.graphics/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    ... 5 more

Process finished with exit code 1
</code></pre>

<p>What should I do to make my project work?</p>
","<p>I created a Java application with openjdk 11 and javafx. Everything is fine as I have defined the javafx dependencies in my maven pom.xml.
So far everything works.</p>

<p>But when I try to automate the build in Gitlab ci with the image <em>maven:3-jdk-11</em> and <em>openjfx</em>, the jar, build by maven, is broken.</p>

<p>Every time I call the generated jar on my machine, this exception appears:</p>

<pre><code>Graphics Device initialization failed for :  d3d, sw
Error initializing QuantumRenderer: no suitable pipeline found
java.lang.RuntimeException: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
        at com.sun.javafx.tk.quantum.QuantumRenderer.getInstance(QuantumRenderer.java:280)
        at com.sun.javafx.tk.quantum.QuantumToolkit.init(QuantumToolkit.java:222)
        at com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:260)
        at com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
        at com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
        at com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
        at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:678)
        at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$2(LauncherImpl.java:195)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.RuntimeException: Error initializing QuantumRenderer: no suitable pipeline found
        at com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.init(QuantumRenderer.java:94)
        at com.sun.javafx.tk.quantum.QuantumRenderer$PipelineRunnable.run(QuantumRenderer.java:124)
        ... 1 more
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48)
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:87)
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:50)
        at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51)
Caused by: java.lang.RuntimeException: No toolkit found
        at com.sun.javafx.tk.Toolkit.getToolkit(Toolkit.java:272)
        at com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:267)
        at com.sun.javafx.application.PlatformImpl.startup(PlatformImpl.java:158)
        at com.sun.javafx.application.LauncherImpl.startToolkit(LauncherImpl.java:658)
        at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:678)
        at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$2(LauncherImpl.java:195)
        at java.base/java.lang.Thread.run(Thread.java:834)
</code></pre>
"
"52495429","Setting -XX:MaxRam","<java><performance><memory><jvm><jvm-hotspot>","60636907","How JVM -XX:MaxRAM option can be correctly used?","<java><docker><centos><jvm>","<p>According to this <a href=""https://developers.redhat.com/blog/author/cflood/"" rel=""noreferrer"">link</a>, there is an option to set MaxRamSize manually to restrict the JVM to not use memory beyond this. But I have not seen any documentation of the same. I've never known this. Is there anything like this or anything similar?
PS. I know and I'm not looking to set heap/stack/metaspace/native memory sizes. I just would like to know if there is an overall memory limiting option. </p>

<p>Trying it did not help as It errored out:</p>

<pre><code>Improperly specified VM option 'MaxRAM=1073741824B'
Could not create the Java Virtual Machine.
A fatal exception has occurred. Program will exit.
</code></pre>

<p>Infact according to this <a href=""https://developers.redhat.com/blog/2017/04/04/openjdk-and-containers/#more-433899"" rel=""noreferrer"">link</a> open-jdk seems to have these options. 
Another <a href=""https://community.oracle.com/thread/1543459"" rel=""noreferrer"">link</a> that I found I believe points to set the heap size. Which again is not looking for. But this is for Oracle I guess.</p>

<p>Why I'm looking for this kind of an option to run the application inside a container (Like Docker) and prevent the application from being killed by the OOM Killer. What I believe is if there is a setting of such the java application would error or crash with a <code>java.lang.OutOfMemoryError</code> rather, than the container being terminated. </p>

<p>My assumptions and understandings may be totally wrong.
This question may also be totally wrong and irrelevant. But of course, asking is the way forward :). </p>
","<p>I run a java application with the following parameters:</p>

<pre><code>#!/bin/bash

export JVM_OPTS=""-XX:MaxRAM=150m""
export JVM_OPTS=""$JVM_OPTS -XX:+UseSerialGC""

java $JVM_OPTS -jar application.jar
</code></pre>

<p>The <code>htop</code> shows:</p>

<ul>
<li>VIRT=475M</li>
<li>RES=238M</li>
<li>SHR=4880</li>
<li>MEM%=24.1</li>
</ul>

<p>As I understand it, I need to look at the <code>RES</code> parameter. But in this case, it greatly exceeds <code>-XX:MaxRAM</code>. Expected that in this case, <code>OutOfMemoryException</code> will happen. What am I doing wrong? How to limit the memory of a java application for a container? Am I incorrectly looking at the used process memory?</p>

<p>I want to minimize the used RAM. OS - ​​CentOS 7</p>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","54045791","Docker rm command throws error when using $()","<docker>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I'm on a Windows 10 Enterprise OS and Docker ver 18.09.</p>

<p>From the <a href=""https://docs.docker.com/engine/reference/commandline/rm/#remove-a-link-specified-with---link-on-the-default-bridge-network"" rel=""nofollow noreferrer"">Docker documentation</a> docker rm $(docker ps -a -q) gives an error:</p>

<p>unknown shorthand flag: 'a' in -aq)<br>
See 'docker rm --help'</p>

<p>I can remove the containers one at a time with the Docker container rm command just fine, but I wanted to know why i'm getting this error.</p>
"
"54121031","Multiple commands on docker ENTRYPOINT","<docker><dockerfile>","66707054","How to run multiple ENTRYPOINT script in docker","<docker>","<p>I'm trying to build a custom tcserver docker image. But I'm having some problems starting the webserver and the tomcat.<br/>
As far as I understand I should use ENTRYPOINT to run the commands I want.<br/>
The question is, is it possible to run multiple commands with ENTRYPOINT?<br/>
Or should I create a small bash script to start all?<br/><br/></p>

<p>Basically what I would like to do is:<br/></p>

<pre><code>ENTRYPOINT /opt/pivotal/webserver/instance1/bin/httpdctl start &amp;&amp; /opt/pivotal/webserver/instance2/bin/httpdctl start &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance1 -i /opt/pivotal/pivotal-tc-server-standard &amp;&amp; /opt/pivotal/pivotal-tc-server-standard/standard-4.0.1.RELEASE/tcserver start instance2 -i /opt/pivotal/pivotal-tc-server-standard
</code></pre>

<p>But I don't know if that is a good practice or if that would even work.</p>
","<p>I have a bunch of script inside my docker image</p>
<pre><code>script01.sh
script02.sh
script03.sh
...
</code></pre>
<p>I would like to run them (in a particular order) in the ENTRYPOINT statemant:</p>
<pre><code>ENTRYPOINT [&quot;script01.sh&quot;, &quot;script02.sh&quot;, &quot;script03.sh&quot;, ...]
</code></pre>
<p>I've tryed different formulations (adding &quot;bash&quot;, &quot;.&quot; in the entrypoint commands), but nothing seems to work.</p>
<p>Any suggestion?</p>
<p>This : <a href=""https://stackoverflow.com/questions/54121031/multiple-commands-on-docker-entrypoint"">Multiple commands on docker ENTRYPOINT</a> did not work in my case so I wrote a new question as the site suggestet me.</p>
"
"54374839","NodeJS could not connect to MYSQL inside Docker Container","<mysql><node.js><docker><sequelize.js>","54384845","NodeJS could not connect to MYSQL latest version inside Docker Container","<mysql><node.js><docker><docker-compose>","<p>I've complied following docker-compose file, encounter error message.</p>

<pre><code>version: '3'
services:
  node:
    build: ./app
    environment:
      - NODE_ENV=production
      - PORT=3000
      - MYSQL_HOST=ppshein-mysql
      - MYSQL_USER=root
      - MYSQL_PASSWORD=ppshein123456
    ports:
      - ""3000:3000""
    networks:
    - my-bridge-network
    depends_on:
      - ""${MYSQL_HOST}""
    container_name: ppshein-api
  db:
    image: mysql:8.0.14
    environment:
      - MYSQL_ROOT_PASSWORD=ppshein123456
    networks:
    - my-bridge-network
    container_name: ppshein-mysql
networks:
  my-bridge-network:
    driver: bridge
</code></pre>

<p>Here is configuration file for mysql sequelize,</p>

<pre><code>""production"": {
    ""username"": ""root"",
    ""password"": ""ppshein123456"",
    ""database"": ""database_production"",
    ""host"": ""ppshein-mysql"",
    ""dialect"": ""mysql"",
    ""logging"": false
}
</code></pre>

<p>and Here is <code>Dockerfile</code> for NodeJS app,</p>

<pre><code>FROM node:9.10.1
ENV NODE_ENV=development
COPY ./ /var/www
WORKDIR /var/www/
RUN yarn install &amp;&amp; yarn add sequelize-cli -g
EXPOSE 3000
CMD /bin/bash ./wait-for-it.sh ppshein-mysql:3306 -- npm run docker
</code></pre>

<p>then, finally I encountered following error message:</p>

<blockquote>
  <p>Loaded configuration file ""config/config.json"". Using environment
  ""production"". Sat, 26 Jan 2019 01:03:45 GMT sequelize deprecated
  String based operators are now deprecated. Please use Symbol based
  operators for better security, read more at
  <a href=""http://docs.sequelizejs.com/manual/tutorial/querying.html#operators"" rel=""nofollow noreferrer"">http://docs.sequelizejs.com/manual/tutorial/querying.html#operators</a> at
  node_modules/sequelize/lib/sequelize.js:242:13</p>
  
  <p>ERROR: connect ECONNREFUSED 172.21.0.2:3306</p>
</blockquote>
","<p>NodeJS cannot connect to MySQL latest version or either 8 onwards and encountered following error message:</p>

<p><code>ERROR: connect ECONNREFUSED 172.21.0.2:3306</code></p>

<p><strong>Here is my docker-compose file</strong></p>

<pre><code>version: '2.1'
services:
  db:
      build: ./db
      networks:
        - ppshein
      environment:
          - MYSQL_ALLOW_EMPTY_PASSWORD=yes
      healthcheck:
          test: ""exit 0""
  node:
    build: ./app
    depends_on:
      db:
        condition: service_healthy
    ports:
      - 3000:3000
    networks:
      - ppshein
networks:
  ppshein:
</code></pre>

<p><strong>Here is db Dockerfiles</strong></p>

<pre><code>FROM mysql:5
COPY init_db.sql /docker-entrypoint-initdb.d/
</code></pre>

<p><strong>init_db.sql</strong></p>

<pre><code>CREATE DATABASE IF NOT EXISTS database_docker;
GRANT ALL PRIVILEGES on database_docker.*
TO 'root'@'%' IDENTIFIED BY 'ppshein123456'
WITH GRANT OPTION;
</code></pre>

<p><strong>NodeJS Dockerfile</strong></p>

<pre><code>FROM node:9.10.1
ENV NODE_ENV=docker
COPY ./ /var/www
WORKDIR /var/www/
RUN yarn install &amp;&amp; yarn add sequelize-cli -g
EXPOSE 3000
ENTRYPOINT [ ""npm"", ""run"", ""docker"" ]
</code></pre>

<p>Config.json</p>

<pre><code>""docker"": {
    ""username"": ""root"",
    ""password"": ""ppshein123456"",
    ""database"": ""database_docker"",
    ""host"": ""db"",
    ""dialect"": ""mysql"",
    ""logging"": false
}
</code></pre>

<p>But everything is working file when I've changed to <code>FROM mysql:5</code> but <code>FROM mysql</code> or <code>FROM mysql:8</code>, I've encountered above error I've mentioned. Please let me know which kind of configuration do I need to miss it?</p>
"
"52640304","standard_init_linux.go:190: exec user process caused ""no such file or directory"" Docker with go basic web app","<docker><go><dockerfile>","66954240","Dockerfile from scratch with Go Lang return error no such file or directory","<docker><go>","<p>The very basic web app is created in Go</p>
<pre><code>package main

import(
   &quot;fmt&quot;
   &quot;net/http&quot;
   &quot;os&quot;
)

func hostHandler(w http.ResponseWriter, r *http.Request){
    name, err :=os.Hostname()

    if err != nil {
           panic(err)
        }

        fmt.Fprintf(w, &quot;&lt;h1&gt;HOSTNAME: %s&lt;/h1&gt;&lt;br&gt;&quot;,name)
        fmt.Fprintf(w, &quot;&lt;h1&gt;ENVIRONMENT VARS: &lt;/h1&gt;&lt;br&gt;&quot;)
        fmt.Fprintf(w, &quot;&lt;ul&gt;&quot;)

        for _, evar := range os.Environ(){
            fmt.Fprintf(w, &quot;&lt;li&gt;%s&lt;/li&gt;&quot;,evar)
        }
        fmt.Fprintf(w, &quot;&lt;/ul&gt;&quot;)

}

func rootHandler(w http.ResponseWriter, r *http.Request){

    fmt.Fprintf(w, &quot;&lt;h1&gt;Awesome site in Go!&lt;/h1&gt;&lt;br&gt;&quot;)
    fmt.Fprintf(w, &quot;&lt;a href='/host/'&gt;Host info&lt;/a&gt;&lt;br&gt;&quot;)

}

func main() {

        http.HandleFunc(&quot;/&quot;, rootHandler)
        http.HandleFunc(&quot;/host/&quot;, hostHandler)
        http.ListenAndServe(&quot;:8080&quot;, nil)


}
</code></pre>
<p>Docker File for it</p>
<pre><code>FROM scratch
WORKDIR /home/ubuntu/go
COPY webapp /
EXPOSE 8080
CMD [&quot;/webapp&quot;]
</code></pre>
<p>The image is built successfully</p>
<pre><code>ubuntu@ip-172-31-32-125:~/go/src/hello$ docker build -t &quot;webapp&quot; .
Sending build context to Docker daemon  6.152MB
Step 1/5 : FROM scratch
 ---&gt;
Step 2/5 : WORKDIR /home/ubuntu/go
 ---&gt; Using cache
 ---&gt; 8810a06c58c7
Step 3/5 : COPY webapp /
 ---&gt; Using cache
 ---&gt; d75222363d3a
Step 4/5 : EXPOSE 8080
 ---&gt; Using cache
 ---&gt; 45de0853de8e
Step 5/5 : CMD [&quot;/webapp&quot;]
 ---&gt; Using cache
 ---&gt; e9f9031f3632
Successfully built e9f9031f3632
Successfully tagged webapp:latest
</code></pre>
<p>But when i run the docker its show error.</p>
<pre><code>ubuntu@ip:~/go/src/hello$ docker run webapp
standard_init_linux.go:190: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Please guide what is the issue, I am new to docker and go.</p>
<p>Environment-related information</p>
<pre><code>ubuntu@ip:~/go/src/hello$ ls
Dockerfile  webapp
ubuntu@ip:~/go/src/hello$ echo $GOPATH
/home/ubuntu/go
</code></pre>
<p>Code was compiled with <strong>go build webapp.go</strong> command</p>
","<p>here is my dockerfile content:</p>
<pre><code>FROM golang:latest as goCompiler

COPY app/main.go .

RUN go build main.go

FROM scratch

COPY --from=goCompiler /go/main /main

ENTRYPOINT [ &quot;/main&quot; ]
</code></pre>
<p>i got this error <em>standard_init_linux.go:219: exec user process caused: no such file or directory</em></p>
<p>after i change scratch to alpine i get same error but if i provide there ubuntu image everything is working</p>
<p>i watched tutorial in internet with C example and everything was good there</p>
<p>Scratch isnt working with Go Lang?</p>
<p>Solution is add flag to compile  CGO_ENABLED=0</p>
<pre><code>RUN CGO_ENABLED=0 go build main.go
</code></pre>
"
"54384845","NodeJS could not connect to MYSQL latest version inside Docker Container","<mysql><node.js><docker><docker-compose>","54374839","NodeJS could not connect to MYSQL inside Docker Container","<mysql><node.js><docker><sequelize.js>","<p>NodeJS cannot connect to MySQL latest version or either 8 onwards and encountered following error message:</p>

<p><code>ERROR: connect ECONNREFUSED 172.21.0.2:3306</code></p>

<p><strong>Here is my docker-compose file</strong></p>

<pre><code>version: '2.1'
services:
  db:
      build: ./db
      networks:
        - ppshein
      environment:
          - MYSQL_ALLOW_EMPTY_PASSWORD=yes
      healthcheck:
          test: ""exit 0""
  node:
    build: ./app
    depends_on:
      db:
        condition: service_healthy
    ports:
      - 3000:3000
    networks:
      - ppshein
networks:
  ppshein:
</code></pre>

<p><strong>Here is db Dockerfiles</strong></p>

<pre><code>FROM mysql:5
COPY init_db.sql /docker-entrypoint-initdb.d/
</code></pre>

<p><strong>init_db.sql</strong></p>

<pre><code>CREATE DATABASE IF NOT EXISTS database_docker;
GRANT ALL PRIVILEGES on database_docker.*
TO 'root'@'%' IDENTIFIED BY 'ppshein123456'
WITH GRANT OPTION;
</code></pre>

<p><strong>NodeJS Dockerfile</strong></p>

<pre><code>FROM node:9.10.1
ENV NODE_ENV=docker
COPY ./ /var/www
WORKDIR /var/www/
RUN yarn install &amp;&amp; yarn add sequelize-cli -g
EXPOSE 3000
ENTRYPOINT [ ""npm"", ""run"", ""docker"" ]
</code></pre>

<p>Config.json</p>

<pre><code>""docker"": {
    ""username"": ""root"",
    ""password"": ""ppshein123456"",
    ""database"": ""database_docker"",
    ""host"": ""db"",
    ""dialect"": ""mysql"",
    ""logging"": false
}
</code></pre>

<p>But everything is working file when I've changed to <code>FROM mysql:5</code> but <code>FROM mysql</code> or <code>FROM mysql:8</code>, I've encountered above error I've mentioned. Please let me know which kind of configuration do I need to miss it?</p>
","<p>I've complied following docker-compose file, encounter error message.</p>

<pre><code>version: '3'
services:
  node:
    build: ./app
    environment:
      - NODE_ENV=production
      - PORT=3000
      - MYSQL_HOST=ppshein-mysql
      - MYSQL_USER=root
      - MYSQL_PASSWORD=ppshein123456
    ports:
      - ""3000:3000""
    networks:
    - my-bridge-network
    depends_on:
      - ""${MYSQL_HOST}""
    container_name: ppshein-api
  db:
    image: mysql:8.0.14
    environment:
      - MYSQL_ROOT_PASSWORD=ppshein123456
    networks:
    - my-bridge-network
    container_name: ppshein-mysql
networks:
  my-bridge-network:
    driver: bridge
</code></pre>

<p>Here is configuration file for mysql sequelize,</p>

<pre><code>""production"": {
    ""username"": ""root"",
    ""password"": ""ppshein123456"",
    ""database"": ""database_production"",
    ""host"": ""ppshein-mysql"",
    ""dialect"": ""mysql"",
    ""logging"": false
}
</code></pre>

<p>and Here is <code>Dockerfile</code> for NodeJS app,</p>

<pre><code>FROM node:9.10.1
ENV NODE_ENV=development
COPY ./ /var/www
WORKDIR /var/www/
RUN yarn install &amp;&amp; yarn add sequelize-cli -g
EXPOSE 3000
CMD /bin/bash ./wait-for-it.sh ppshein-mysql:3306 -- npm run docker
</code></pre>

<p>then, finally I encountered following error message:</p>

<blockquote>
  <p>Loaded configuration file ""config/config.json"". Using environment
  ""production"". Sat, 26 Jan 2019 01:03:45 GMT sequelize deprecated
  String based operators are now deprecated. Please use Symbol based
  operators for better security, read more at
  <a href=""http://docs.sequelizejs.com/manual/tutorial/querying.html#operators"" rel=""nofollow noreferrer"">http://docs.sequelizejs.com/manual/tutorial/querying.html#operators</a> at
  node_modules/sequelize/lib/sequelize.js:242:13</p>
  
  <p>ERROR: connect ECONNREFUSED 172.21.0.2:3306</p>
</blockquote>
"
"53460628","Adding a file to docker image results in ""permission denied"" error","<docker>","60339290","OCI runtime create failed: container_linux.go:345","<docker><go><github><google-kubernetes-engine><github-actions>","<p>I am creating a docker golang image, but my golang app needs to read a config.yaml on start. I tried to add the file as shown in the dockerfile below:</p>

<pre><code>FROM golang:alpine as builder
# Install git + SSL ca certificates
RUN apk update &amp;&amp; apk add git &amp;&amp; apk add ca-certificates
# Create appuser
COPY . $GOPATH/src/github.com/user/app/
WORKDIR $GOPATH/src/github.com/user/app/
#get dependancies
RUN go get -d -v
#build the binary
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o /go/bin/app
# STEP 2 build a small image
# start from scratch
FROM scratch
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
ADD ./config.yaml /go/bin/app/
# Copy our static executable
COPY --from=builder /go/bin/app /go/bin/app
EXPOSE 3000
ENTRYPOINT [""/go/bin/app""]
</code></pre>

<p>But I get the following error:</p>

<blockquote>
  <p>docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""/go/bin/app\"": permission denied"": unknown.</p>
</blockquote>
","<p>Currently we are migrating from GitLab to GitHub and we've decided to move the CI/CD process to GitHub actions. The pipeline process works like a charm but when GKE tries to spin up the newly pushed image it gives back this error:</p>

<p><code>'OCI runtime create failed: container_linux.go:345: starting container process caused ""exec: \""/socket-server\"": permission denied"": unknown'</code></p>

<p>It's important to note here that this whole process was working on GitLab. Anyway, the GitHub workflow yaml file looks like this:</p>

<pre><code>name: Build and deploy
on:
  - push
  - pull_request

jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    steps:
      - uses: fusion-engineering/setup-git-credentials@v2
        with:
          credentials: https://${{ secrets.MACHINE_ACCOUNT_ACCESS_TOKEN }}:x-oauth-basic@github.com/

      - name: Setup environment
        shell: bash
        run: |
          echo ""::set-env name=GOPATH::${{ github.workspace }}/go""
          echo ""::add-path::${{ github.workspace }}/go/bin""

      - name: Install Go
        uses: actions/setup-go@v1
        with:
          go-version: 1.12.4

      - name: Checkout code
        uses: actions/checkout@v2
        with:
          path: go/src/github.com/${{ github.repository }}

      - name: Prepare environment
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make prepare

      - name: Format code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make fmt &amp;&amp; git diff --exit-code

      - name: Lint code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make lint

      - name: Vet code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make vet

      - name: Test code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make cover

      - name: Build code
        run: |
          cd $GOPATH/src/github.com/${{ github.repository }}
          make build

      - name: Upload artifact
        uses: actions/upload-artifact@v1
        with:
          name: socket-server
          path: go/src/github.com/${{ github.repository }}/socket-server

  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [build]
    if: contains(github.ref, 'refs/tags')

    steps:
      - name: Set release version
        run: echo ::set-env name=CI_COMMIT_TAG::${GITHUB_REF/refs\/tags\//}

      - name: Checkout code
        uses: actions/checkout@v2

      - name: Get artifact from build step
        uses: actions/download-artifact@v1
        with:
          name: socket-server

      - name: Set ci auth
        run: echo ::set-env name=CI_AUTH::$(cat ci_auth.json | base64)

      - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master
        with:
          service_account_key: ${{ env.CI_AUTH }}

      - name: Configure gcloud docker authentication
        run: |
          gcloud config set project foo
          gcloud auth configure-docker

      - name: Build, push and deploy container
        run: |
          bash deploy.sh
</code></pre>

<p>The deploy.sh file looks like this:</p>

<pre><code>#!/bin/bash

if [[ -z ""${CI_COMMIT_TAG}"" ]]; then
    echo ""CI_COMMIT_TAG is empty, this stage should not run""
    exit 0
fi

export ENV=""stage""

if [[ ""$CI_COMMIT_TAG"" != ""${CI_COMMIT_TAG%-release}"" ]]; then
    export ENV=""prod""
fi

echo ""Current environment: $ENV""

make deploy
</code></pre>

<p>The deploy step in the Makefile looks like this:</p>

<pre><code>deploy:
    ( echo ""cat &lt;&lt;EOF"" ; cat k8s.yml.template; ) | sh &gt; k8s-${ENV}.yml
    docker build --no-cache \
    --build-arg RELEASE=${CI_COMMIT_TAG} \
    --build-arg ENV=${ENV} \
    -t gcr.io/foo/socket-server:${CI_COMMIT_TAG} .
    docker push gcr.io/foo/socket-server:${CI_COMMIT_TAG}
    gcloud container clusters get-credentials api-${ENV} --zone=europe-west1-b
    kubectl apply -f k8s-${ENV}.yml
</code></pre>

<p>And the Dockerfile looks like this:</p>

<pre><code>FROM alpine:latest as certs
RUN apk --update add ca-certificates

FROM scratch
COPY --from=certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt

ARG RELEASE
ARG ENV

ADD ./socket-server /socket-server
ADD ./config.yml /config.yml
ADD ./dbconfig.yml /dbconfig.yml
ADD ./migrations /migrations

ENV SOCKET_SERVER_SENTRY_DSN https://foo@sentry.io/bar
ENV SOCKET_SERVER_SENTRY_RELEASE $RELEASE
ENV SOCKET_SERVER_SENTRY_ENVIRONMENT $ENV

CMD [""/socket-server"", ""--port"", ""9345"", ""--host"", """"]
</code></pre>

<p>I have already tried to <code>chmod +x socket-server</code> on the pipeline and also in the Dockerfile (<a href=""https://stackoverflow.com/questions/53460628/adding-a-file-to-docker-image-results-in-permission-denied-error"">as suggested here</a>). When I do it in the Dockerfile it fails with the following error:</p>

<pre><code>Step 14/15 : RUN chmod +x socket-server
 ---&gt; Running in 9c66aef0c35b
OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""/bin/sh\"": stat /bin/sh: no such file or directory"": unknown
</code></pre>

<p>Am I missing something on the GitHub workflow or is there anybody who is seeing something that I don't? Some help is appreciated!</p>
"
"55115080","How to specify different port for a Docker postgres instance?","<docker><docker-compose>","66611842","keycloak cannot connect to postgres when containerized","<postgresql><docker><keycloak>","<p>I am trying to get a postgres container running on a different port (5433 instead of the default 5432) per several online threads like this one:
<a href=""https://github.com/docker-library/postgres/issues/196#issuecomment-310209118"" rel=""noreferrer"">https://github.com/docker-library/postgres/issues/196#issuecomment-310209118</a></p>

<p>(The reason for the port change is because an unrelated project that I leave running is already using port 5432 so I'd like to be able to run rspec simultaneously on multiple projects.)</p>

<p>In my test ENV I have 
DATABASE_URL=postgresql://postgres:@db.local:5433/test_agile_self</p>

<p>Everything worked when I was using port 5432 (in DATABASE_URL and docker-compose.yml)</p>

<p>After changing the port to 5433, when I run rspec I get:</p>

<pre><code>PG::ConnectionBad:
  could not connect to server: Connection refused
    Is the server running on host ""db.local"" (172.22.0.2) and accepting
    TCP/IP connections on port 5433?
# ./spec/spec_helper.rb:62:in `block (2 levels) in &lt;top (required)&gt;'
</code></pre>

<p>The container does indeed seem to be running on port 5433 and IP address 172.22.0.2:</p>

<pre><code>$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
a8f5dee227e8        postgres:10.5       ""docker-entrypoint.s…""   27 minutes ago      Up 14 seconds       0.0.0.0:5433-&gt;5432/tcp   zarc_db.local_1
</code></pre>

<pre><code>$ docker inspect a8f5dee227e8 | grep ""IPAddress""
            ""SecondaryIPAddresses"": null,
            ""IPAddress"": """",
                    ""IPAddress"": ""172.22.0.2"",
</code></pre>

<p>My docker-compose.yml uses the <code>ports: ""5433:5432""</code> as per the discussion linked above.</p>

<pre><code>#docker-compose.yml
version: '3'
services:
  web:
    build: .
    ports:
      - ""3010:3010""
    volumes:
      - .:/my_app
    links:
      - db.local
    depends_on:
      - db.local

  db.local:
    image: postgres:10.5
    ports:
      - ""5433:5432""
</code></pre>

<p>If I change 5433 back to 5432 in those two spots (ENV and docker-compose.yml) it works again.</p>

<p>This is on a Mac running Mohave 10.14.3, and Docker 18.09.2</p>
","<p>I have the following docker-compose</p>
<pre><code>  postgres_db:
    image: postgres:13
    ports:
      - 5433:5432
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: root
      POSTGRES_DB: keycloak
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
  keycloak:
    image: jboss/keycloak
    ports:
      - 9800:8080
    environment:
      DB_VENDOR: postgres
      DB_ADDR: postgres_db
      DB_PORT: 5433
      DB_USER: root
      DB_PASSWORD: root
      DB_DATABASE: keycloak
      DB_SCHEMA: public
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
    depends_on:
      - postgres_db
</code></pre>
<p>When run, I am able to connect with a database client to the postgres_db container with the provided credentials but Keycloak container doesn't want to start because it cannot reach postgres.</p>
<pre><code>        ... 50 more
Caused by: org.postgresql.util.PSQLException: Connection to postgres_db:5433 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:280)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
        at org.postgresql.jdbc@42.2.5//org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195)
        at org.postgresql.jdbc@42.2.5//org.postgresql.Driver.makeConnection(Driver.java:454)
        at org.postgresql.jdbc@42.2.5//org.postgresql.Driver.connect(Driver.java:256)
        at org.jboss.ironjacamar.jdbcadapters@1.4.23.Final//org.jboss.jca.adapters.jdbc.local.LocalManagedConnectionFactory.createLocalManagedConnection(LocalManagedConnectionFactory.java:321)
        ... 57 more
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)
        at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)
        at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.base/java.net.Socket.connect(Socket.java:609)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:70)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)
        at org.postgresql.jdbc@42.2.5//org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)
        ... 62 more
</code></pre>
<p>Keycloak has also a sample docker-compose file for configuring it with Postgres: <a href=""https://github.com/keycloak/keycloak-containers/blob/master/docker-compose-examples/keycloak-postgres.yml"" rel=""nofollow noreferrer"">https://github.com/keycloak/keycloak-containers/blob/master/docker-compose-examples/keycloak-postgres.yml</a></p>
<p>I also tried replacing <code>DB_ADDR: postgres_db</code> with <code>DB_ADDR: localhost</code> or <code>DB_ADDR: postgres_db:5433</code> or <code>DB_ADDR: localhost:5433</code> and it didn't work.</p>
<p>What am I missing?</p>
"
"54374724","Docker container save logs on the host directory","<docker><docker-compose><containers><dockerfile><docker-container>","66774731","How to copy docker container logs to a file on host machine in real time?","<docker><logging><docker-compose><consul>","<p>I have a question similar to <a href=""https://stackoverflow.com/questions/41358285/log-doesnt-save-to-file-in-docker-environment"">this one</a>. When run my <code>docker-compose.yml</code> file, I automatically create a docker image and as specified in my <code>dockerfile</code>, I run certain apps. These apps produce some logs, but these logs are written <em>inside</em> the docker container, in <code>/home/logs/</code> folder.</p>
<p>How can I indicate that these logs be writted <em>outside</em> the container, on my <code>/path/on/host</code> address? Simply because if the container was failed, I need to see the logs and not lose them!</p>
<p>This is my <code>docker-compose.yml</code>:</p>
<pre><code>version: '3'
services:
  myapp:
    build: .
    image: myapp
    ports:
      - &quot;9001:9001&quot;
</code></pre>
<p>and here is my <code>dockerfile</code>:</p>
<pre><code>FROM java:latest
COPY myapp-1.0.jar /home
CMD java -jar /home/myapp-1.0.jar
</code></pre>
<p>And I simply run it on production machine with <code>docker-compose up -d</code>.</p>
<p>(BTW, I'm new to dockers. Are all of my steps correct? Am I missing anything?! I see all is fine and myapp is running though!)</p>
","<p>I have a consul server running as a docker image in a docker-compose environment, I want to copy the logs to a file in the host machine in real-time.</p>
<p>I want to automate the process of copying files.</p>
<p><strong>NOTE</strong>: I am aware of other similar answers, but none of them work for me, as I want the host machine logs file to be updated in real-time.</p>
"
"54862460","How to determine which GC I use?","<java><garbage-collection><jvm><jvm-arguments>","60401648","In Java 8 it is shown as none of the available 4 collectors (GC) are selected by default","<java><docker><java-8><jvm><jvm-arguments>","<p>I didn't specify any GC, and I think my JVM have not any GC be enabled by default.</p>

<p>Of course I know that OpenJDK8 use ParallelGC by default, but I think it should can print by command line, like this:</p>

<p><code>java -XX:+PrintFlagsFinal|grep Use|grep GC</code> </p>

<p>I expect that the output contains 
<code>bool UseParallelOldGC = true {product}</code> but it is no :</p>

<pre><code>     bool ParGCUseLocalOverflow                     = false                               {product}
     bool UseAdaptiveGCBoundary                     = false                               {product}
     bool UseAdaptiveSizeDecayMajorGCCost           = true                                {product}
     bool UseAdaptiveSizePolicyWithSystemGC         = false                               {product}
     bool UseAutoGCSelectPolicy                     = false                               {product}
     bool UseConcMarkSweepGC                        = false                               {product}
     bool UseDynamicNumberOfGCThreads               = false                               {product}
     bool UseG1GC                                   = false                               {product}
     bool UseGCLogFileRotation                      = false                               {product}
     bool UseGCOverheadLimit                        = true                                {product}
     bool UseGCTaskAffinity                         = false                               {product}
     bool UseMaximumCompactionOnSystemGC            = true                                {product}
     bool UseParNewGC                               = false                               {product}
     bool UseParallelGC                             = false                               {product}
     bool UseParallelOldGC                          = false                               {product}
     bool UseSerialGC                               = false                               {product}
</code></pre>

<p>and </p>

<p><code>java -XX:+PrintCommandLineFlags -version</code> </p>

<p>I expect that the output contains: <code>XX:+UseParallelGC</code> but it is no too:</p>

<pre><code>-XX:InitialHeapSize=460493056 -XX:MaxHeapSize=7367888896 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops 
.
</code></pre>

<p>My JVM options: </p>

<pre><code>-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -verbose:gc -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime
.
</code></pre>

<p>the above output show that no any GC be enabled(I think so), I am very confuse by this situation.</p>

<p>The GC logs output like:</p>

<pre><code>OpenJDK 64-Bit Server VM (25.181-b13) for linux-amd64 JRE (1.8.0_181-b13), built on Oct 23 2018 11:39:12 by ""buildozer"" with gcc 6.4.0
Memory: 4k page, physical 28780816k(6283132k free), swap 0k(0k free)
CommandLine flags: -XX:InitialHeapSize=460493056 -XX:MaxHeapSize=7367888896 -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDeta
ils -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+UseCompressedClassPointers -XX:+UseCompressedOops 
{Heap before GC invocations=0 (full 0):
 def new generation   total 135168K, used 120192K [0x0000000608c00000, 0x0000000611ea0000, 0x000000069b2a0000)
  eden space 120192K, 100% used [0x0000000608c00000, 0x0000000610160000, 0x0000000610160000)
  from space 14976K,   0% used [0x0000000610160000, 0x0000000610160000, 0x0000000611000000)
  to   space 14976K,   0% used [0x0000000611000000, 0x0000000611000000, 0x0000000611ea0000)
 tenured generation   total 300416K, used 0K [0x000000069b2a0000, 0x00000006ad800000, 0x00000007c0000000)
   the space 300416K,   0% used [0x000000069b2a0000, 0x000000069b2a0000, 0x000000069b2a0200, 0x00000006ad800000)
 Metaspace       used 20532K, capacity 20780K, committed 20992K, reserved 1067008K
  class space    used 2628K, capacity 2726K, committed 2816K, reserved 1048576K
2019-02-25T06:29:46.105+0000: 2.890: [GC (Allocation Failure) 2019-02-25T06:29:46.105+0000: 2.890: [DefNew
Desired survivor size 7667712 bytes, new threshold 1 (max 15)
- age   1:   15335424 bytes,   15335424 total
: 120192K-&gt;14976K(135168K), 0.0238110 secs] 120192K-&gt;18041K(435584K), 0.0238765 secs] [Times: user=0.01 sys=0.01, real=0.02 secs] 
Heap after GC invocations=1 (full 0):
 def new generation   total 135168K, used 14976K [0x0000000608c00000, 0x0000000611ea0000, 0x000000069b2a0000)
  eden space 120192K,   0% used [0x0000000608c00000, 0x0000000608c00000, 0x0000000610160000)
  from space 14976K, 100% used [0x0000000611000000, 0x0000000611ea0000, 0x0000000611ea0000)
  to   space 14976K,   0% used [0x0000000610160000, 0x0000000610160000, 0x0000000611000000)
 tenured generation   total 300416K, used 3065K [0x000000069b2a0000, 0x00000006ad800000, 0x00000007c0000000)
   the space 300416K,   1% used [0x000000069b2a0000, 0x000000069b59e660, 0x000000069b59e800, 0x00000006ad800000)
 Metaspace       used 20532K, capacity 20780K, committed 20992K, reserved 1067008K
  class space    used 2628K, capacity 2726K, committed 2816K, reserved 1048576K
}
2019-02-25T06:29:46.129+0000: 2.914: Total time for which application threads were stopped: 0.0241189 seconds, Stopping threads took: 0.0000181 seconds
{Heap before GC invocations=1 (full 0):
 def new generation   total 135168K, used 21860K [0x0000000608c00000, 0x0000000611ea0000, 0x000000069b2a0000)
  eden space 120192K,   5% used [0x0000000608c00000, 0x00000006092b93f8, 0x0000000610160000)
  from space 14976K, 100% used [0x0000000611000000, 0x0000000611ea0000, 0x0000000611ea0000)
  to   space 14976K,   0% used [0x0000000610160000, 0x0000000610160000, 0x0000000611000000)
 tenured generation   total 300416K, used 3065K [0x000000069b2a0000, 0x00000006ad800000, 0x00000007c0000000)
   the space 300416K,   1% used [0x000000069b2a0000, 0x000000069b59e660, 0x000000069b59e800, 0x00000006ad800000)
 Metaspace       used 20982K, capacity 21132K, committed 21248K, reserved 1067008K
  class space    used 2667K, capacity 2758K, committed 2816K, reserved 1048576K
2019-02-25T06:29:46.187+0000: 2.972: [Full GC (Metadata GC Threshold) 2019-02-25T06:29:46.187+0000: 2.972: [Tenured: 3065K-&gt;9617K(300416K), 0.0270556 secs] 24926K-
&gt;9617K(435584K), [Metaspace: 20982K-&gt;20982K(1067008K)], 0.0271334 secs] [Times: user=0.03 sys=0.00, real=0.03 secs] 
Heap after GC invocations=2 (full 1):
 def new generation   total 135296K, used 0K [0x0000000608c00000, 0x0000000611ec0000, 0x000000069b2a0000)
  eden space 120320K,   0% used [0x0000000608c00000, 0x0000000608c00000, 0x0000000610180000)
  from space 14976K,   0% used [0x0000000610180000, 0x0000000610180000, 0x0000000611020000)
  to   space 14976K,   0% used [0x0000000611020000, 0x0000000611020000, 0x0000000611ec0000)
 tenured generation   total 300416K, used 9617K [0x000000069b2a0000, 0x00000006ad800000, 0x00000007c0000000)
   the space 300416K,   3% used [0x000000069b2a0000, 0x000000069bc04698, 0x000000069bc04800, 0x00000006ad800000)
 Metaspace       used 20982K, capacity 21132K, committed 21248K, reserved 1067008K
  class space    used 2667K, capacity 2758K, committed 2816K, reserved 1048576K
}
</code></pre>

<p>(from app starting until first major GC)</p>

<p>The log show that JVM heap is divided into <code>new</code> and <code>tenured</code>, but without print GC type.</p>

<p>Sadly, I also cannot use <code>jmap -heap {pid}</code> to get the GC type because jmap without this option(-heap) in my env.</p>

<p>So I wanted knows that:</p>

<ol>
<li>Which GC is what I use really?</li>
<li>Is the command line options(<code>-XX:+PrintCommandLineFlags</code> and <code>-XX:+PrintFlagsFinal</code>) output information correct?</li>
</ol>

<p>My env:
k8s+docker, Alpine OpenJKD8</p>
","<p>I have a container running a Spring Boot microservice. I am using <strong>openjdk version ""1.8.0_212"" under OpenJDK Runtime Environment (IcedTea 3.12.0) (Alpine 8.212.04-r0)</strong> When I use -<strong>XX:+PrintFlagsFinal</strong> flag and print the JVM parameters I <strong>expected</strong> to see </p>

<blockquote>
  <p>-XX:+UseParallelGC as true</p>
</blockquote>

<p>But to my surprise none of the 4 collectors (-XX:+UseSerialGC,-XX:+UseParallelGC,-XX:+UseConcMarkSweepGC,–XX:+UseG1GC) were active.</p>

<p>Shown below is my dockerfile:</p>

<pre><code>FROM openjdk:8-jdk-alpine
ADD  ./demo-0.0.1-SNAPSHOT.jar /usr/src/factorial/
WORKDIR /usr/src/factorial
EXPOSE 8080
CMD java $JAVA_OPTIONS -jar demo-0.0.1-SNAPSHOT.jar
</code></pre>

<p>I'm running this using:</p>

<pre><code>  docker run -d --rm --name factorialorialContainer --memory='512m' --cpus=2 -p 8080:8080 -e JAVA_OPTIONS=""$(cat /Users/sulekahelmini/Documents/fyp/fyp_work/MLscripts/$3)"" suleka96/factorial:latest
</code></pre>

<p>The flags_base.txt</p>

<pre><code>-Xms512m -Xmx512m -XX:+PrintFlagsFinal -version
</code></pre>

<p>This is part of the output of -XX:+PrintFlagsFinal</p>

<pre><code>     intx UnguardOnExecutionViolation               = 0                                   {product}
     bool UnlinkSymbolsALot                         = false                               {product}
     bool Use486InstrsOnly                          = false                               {ARCH product}
     bool UseAES                                    = true                                {product}
     bool UseAESIntrinsics                          = true                                {product}
     intx UseAVX                                    = 2                                   {ARCH product}
     bool UseAdaptiveGCBoundary                     = false                               {product}
     bool UseAdaptiveGenerationSizePolicyAtMajorCollection  = true                                {product}
     bool UseAdaptiveGenerationSizePolicyAtMinorCollection  = true                                {product}
     bool UseAdaptiveNUMAChunkSizing                = true                                {product}
     bool UseAdaptiveSizeDecayMajorGCCost           = true                                {product}
     bool UseAdaptiveSizePolicy                     = true                                {product}
     bool UseAdaptiveSizePolicyFootprintGoal        = true                                {product}
     bool UseAdaptiveSizePolicyWithSystemGC         = false                               {product}
     bool UseAddressNop                             = true                                {ARCH product}
     bool UseAltSigs                                = false                               {product}
     bool UseAutoGCSelectPolicy                     = false                               {product}
     bool UseBMI1Instructions                       = true                                {ARCH product}
     bool UseBMI2Instructions                       = true                                {ARCH product}
     bool UseBiasedLocking                          = true                                {product}
     bool UseBimorphicInlining                      = true                                {C2 product}
     bool UseBoundThreads                           = true                                {product}
     bool UseCLMUL                                  = true                                {ARCH product}
     bool UseCMSBestFit                             = true                                {product}
     bool UseCMSCollectionPassing                   = true                                {product}
     bool UseCMSCompactAtFullCollection             = true                                {product}
     bool UseCMSInitiatingOccupancyOnly             = false                               {product}
     bool UseCRC32Intrinsics                        = true                                {product}
     bool UseCodeCacheFlushing                      = true                                {product}
     bool UseCompiler                               = true                                {product}
     bool UseCompilerSafepoints                     = true                                {product}
     bool UseCompressedClassPointers               := true                                {lp64_product}
     bool UseCompressedOops                        := true                                {lp64_product}
     bool UseConcMarkSweepGC                        = false                               {product}
     bool UseCondCardMark                           = false                               {C2 product}
     bool UseContainerSupport                       = true                                {product}
     bool UseCountLeadingZerosInstruction           = true                                {ARCH product}
     bool UseCountTrailingZerosInstruction          = true                                {ARCH product}
     bool UseCountedLoopSafepoints                  = false                               {C2 product}
     bool UseCounterDecay                           = true                                {product}
     bool UseDivMod                                 = true                                {C2 product}
     bool UseDynamicNumberOfGCThreads               = false                               {product}
     bool UseFPUForSpilling                         = true                                {C2 product}
     bool UseFastAccessorMethods                    = false                               {product}
     bool UseFastEmptyMethods                       = false                               {product}
     bool UseFastJNIAccessors                       = true                                {product}
     bool UseFastStosb                              = true                                {ARCH product}
     bool UseG1GC                                   = false                               {product}
     bool UseGCLogFileRotation                      = false                               {product}
     bool UseGCOverheadLimit                        = true                                {product}
     bool UseGCTaskAffinity                         = false                               {product}
     bool UseHeavyMonitors                          = false                               {product}
     bool UseHugeTLBFS                              = false                               {product}
     bool UseInlineCaches                           = true                                {product}
     bool UseInterpreter                            = true                                {product}
     bool UseJumpTables                             = true                                {C2 product}
     bool UseLWPSynchronization                     = true                                {product}
     bool UseLargePages                             = false                               {pd product}
     bool UseLargePagesInMetaspace                  = false                               {product}
     bool UseLargePagesIndividualAllocation         = false                               {pd product}
     bool UseLinuxPosixThreadCPUClocks              = true                                {product}
     bool UseLockedTracing                          = false                               {product}
     bool UseLoopCounter                            = true                                {product}
     bool UseLoopInvariantCodeMotion                = true                                {C1 product}
     bool UseLoopPredicate                          = true                                {C2 product}
     bool UseMathExactIntrinsics                    = true                                {C2 product}
     bool UseMaximumCompactionOnSystemGC            = true                                {product}
     bool UseMembar                                 = false                               {pd product}
     bool UseMontgomeryMultiplyIntrinsic            = true                                {C2 product}
     bool UseMontgomerySquareIntrinsic              = true                                {C2 product}
     bool UseMulAddIntrinsic                        = true                                {C2 product}
     bool UseMultiplyToLenIntrinsic                 = true                                {C2 product}
     bool UseNUMA                                   = false                               {product}
     bool UseNUMAInterleaving                       = false                               {product}
     bool UseNewLongLShift                          = false                               {ARCH product}
     bool UseOSErrorReporting                       = false                               {pd product}
     bool UseOldInlining                            = true                                {C2 product}
     bool UseOnStackReplacement                     = true                                {pd product}
     bool UseOnlyInlinedBimorphic                   = true                                {C2 product}
     bool UseOprofile                               = false                               {product}
     bool UseOptoBiasInlining                       = true                                {C2 product}
     bool UsePSAdaptiveSurvivorSizePolicy           = true                                {product}
     bool UseParNewGC                               = false                               {product}
     bool UseParallelGC                             = false                               {product}
     bool UseParallelOldGC                          = false                               {product}
     bool UsePerfData                               = true                                {product}
     bool UsePopCountInstruction                    = true                                {product}
     bool UseRDPCForConstantTableBase               = false                               {C2 product}
     bool UseRTMDeopt                               = false                               {ARCH product}
     bool UseRTMLocking                             = false                               {ARCH product}
     bool UseSHA                                    = false                               {product}
     bool UseSHA1Intrinsics                         = false                               {product}
     bool UseSHA256Intrinsics                       = false                               {product}
     bool UseSHA512Intrinsics                       = false                               {product}
     bool UseSHM                                    = false                               {product}
     intx UseSSE                                    = 4                                   {product}
     bool UseSSE42Intrinsics                        = true                                {product}
     bool UseSerialGC                               = false                               {product}
     bool UseSharedSpaces                           = false                               {product}
     bool UseSignalChaining                         = true                                {product}
     bool UseSquareToLenIntrinsic                   = true                                {C2 product}
     bool UseStoreImmI16                            = false                               {ARCH product}
     bool UseStringDeduplication                    = false                               {product}
     bool UseSuperWord                              = true                                {C2 product}
     bool UseTLAB                                   = true                                {pd product}
     bool UseThreadPriorities                       = true                                {pd product}
     bool UseTransparentHugePages                   = false                               {product}
     bool UseTypeProfile                            = true                                {product}
     bool UseTypeSpeculation                        = true                                {C2 product}
     bool UseUnalignedLoadStores                    = false                               {ARCH product}
     bool UseVMInterruptibleIO                      = false                               {product}
     bool UseXMMForArrayCopy                        = true                                {product}
     bool UseXmmI2D                                 = false                               {ARCH product}
     bool UseXmmI2F                                 = false                               {ARCH product}
     bool UseXmmLoadAndClearUpper                   = true                                {ARCH product}
     bool UseXmmRegToRegMoveAll                     = true             
</code></pre>

<p>After running </p>

<blockquote>
  <p>jcmd [pid]  VM.flags</p>
</blockquote>

<p>I am getting the below output which also does not specify a collector:</p>

<pre><code>-XX:CICompilerCount=2 -XX:InitialHeapSize=536870912 -XX:MaxHeapSize=536870912 -XX:MaxNewSize=178913280 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=178913280 -XX:OldSize=357957632 -XX:+StartAttachListener -XX:+UseCompressedClassPointers -XX:+UseCompressedOops 
</code></pre>

<p>What Is happening here? Is there anything I'm doing wrong?</p>
"
"63889102","Unable to run docker image due to libGl error","<python-3.x><docker><opencv><matplotlib><dockerfile>","66921547","Failed importing cv2 into Docker [python]","<python><docker><cv2>","<p>Dockerfile</p>
<pre><code>FROM python:3.6.8
COPY . /app
WORKDIR /app
RUN pip3 install --upgrade pip
RUN pip3 install opencv-python==4.3.0.38
RUN pip3 install -r requirements.txt
EXPOSE 80
CMD [&quot;python3&quot;, &quot;server.py&quot;]
</code></pre>
<p>requirements.txt</p>
<pre><code>Flask==0.12
Werkzeug==0.16.1
boto3==1.14.40
torch
torchvision==0.7.0
numpy==1.15.4
sklearn==0.0
scipy==1.2.1
scikit-image==0.14.2
pandas==0.24.2

</code></pre>
<p>The docker build succeeds but the docker run fails with the error</p>
<pre><code>INFO:matplotlib.font_manager:Generating new fontManager, this may take some time...
PyTorch Version:  1.6.0
Torchvision Version:  0.7.0
Traceback (most recent call last):
  File &quot;server.py&quot;, line 7, in &lt;module&gt;
    from pipeline_prediction.pipeline import ml_pipeline 
  File &quot;/app/pipeline_prediction/pipeline.py&quot;, line 3, in &lt;module&gt;
    from segmentation_color import get_swatch_color_from_segmentation
  File &quot;pipeline_prediction/segmentation_color.py&quot;, line 7, in &lt;module&gt;
    import cv2
  File &quot;/usr/local/lib/python3.6/site-packages/cv2/__init__.py&quot;, line 5, in &lt;module&gt;
    from .cv2 import *
ImportError: libGL.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I looked at answer <a href=""https://stackoverflow.com/questions/47710022/import-matplotlib-pyplot-as-plt-importerror-libgl-so-1-cannot-open-shared-obj"">import matplotlib.pyplot as plt, ImportError: libGL.so.1: cannot open shared object file: No such file or directory</a>  relating to it and replaced</p>
<pre><code>import matplotlib.pyplot as plt
</code></pre>
<p>with</p>
<pre><code>import matplotlib
matplotlib.use(&quot;Agg&quot;)
import matplotlib.pyplot as plt
</code></pre>
<p>but it is not working for me. Also looked at <a href=""https://stackoverflow.com/questions/55313610/importerror-libgl-so-1-cannot-open-shared-object-file-no-such-file-or-directo"">ImportError: libGL.so.1: cannot open shared object file: No such file or directory</a> but I do not have Ubuntu as base image so this installation would not work for me as listed in the answer.</p>
<p>Let me know a way to make this work.</p>
","<p>I am using the Opencv library, in my IDE I run everything fine, but when I run it in a docker container I get the following error</p>
<p><strong>Error code</strong></p>
<pre><code>Traceback (most recent call last):
File &quot;/usr/src/app/./main.py&quot;, line 2, in &lt;module&gt;
from cv2 import cv2
File &quot;/usr/local/lib/python3.9/site-packages/cv2/__init__.py&quot;, line 5, in &lt;module&gt;
from .cv2 import *
ImportError: libGL.so.1: cannot open shared object file: No such file or directory

</code></pre>
<p><strong>Mi dockerfile</strong></p>
<pre><code>FROM python:3
WORKDIR /usr/src/app
COPY requirements.txt ./
RUN pip3 install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;,&quot;./main.py&quot;]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>numpy==1.20.2
opencv-python==4.5.1.48
</code></pre>
<p>Any idea why this error is happening, or what I'm doing wrong</p>
"
"65278648","Mosquitto: Starting in local only mode","<networking><mqtt><iot><mosquitto>","66694290","Can't connect to my broker in a docker container","<docker><mqtt>","<p>I have a virtual machine that is supposed to be the host, which can receive and send data. The first picture is the error that I'm getting on my main machine (from which I'm trying to send data from). The second picture is the mosquitto log on my virtual machine. Also I'm using the default config, which as far as I know can't cause these problems, at least from what I have seen from other examples. I have very little understanding on how all of this works, so any help is appreciated.</p>
<p>What I have tried on the host machine:</p>
<ol>
<li>Disabling Windows defender</li>
<li>Adding firewall rules for &quot;mosquitto.exe&quot;</li>
<li>Installing mosquitto on a linux machine</li>
</ol>
<p><img src=""https://i.stack.imgur.com/3acFZ.png"" alt=""First error"" /></p>
<p><img src=""https://i.stack.imgur.com/lHvpX.png"" alt=""Second error"" /></p>
<p><img src=""https://i.stack.imgur.com/50yFr.png"" alt=""Netstat info"" /></p>
","<p>I'm trying to setup mqtt broker in a docker container. I pulled the following docker image (<a href=""https://hub.docker.com/_/eclipse-mosquitto"" rel=""nofollow noreferrer"">https://hub.docker.com/_/eclipse-mosquitto</a>) on my machine and I can succesfully launch the docker container with the following command:</p>
<pre><code> docker run -it -p 1883:1883 -p 9001:9001 --network=host eclipse-mosquitto
</code></pre>
<p>If I run it with that command i get the following output:</p>
<pre><code>WARNING: Published ports are discarded when using host network mode
1616081533: mosquitto version 2.0.9 starting
1616081533: Config loaded from /mosquitto/config/mosquitto.conf.
1616081533: Starting in local only mode. Connections will only be possible from clients running on this machine.
1616081533: Create a configuration file which defines a listener to allow remote access.
1616081533: Opening ipv4 listen socket on port 1883.
1616081533: Opening ipv6 listen socket on port 1883.
1616081533: mosquitto version 2.0.9 running
</code></pre>
<p>So then I start Mqqtfx and I set up a connection to 127.0.0.1 and port 1883 but the mqtt client is unable to connect to my broker. What am i doing wrong?</p>
<p>Thanks in advance!</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","60738092","docker-compose up takes lot of time for mac but runs fine on windows","<node.js><reactjs><docker><docker-compose>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I have three services starting up from my docker-compose.yml file. A React frontend, a node js backend and a mongodb service. It starts up and run absolutely fine on windows but it takes lots of time on mac that most of the time it throws http timeout error.</p>

<p>My docker-compose file -</p>

<pre><code>version: ""2""
services:
  client:
    build:
      context: ./react-ui
      args:
        NPM_TOKEN: $NPM_TOKEN
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NPM_TOKEN=${NPM_TOKEN}
    ports:
      - ""3001:3001""
    volumes:
      - /app/node_modules
      - ./react-ui:/app
  server:
    command: yarn dev -- -L
    build:
      context: ./server
    restart: always
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NODE_ENV=development
      - MONGODB_URI=mongodb://mongodb:27017/react-node-project
    ports:
      - ""3000:3000""
    volumes:
      - /app/node_modules
      - ./server:/app
    depends_on:
      - mongodb
  mongodb:
    image: mongo
    ports:
      - ""27017:27017""
</code></pre>

<p>My react dockerfile -</p>

<pre><code>FROM node:alpine

ARG NPM_TOKEN

ENV NPM_TOKEN=${NPM_TOKEN}

WORKDIR ""/app""
COPY ./package.json ./
COPY ./.npmrc . 
RUN  yarn install
COPY . .
CMD [""yarn"", ""start""]
</code></pre>

<p>My server dockerfile - </p>

<pre><code>FROM node:8.11

EXPOSE 3000

ARG NODE_ENV
ENV NODE_ENV $NODE_ENV

WORKDIR /app
COPY ./package.json .
RUN yarn install
COPY . .

CMD [""yarn"", ""docker:start""]
</code></pre>

<p>docker:start script does <code>nodemon ./src/index.js</code></p>

<p>I have made sure to add node_modules in my .dockerignore file. Also the client service specifically is taking lots of time for mac. Maybe I have written something incorrectly there which is taking so much time.</p>

<p>I have tried on 2-3 mac machines, the behaviour is same.</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","66230719","Laravel Docker For Mac Very Slowly","<php><laravel><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I am following this tutorial (<a href=""https://tighten.co/blog/converting-a-legacy-app-to-laravel/"" rel=""nofollow noreferrer"">https://tighten.co/blog/converting-a-legacy-app-to-laravel/</a>) to migrate a legacy app to Laravel and have made it as far as the &quot;Spring Cleaning&quot; section. My legacy code is in a <code>legacy</code> directory inside my Laravel build.</p>
<p>Our development environment uses Docker Compose to build a container on the host machine (which in my case is a Mac but can be either Windows or Linux as well depending on the developer). The source code is in a volume mounted to the container so that the developer's updates can be seen in the browser as soon as the developer reloads the page.</p>
<p>When I go to get Laravel to load it's default route (the basic route that Laravel builds with) I get page load times in excess of 6 minutes.</p>
<p>I have tried using cached volumes like so: <code>./:/var/www/portal/:cached</code></p>
<p>I have also tried following this tutorial (<a href=""https://www.jeffgeerling.com/blog/2020/revisiting-docker-macs-performance-nfs-volumes"" rel=""nofollow noreferrer"">https://www.jeffgeerling.com/blog/2020/revisiting-docker-macs-performance-nfs-volumes</a>) for using an NFS mount. I got it working but the page load size was still over 6 minutes for the default page.</p>
<p>What is causing this extremely slow page-load time? How are Dockerized Laravel Development Environments supposed to be structured to avoid this issue with Docker's VM on Mac and Windows?</p>
<p>I didn't have this problem when developing just in the legacy app. I'm not inclined to think that Laravel itself is causing so large a slowdown. My thought is that it is the Docker VM running on Mac but I haven't been able to prove that yet.</p>
"
"57496004","Why would pom.xml fail to find Maven repository?","<maven><spring-boot><intellij-idea><vertica>","60384297","Cannot resolve com.vertica:vertica-jdbc:9.0.1","<maven><docker><jdbc><vertica>","<p>I need to use <a href=""https://mvnrepository.com/artifact/com.vertica/vertica-jdbc/9.1.1"" rel=""nofollow noreferrer"">this repository</a> from maven central for my application. But my application is failing to locate it. I use IntelliJ Idea and the Sync window gives me error - </p>

<pre><code>Could not find artifact com.vertica:vertica-jdbc:pom:9.1.1 in central (https://repo.maven.apache.org/maven2)
</code></pre>

<p>This is how my pom.xml looks like - </p>

<p><a href=""https://i.stack.imgur.com/2Lde4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Lde4.png"" alt=""enter image description here""></a></p>

<p>Can anyone please let me know how can i configure my application so it can locate this <code>vertica jdbc driver</code> from maven central repo?</p>
","<p>I'm trying to use a vertica driver with my Maven application.
Here is a snapshot of my pom.xml:</p>

<pre><code>    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;icm&lt;/id&gt;
            &lt;url&gt;http://maven.icm.edu.pl/artifactory/repo/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.vertica&lt;/groupId&gt;
            &lt;artifactId&gt;vertica-jdbc&lt;/artifactId&gt;
            &lt;version&gt;9.0.1&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
</code></pre>

<p>but i'm getting the error <code>Cannot resolve com.vertica:vertica-jdbc:9.0.1</code>
I'm not sure what i'm missing. Any help would be appreciated. </p>

<p>Maybe it's worth noting that i intend to run this with Docker. I've tried local install of the driver and it works. I've tried local install in Docker with the command </p>

<pre><code>mvn install:install-file -Dfile=opt/vertica/java/lib/vertica-jdbc-9.1.1-0.jar -DgroupId=com.vertica -DartifactId=vjdbc9 -Dversion=9.1.1 -Dpackaging=jar
</code></pre>

<p>and i get this below error:</p>

<pre><code>Java.sql.SQLException: No suitable driver found for jdbc:vertica://rnd-vt1.dev1.com:5433/en

</code></pre>

<p>Here is my simple java application that i indend to use</p>

<pre><code>public class TestConnection {
public static void main(String[] args) throws Exception {

    Properties myProp = new Properties();
    myProp.put(""user"", ""admin1"");
    myProp.put(""password"", ""1234"");
    Connection conn;
    try {
        conn = DriverManager.getConnection(""jdbc:vertica://rnd-vt1.dev1.com:5433/en"", myProp);
        String query = ""SELECT * FROM Clients WHERE id = 1"";
        Statement st = conn.createStatement();
        ResultSet rs = st.executeQuery(query);
        conn.close();
    } catch (SQLException e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>}</p>
"
"57842718","Gunicorn Flask application in Docker Container not getting Exposed","<python><docker><flask><kubernetes><gunicorn>","60705576","Docker Container (Django Server) not accessible from local machine","<python><docker>","<p>The application inside the container is inaccessible from the outside i.e if I exec into the docker container and do </p>

<pre class=""lang-sh prettyprint-override""><code>curl localhost:5000 
</code></pre>

<p>it works correctly but not on the browser in my computer i get error : This site cant be reached</p>

<p>My Dockerfile:</p>

<pre><code># Use an official Python runtime as a parent image
FROM python:3.7-slim

# Set the working directory to /app
WORKDIR /web-engine

# Copy the current directory contents into the container at /app
COPY . /web-engine

# Install Gunicorn3
RUN apt-get update &amp;&amp; apt-get install default-libmysqlclient-dev gcc -y

# Install any needed packages specified in requirements.txt
RUN pip3 install --trusted-host pypi.python.org -r requirements.txt

# Make port 5000 available to the world outside this container
EXPOSE 5000

# Define environment variable
ENV username root

# Run app.py when the container launches
CMD gunicorn --workers 4 --bind 127.0.0.1:5000 application:app --threads 1

</code></pre>

<p>UPON executing docker in this way:</p>

<pre class=""lang-sh prettyprint-override""><code>sudo docker run -e password=$password -p 5000:5000 $reg/web-engine:ve0.0.2
</code></pre>

<p>I get the following output:</p>

<pre><code>[2019-09-08 11:53:36 +0000] [6] [INFO] Starting gunicorn 19.9.0
[2019-09-08 11:53:36 +0000] [6] [INFO] Listening at: http://127.0.0.1:5000 (6)
[2019-09-08 11:53:36 +0000] [6] [INFO] Using worker: sync
[2019-09-08 11:53:36 +0000] [9] [INFO] Booting worker with pid: 9
[2019-09-08 11:53:36 +0000] [10] [INFO] Booting worker with pid: 10
[2019-09-08 11:53:36 +0000] [11] [INFO] Booting worker with pid: 11
[2019-09-08 11:53:36 +0000] [12] [INFO] Booting worker with pid: 12
</code></pre>

<p>So as you can see I'm mapping port 5000 of the container to port 5000 of my computer but localhost:5000 is not working</p>

<p>Therefore i tried everthing the same but with the development server of Flask
with the following change in My dockerfile </p>

<p>FRom</p>

<pre class=""lang-sh prettyprint-override""><code>CMD gunicorn --workers 4 --bind 127.0.0.1:5000 application:app --threads 1

</code></pre>

<p>TO </p>

<pre class=""lang-sh prettyprint-override""><code>CMD python3.7 application.py
</code></pre>

<p>and IT WORKED; I goto localhost:5000 and see the application is working</p>

<p>There is nothing wrong with the application. I suppose there's an error in gunicorn server</p>

<p>the requirements.txt file :</p>

<pre><code>Flask
Flask-SQLAlchemy
mysqlclient
gunicorn
bs4
html5lib
</code></pre>

<p>Please help me out</p>

<p>I also tried different forms of gunicorn and docker run command combinations like</p>

<pre class=""lang-sh prettyprint-override""><code>CMD gunicorn -application:app &amp;&amp; sudo docker run -e password=$password -p 5000:8000 $reg/web-engine:ve0.0.2
</code></pre>

<p>It didnt work
<a href=""https://i.stack.imgur.com/Z6ors.png"" rel=""nofollow noreferrer"">terminal image of container working with development server</a></p>

<p>I would appreciate a solution involving nothing outside whats mentioned here like nginx, supervisor etc
 SOmeone please helllppp meeeeee.......😢</p>
","<p>I'm trying to run a simple container from a custom Dockerfile, but cannot access it when I go to my localhost. Anyone know why this could be? Here is the Dockerfile:</p>

<pre><code>FROM python:3.7.7-buster

RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
RUN echo deb ""https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list
RUN apt-get update
RUN yes | apt-get install npm
RUN yes | apt-get install yarn

COPY . .
RUN ./build.sh

EXPOSE 8000
CMD gunicorn -b 127.0.0.1:8000 project_config.wsgi.prod:application
</code></pre>

<p>And I use the following command to start the server:</p>

<pre><code>docker run -p 8000:8000 -e SECRET_KEY=supersecretkey mycontainer
</code></pre>

<p>I can see the server is running fine but when I got to localhost port 8000, I get nothing. Any help appreciated!</p>
"
"59299133","How to silent install Postgresql in Ubuntu via. Dockerfile?","<postgresql><docker><dockerfile>","66574785","Why does docker build keep asking me for the time zone even after I set it?","<docker><timezone><dockerfile>","<p>I have the following docker file, and I am using the command <code>docker build -t demo:v1 .</code> to build the image.</p>

<pre><code>FROM ubuntu:18.04
WORKDIR /app
RUN apt update \
    &amp;&amp; apt -y upgrade \
    &amp;&amp; apt install -y python3 \
    &amp;&amp; apt install -y python3-pip \
    &amp;&amp; apt install -y poppler-utils \
    &amp;&amp; apt install -y libsm6 libxext6 libxrender-dev

RUN apt install -y postgresql

COPY requirements.txt /app/requirements.txt

RUN pip3 install -r requirements.txt

COPY . /app

CMD gunicorn -t 300 --workers 5 --bind  0.0.0.0:8080 wsgi
</code></pre>

<p>When I build an image using this, while installing postgresql, it expects input and stops the building process like this </p>

<pre><code>.
.
.
.
Setting up libpopt0:amd64 (1.16-11) ...
Setting up tzdata (2019c-0ubuntu0.18.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration
questions will narrow this down by presenting a list of cities, representing
the time zones in which they are located.

  1. Africa      4. Australia  7. Atlantic  10. Pacific  13. Etc
  2. America     5. Arctic     8. Europe    11. SystemV
  3. Antarctica  6. Asia       9. Indian    12. US
Geographic area:
</code></pre>

<p>So, how can I setup postgresql inside my image, so that it builds without expecting this input? Also, surprisingly, even after I input my option, nothing happens further, and the process gets stuck. </p>
","<p>Why does creating a new docker image using the dockerfile below keep asking me to select the timezone? I have tried multiple variations of the <code>ENV TZ</code> command to no avail.</p>
<pre><code> =&gt; =&gt; # Please select the geographic area in which you live. Subsequent configuration
 =&gt; =&gt; # questions will narrow this down by presenting a list of cities, representing
 =&gt; =&gt; # the time zones in which they are located.
 =&gt; =&gt; #   1. Africa   3. Antarctica  5. Arctic  7. Atlantic  9. Indian    11. US
 =&gt; =&gt; #   2. America  4. Australia   6. Asia    8. Europe    10. Pacific  12. Etc
 =&gt; =&gt; # Geographic area:
</code></pre>
<p>I asks this after the <code>apt-get install</code> is finished.</p>
<p>Dockerfile:</p>
<pre><code>FROM ubuntu:20.10

ENV TZ Europe/London

RUN apt-get update \
 &amp;&amp; apt-get upgrade -y \
 &amp;&amp; apt-get -y install autoconf automake bison bzip2 cmake doxygen diffutils flex g++ gcc git gzip libarchive13 libcurl4 libelf1 libgpgme11 libssl1.1 libtool libusb-dev m4 make libncurses-dev patch pkg-config python3 readline-common subversion tar tcl texinfo unzip wget xz-utils
</code></pre>
<p>Thanks!</p>
"
"59693655","Building tensorflow from source with Docker","<python><git><docker><tensorflow>","66967968","how to download the latest tensorflow within docker container","<linux><docker><tensorflow>","<p>Sorry for this basic question, bit of a Docker noob here.</p>

<p>I'm trying to <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">build tensorflow from source</a> following the Docker instructions:</p>

<blockquote>
  <p>docker pull tensorflow/tensorflow:devel</p>
  
  <p>docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
      tensorflow/tensorflow:devel bash</p>
  
  <p>git pull  # within the container, download the latest source code</p>
</blockquote>

<p>Here are the commands I run in the terminal (on Ubuntu), along with their output:</p>

<pre><code>$ docker --version
Docker version 19.03.2, build 6a30dfc

$ docker pull tensorflow/tensorflow:devel
devel: Pulling from tensorflow/tensorflow
Digest: sha256:0ee065743f0001f922561bcba914013929a88263ec2a5af21ba35899c3ac85a7
Status: Image is up to date for tensorflow/tensorflow:devel
docker.io/tensorflow/tensorflow:devel

$ docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
&gt;     tensorflow/tensorflow:devel bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...

root@4746a002f18e:/tensorflow# 

</code></pre>

<p>But now, if I run <code>git pull</code> as instructed, I get</p>

<pre><code>fatal: not a git repository (or any of the parent directories): .git
</code></pre>

<p>How should I instead be running these commands?</p>
","<p>I'm trying to build TensorFlow from source using docker.
I followed instructions in <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/source</a></p>
<p>Now I'm in TensorFlow container and reached to this line:</p>
<pre><code>git pull  # within the container, download the latest source code
</code></pre>
<p>What does the comment mean? If I just run <code>git pull</code> it shows:</p>
<pre><code>root@7cd7f965a7b3:/tensorflow# git pull
fatal: not a git repository (or any of the parent directories): .git
</code></pre>
"
"60235353","Docker is not running on Colab","<docker><google-colaboratory>","60614614","Unable to start docker in Google-Colab","<docker><google-colaboratory>","<p>I have tried  to install Docker on google Colab through the following ways:</p>

<p>(1)<a href=""https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://phoenixnap.com/kb/how-to-install-docker-on-ubuntu-18-04</a> </p>

<p>(2)<a href=""https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04"" rel=""noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04</a></p>

<p>(3)<a href=""https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP"" rel=""noreferrer"">https://colab.research.google.com/drive/10OinT5ZNGtdLLQ9K399jlKgNgidxUbGP</a></p>

<p>I started the docker service and saw the status, but it showed 'Docker is not running'. Maybe the docker can not work on the Colab.
<a href=""https://i.stack.imgur.com/Xxs7O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xxs7O.png"" alt=""enter image description here""></a></p>

<p>I feel confused and want to know the reason.</p>

<p>Thanks</p>
","<p>I'm trying to build and run my docker image in Google Colab. I have installed docker in colab and I get the following error message ""docker: Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?"" even when I try to run the default hello-world container using <code>docker run hello-world</code></p>
"
"60326148","How to create table postgresql when start by docker compose","<postgresql><docker><docker-compose>","60326200","How to create table postgresql when start by docker compose (given answer doesn't work)","<postgresql><docker><docker-compose>","<p>I know this question is already asked and also answer given. But it is not working for me. I follow the same. My postgres container is running fine. I checked inside the container that /docker-entrypoint-initdb.d/init.sql exist.I used the following docker-compose.yml.</p>

<pre><code>version: ""3""
services:
  postgres:
    image: postgres:latest
    network_mode: bridge
    container_name: postgres
    expose:
    - 5432
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
         - POSTGRES_PASSWORD=admin
         - POSTGRES_USER=postgres
         - POSTGRES_DB=dev
    restart: unless-stopped

# APP
  profile_editor2:
    build:
      context: .
      dockerfile: ./Dockerfile
    network_mode: bridge
    container_name: profile_editor2
    volumes:
      - ./image:/app/image
    expose:
      - 8080
    ports:
      - 8080:8080
    restart: unless-stopped
    depends_on:
      - postgres
    links:
      - postgres
volumes:
  postgres-data:
</code></pre>

<p>init.sql:-</p>

<pre><code>  create table sometable(id int);   
</code></pre>

<p>No table created. I can see only the database is created. How do I create a table and also if possible insert some data into the table?       </p>
","<p>I know this question is already asked <a href=""https://stackoverflow.com/questions/33309121/using-docker-compose-to-create-tables-in-postgresql-database"">here</a> and also answer given. But it is not working for me. I follow the same. My postgres container is running fine. I checked inside the container that /docker-entrypoint-initdb.d/init.sql exist.I used the following docker-compose.yml.</p>

<pre><code>version: ""3""
services:
  postgres:
    image: postgres:latest
    network_mode: bridge
    container_name: postgres
    expose:
    - 5432
    ports:
      - 5432:5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
         - POSTGRES_PASSWORD=admin
         - POSTGRES_USER=postgres
         - POSTGRES_DB=dev
    restart: unless-stopped

# APP
  profile_editor2:
    build:
      context: .
      dockerfile: ./Dockerfile
    network_mode: bridge
    container_name: profile_editor2
    volumes:
      - ./image:/app/image
    expose:
      - 8080
    ports:
      - 8080:8080
    restart: unless-stopped
    depends_on:
      - postgres
    links:
      - postgres
volumes:
  postgres-data:
</code></pre>

<p>init.sql:-</p>

<pre><code>  create table sometable(id int);   
</code></pre>

<p>No table created. I can see only the database is created. How do I create a table and also if possible insert some data into the table?       </p>
"
"60606669","How do I upload my own binary (Python module) as a resource for my Kubernetes application?","<python><kubernetes><dockerfile><kubernetes-helm>","60714814","How do I install my plugins to the app's filesystem before the app container starts?","<docker><kubernetes><google-kubernetes-engine><kubernetes-helm>","<p>I have my own Python module (an '.so' file that I'm able to import locally) that I want to make available to my application running in Kubernetes. I am wholly unfamiliar with Kubernetes and Helm, and the documentation and attempts I've made so far haven't gotten me anywhere. </p>

<p>I looked into <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/"" rel=""nofollow noreferrer"">ConfigMaps</a>, trying <code>kubectl.exe create configmap mymodule --from-file=MyModule.so</code>, but kubectl says ""Request entity too large: limit is 3145728"". (My binary file is ~6mb.) I don't know if this is even the appropriate way to get my file there. I've also looked at <a href=""https://helm.sh/docs/topics/charts/"" rel=""nofollow noreferrer"">Helm Charts</a>, but I see nothing about how to package up a file to upload. Helm Charts look more like a way to configure deployment of existing services.</p>

<p>What's the appropriate way to package up my file, upload it, and use it within my application (ensuring that Python will be able to <code>import MyModule</code> successfully when running in my AKS cluster)?</p>
","<p>I've an app, packaged as docker image. The app has some default plugins, installed in <code>/opt/myapp/plugins/</code>. I want to install some additional plugins, which basically means copying the plugins to the app's aforementioned path:</p>

<pre class=""lang-sh prettyprint-override""><code>cp /path/to/more-plugins/* /opt/myapp/plugins/
</code></pre>

<p>How do I do that? Is <code>initContainers</code> helpful in such cases? Does init-container have access to the filesystem of the app-container, so that the above command can be executed? I tried to use <code>busybox</code> in <code>initContainers</code> and ran <code>ls -lR /opt/myapp</code> just to see if such a path even exists. But it seems, init container doesn't have access to the app-filesystem. </p>

<p>So what are the different solutions to this problem? And what is the best one?</p>
"
"60677656","Listing the contents of a directory in a shared volume with 200K files from within a ubuntu container hosted in windows gets stuck","<docker><windows-subsystem-for-linux><docker-volume>","60644956","""ls"" in a container within a volume shared with Windows host gets stuck when there are many files in a directory","<java><spring-boot><docker><docker-desktop><volumes>","<h1>Problem</h1>

<p>Listing the contents of a directory in a shared volume with 200K files from within a ubuntu container hosted in windows gets stuck. </p>

<h1>How to reproduce</h1>

<ol>
<li>Install Docker Desktop 2.2.0.3 (42716) stable on Windows 10 </li>
<li>Configure docker to work in WSL (Ubuntu 16 distribution)

<ul>
<li>Expose daemon on <code>tcp://localhost:2375 without TLS</code></li>
<li>In WSL, define environment variable <code>DOCKER_HOST=localhost:2375</code></li>
<li>In WSL, in <code>/etc/wsl.conf</code> in the <code>[automount]</code> section, define <code>root = /</code></li>
</ul></li>
<li>Create a folder on the Windows host that contains 200,000 files (even small ones). e.g. <code>c:/temp</code></li>
<li>Run a docker container, mapping c:/temp on the host to /opt in the container as follows:
<code>docker run -it -v /c/temp:/opt ubuntu bash</code></li>
<li>In the bash prompt of the container run <code>cd /opt</code> and then <code>ls</code></li>
</ol>

<h1>Expected Result</h1>

<p>The <code>ls</code> command will start to list all files in the c:/temp folder.<br>
This can be expected to take some time, but it will begin almost immediately.</p>

<h1>Actual Result</h1>

<p>Nothing is printed and the shell does not respond.<br>
Moreover, CTR C does not interrupt the <code>ls</code> command.<br>
The only way I found to stop the container is to restart Docker Desktop</p>
","<h1>Background</h1>

<p>I am running a Spring Boot application in a docker container (Ubuntu image).
The code is written in Kotlin and it walks through a directory on disk that contains 300,000 files.<br>
I run the following code:  </p>

<pre><code>  File(dir)
    .walk()
    .forEach{logger.info(""{}"", it.name)}
</code></pre>

<p>and this code blocks for at least 10 minutes.  </p>

<p>I would expect that lines should start being printed very shortly after invoking walk.<br>
Indeed the code works as expected when running it from Intellij - that is, not in a container.</p>

<h1>Question:</h1>

<p>Why is this happening and how can I fix it?</p>

<hr>

<h2>What I Have Tried</h2>

<h2>First Trial</h2>

<p>I have tried just calling File.listfiles and logging the number of files like so:  </p>

<pre><code>   val count = File(dir).listFiles().size
   logger.info(""{}"", count)
</code></pre>

<p>This also blocked for a very long time and eventually logged the value 0.</p>

<h2>Second Trial</h2>

<p>I changed the settings for Docker Desktop.
I increased the RAM to 20 GB and the swap file to to 1 GB<br>
This had no effect on the result.</p>
"
"62154016","docker on wsl2 very slow","<docker><windows-10><wsl-2>","66549583","Docker - Windows10 WSL2 - Compilation Speed ""Super-BAD""","<docker><windows-subsystem-for-linux>","<p>After having read about the performance improvements when running Docker on wsl2, I have been waiting for the official release of Windows 10 that supports wsl2.
I updated Windows and Docker and switched on the Docker flag to use wsl2 and was hoping for some performance boost for my Oracle Database running in a Docker container but unfortunately the change slowed down the container and my laptop dramatically.
The performance of the container is about 10x slower and my laptop is pretty much stuck when starting the container.
It seems as if the memory consumption would completely use up my 8GB and heavy memory swapping starts to take place.
Is there anything I can do to improve the performance of Docker on wsl2 or at least to better understand what's wrong in my setup?</p>

<p>My environment:</p>

<ul>
<li>Processor Intel(R) Core(TM) i7-2620M CPU @ 2.70GHz, 2 Core(s)</li>
<li>Installed Physical Memory (RAM)   8.00 GB</li>
<li>Microsoft Windows 10 Pro Version 10.0.19041 Build 19041</li>
<li>Docker version 19.03.8, build afacb8b</li>
</ul>
","<p>In our company we use a special docker image to execute the build for our projects.
The docker image has:</p>
<ul>
<li>ARM GCC with ccache</li>
<li>CMAKE</li>
<li>NINJA</li>
</ul>
<p>we have 2 setups that work super great and one setup which is super slow:</p>
<p>great:</p>
<ul>
<li>docker on a linux host</li>
<li>docker on a virtualbox-vm with windows7</li>
</ul>
<p>super-bad:</p>
<ul>
<li>docker on windows10wsl2 host</li>
</ul>
<p>what we see if the compilation is runs on wsl2, is that the cpu/ram usage max out (cpu usage is expected as we use max parallel threads) but actual task takes more than 30 times longer than on either one of the good options listed above.</p>
<p>does someone have a good tipp for us why wsl2 performs so badly?</p>
<p>i currently assume its the filesystem, so windows uses ntfs, and wsl accesses files from the windows file system (our project to build).
what makes matters worse is that building c code means compiling 100erts of small files, in words of a file system this means, fetching, processing and writing 100erts of files super fast.</p>
"
"62441307","How can I change the location of docker images when using Docker Desktop on WSL2 with Windows 10 Home?","<docker><windows-10><docker-desktop><wsl-2>","66570306","I need to use my D disc to run my Docker. How can I do?","<docker>","<p>I've just upgraded to Windows 10 Home May 2020, <a href=""https://docs.microsoft.com/en-us/windows/wsl/install-win10"" rel=""noreferrer"">activated WSL2</a>, and installed <a href=""https://hub.docker.com/editions/community/docker-ce-desktop-windows"" rel=""noreferrer"">Docker Desktop</a>.</p>
<p>WSL2 must be installed in my system disk, which is a small SSD. I don't want to fill it with docker images. How do I change the docker images path? I'd like to use a path in my big Windows filesystem.</p>
<p>The <a href=""https://stackoverflow.com/questions/42250222/what-is-docker-image-location-on-windows-10/"">image location</a> is somewhat confusing. I believe it is in <code>/mnt/wsl/docker-desktop-data/</code>.</p>
<p>How do I change the directory of docker images inside WSL2? May I change docker configuration to select a path inside <code>/mnt/d</code>, or mount a path from /mnt/d over docker data dirs?</p>
","<p>I checked that the Docker container has only about 2 GB disk space in my C disc so I need to use my D disc to run my Docker. How can I do? My C disk is not enough, what should I do for docker for everything?</p>
<p><img src=""https://i.stack.imgur.com/mlLHJ.jpg"" alt=""my disc spaces"" /></p>
"
"64804749","Docker build not showing any output from commands","<docker><docker-build>","66968821","How to show container logs from remote Docker?","<windows><docker><sh><remote-access>","<p>Snippet from dockerfile:</p>
<pre><code>FROM node:12.18.0
RUN echo &quot;hello world&quot;
RUN psql --version
</code></pre>
<p>When I run <code>docker build .</code> I don't see any output from these two commands even if they are not cached. The documentation says that <code>docker build</code> is verbose by default. Why am I not seeing the output from commands? I used to see them before.</p>
<p>The output while building: <a href=""https://i.stack.imgur.com/xHI6A.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xHI6A.png"" alt=""enter image description here"" /></a>
The output I am seeing after building finishes: <a href=""https://i.stack.imgur.com/mBjhy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mBjhy.png"" alt=""enter image description here"" /></a></p>
<p>Dockerfile is created from node:12.18.0 which is based on Debian 9</p>
<p>Docker version 19.03.13, build 4484c46d9d.</p>
","<p>I've setup Docker at my Windows machine, since my Windows machine is more powerful than my Macbook. However, I do like MacOS more.</p>
<p>So I've setup a Docker context to use remote Windows machine using command <code>docker context create remote ‐‐docker “host=ssh://user@host”</code>.</p>
<p>Now when I try to use following set of commands:</p>
<pre><code>docker context use remote
docker-compose up --build
</code></pre>
<p>Everything works ok, but I don't get live logs from my containers.
When I do <code>docker-compose up --build</code> locally - I get live logs.</p>
<p>What can I do to get live logs from remote containers?</p>
<p>Here is what I have when I execute both remotely and locally:</p>
<pre><code>Creating network &quot;target_default&quot; with the default driver
Creating volume &quot;target_storage&quot; with default driver
Building server
[+] Building 10.1s (10/10) FINISHED
 =&gt; [internal] load build definition from Dockerfile                                                               0.3s
 =&gt; =&gt; transferring dockerfile: 390B                                                                               0.0s
 =&gt; [internal] load .dockerignore                                                                                  0.3s
 =&gt; =&gt; transferring context: 2B                                                                                    0.0s
 =&gt; [internal] load metadata for docker.io/library/openjdk:8-jre-slim                                              0.6s
 =&gt; CACHED [1/5] FROM docker.io/library/openjdk:8-jre-slim@sha256:0330883ffeb5e14c4c15271004cdf6a2df21e827420b71d  0.0s
 =&gt; [internal] load build context                                                                                  8.2s
 =&gt; =&gt; transferring context: 91.16MB                                                                               8.2s
 =&gt; [2/5] RUN groupadd -r user &amp;&amp;     useradd -r -g user -m -s /usr/bin/nologin -c &quot;Docker image user&quot; user        0.3s
 =&gt; [3/5] COPY --chown=user:user [server-[{0-9.}]*-SNAPSHOT.jar, server.sh, /home/user/]                         0.2s
 =&gt; [4/5] WORKDIR /home/user                                                                                       0.0s
 =&gt; [5/5] RUN sed -i 's/localhost/data/g' server.sh &amp;&amp;     mkdir ./storage                                   0.3s
 =&gt; exporting to image                                                                                             0.4s
 =&gt; =&gt; exporting layers                                                                                            0.3s
 =&gt; =&gt; writing image sha256:3daf841e0b9749e1dcb7135e85b16a81104b84caa35930a423ecc6d460d6e8fd                       0.0s
 =&gt; =&gt; naming to docker.io/library/server:1.0-8-jre-slim                                                   0.0s
Successfully built 3daf841e0b9749e1dcb7135e85b16a81104b84caa35930a423ecc6d460d6e8fd
Creating target_server_1    ... done
Attaching to target_server_1    
</code></pre>
<p>But when I do it locally, after the last line above I get all my logs from 'target_server_1' live.
Remotely it just stops there, silently.</p>
"
"66472758","Creating a symbolic in shared volume of docker and accessing it in host machine","<python><linux><docker><unix><symlink>","66566429","How to make symbolic links point to files inside a docker container when mounting a directory","<docker><symlink><mount>","<p>I am creating a symbolic link in mounted volume of a host machine inside a docker. But I am unable to access it in host machine. Is it possible to do it. If yes how can I do that.</p>
<p>I used the following command to mount directory</p>
<pre><code>docker run -it --rm --net host -v $(pwd):/workspace --name myproject my-container:dev
</code></pre>
<p>Then I created a symbolic link using</p>
<pre><code>import os
fname = '/workspace/log/project_info_hostinfo_timeinfo_exe_param.log'
symlink_name = '/workspace/log/project_info.log'
os.symlink(fname, symlink_name)
</code></pre>
<p>Now when I am trying to see log info it looks like</p>
<pre><code>$ls
lrwxrwxrwx 1 root root   66 Mar  4 14:54 project_info.log -&gt; /workspace/log/project_info_hostinfo_timeinfo_exe_param.log
-rw-r--r-- 1 root root  206 Mar  4 14:54 project_info_hostinfo_timeinfo_exe_param.log
</code></pre>
<p>But when I try to open file I got message like</p>
<pre><code>$tail -f project_info.log
tail: cannot open 'project_info.log' for reading: No such file or directory
tail: no files remaining
</code></pre>
","<p>I have created a symbolic link to a file on a host machine. Then I have mounted a directory which contains the file and the link to a docker container. I expect the link inside the container to point to the file inside the container. However it points to the file on host, so it is broken inside the container. Is there a way to make all the links inside a container point to files inside a container and links on host point to files on host? Note that I wouldn't like to modify the links itself.</p>
<p>An example:</p>
<pre><code>cd /home/myuser/work/tmp
touch a.txt
ln -s &quot;$(pwd)&quot;/a.txt b.link
docker run -it -v &quot;$(pwd)&quot;:/some ubuntu bash
ls -lah /some/b.link
lrwxrwxrwx 1 1010 1014 30 Mar 10 13:45 /some/b.link -&gt; /home/myuser/work/tmp/a.txt
</code></pre>
<p>Expected output:</p>
<pre><code>lrwxrwxrwx 1 1010 1014 30 Mar 10 13:45 /some/b.link -&gt; /some/a.txt
</code></pre>
"
"66508965","Mongo docker setup broken after reboot (unifi controller on raspberry pi)","<mongodb><docker><docker-compose>","66510896","mongod --repair causing backtrace","<mongodb><docker>","<p>i seem to have run into an unclean shutdown after a power failure that I can't seem to recover from. I've tried running mongod --repair within my controller container but it doesn't seem to help. Any suggestions? I don't want to just blow away my unifi_mongo container, since I'm not sure if I'll lose all my configs.</p>
<p>As a somewhat related question, should I be enabling journaling somehow in this config even though I'm on a 32-bit raspbian lite OS? Not sure how I'd do that, but maybe it'd prevent these sorts of issues in the future?</p>
<p><strong>docker logs -f unifi_mongo</strong></p>
<pre><code>2021-03-06T18:35:51.917+0000 I STORAGE  [initandlisten] exception in initAndListen: 12596 old lock file, terminating
2021-03-06T18:35:51.917+0000 I CONTROL  [initandlisten] dbexit:  rc: 100
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 32-bit host=a282e3addaec
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T18:36:44.913+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T18:36:44.914+0000 I CONTROL  [initandlisten] options: { storage: { journal: { enabled: true } } }
2021-03-06T18:36:44.935+0000 W -        [initandlisten] Detected unclean shutdown - /data/db/mongod.lock is not empty.
2021-03-06T18:36:44.972+0000 I STORAGE  [initandlisten] **************
old lock file: /data/db/mongod.lock.  probably means unclean shutdown,
but there are no journal files to recover.
this is likely human error or filesystem corruption.
please make sure that your journal directory is mounted.
found 3 dbs.
see: http://dochub.mongodb.org/core/repair for more information
</code></pre>
<p><strong>docker-compose.yml:</strong></p>
<pre><code>version: '2.3'
services:
 mongo:
   #   image: mongo
   image: andresvidal/rpi3-mongodb3
   container_name: ${COMPOSE_PROJECT_NAME}_mongo
   networks:
     - unifi
   restart: always
   volumes:
     - db:/data/db
     - dbcfg:/data/configdb
 controller:
   image: &quot;jacobalberty/unifi:${TAG:-latest}&quot;
   container_name: ${COMPOSE_PROJECT_NAME}_controller
   depends_on:
     - mongo
   init: true
   networks:
     - unifi
   restart: always
   privileged: true
   volumes:
     - dir:/unifi
     - data:/unifi/data
     - log:/unifi/log
     - cert:/unifi/cert
     - init:/unifi/init.d
     - run:/var/run/unifi
     # Mount local folder for backups and autobackups
     - ./backup:/unifi/data/backup
   user: unifi
   sysctls:
     net.ipv4.ip_unprivileged_port_start: 0
   environment:
     DB_URI: mongodb://mongo/unifi
     STATDB_URI: mongodb://mongo/unifi_stat
     DB_NAME: unifi
     TZ: America/Toronto
   ports:
     - &quot;3478:3478/udp&quot; # STUN
     - &quot;1900:1900/udp&quot;
     - &quot;6789:6789/tcp&quot; # Speed test
     - &quot;8080:8080/tcp&quot; # Device/ controller comm.
     - &quot;8443:8443/tcp&quot; # Controller GUI/API as seen in a web browser
     - &quot;8880:8880/tcp&quot; # HTTP portal redirection
     - &quot;8843:8843/tcp&quot; # HTTPS portal redirection
     - &quot;10001:10001/udp&quot; # AP discovery
 logs:
   image: bash
   container_name: ${COMPOSE_PROJECT_NAME}_logs
   depends_on:
     - controller
   command: bash -c 'tail -F /unifi/log/*.log'
   restart: always
   volumes:
     - log:/unifi/log


volumes:
 db:
 dbcfg:
 data:
 log:
 cert:
 init:
 dir:
 run:

networks:
 unifi:
</code></pre>
<p>I tried blowing away the lock and re-running &quot;docker-compose up -d&quot; but it didn't solve the problem.</p>
<pre><code>unifi@me:/unifi/data/db$ ls
local  local.0  local.ns  storage.bson  version
</code></pre>
<p>Output of docker ps:</p>
<pre><code>docker ps
CONTAINER ID   IMAGE                       COMMAND                  CREATED       STATUS                            PORTS                                                                                                                                                                                              NAMES
aaaaaaaaaaaa   jacobalberty/unifi:latest   &quot;/usr/local/bin/dock…&quot;   4 weeks ago   Up 3 days (unhealthy)             0.0.0.0:1900-&gt;1900/udp, 0.0.0.0:6789-&gt;6789/tcp, 0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:8443-&gt;8443/tcp, 0.0.0.0:8843-&gt;8843/tcp, 0.0.0.0:3478-&gt;3478/udp, 0.0.0.0:10001-&gt;10001/udp, 0.0.0.0:8880-&gt;8880/tcp   unifi_controller
bbbbbbbbbbbb   andresvidal/rpi3-mongodb3   &quot;/docker-entrypoint.…&quot;   4 weeks ago   Restarting (100) 13 seconds ago                                                                                                                                                                                                      unifi_mongo
</code></pre>
<p>Do I run mongod --repair inside the mongo container? How do I do that if it keeps restarting?</p>
<p>Thanks</p>
<p>Edit:</p>
<p>I tried setting an entrypoint in the docker-compose.yml to run mongod --repair instead of the normal mongo startup, but i got this backtrace:</p>
<pre><code>2021-03-06T18:52:46.599+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T18:52:46.649+0000 I -        [initandlisten] Fatal Assertion 17441
2021-03-06T18:52:46.769+0000 I CONTROL  [initandlisten]
 0x1622348 0x15c50a0 0x15abc08 0xdc63ec 0x13e0730 0x13e026c 0x13f883c 0x13faf84 0x122a440 0xcf1838 0xcf2f1c 0xcf3acc 0xcf4e00 0xcf3e28 0x76bc9678
----- BEGIN BACKTRACE -----
{&quot;backtrace&quot;:[{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;1612348&quot;,&quot;s&quot;:&quot;_ZN5mongo15printStackTraceERSo&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;15B50A0&quot;,&quot;s&quot;:&quot;_ZN5mongo10logContextEPKc&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;159BC08&quot;,&quot;s&quot;:&quot;_ZN5mongo13fassertFailedEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;DB63EC&quot;,&quot;s&quot;:&quot;_ZN5mongo7fassertEib&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D0730&quot;,&quot;s&quot;:&quot;
_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D026C&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13E883C&quot;,&quot;s&quot;:&quot;_ZN5mongo27SimpleRecordStoreV1Iterator7getNe
xtEv&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13EAF84&quot;,&quot;s&quot;:&quot;_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;121A440&quot;,&quot;s&quot;:&quot;_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_str
ingIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE1838&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE2F1C&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3ACC&quot;,&quot;s&quot;:&quot;_ZN5mongo13initAndListenEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE4E00&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3E28&quot;,&quot;s&quot;:&quot;main&quot;},{&quot;b&quot;:&quot;76BB3000&quot;,&quot;o&quot;:&quot;16678&quot;,&quot;s&quot;:&quot;__libc_start_main&quot;}],&quot;processInfo&quot;:{ &quot;m
ongodbVersion&quot; : &quot;3.0.14&quot;, &quot;gitVersion&quot; : &quot;08352afcca24bfc145240a0fac9d28b978ab77f3&quot;, &quot;uname&quot; : { &quot;sysname&quot; : &quot;Linux&quot;, &quot;release&quot; : &quot;5.4.83-v7+&quot;, &quot;version&quot; : &quot;#1379 SMP Mon Dec 14 13:08:57 GMT 2020&quot;, &quot;machine&quot; : &quot;armv7l&quot; }, &quot;somap&quot; : [ { &quot;elfType&quot; : 2, &quot;b&quot; : &quot;10000&quot;, &quot;buildId&quot; : &quot;77BB9B
C6C28CA032211CCD119B903FDEE2C6A7D8&quot; }, { &quot;b&quot; : &quot;7EDCA000&quot;, &quot;path&quot; : &quot;linux-vdso.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;8E8ADD944B36D89CB5A4AE6DAB825D428D5407ED&quot; }, { &quot;b&quot; : &quot;76F22000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/librt.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4C7E415AA306267E5BA73CD0FE8F6F
8ABC5D9370&quot; }, { &quot;b&quot; : &quot;76F0F000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libdl.so.2&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;99B3CD788031A72A37B9C9F10C5A63FEABF1BCDB&quot; }, { &quot;b&quot; : &quot;76DC7000&quot;, &quot;path&quot; : &quot;/usr/lib/arm-linux-gnueabihf/libstdc++.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;5909F48F93D947CDD017977DA4
79EC563E8B426E&quot; }, { &quot;b&quot; : &quot;76D48000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libm.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;1128E26D3F2FA311FE65EDF9E3930D2162AF9BE8&quot; }, { &quot;b&quot; : &quot;76D1B000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libgcc_s.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;030EF284554E9F6259572226A3F2
6F86F86E1B35&quot; }, { &quot;b&quot; : &quot;76CF2000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libpthread.so.0&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4B15D4A8FE60C9A013D924976C36C1281A60E04D&quot; }, { &quot;b&quot; : &quot;76BB3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libc.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;B84C7156F66DE515C6257D0A4A71
D9F31CE6F9CF&quot; }, { &quot;b&quot; : &quot;76F39000&quot;, &quot;path&quot; : &quot;/lib/ld-linux-armhf.so.3&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;21F72FB00897D4F06093D6F0451C9CA7D1F6E14C&quot; } ] }}
 mongod(_ZN5mongo15printStackTraceERSo+0x2C) [0x1622348]
 mongod(_ZN5mongo10logContextEPKc+0x88) [0x15c50a0]
 mongod(_ZN5mongo13fassertFailedEi+0x78) [0x15abc08]
 mongod(_ZN5mongo7fassertEib+0x34) [0xdc63ec]
 mongod(_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE+0x90) [0x13e0730]
 mongod(_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE+0x30) [0x13e026c]
 mongod(_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv+0x8C) [0x13f883c]
 mongod(_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0xD00) [0x13faf84]
 mongod(_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0x1C8) [0x122a440]
 mongod(+0xCE1838) [0xcf1838]
 mongod(+0xCE2F1C) [0xcf2f1c]
 mongod(_ZN5mongo13initAndListenEi+0x20) [0xcf3acc]
 mongod(+0xCE4E00) [0xcf4e00]
 mongod(main+0x28) [0xcf3e28]
 libc.so.6(__libc_start_main+0x114) [0x76bc9678]
-----  END BACKTRACE  -----
2021-03-06T18:52:46.770+0000 I -        [initandlisten]

***aborting after fassert() failure
</code></pre>
<p>Edit2: Trying to run a repair manually doesn't seem to solve the problem</p>
<pre><code>docker run -it -v db:/data/db andresvidal/rpi3-mongodb3:latest mongod --repair
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm/v7) and no specific platform was requested
2021-03-06T19:23:47.049+0000 I CONTROL
2021-03-06T19:23:47.049+0000 W CONTROL  32-bit servers don't have journaling enabled by default. Please use --journal if you want durability.
2021-03-06T19:23:47.049+0000 I CONTROL
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 32-bit host=fa58e6e86cff
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T19:23:47.075+0000 I CONTROL  [initandlisten] options: { repair: true }
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] ** NOTE: This is a 32 bit MongoDB binary.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       32 bit builds are limited to less than 2GB of data (or less with --journal).
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       Note that journaling defaults to off for 32 bit and is currently off.
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten] **       See http://dochub.mongodb.org/core/32bit
2021-03-06T19:23:47.159+0000 I CONTROL  [initandlisten]
2021-03-06T19:23:47.166+0000 I STORAGE  [initandlisten] finished checking dbs
2021-03-06T19:23:47.167+0000 I CONTROL  [initandlisten] now exiting
2021-03-06T19:23:47.167+0000 I NETWORK  [initandlisten] shutdown: going to close listening sockets...
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] removing socket file: /tmp/mongodb-27017.sock
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] shutdown: going to flush diaglog...
2021-03-06T19:23:47.168+0000 I NETWORK  [initandlisten] shutdown: going to close sockets...
2021-03-06T19:23:47.168+0000 I STORAGE  [initandlisten] shutdown: waiting for fs preallocator...
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] shutdown: closing all files...
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] closeAllFiles() finished
2021-03-06T19:23:47.169+0000 I STORAGE  [initandlisten] shutdown: removing fs lock...
2021-03-06T19:23:47.169+0000 I CONTROL  [initandlisten] dbexit:  rc: 0
</code></pre>
<p>Edit 3:</p>
<p>mongodump --repair -d /data/db on a stopped instance can't find the database</p>
<p>mongodump --repair -d /data/db on a running mongo gives me the following, after which my container crashes again.</p>
<pre><code> Failed: error getting collections for database `/data/db`: error running `listCollections`. Database: `/data/db` Err: Invalid ns [/data/db.$cmd]
</code></pre>
<p>mongodump --repair on a running mongo instance gives me:</p>
<pre><code>2021-03-07T17:31:44.556+0000    writing repair of unifi.wlanconf to dump/unifi/wlanconf.bson
2021-03-07T17:31:44.560+0000            repair cursor found 4 documents in unifi.wlanconf
2021-03-07T17:31:44.561+0000    writing unifi.wlanconf metadata to dump/unifi/wlanconf.metadata.json
2021-03-07T17:31:44.564+0000    done dumping unifi.wlanconf (0 documents)
2021-03-07T17:31:44.565+0000    writing repair of unifi.site to dump/unifi/site.bson
2021-03-07T17:31:44.569+0000            repair cursor found 4 documents in unifi.site
2021-03-07T17:31:44.569+0000    writing unifi.site metadata to dump/unifi/site.metadata.json
2021-03-07T17:31:44.572+0000    done dumping unifi.site (0 documents)
2021-03-07T17:31:44.572+0000    writing repair of unifi.networkconf to dump/unifi/networkconf.bson
2021-03-07T17:31:44.576+0000            repair cursor found 4 documents in unifi.networkconf
2021-03-07T17:31:44.576+0000    writing unifi.networkconf metadata to dump/unifi/networkconf.metadata.json
2021-03-07T17:31:44.579+0000    done dumping unifi.networkconf (0 documents)
2021-03-07T17:31:44.580+0000    writing repair of unifi.privilege to dump/unifi/privilege.bson
2021-03-07T17:31:44.610+0000            repair cursor found 4 documents in unifi.privilege
2021-03-07T17:31:44.610+0000    writing unifi.privilege metadata to dump/unifi/privilege.metadata.json
2021-03-07T17:31:44.616+0000    done dumping unifi.privilege (0 documents)
2021-03-07T17:31:44.616+0000    writing repair of unifi.apgroup to dump/unifi/apgroup.bson
2021-03-07T17:31:44.640+0000            repair cursor found 2 documents in unifi.apgroup
2021-03-07T17:31:44.640+0000    writing unifi.apgroup metadata to dump/unifi/apgroup.metadata.json
2021-03-07T17:31:44.672+0000    done dumping unifi.apgroup (0 documents)
2021-03-07T17:31:44.675+0000    Failed: repair error: error reading collection: EOF
</code></pre>
","<p>I have a unifi controller running on docker on raspberry pi, but I seem to have some corruption in my mongo database due to an unclean shutdown. I've tried to mongod --repair, but I get the following error. What should I do to recover this? Thanks!</p>
<pre><code>021-03-06T21:34:14.261+0000 I CONTROL
2021-03-06T21:34:14.265+0000 W CONTROL  32-bit servers don't have journaling enabled by default. Please use --journal if you want durability.
2021-03-06T21:34:14.265+0000 I CONTROL
2021-03-06T21:34:14.288+0000 I CONTROL  [initandlisten] MongoDB starting : pid=15 port=27017 dbpath=/data/db 32-bit host=ae6010a02591
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] db version v3.0.14
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] git version: 08352afcca24bfc145240a0fac9d28b978ab77f3
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] build info: Linux raspberrypi 4.9.41-v7+ #1023 SMP Tue Aug 8 16:00:15 BST 2017 armv7l BOOST_LIB_VERSION=1_49
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] allocator: system
2021-03-06T21:34:14.289+0000 I CONTROL  [initandlisten] options: { repair: true }
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten]
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten]
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] ** NOTE: This is a 32 bit MongoDB binary.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       32 bit builds are limited to less than 2GB of data (or less with --journal).
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       Note that journaling defaults to off for 32 bit and is currently off.
2021-03-06T21:34:14.370+0000 I CONTROL  [initandlisten] **       See http://dochub.mongodb.org/core/32bit
2021-03-06T21:34:14.371+0000 I CONTROL  [initandlisten]
</code></pre>
<p>...</p>
<pre><code>2021-03-06T21:34:34.406+0000 I STORAGE  [initandlisten] repairDatabase unifi_stat
2021-03-06T21:34:35.338+0000 I INDEX    [initandlisten] allocating new ns file /data/db/_tmp_repairDatabase_16/unifi_stat.ns, filling with zeroes...
2021-03-06T21:34:36.845+0000 I STORAGE  [FileAllocator] allocating new datafile /data/db/_tmp_repairDatabase_16/unifi_stat.0, filling with zeroes...
2021-03-06T21:34:36.846+0000 I STORAGE  [FileAllocator] creating directory /data/db/_tmp_repairDatabase_16/_tmp
2021-03-06T21:34:36.863+0000 I STORAGE  [FileAllocator] done allocating datafile /data/db/_tmp_repairDatabase_16/unifi_stat.0, size: 64MB,  took 0.011 secs
2021-03-06T21:34:36.885+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.885+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:36.888+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { site_id: 1, datetime: 1 }, name: &quot;site_id_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.888+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:36.893+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_5minutes properties: { v: 1, key: { datetime: 1, o: 1, site_id: 1, oid: 1 }, name: &quot;datetime_1_o_1_site_id_1_oid_1&quot;, ns: &quot;unifi_stat.stat_5minutes&quot; }
2021-03-06T21:34:36.893+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.449+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.449+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.452+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { site_id: 1, o: 1, oid: 1, datetime: 1 }, name: &quot;site_id_1_o_1_oid_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.453+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:37.456+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_archive properties: { v: 1, key: { datetime: 1 }, name: &quot;datetime_1&quot;, ns: &quot;unifi_stat.stat_archive&quot; }
2021-03-06T21:34:37.456+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.075+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.075+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.077+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { site_id: 1, datetime: 1 }, name: &quot;site_id_1_datetime_1&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.077+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:38.080+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_daily properties: { v: 1, key: { datetime: 1, o: 1, site_id: 1, oid: 1 }, name: &quot;datetime_1_o_1_site_id_1_oid_1&quot;, ns: &quot;unifi_stat.stat_daily&quot; }
2021-03-06T21:34:38.081+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.454+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_dpi properties: { v: 1, key: { _id: 1 }, name: &quot;_id_&quot;, ns: &quot;unifi_stat.stat_dpi&quot; }
2021-03-06T21:34:39.454+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.457+0000 I INDEX    [initandlisten] build index on: unifi_stat.stat_dpi properties: { v: 1, key: { site_id: 1, o: 1, oid: 1, datetime: 1, app: 1, cat: 1 }, name: &quot;site_id_1_o_1_oid_1_datetime_1_app_1_cat_1&quot;, ns: &quot;unifi_stat.stat_dpi&quot; }
2021-03-06T21:34:39.457+0000 I INDEX    [initandlisten]          building index using bulk method
2021-03-06T21:34:39.490+0000 I -        [initandlisten] Fatal Assertion 17441
2021-03-06T21:34:39.629+0000 I CONTROL  [initandlisten]
 0x1622348 0x15c50a0 0x15abc08 0xdc63ec 0x13e0730 0x13e026c 0x13f883c 0x13faf84 0x122a440 0xcf1838 0xcf2f1c 0xcf3acc 0xcf4e00 0xcf3e28 0x76b7a678
----- BEGIN BACKTRACE -----
{&quot;backtrace&quot;:[{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;1612348&quot;,&quot;s&quot;:&quot;_ZN5mongo15printStackTraceERSo&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;15B50A0&quot;,&quot;s&quot;:&quot;_ZN5mongo10logContextEPKc&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;159BC08&quot;,&quot;s&quot;:&quot;_ZN5mongo13fassertFailedEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;DB63EC&quot;,&quot;s&quot;:&quot;_ZN5mongo7fassertEib&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D0730&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13D026C&quot;,&quot;s&quot;:&quot;_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13E883C&quot;,&quot;s&quot;:&quot;_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;13EAF84&quot;,&quot;s&quot;:&quot;_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;121A440&quot;,&quot;s&quot;:&quot;_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE1838&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE2F1C&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3ACC&quot;,&quot;s&quot;:&quot;_ZN5mongo13initAndListenEi&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE4E00&quot;},{&quot;b&quot;:&quot;10000&quot;,&quot;o&quot;:&quot;CE3E28&quot;,&quot;s&quot;:&quot;main&quot;},{&quot;b&quot;:&quot;76B64000&quot;,&quot;o&quot;:&quot;16678&quot;,&quot;s&quot;:&quot;__libc_start_main&quot;}],&quot;processInfo&quot;:{ &quot;mongodbVersion&quot; : &quot;3.0.14&quot;, &quot;gitVersion&quot; : &quot;08352afcca24bfc145240a0fac9d28b978ab77f3&quot;, &quot;uname&quot; : { &quot;sysname&quot; : &quot;Linux&quot;, &quot;release&quot; : &quot;5.4.83-v7+&quot;, &quot;version&quot; : &quot;#1379 SMP Mon Dec 14 13:08:57 GMT 2020&quot;, &quot;machine&quot; : &quot;armv7l&quot; }, &quot;somap&quot; : [ { &quot;elfType&quot; : 2, &quot;b&quot; : &quot;10000&quot;, &quot;buildId&quot; : &quot;77BB9BC6C28CA032211CCD119B903FDEE2C6A7D8&quot; }, { &quot;b&quot; : &quot;7ECB5000&quot;, &quot;path&quot; : &quot;linux-vdso.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;8E8ADD944B36D89CB5A4AE6DAB825D428D5407ED&quot; }, { &quot;b&quot; : &quot;76ED3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/librt.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4C7E415AA306267E5BA73CD0FE8F6F8ABC5D9370&quot; }, { &quot;b&quot; : &quot;76EC0000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libdl.so.2&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;99B3CD788031A72A37B9C9F10C5A63FEABF1BCDB&quot; }, { &quot;b&quot; : &quot;76D78000&quot;, &quot;path&quot; : &quot;/usr/lib/arm-linux-gnueabihf/libstdc++.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;5909F48F93D947CDD017977DA479EC563E8B426E&quot; }, { &quot;b&quot; : &quot;76CF9000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libm.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;1128E26D3F2FA311FE65EDF9E3930D2162AF9BE8&quot; }, { &quot;b&quot; : &quot;76CCC000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libgcc_s.so.1&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;030EF284554E9F6259572226A3F26F86F86E1B35&quot; }, { &quot;b&quot; : &quot;76CA3000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libpthread.so.0&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;4B15D4A8FE60C9A013D924976C36C1281A60E04D&quot; }, { &quot;b&quot; : &quot;76B64000&quot;, &quot;path&quot; : &quot;/lib/arm-linux-gnueabihf/libc.so.6&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;B84C7156F66DE515C6257D0A4A71D9F31CE6F9CF&quot; }, { &quot;b&quot; : &quot;76EEA000&quot;, &quot;path&quot; : &quot;/lib/ld-linux-armhf.so.3&quot;, &quot;elfType&quot; : 3, &quot;buildId&quot; : &quot;21F72FB00897D4F06093D6F0451C9CA7D1F6E14C&quot; } ] }}
 mongod(_ZN5mongo15printStackTraceERSo+0x2C) [0x1622348]
 mongod(_ZN5mongo10logContextEPKc+0x88) [0x15c50a0]
 mongod(_ZN5mongo13fassertFailedEi+0x78) [0x15abc08]
 mongod(_ZN5mongo7fassertEib+0x34) [0xdc63ec]
 mongod(_ZNK5mongo17RecordStoreV1Base21getNextRecordInExtentEPNS_16OperationContextERKNS_7DiskLocE+0x90) [0x13e0730]
 mongod(_ZNK5mongo17RecordStoreV1Base13getNextRecordEPNS_16OperationContextERKNS_7DiskLocE+0x30) [0x13e026c]
 mongod(_ZN5mongo27SimpleRecordStoreV1Iterator7getNextEv+0x8C) [0x13f883c]
 mongod(_ZN5mongo12MMAPV1Engine14repairDatabaseEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0xD00) [0x13faf84]
 mongod(_ZN5mongo14repairDatabaseEPNS_16OperationContextEPNS_13StorageEngineERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0x1C8) [0x122a440]
 mongod(+0xCE1838) [0xcf1838]
 mongod(+0xCE2F1C) [0xcf2f1c]
 mongod(_ZN5mongo13initAndListenEi+0x20) [0xcf3acc]
 mongod(+0xCE4E00) [0xcf4e00]
 mongod(main+0x28) [0xcf3e28]
 libc.so.6(__libc_start_main+0x114) [0x76b7a678]
-----  END BACKTRACE  -----
2021-03-06T21:34:39.630+0000 I -        [initandlisten]

***aborting after fassert() failure
</code></pre>
"
"66544508","golang Infinite for loop problem with docker run","<docker><go><dockerfile>","66576561","Wait for user input golang cli app on docker","<docker><go>","<p>I tried to do simple infinite for loop task. It is working fine without using docker. But when i used the docker,it only executes the else part of for loop infinitely. What may be problem actually? Is docker having problem with infinite for loop?
My main.go file is shown below.</p>
<pre><code>package main

 import (
&quot;bufio&quot;
&quot;fmt&quot;
&quot;os&quot;
 )

func main() {
 fmt.Println(&quot;Hello, World!.....&quot;)

 for {
    fmt.Print(&quot;-&gt; &quot;)
    var i int
    fmt.Scan(&amp;i)
    if i == 1 {
        fmt.Println(&quot;Hello, World! 1&quot;)
    } else if i == 2 {

        fmt.Println(&quot;Hello, World! 2&quot;)
    } else if i == 3 {
        fmt.Println(&quot;Hello, World! 3&quot;)
    } else if i == 4 {
        fmt.Println(&quot;Hello, World! 4&quot;)
    } else if i == 5 {
        fmt.Println(&quot;Hello, World! 5&quot;)
    } else {
        fmt.Println(&quot;Hello, World! else&quot;)
        
    }

 }
}
</code></pre>
<p>I tried these link as well. <a href=""https://stackoverflow.com/questions/40035635/read-line-in-golang/40061275#40061275"">Read line in golang</a>   <a href=""https://stackoverflow.com/questions/46476170/how-do-i-break-out-of-an-infinite-loop-in-golang"">How do I break out of an infinite loop in Golang</a>  But still of no use. I am trying to solve the problem since yesterday.</p>
<p>The docker file is as given below:</p>
<pre><code>FROM golang:1.12.0-alpine3.9

RUN mkdir /app

ADD . /app

WORKDIR /app

RUN go build -o main .

CMD [&quot;go&quot;,&quot;run&quot;,&quot;/app/main.go&quot;]
</code></pre>
<p>I tried to build the docker using
<strong>docker build -t hello .</strong>
and run using <code>docker run hello</code></p>
<p>Running with</p>
<blockquote>
<p>docker run hello</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/vpK7c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vpK7c.png"" alt=""I tried to run using docker run hello, it only prints else part infinetely"" /></a></p>
<p>Executing with console without docker <code>go run main.go</code>
<a href=""https://i.stack.imgur.com/dWpzr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dWpzr.png"" alt=""I tried to run using go run main.go and it works fine"" /></a></p>
","<p>Hello everyone i am trying to learn how to wait for user input with golang cli app in docker, her is my main.go file</p>
<pre><code>package main

import (
  &quot;bufio&quot;
  &quot;fmt&quot;
  &quot;os&quot;
  &quot;strings&quot;
  &quot;strconv&quot;
)

func main(){
    reader := bufio.NewReader(os.Stdin)
    var errConv error
    for {
        fmt.Print(&quot;-&gt; &quot;)
        inputText, _ := reader.ReadBytes('\n')
        // Receive input
        text := string(inputText)
        var command []string 
        for _, word := range strings.Fields(text) {
            command = append(command,word)
        }

        if command[0] == &quot;create_day_max&quot;{
           //do something
        }
    }
}
</code></pre>
<p>this is my Dockerfile</p>
<pre><code>    # Start from golang base image
    FROM golang:1.15.2
    
    # Copy the source from the current directory to the working Directory inside the container 
    COPY . /usr/src/golang-tunaiku
    
    # Move to working directory
    WORKDIR /usr/src/golang-tunaiku
    
    #install depedencies
    RUN go mod download
    
    # Build the application
    RUN go build -o golang-tunaiku
    
    #Command to run the executable
    CMD [ &quot;./golang-tunaiku&quot; ]

and this is my docker-compose.yml file

version: '3'
services:
  app:
    container_name: golang-linkaja
    build: .
</code></pre>
<p>it run fine when i do go run main.go but when i use <code>docker-compose up --build</code> , it just does not wait for user input, thus it create error like this</p>
<p><code>-&gt; panic: runtime error: index out of range</code></p>
<br>
which is obvious since it is forcing to read index 0 while there is no user input, 
How do i wait for user input on golang clip app with docker compose?
Thanks in advance i am stil learning
"
"18073329","JAVA - path issue (works in eclipse, not in cmd)","<java>","66471954","Jasper Report--java.lang.IllegalArgumentException: name","<java><spring-boot><docker><google-cloud-platform><jasper-reports>","<p>Why the following iniciation works in eclipse:</p>

<pre><code>private static MaxentTagger maxentTagger = new MaxentTagger(""c:\\DP\\lemma\\models\\english-left3words-distsim.tagger"");
</code></pre>

<p>but in command line it throws: </p>

<pre><code>java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoa
der.java:58)
Caused by: java.lang.ExceptionInInitializerError
        at dp.beans.MySearch.&lt;init&gt;(MySearch.java:122)
        at dp.runable.Main.main(Main.java:25)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: name
        at sun.misc.URLClassPath$Loader.findResource(Unknown Source)
        at sun.misc.URLClassPath.findResource(Unknown Source)
        at java.net.URLClassLoader$2.run(Unknown Source)
        at java.net.URLClassLoader$2.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findResource(Unknown Source)
        at java.lang.ClassLoader.getResource(Unknown Source)
        at java.net.URLClassLoader.getResourceAsStream(Unknown Source)
        at edu.stanford.nlp.io.IOUtils.findStreamInClasspathOrFileSystem(IOUtils.java:370)
        at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:399)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:646)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:284)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.&lt;init&gt;(MaxentTagger.java:248)
        at dp.data.Settings.&lt;clinit&gt;(Settings.java:80)
        ... 7 more
</code></pre>

<p><code>Settings.java:80</code> corresponding with <code>MaxentTagger</code> iniciation..</p>

<p>Is there a different way to declare windows path, which works in both, eclipse and cmd?</p>

<p><strong>update</strong> (the <code>findStreamInClasspathOrFileSystem</code> method):</p>

<pre><code>private static InputStream  [More ...] findStreamInClasspathOrFileSystem(String name) throws FileNotFoundException 
{
  String path = null;
  if (name.startsWith(""/"")) {
    path = name.substring(1);
  }

  // - even though this may look like a regular file, it may be a path inside a jar in the CLASSPATH
  // - check for this first. This takes precedence over the file system.
  InputStream is = null;
  if (path != null) {
    is = IOUtils.class.getClassLoader().getResourceAsStream(path);

    // windows File.separator is \, but getting resources only works with /
    if (is == null) {
      is = IOUtils.class.getClassLoader().getResourceAsStream(path.replaceAll(""\\\\"", ""/""));
    }
  }

  // if not found in the CLASSPATH, load from the file system
  if (is == null) is = new FileInputStream(name);
  return is;
}
</code></pre>

<p><strong>update:</strong> no matter if i change the path to:</p>

<pre><code>  ""c:/DP/lemma/models/english-left3words-distsim.tagger"");
  ""c:\\\\DP\\\\lemma\\\\models\\\\english-left3words-distsim.tagger"");
</code></pre>

<p>its behaviour is still the same (works in eclipce, not in cmd)</p>
","<p>My requirement:
Download a generated pdf report in client side using restApi with jasper report template(.jrxml file)</p>
<p>I am creating reports using jasper report integrate with spring boot. I need to print pdf file for that i created the rest api.The code looks like</p>
<pre><code>@PostMapping(&quot;/client-projects&quot;)
public void clientProject(@RequestParam String format,@RequestBody List&lt;ProjectReportModel&gt; project,HttpServletResponse response)  {
    
       report.clientProjectReport(format,response,project);
         
    }
</code></pre>
<p>service class looks like this</p>
<pre><code> public void clientProjectReport(String format,List&lt;ProjectReportModel&gt; project,HttpServletResponseresponse){
    
    InputStream stream = this.getClass().getResourceAsStream(&quot;/clientProject.jrxml&quot;);
    JasperReport report = JasperCompileManager.compileReport(stream);
    JRBeanCollectionDataSource source = new JRBeanCollectionDataSource(project);
    JasperPrint print = JasperFillManager.fillReport(report, null, source);
    if(format.equals(&quot;pdf&quot;)){
        JasperExportManager.exportReportToPdfStream(print, response.getOutputStream());
         response.setContentType(&quot;application/pdf&quot;);
         response.addHeader(&quot;Content-Disposition&quot;, &quot;inline; filename=jasper.pdf;&quot;);
    }
</code></pre>
<p>Everything works fine when i run the code in the local but when i deploy the code in server(GCP) i faced the following error</p>
<pre><code>java.lang.IllegalArgumentException: name
at java.base/jdk.internal.loader.URLClassPath$Loader.findResource(URLClassPath.java:600) ~[na:na]
at java.base/jdk.internal.loader.URLClassPath.findResource(URLClassPath.java:291) ~[na:na]
at java.base/java.net.URLClassLoader$2.run(URLClassLoader.java:655) ~[na:na]
at java.base/java.net.URLClassLoader$2.run(URLClassLoader.java:653) ~[na:na]
at java.base/java.security.AccessController.doPrivileged(Native Method) ~[na:na]
at java.base/java.net.URLClassLoader.findResource(URLClassLoader.java:652) ~[na:na]
at org.springframework.boot.loader.LaunchedURLClassLoader.findResource(LaunchedURLClassLoader.java:100) ~[app.jar:0.0.1-SNAPSHOT]
at java.base/java.lang.ClassLoader.getResource(ClassLoader.java:1401) ~[na:na]
at org.apache.catalina.loader.WebappClassLoaderBase.getResource(WebappClassLoaderBase.java:1048) ~[tomcat-embed-core-9.0.39.jar!/:9.0.39]
at net.sf.jasperreports.engine.util.JRResourcesUtil.findClassLoaderResource(JRResourcesUtil.java:457) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.repo.DefaultRepositoryService.getInputStream(DefaultRepositoryService.java:131) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.repo.InputStreamPersistenceService.load(InputStreamPersistenceService.java:51) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.repo.DefaultRepositoryService.getResource(DefaultRepositoryService.java:196) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.repo.RepositoryUtil.findInputStream(RepositoryUtil.java:195) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.repo.RepositoryUtil.getBytesFromLocation(RepositoryUtil.java:211) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.renderers.util.RendererUtil.getNonLazyRenderable(RendererUtil.java:179) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFillImage.evaluateImage(JRFillImage.java:564) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFillImage.evaluate(JRFillImage.java:489) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFillElementContainer.evaluate(JRFillElementContainer.java:383) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFillBand.evaluate(JRFillBand.java:548) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRVerticalFiller.fillTitle(JRVerticalFiller.java:323) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRVerticalFiller.fillReportStart(JRVerticalFiller.java:256) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRVerticalFiller.fillReport(JRVerticalFiller.java:110) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRBaseFiller.fill(JRBaseFiller.java:615) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.BaseReportFiller.fill(BaseReportFiller.java:432) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFiller.fill(JRFiller.java:162) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.fill.JRFiller.fill(JRFiller.java:145) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.JasperFillManager.fill(JasperFillManager.java:689) ~[jasperreports-6.6.0.jar!/:6.6.0]
at net.sf.jasperreports.engine.JasperFillManager.fillReport(JasperFillManager.java:1005) ~[jasperreports-6.6.0.jar!/:6.6.0]
at com.yenmin.proton.report.service.ReportService.clientProjectReport(ReportService.java:135) ~[classes!/:0.0.1-SNAPSHOT]
at com.yenmin.proton.report.controller.ReportController.clientProject(ReportController.java:42) ~[classes!/:0.0.1-SNAPSHOT]
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:197) ~[spring-web-5.3.1.jar!/:5.3.1]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:141) ~[spring-web-5.3.1.jar!/:5.3.1]
</code></pre>
<p>It didn't fill the report ,there it got error don't know why this error occurs we are using docker also
while deploying the code in gcp.</p>
<p>If i create a file as tmp file like</p>
<pre><code>  File pdfFile = File.createTempFile(&quot;jasper&quot;, &quot;.pdf&quot;);
    
        try(FileOutputStream pos = new FileOutputStream(pdfFile))
        {
            final JasperReport report = loadTemplate();
            JRBeanCollectionDataSource source = new JRBeanCollectionDataSource(project);
            System.out.println(&quot;Fill report error&quot;);
            JasperReportsUtils.renderAsPdf(report, null, source, pos);
        
                       
        }
        catch (final Exception e)
        {
            LOG.error(e.getLocalizedMessage(), e);
            
        }
       return pdfFile;
        
  }
 private JasperReport loadTemplate() throws JRException {

 final InputStream reportInputStream = getClass().getResourceAsStream(&quot;/clientproject.jrxml&quot;);
 final JasperDesign jasperDesign = JRXmlLoader.load(reportInputStream);
 return JasperCompileManager.compileReport(jasperDesign);
        }
</code></pre>
<p>everything works but dont know where the files are saved.but file path looks likes &quot;/tmp/client065346395.pdf&quot; i am not clear with how to retrieve the file from tmp folder.</p>
<p>kindly let me know how to resolve this issue or provide me some useful links... Thanks in advance</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","66894494","how do I connect a docker container to local redis server running on the host","<docker><docker-compose><dockerfile>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a redis-server running on the host computer, and a dockerized python app (single container).</p>
<p>How do I make the dockerized app talk to my local redis-server on the host?</p>
<p>I know I can create a docker-compose.yaml and connect both the containers but I want to connect to this local redis server running on the host.</p>
<p>Thanks</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","66762567","Dockerfile: Copy directory from outside of docker folder","<docker><dockerfile>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I would like to copy a directory from program files folder to a docker container.</p>
<pre><code>ARG xsrc=&quot;C:\\Program Files (x86)\\x&quot;
ARG xdstdst=&quot;/home/workdir/.wine/Program Files (x86)/x&quot;
RUN mkdir -p ${xdst}

COPY ${xsrc} ${xdst}
</code></pre>
<pre class=""lang-sh prettyprint-override""><code>docker build -t x .
</code></pre>
<p>Results in this error.</p>
<pre><code>failed to compute cache key: &quot;/C:\\Program Files (x86)\\x&quot; not found: not found
</code></pre>
<p>I am kind of new to docker and guess thatt there is some design choice that you should not be able to access files outside the projects folder. But if I want to include files in my docker build like this, how should I do?</p>
"
"30740828","Commit data in a mysql container","<mysql><linux><database><docker><virtualization>","66734991","Cannot save database changes to MySQL docker image","<mysql><docker>","<p>I created a mysql container using the officially supported <a href=""https://registry.hub.docker.com/_/mysql/"" rel=""noreferrer"">mysql image</a>. I ran the image mounting a folder that contains a sql dump, then I created a new database in the container and imported the .sql dump in it:</p>

<pre><code>sudo docker run --name mysql-psat1 -v /opt/Projets/P1/sqldumps:/mnt -e MYSQL_ROOT_PASSWORD=secret -d mysql:latest
sudo docker exec -it mysql-psat1 bash
&gt; mysql -uroot -psecret -e 'create database liferay_psat1;'
&gt; mysql -uroot -psecret liferay_psat1 &lt; /mnt/liferay_sql_dump.sql
</code></pre>

<p>Then I listed the running containers to get that container's id:</p>

<pre><code>sudo docker ps -a
</code></pre>

<p>Then, I commited the container (with the imported sql) as a new container image</p>

<pre><code>sudo docker commit -m ""Imported liferay sql dump"" &lt;id-of-the-container&gt; jihedamine/mysql-psat1:v1
</code></pre>

<p>However, when if I start a container using that new image, the mysql database doesn't contain the newly created database liferay_psat1.</p>

<pre><code>sudo docker run -ti jihedamine/mysql-psat1:v1 bash
&gt; mysql -uroot -psecret
# show databases;
</code></pre>

<p>What am I doing wrong?</p>

<p>Thanks for your help!</p>
","<p>I am attempting to make a change to a MySQL docker instance of a database and save this as an image to use at later point. I can spawn the MySQL database, make changes to it, but when I <em><strong>COMMIT</strong></em> the container instance, then the database changes are not saved. If I spawn a container from the image, it still has the old changes.</p>
<p>These are the steps that I executed :</p>
<ol>
<li>I first spawn a MySQL instance like so:</li>
</ol>
<blockquote>
<pre><code>docker run -d -p 6033:3306 --name=test1 --env=&quot;MYSQL_ROOT_PASSWORD=root&quot; --env=&quot;MYSQL_PASSWORD=root&quot; --env=&quot;MYSQL_DATABASE=myDB&quot; mysql
</code></pre>
</blockquote>
<ol start=""2"">
<li>Next I pull in my SCHEMA.sql which creates some new tables like so :</li>
</ol>
<blockquote>
<pre><code>docker exec -i test1 mysql -uroot -proot myDB &lt; SCHEMA.sql
</code></pre>
</blockquote>
<ol start=""3"">
<li>Next I inspect my instance to see if the new tables are there.</li>
</ol>
<blockquote>
<pre><code>docker exec -it test1 bash
mysql -uroot -proot
show databases;
use myDB;
show tables;
</code></pre>
</blockquote>
<p>My new tables are there!!</p>
<p>My plan now is to take an image of this and recreate another container instance.</p>
<ol start=""4"">
<li>I commit</li>
</ol>
<blockquote>
<pre><code>docker commit test1
</code></pre>
</blockquote>
<ol start=""5"">
<li>I tag the image</li>
</ol>
<blockquote>
<pre><code>docker tag e2d6f1b70bbe new_image:1.1
</code></pre>
</blockquote>
<ol start=""6"">
<li>Now I run :</li>
</ol>
<blockquote>
<pre><code>docker run -d -p 3401:3306 --name test2 new_image:1.1
</code></pre>
</blockquote>
<ol start=""7"">
<li>Next I Inspect to see if tables are there</li>
</ol>
<blockquote>
<pre><code>docker exec -it test2 bash
mysql -uroot -proot
show databases;
use myDB;
show tables;
</code></pre>
</blockquote>
<p>My tables are NOT there!! (and that makes me sad :´( )</p>
"
"39527571","Are shell scripts sensitive to encoding and line endings?","<bash><shell><sh>","66474675","AzerothCore Docker on Windows: '\r': command not found","<windows><bash><docker><azerothcore>","<p>I am making a NW.js app on Mac, and want to run the app in dev mode by double-clicking on an icon. First step, I'm trying to make my shell script work.</p>

<p>Using VSCode on Windows (I wanted to gain time), I have created a <code>run-nw</code> file at the root of my project, containing this:</p>

<pre><code>#!/bin/bash

cd ""src""
npm install

cd ..
./tools/nwjs-sdk-v0.17.3-osx-x64/nwjs.app/Contents/MacOS/nwjs ""src"" &amp;
</code></pre>

<p>but I get this output:</p>

<pre><code>$ sh ./run-nw

: command not found  
: No such file or directory  
: command not found  
: No such file or directory  

Usage: npm &lt;command&gt;

where &lt;command&gt; is one of:  (snip commands list)

(snip npm help)

npm@3.10.3 /usr/local/lib/node_modules/npm  
: command not found  
: No such file or directory  
: command not found
</code></pre>

<p>I really don't understand:</p>

<ul>
<li>it seems that it takes empty lines as commands. In my editor (VSCode) I have tried to replace <code>\r\n</code> with <code>\n</code> (in case the <code>\r</code> creates problems) but it changes nothing.</li>
<li>it seems that it doesn't find the folders (with or without the <code>dirname</code> instruction), or maybe it doesn't know about the <code>cd</code> command ?</li>
<li>it seems that it doesn't understand the <code>install</code> argument to <code>npm</code></li>
<li>the part that really weirds me out, is that it still runs the app (if I did a <code>npm install</code> manually)...</li>
</ul>

<p>Not able to make it work properly, and suspecting something weird with the file itself, I created a new one directly on the Mac, using vim this time. I entered the exact same instructions, and... now it works without any issue.<br>
A diff on the two files reveals exactly zero difference.</p>

<p>What can be the difference? What can make the first script not work? How can I find out?</p>

<h2>Update</h2>

<p>Following the accepted answer's recommandations, after the wrong line endings came back, I checked multiple things. It turns out that since I copied my <code>~/.gitconfig</code> from my Windows machine, I had <code>autocrlf=true</code>, so every time I modified the bash file under Windows, it re-set the line endings to <code>\r\n</code>.<br>
So, in addition to running dos2unix (which you will have to install using Homebrew on mac), if you're using Git, check your config.</p>
","<p>Getting this error while trying to run <a href=""https://www.azerothcore.org/wiki/Install-with-Docker"" rel=""nofollow noreferrer"">AzerothCore using Docker</a> on Windows:</p>
<pre><code>Step 9/15 : RUN ./azerothcore/bin/acore-db-asm 1
 ---&gt; Running in 17cc370dcab8
/azerothcore/apps/db_assembler/includes/../../bash_shared/includes.sh: line 2: $'\r': command not found
ERROR: Service 'ac-database' failed to build : The command '/bin/sh -c ./azerothcore/bin/acore-db-asm 1' returned a non-zero code: 127
</code></pre>
<p><a href=""https://i.stack.imgur.com/OrtBw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OrtBw.png"" alt=""AzerothCore error Docker on Windows"" /></a></p>
"
"39626579","Is there a way to combine Docker images into 1 container?","<docker><dockerfile><docker-image>","66891263","Merging two dockerfiles in one","<docker><ubuntu><dockerfile><conda>","<p>I have a few Dockerfiles right now.</p>

<p>One is for Cassandra 3.5, and it is <code>FROM cassandra:3.5</code></p>

<p>I also have a Dockerfile for Kafka, but t is quite a bit more complex. It is <code>FROM java:openjdk-8-fre</code> and it runs a long command to install Kafka and Zookeeper.</p>

<p>Finally, I have an application written in Scala that uses SBT. </p>

<p>For that Dockerfile, it is <code>FROM broadinstitute/scala-baseimage</code>, which gets me Java 8, Scala 2.11.7, and STB 0.13.9, which are what I need.</p>

<p>Perhaps, I don't understand how Docker works, but my Scala program has Cassandra and Kafka as dependencies and for development purposes, I want others to be able to simply clone my repo with the <code>Dockerfile</code> and then be able to build it with Cassandra, Kafka, Scala, Java and SBT all baked in so that they can just compile the source. I'm having a lot of issues with this though. </p>

<p>How do I combine these Dockerfiles? How do I simply make an environment with those things baked in?</p>
","<p>I have a custom docker image that i work upon.
The dockerfile i created starts with</p>
<pre><code>FROM ubuntu
</code></pre>
<p>Now, i want to add cadquery to my packages.
Hopefully, there is a dockerfile for doing that.
Here it is:</p>
<pre><code>FROM continuumio/miniconda3
RUN apt install -y libgl1-mesa-glx
RUN conda install conda-build
RUN apt-get install -y git
WORKDIR /
RUN git clone https://github.com/cadquery/cadquery.git
WORKDIR /cadquery
RUN conda env create -n cq -f environment.yml
RUN echo &quot;source activate cq&quot; &gt; ~/.bashrc
ENV PATH /opt/conda/envs/cq/bin:$PATH
WORKDIR /testing
</code></pre>
<p>(source: <a href=""https://github.com/RubenRubens/cq-testing/blob/master/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/RubenRubens/cq-testing/blob/master/Dockerfile</a>)</p>
<p>However, as you can see, it uses a different <code>FROM</code> command than mine.
What i want to do, is keep my dockerfile, and just add conda, and cadquery somehow.</p>
<p>I researched a bit into the <code>continuumio/miniconda3</code> image, but i am to sure what is the best startegy to use after all.</p>
<p>I know that you can have two different <code>FROM</code> commands in your dockerfile, but is this recommended?</p>
<p>If yes, how the structure of the dockerfile should be?</p>
<p>IF no, meaning it is NOT recommended to use two different <code>FROM</code> commands, how should i structure my dockerfile?</p>
<p>Should i use the <code>FROM</code> ubuntu dockerfile, and just add conda manually, and then proceed like this dockerfile i pasted here?</p>
<p>EDIT: I forgot to say that one important thing to note, is that miniconda is installed from a <code>.sh</code> file that you download
(source: <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html"" rel=""nofollow noreferrer"">https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</a>)</p>
<p>So i am not sure if using two <code>FROM</code> commands (and how), or just having the basic ubuntu image and downloading that <code>.sh</code> would work best.</p>
"
"40182121","What's the source of Error: getaddrinfo EAI_AGAIN?","<javascript><node.js><error-handling><dns><shopify>","66730512","Why does mongo crash inside of docker at random?","<node.js><mongodb><docker><mongoose><docker-compose>","<p>My server threw this today, which is a Node.js error I've never seen before:</p>

<pre><code>Error: getaddrinfo EAI_AGAIN my-store.myshopify.com:443
    at Object.exports._errnoException (util.js:870:11)
    at errnoException (dns.js:32:15)
    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:78:26)
</code></pre>

<p>I'm wondering if this is related to the DynDns DDOS attack which affected Shopify and many other services today. <a href=""http://thenextweb.com/security/2016/10/21/massive-ddos-attack-dyn-dns-causing-havoc-online/"" rel=""noreferrer"">Here's an article about that.</a></p>

<p>My main question is what does <code>dns.js</code> do? What part of node is it a part of? How can I recreate this error with a different domain?</p>
","<p><em>I have a docker-compose file that creates a Nodejs app that depends on mongo that is on another docker.</em></p>
<p><strong>The compose runs properly but than at random I get the following error, why do I get this ?</strong></p>
<pre><code>{&quot;level&quot;:&quot;error&quot;,&quot;type&quot;:&quot;none&quot;,&quot;message&quot;:&quot;Error: MongooseServerSelectionError: getaddrinfo EAI_AGAIN mongo&quot;,&quot;service&quot;:&quot;portal&quot;,&quot;timestamp&quot;:&quot;21-03-21 01:54:24&quot;}
</code></pre>
<p>the docker-compose file</p>
<pre><code>version: &quot;3.2&quot;

services:
  portal:
    container_name: portal
    image: keybraker/portal:latest
    build:
      context: .
      dockerfile: ./docker/Dockerfile
    depends_on:
      - mongo
    volumes:
      - type: bind
        source: ../logs
        target: /logs

  mongo:
    container_name: mongo
    restart: always
    image: mongo:4.4.3
    ports:
      - &quot;29017:27017&quot;
    volumes:
      - type: bind
        source: ../mongo/data/db
        target: /data/db
</code></pre>
"
"46440909","How to configure different dockerfile for development and production","<docker><dockerfile>","66748919","Docker for dev and prod","<php><docker><docker-compose><xdebug>","<p>I use docker for development and in production for laravel project. I have slightly different dockerfile for development and production. For example I am mounting local directory to docker container in development environment so that I don't need to do docker build for every change in code. </p>

<p>As mounted directory will only be available when running the docker container I can't put commands like ""composer install"" or ""npm install"" in dockerfile for development.</p>

<p>Currently I am managing two docker files, is there any way that I can do this with single docker file and decide which commands to run when doing docker build by sending parameters.</p>

<p><strong>What I am trying to achieve is</strong> </p>

<p><strong>In docker file</strong></p>

<pre><code>...
IF PROD THEN RUN composer install
...
</code></pre>

<p><strong>During docker build</strong></p>

<pre><code>docker build [PROD] -t mytag .
</code></pre>
","<p>I want to use docker for development and production. I know that in theory the settings should be the same, but I came across the fact that, for example, I only need xbedug during development. In the future, there will probably be other things that are not needed in production.</p>
<p>Right now my docker-compose contains</p>
<pre><code>networks:
  appuser:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${SUBNET_IP}/${SUBNET_MASK}
</code></pre>
<p>for xdebug.</p>
<p>And in the dockerfile for php, I try to use &quot;if&quot;</p>
<pre><code>RUN if [ ${APP_ENV} == &quot;dev&quot; ]; then pecl install xdebug; fi
RUN if [ ${APP_ENV} == &quot;dev&quot; ]; then docker-php-ext-enable xdebug; fi
</code></pre>
<p>This does not generate errors, but it does not install xdebug either.</p>
<p>Hence the question: what is the best way to solve the problem with unnecessary settings and modules in prod? There must be some kind of best practice or something like that. I’m hardly the first person to come across such a question. Use different docker-compose files for prod and dev? I don't really like this solution. You will have to create almost identical docker-compose and dockerfile. IF in docker is also not trustworthy, especially since it does not work and extra instructions remain in docker-compose.</p>
"
"47854463","Docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock","<docker><jenkins><jenkins-pipeline>","66497967","Jenkins error with Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock","<docker><jenkins>","<p>I am new to docker. I just tried to use docker in my local machine(Ubuntu 16.04) with Jenkins. </p>

<p>I configured a new job with below pipeline script.</p>

<pre class=""lang-groovy prettyprint-override""><code>node {
    stage('Build') {
      docker.image('maven:3.3.3').inside {
        sh 'mvn --version'
      }
    }
}
</code></pre>

<p>But it fails with below error.</p>

<p><a href=""https://i.stack.imgur.com/nz6Ig.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nz6Ig.png"" alt=""enter image description here""></a></p>
","<p>I have a jenkins job that does a docker ps command. It is a pipeline script as follows:</p>
<pre><code>node {
    wrap([$class: 'BuildUser']) {
        sh 'echo &quot;The build user: &quot;'
        sh 'echo &quot;${BUILD_USER}&quot;'
    }
    stage 'run ps command'
    sh &quot;\$(docker ps)&quot;
}
</code></pre>
<p>It prints:</p>
<pre><code>+ echo 'The build user: '
The build user: 
[Pipeline] sh
+ echo 'Anonymous'
Anonymous
[Pipeline] }
[Pipeline] // wrap
[Pipeline] stage (run ps command)
Using the ‘stage’ step without a block argument is deprecated
Entering stage run ps command
Proceeding
[Pipeline] sh
++ docker ps
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json: dial unix /var/run/docker.sock: connect: permission denied
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
ERROR: script returned exit code 1
Finished: FAILURE
</code></pre>
<p>It is my understanding that jenkins job will always run as the same OS user, so in thise case &quot;<strong>jenkinsuser</strong>&quot;. I have already added this user to docker group and restarted jenkins multiple times.</p>
<pre><code>$&gt; grep docker /etc/group
docker:x:497:jenkinsuser

$&gt; ps -ef | grep jenkins
root     11177   978  0 15:48 ?        00:00:00 sshd: jenkinsuser [priv]
jenkinsuser 11180 11177  0 15:48 ?        00:00:00 sshd: jenkinsuser@pts/0
jenkinsuser 13276     1  0  2020 ?        1-19:07:35 java -jar jenkins.war
jenkinsuser 20929 11181  0 20:04 pts/0    00:00:00 grep --color=auto jenkins
</code></pre>
<p>Is there something I am missing?</p>
<p>When i am on the linux box, I can successfully do docker ps:</p>
<pre><code>$&gt; whoami
jenkinsuser

$&gt; docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
</code></pre>
<p>but this does not work when i do it as part of a jenkins pipeline build job!! Why?!?</p>
"
"48910876","Error: EACCES: permission denied, access '/usr/local/lib/node_modules'","<node.js><npm><permission-denied>","66447140","Permission error on react docker project : EACCES: permission denied, mkdir '/app/node_modules/.cache'","<reactjs><docker><docker-compose>","<p>What might be causing the error <code>Error: EACCES: permission denied, access '/usr/local/lib/node_modules'</code>?</p>

<pre><code>npm ERR! path /usr/local/lib/node_modules
npm ERR! code EACCES
npm ERR! errno -13
npm ERR! syscall access
npm ERR! Error: EACCES: permission denied, access '/usr/local/lib/node_modules'
npm ERR!  { Error: EACCES: permission denied, access '/usr/local/lib/node_modules'
npm ERR!   errno: -13,
npm ERR!   code: 'EACCES',
npm ERR!   syscall: 'access',
npm ERR!   path: '/usr/local/lib/node_modules' }
npm ERR! 
npm ERR! Please try running this command again as root/Administrator.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/macbookmd101/.npm/_logs/2018-02-21T16_26_08_421Z-debug.log
</code></pre>
","<p>Good evening I created a react project, with a docker file, and a docker-composes file but when the dockerfile build there is a permission error in the node_module foalder located in the container.</p>
<blockquote>
<p>_1  | Failed to compile.
web_1  |
web_1  | EACCES: permission denied, mkdir '/app/node_modules/.cache'</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/kG6wM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kG6wM.png"" alt=""enter image description here"" /></a></p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","66759002","How to protect docker file from users within a system?","<docker><file><dockerfile><password-protection><password-encryption>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I am working on a project and to show the demo of project the code has to be pushed into sever of the third party, but my code has to be protected so that other users in the system cannot access or modify my code. The code runs on docker, so is there anyways to protect my docker file from other users within the system?</p>
"
"66159404","Parameterising Docker Image prefix","<docker><dockerfile><aws-codebuild><amazon-ecr>","66875286","How do I avoid code duplication in Dockerfile?","<docker><dockerfile>","<p>I have a Dockerfile which has the following content</p>
<pre><code># Build
FROM ${ECR_PREFIX}/maven:3.6.3-jdk-11 AS build
COPY src /home/app/src
COPY pom.xml /home/app
RUN mvn -f /home/app/pom.xml clean package

# Package
FROM ${ECR_PREFIX}/openjdk:11-jre-slim
COPY --from=build /home/app/target/application.jar application.jar
EXPOSE 8080
ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;application.jar&quot;]
</code></pre>
<p>I tried to build this using</p>
<pre><code>export PREFIX=${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
docker build -t backend --build-arg ECR_PREFIX=$PREFIX .
</code></pre>
<p>but this would not work. I am really trying not to hard code the prefix especially <strong>${AWS::AccountId}</strong> part of it because of various reasons.</p>
<p>Any pointers here?</p>
","<p>In my Dockerfile I have two almost identical FROM statements:</p>
<pre><code>FROM 9999999999999999.dkr.ecr.us-east.amazonaws.com/sqlserver:myTag AS builder

...

FROM 9999999999999999.dkr.ecr.us-east.amazonaws.com/sqlserver:myTag 
</code></pre>
<p>How do I avoid code duplication here?</p>
"
"23513045","How to check if a process is running inside docker container?","<shell><docker><containers>","65895493","How to determine whether a process is running in a container or not (in linux)?","<c><linux><docker><linux-kernel><containers>","<p>[Updated1] I have a shell which will change TCP kernel parameters in some functions, but now I need to make this shell run in Docker container, that means, the shell need to know it is running inside a container and stop configuring the kernel. </p>

<p>Now I'm not sure how to achieve that, here is the contents of <code>/proc/self/cgroup</code> inside the container: </p>

<pre><code>9:hugetlb:/
8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>Any flags above can I use to figure out if this process is running inside a container?</p>

<p>[Updated2]: I have also noticed <a href=""https://stackoverflow.com/questions/20010199/determining-if-a-process-runs-inside-lxc-docker"">Determining if a process runs inside lxc/Docker</a>, but it seems not working in this case, the content in <code>/proc/1/cgroup</code> of my container is:</p>

<pre><code>8:perf_event:/
7:blkio:/
6:freezer:/
5:devices:/
4:memory:/
3:cpuacct:/
2:cpu:/docker/25ef774c390558ad8c4e9a8590b6a1956231aae404d6a7aba4dde320ff569b8b
1:cpuset:/
</code></pre>

<p>No /lxc/containerid</p>
","<p>I am trying to determine whether a process is running in a container or not? Is there any method to see from the point of host view? Is there any method to check it by using linux kernel namespace?</p>
"
"28089344","Docker, what is it and what is the purpose","<docker>","66073824","why is 'sys.platform' output different between 'docker run python' and just 'python'?","<linux><docker>","<p>I've heard about Docker some days ago and wanted to go across.</p>

<p>But in fact, I don't know what is the purpose of this ""container""? </p>

<p>What is a container?</p>

<p>Can it replace a virtual machine dedicated to development?</p>

<p>What is the purpose, in simple words, of using Docker in companies? The main advantage?</p>
","<p>I'm really new to Docker, and I'm using Mac.
I heard that docker is not VM, and docker is built upon the existing OS of the machine.
Then why does that difference occur? Are all docker containers running on Linux? Then what is the difference btw VM and docker?</p>
<pre><code>python
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.platform
'darwin'

docker run --rm -it python
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.platform
'linux'
</code></pre>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","66071282","Docker Container server not sending data","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I've just started learning docker and I'm trying to run a very simple flask server in a container. Whenever I run it, it works just fine. However, when I access the website, chrome displays this:</p>
<pre><code>This page isn’t working
127.0.0.1 didn’t send any data.
ERR_EMPTY_RESPONSE
</code></pre>
<p>Here is my code:</p>
<pre><code>from flask import Flask

app = Flask(__name__)

@app.route(&quot;/&quot;)
def index():
    return &quot;Hello World&quot;

if __name__ == &quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
<p>The Dockerfile:</p>
<pre><code>FROM python:3

RUN mkdir /test
WORKDIR /test
RUN pip3 install flask

COPY . .
EXPOSE 5000

cmd [&quot;python3&quot;, &quot;application.py&quot;]
</code></pre>
<p>And the commands I'm using to build the image and run the container:</p>
<pre><code>docker build . -t test
docker start -p 5000:5000 test
</code></pre>
<p>Could someone help me figure this out?</p>
"
"31746182","Docker Compose wait for container X before starting Y","<docker-compose>","65987900","How to run RabbitMQ consumer using Docker - NodeJS","<node.js><docker><rabbitmq><microservices>","<p>I am using rabbitmq and a simple python sample from <a href=""https://www.rabbitmq.com/tutorials/tutorial-one-python.html"" rel=""noreferrer"">here</a>
together with docker-compose. My problem is that I need to wait for rabbitmq to be fully started. From what I searched so far, I don't know how to wait with container x (in my case worker) until y (rabbitmq) is started.</p>
<p>I found this <a href=""http://blog.chmouel.com/2014/11/04/avoiding-race-conditions-between-containers-with-docker-and-fig/"" rel=""noreferrer"">blog post</a> where he checks if the other host is online.
I also found this <a href=""https://docs.docker.com/reference/commandline/wait/"" rel=""noreferrer"">docker command</a>:</p>
<blockquote>
<p><strong>wait</strong></p>
<p>Usage: docker wait CONTAINER [CONTAINER...]</p>
<p>Block until a container stops, then print its exit code.</p>
</blockquote>
<p>Waiting for a container to stop is maybe not what I am looking for but if
it is, is it possible to use that command inside the docker-compose.yml?
My solution so far is to wait some seconds and check the port, but is this the way to achieve this? If I don't wait, I get an error.</p>
<p><strong>docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>worker:
    build: myapp/.
    volumes:
    - myapp/.:/usr/src/app:ro

    links:
    - rabbitmq
rabbitmq:
    image: rabbitmq:3-management
</code></pre>
<p><strong>python hello sample (rabbit.py):</strong></p>
<pre class=""lang-py prettyprint-override""><code>import pika
import time

import socket

pingcounter = 0
isreachable = False
while isreachable is False and pingcounter &lt; 5:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        s.connect(('rabbitmq', 5672))
        isreachable = True
    except socket.error as e:
        time.sleep(2)
        pingcounter += 1
    s.close()

if isreachable:
    connection = pika.BlockingConnection(pika.ConnectionParameters(
            host=&quot;rabbitmq&quot;))
    channel = connection.channel()

    channel.queue_declare(queue='hello')

    channel.basic_publish(exchange='',
                          routing_key='hello',
                          body='Hello World!')
    print (&quot; [x] Sent 'Hello World!'&quot;)
    connection.close()
</code></pre>
<p><strong>Dockerfile for worker:</strong></p>
<pre><code>FROM python:2-onbuild
RUN [&quot;pip&quot;, &quot;install&quot;, &quot;pika&quot;]

CMD [&quot;python&quot;,&quot;rabbit.py&quot;]
</code></pre>
<p><strong>Update Nov 2015</strong>:</p>
<p>A shell script or waiting inside your program is maybe a possible solution. But after seeing this <a href=""https://github.com/docker/compose/issues/374"" rel=""noreferrer"">Issue</a> I am looking for a command or feature of docker/docker-compose itself.</p>
<p>They mention a  solution for implementing a health check, which may be the best option. A open tcp connection does not mean your service is ready or may remain ready. In addition to that I need to change my entrypoint in my dockerfile.</p>
<p>So I am hoping for an answer with docker-compose on board commands, which will hopefully the case if they finish this issue.</p>
<p><strong>Update March 2016</strong></p>
<p>There is a <a href=""https://github.com/docker/docker/issues/21142"" rel=""noreferrer"">proposal</a> for providing a built-in way to determine if a container is &quot;alive&quot;. So docker-compose can maybe make use of it in near future.</p>
<p><strong>Update June 2016</strong></p>
<p>It seems that the healthcheck will be <a href=""https://github.com/docker/docker/pull/23218"" rel=""noreferrer"">integrated</a> into docker in Version 1.12.0</p>
<p><strong>Update January 2017</strong></p>
<p>I found a docker-compose solution see:
<a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y/41854997#41854997"">Docker Compose wait for container X before starting Y</a></p>
","<p>I have a microservice application that uses RabbitMQ. How can I run the RabbitMQ consumer from the application backend container only after the RabbitMQ is up and running. My compose file is as below.</p>
<pre><code>certichain_backend:
    depends_on:
      - rabbitmq

    working_dir: /app/backend/src 
    command: sh sleep 20 &amp; nohup node /app/backend/src/services/amqp_consumer.js &amp;&amp; npm run start;

rabbitmq:
     image: &quot;rabbitmq:3-management&quot;
     hostname: &quot;rabbitmq&quot;
     restart: always
     expose:
       - 15672
       - 5672
     labels:
       NAME: &quot;rabbitmq&quot;
     volumes:
       - ./rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
                
</code></pre>
<p>I have given the backend the 'depends_on' to rabbitmq. But what I have observed is, the rabbitmq container starting process is initiated. But the backend container is not waiting for the rabbitmq container to up completely. My consumer is running along with the backend. So the consumer cannot connect to amqp server since it is not running at that moment. Thus I added a sleep parameter. So that it gets some time while rabbitmq to bringing up.</p>
<p>This method is very inconsistent. I'm sure this is not the right way to achieve this.</p>
"
"34254200","How to pass arguments to a Dockerfile?","<docker><dockerfile><docker-compose><docker-registry><dockerhub>","65931367","How to pass arguments to a bash script through Dockerfile","<bash><docker>","<p>I am using RUN instruction within a <code>Dockerfile</code> to install a rpm</p>

<pre><code>RUN yum -y install samplerpm-2.3
</code></pre>

<p>However, I want to pass the value ""2.3"" as an argument.
My RUN instruction should look something like:</p>

<pre><code>RUN yum -y install samplerpm-$arg
</code></pre>

<p>where <code>$arg=2.3</code></p>
","<p>I have a simple <code>Dockerfile</code> as below:</p>
<pre><code>FROM kalilinux/kali-rolling

WORKDIR /attack

COPY . /attack
RUN ls
RUN chmod +x attack.sh
RUN ./attack.sh $aws_access_key_id $aws_secret_access_key $default_region $bucket
</code></pre>
<p>And I have this <code>attack.sh</code> and it contains content as below:</p>
<pre><code>#!/bin/bash

# Getting the host passed as an argument to the script
/usr/share/zaproxy/zap.sh -cmd -addoninstall exportreport
mkdir -p /root/test

test() {
    echo &quot;access key = ${1}&quot;
    echo &quot;secret key = ${2}&quot;
    echo &quot;default region = ${3}&quot;
    echo &quot;bucket = ${4}&quot;
    echo &quot;line = ${5}&quot;

}

while IFS= read -r line; do
    # echo $line
    test $1 $2 $3 $4 $line 
done &lt; domains.txt

</code></pre>
<p>And if you are wondering how I pass the value to the <code>Dockerfile</code>, it is like this:</p>
<pre><code>docker build --build-arg aws_access_key_id=${SPOT_RUNNER_ACCESS_KEY} --build-arg aws_secret_access_key=${SPOT_RUNNER_SECRET_KEY} --build-arg default_region=ap-southeast-2 --build-arg bucket=${BUCKET_NAME} -t run-test .
</code></pre>
<p>So the arguments are passed in below order</p>
<pre><code>docker build ---&gt; Dockerfile ---&gt; attack.sh
</code></pre>
<p>But, this is not working, it gives an empty values for these variables.</p>
<p>Can someone please help me?</p>
"
"36243559","How to connect Docker web app container to Docker PostgreSQL container?","<postgresql><go><docker><docker-compose>","65921326","Connecting to Postgresql in a docker container","<postgresql><docker><remote-connection>","<p>I'm practicing making a Golang web app that interacts with a PostgreSQL database, each running on their own container.</p>

<p>I'm running the containers with <code>docker-compose up</code></p>

<p>But I seem to be failing on getting the postgres container properly set up.</p>

<p>For brevity, links to <code>Dockerfile</code>s and other settings files are <a href=""https://gist.github.com/andradei/922e11f469aec277ab6e"" rel=""nofollow"">on this gist</a> (let me know if you want it here instead).</p>

<pre><code>version: '2'
services:
  web_app:
    build: dockerfiles/web_app
    ports:
      - ""9000:9000""
    volumes:
      - .:/go/src/gitlab.com/repo/web_app
    # links might be replaced by depends_on.
    # links:
    #   - db
    depends_on:
      - db
    # tty and stdin_open cause docker-compose to disconnect from docker-machine after 60sec.
    # A fix is on the way.
    # tty: true
    # stdin_open: true
  db:
    build: dockerfiles/db
    volumes:
      - data:/var/lib/postgresql/data
volumes:
  data: {}
</code></pre>

<p><code>docker-compose up</code> works fine. But when the application tries to open a database connection with:</p>

<pre><code>var pgConf string = ""user=web_app dbname=web_app sslmode=verify-full password=password""

db, err := sql.Open(""postgres"", pgConf)
</code></pre>

<p>I get the following error from docker compose:</p>

<pre><code>Error creating new user:  dial tcp [::1]:5432: getsockopt: connection refused
</code></pre>

<p>What can I do to make both containers talk to each other?</p>

<p>Thank you in advance.</p>
","<p>I have a GO project, inside which I am implementing work with the Postgresql database (P.s. I use the pgx driver, if it matters). I have uploaded the whole project to a docker container, and when I run the project image, I get an error that it cannot connect to the database. I understand that the image is looking for a database in itself. Question: How do I connect to the database?</p>
<p>In main func I have this code:</p>
<pre><code>addr := flag.String(&quot;addr&quot;, &quot;:4000&quot;, &quot;HTTP network address&quot;)
flag.Parse()

infoLog := log.New(os.Stdout, &quot;INFO\t&quot;, log.Ldate|log.Ltime)
errorLog := log.New(os.Stderr, &quot;ERROR\t&quot;, log.Ldate|log.Ltime|log.Lshortfile)

pool, err := pgxpool.Connect(context.Background(), &quot;user=postgres password=root host=localhost port=5432 dbname=snippetbox sslmode=disable pool_max_conns=10&quot;)
if err != nil {
    log.Fatalf(&quot;Unable to connection to database&quot;)
}
defer pool.Close()
</code></pre>
<p><a href=""https://i.stack.imgur.com/cXXwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cXXwI.png"" alt=""enter image description here"" /></a></p>
"
"39023640","How to cache package manager downloads for docker builds?","<caching><docker><composer-php><docker-volume><diskcache>","66042906","Shared volume across multiple docker-compose projects","<performance><docker><docker-compose><composer-php><yarnpkg>","<p>If I run <code>composer install</code> from my host, I hit my local composer cache:</p>

<pre><code>  - Installing deft/iso3166-utility (1.0.0)
    Loading from cache
</code></pre>

<p>Yet when building a container having in its Dockerfile:</p>

<pre><code>RUN composer install -n -o --no-dev
</code></pre>

<p>I download all the things, e.g.:</p>

<pre><code>  - Installing deft/iso3166-utility (1.0.0)
    Downloading: 100%         
</code></pre>

<p>It's expected, yet I like to avoid it. As even on a rebuilt, it would also download everything again.</p>

<p>I would like to have a universal cache for composer that I could also reshare for other docker projects.</p>

<p>I looked into this and found the approach to <a href=""https://git.framasoft.org/Chill-project/docker-ci-image/commit/002aeaf8ae5845c71eb8191821a98aa7a8565f0e"" rel=""noreferrer"">define a volume in the Dockerfile</a>:</p>

<pre><code>ENV COMPOSER_HOME=/var/composer
VOLUME /var/composer
</code></pre>

<p>I added that to my <code>Dockerfile</code>, and expected to only download the files once, and hit the cache afterwards.</p>

<p>Yet when I modify my <code>composer</code>, e.g. remove the <code>-o</code> flag, and rerun <code>docker build .</code>, I expected to hit the cache on build, yet I still download the vendors again.</p>

<p>How are volumes supposed to work to have a data cache inside a docker container?</p>
","<p>I'm using <code>docker-compose</code> to orchestrate containers for multiple separate projects. Each of these projects has their own set of containers and do not relate to other projects.</p>
<p>For example:</p>
<pre><code>/my-projects/project-1/docker-compose.yml
/my-projects/project-2/docker-compose.yml
/my-projects/project-3/docker-compose.yml
</code></pre>
<p>These projects are, however, similar in that they are all PHP projects and use webpack for front-end assets, thus share the same package managers: <code>composer</code> and <code>yarn</code>.</p>
<p>I was wondering, in the interest of performance, if it would be possible to mount a shared volume outside the directory root of all the projects for package manager caches?</p>
<p>For example:</p>
<pre><code>/my-projects/caches/composer
/my-projects/caches/npm
/my-projects/project-1/docker-compose.yml
/my-projects/project-2/docker-compose.yml
/my-projects/project-3/docker-compose.yml
</code></pre>
<p>Where <code>/my-projects/caches/composer</code> and <code>/my-projects/caches/npm</code> get mounted inside the relevant containers within each project. In case it's not clear, only one project would be spun up at a time.</p>
<p>At the moment, if two projects share the same deps then each downloads and caches it individually. A more performant (in terms of build times) would be to mount a common volume and point the package manager's caches there so that when &quot;Project A&quot; downloads an update to a dip, &quot;Project B&quot; can load it from cache.</p>
"
"39564861","Mount ""named volume"" as non-root in Docker","<docker><mount><mount-point>","65982541","Docker volumes as non root user (Create and docker-compose)","<docker><docker-compose>","<p>Is there any way to mount a named volume as a non-root user? I am trying to avoid having to run a <code>chown</code> in each Dockerfile but I need the mount to be writable by a non-root user to be able to write the artifacts created by a build in the image</p>

<p>This is what I'm trying</p>

<pre><code>docker run --rm -it -v /home/bob/dev/:/src/dev -v builds:/mnt/build --name build hilikus/build /bin/bash
</code></pre>

<p>but for the second mount I get</p>

<pre><code>[user@42f237282128 ~]$ ll /mnt
total 4
drwxr-xr-x 2 root root 4096 Sep 18 19:29 build
</code></pre>

<p>My other mount (<code>/src/dev/</code>) is owned by <em>user</em>, not by root so it gives what I need; however, I haven't been able to do the same with the named volume.</p>
","<p>I use a docker-compose file to create some containers. Everything works fine except the volumes that are being created by docker-compose are created with <code>root:root</code>. That makes it very hard to move around as non root and I don't want to have to use sudo all the time to change, copy, or read files in that file tree.</p>
<p>I can run docker as the main user &quot;jack&quot; without a problem--docker-compose for example is being run as the local user &quot;jack&quot;. <code>sudo chown -R jack:docker /opt/docker/volumes/</code> changes permissions without breaking the function of the container but that would require to run chown periodically which I don't want to do.</p>
<p>My question is: How can I make docker-compose create the volumes as anybody else than root? Ideally, all volumes would be created with the ownership set to docker:docker or jack:docker.</p>
<p>The compose file looks like this:</p>
<pre><code>bitwarden:
    image: bitwardenrs/server:latest
    container_name: bitwarden
    environment:
      - PUID=${PUID} #ie. 1000
      - PGID=${PGID} #ie. 1000
      - TZ=${TZ}
      - [...]
    volumes:
      - bitwarden-vol-config:/config
      - bitwarden-vol-data:/data1
    restart: unless-stopped

[...]
volumes:
  bitwarden-vol-config:
  bitwarden-vol-data:
</code></pre>
"
"41935435","Understanding ""VOLUME"" instruction in DockerFile","<docker><dockerfile>","65885555","Directory is not mounted","<docker><docker-compose><dockerfile>","<p>Below is the content of my ""Dockerfile"" </p>

<pre><code>FROM node:boron

# Create app directory
RUN mkdir -p /usr/src/app

# change working dir to /usr/src/app
WORKDIR /usr/src/app

VOLUME . /usr/src/app

RUN npm install

EXPOSE 8080

CMD [""node"" , ""server"" ]
</code></pre>

<p>In this file I am expecting ""VOLUME . /usr/src/app"" instruction to mount contents of present working directory in host to be mounted on /usr/src/app folder of container.</p>

<p>Please let me know if this is the correct way ?</p>
","<pre><code>-- project
----&gt; configs
...
------&gt; nodejs
-------- Dockerfile
-- databases
-- production // (PHP + VUE)
</code></pre>
<p><strong>Dockerfile</strong>:</p>
<pre><code>FROM node:lts-alpine
VOLUME ../../frontend /var/www/project/production/frontend
WORKDIR /var/www/finex_project
RUN ls -la
</code></pre>
<p>Why the directory(frontend) is not mounted?</p>
"
"43099116","Error ""The input device is not a TTY""","<docker><jenkins><jenkins-pipeline>","65876075","Why run Docker container with -t?","<docker><terminal>","<p>I am running the following command from my <code>Jenkinsfile</code>. However, I get the error <em>""The input device is not a TTY""</em>.</p>

<pre><code>docker run -v $PWD:/foobar -it cloudfoundry/cflinuxfs2 /foobar/script.sh
</code></pre>

<p>Is there a way to run the script from the <code>Jenkinsfile</code> without doing interactive mode?</p>

<p>I basically have a file called <code>script.sh</code> that I would like to run inside the Docker container.</p>
","<p>The Docker Run Reference says that running a container with <code>-t</code></p>
<blockquote>
<p>-t              : Allocate a pseudo-tty</p>
</blockquote>
<p>But only running it with <code>-i</code> allows the user to interact with the containerized process through the terminal. So I wonder, what is the meaning of &quot;Allocating a pseudo-tty&quot;, since even when running without <code>-t</code>, content written to <code>STDOUT</code> by the process will be passed to the terminal (The process will have a pipe as stdout instead of a tty) ?</p>
<p>I read <a href=""https://stackoverflow.com/questions/43099116/error-the-input-device-is-not-a-tty"">this answer</a> which says that you may run <code>docker run -t</code> to have &quot;Terminal support&quot;, such as text coloring etc. Well I already done the following experiment:</p>
<pre><code>// Dockerfile

FROM ubuntu:latest

CMD [&quot;echo&quot;, &quot;-e&quot;, &quot;\u001b[31mHello World&quot;]
</code></pre>
<p>And ran this image with no <code>-t</code>. Since I'm running it from a terminal (<code>docker run</code> will always run from some terminal won't it?) I can see a red &quot;Hello World&quot;. So I still don't understand why running with <code>-t</code> alone...</p>
"
"43181654","Locating data volumes in Docker Desktop (Windows)","<docker><docker-for-windows><docker-desktop>","65939775","Where is docker volume and images created on local windows 10","<docker><docker-desktop>","<p>I'm trying to learn docker at the moment and I'm getting confused about where data volumes actually exist.</p>

<p>I'm using <strong>Docker Desktop for Windows</strong>. (Windows 10)</p>

<p>In the docs they say that running docker inspect on the object will give you the source:<a href=""https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume"" rel=""noreferrer"">https://docs.docker.com/engine/tutorials/dockervolumes/#locating-a-volume</a></p>

<pre><code>$ docker inspect web

""Mounts"": [
    {
        ""Name"": ""fac362...80535"",
        ""Source"": ""/var/lib/docker/volumes/fac362...80535/_data"",
        ""Destination"": ""/webapp"",
        ""Driver"": ""local"",
        ""Mode"": """",
        ""RW"": true,
        ""Propagation"": """"
    }
]
</code></pre>

<p>however I don't see this, I get the following:</p>

<pre><code>$ docker inspect blog_postgres-data
[
    {
        ""Driver"": ""local"",
        ""Labels"": null,
        ""Mountpoint"": ""/var/lib/docker/volumes/blog_postgres-data/_data"",
        ""Name"": ""blog_postgres-data"",
        ""Options"": {},
        ""Scope"": ""local""
    }
]
</code></pre>

<p>Can anyone help me? I just want to know where my data volume actually exists is it on my host machine? If so how can i get the path to it?</p>
","<p>I ran command <code>docker volume create --name data-postgresql --driver local</code>, and I am unable to view this volume before I could use it. I am naïve to docker.</p>
"
"43308319","How can I run bash in a new container of a docker image?","<bash><docker>","66114411","docker run exited right away?","<docker>","<p>I am able to run arbitrary shell commands in a container created from docker/whalesay image.</p>

<pre><code>$ docker run docker/whalesay ls -l
total 56
-rw-r--r-- 1 root root  931 May 25  2015 ChangeLog
-rw-r--r-- 1 root root  385 May 25  2015 INSTALL
-rw-r--r-- 1 root root 1116 May 25  2015 LICENSE
-rw-r--r-- 1 root root  445 May 25  2015 MANIFEST
-rw-r--r-- 1 root root 1610 May 25  2015 README
-rw-r--r-- 1 root root  879 May 25  2015 Wrap.pm.diff
drwxr-xr-x 2 root root 4096 May 25  2015 cows
-rwxr-xr-x 1 root root 4129 May 25  2015 cowsay
-rw-r--r-- 1 root root 4690 May 25  2015 cowsay.1
-rw-r--r-- 1 root root   54 May 25  2015 install.pl
-rwxr-xr-x 1 root root 2046 May 25  2015 install.sh
-rw-r--r-- 1 root root  631 May 25  2015 pgp_public_key.txt
$ docker run docker/whalesay lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.2 LTS
Release:    14.04
Codename:   trusty
</code></pre>

<p>However, I am unable to run a shell in a container created from this image.</p>

<pre><code>$ docker run docker/whalesay bash
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                         PORTS               NAMES
7ce600cc9904        docker/whalesay     ""bash""                   5 seconds ago       Exited (0) 3 seconds ago                           loving_mayer
</code></pre>

<p>Why did it not work? How can I make it work?</p>
","<p>I have the following Dockerfile so I can login the container to check the image Some_Linux_Img.</p>
<pre><code>FROM Some_Linux_Img
ENTRYPOINT [&quot;/bin/bash&quot;]
</code></pre>
<p>And I built it</p>
<pre><code>docker build -t test:v2 .
Sending build context to Docker daemon 2.048 kB
Step 1/2 : FROM abc.com/shared/miniconda
 ---&gt; ec1a66fb9030
Step 2/2 : ENTRYPOINT /bin/bash
 ---&gt; Running in ea14b4ce6c6e
 ---&gt; 21ebe99c7ef1
Removing intermediate container ea14b4ce6c6e
Successfully built 21ebe99c7ef1
</code></pre>
<p>However, it exited right after <code>docker run</code>?</p>
<p>$ docker run --name test test:v2
$ docker ps -a</p>
<p>Status showed &quot;Exited (0) x seconds ago&quot;. I wanted to run some bash commands inside the container to check the Linux image <code>Some_Linux_Img</code>. (<code>docker exec -it test /bin/bash</code>)</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","65966577","docker compose access shell script from other container","<docker><docker-compose>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>I have a docker-compose file with 2 containers.</p>
<p>On container A I am running a shell script which should execute a shell script that is in Container B. I have tried a shared_volume but then the needed files to start container B are missing.</p>
<p>So in my file, I would like to execute the init.sh (which is workig so far) but it should execute a file from the service &quot;keycloak&quot;</p>
<pre><code>keycloak:
    build:
      context: .
      dockerfile: Dockerfile_2
    volumes: 
      - ./imports/cache_reload/disable-theme-cache.cli:/opt/jboss/startup-scripts/disable-theme-cache.cli
      - ./imports/themes/custom/:/opt/jboss/keycloak/themes/custom-theme/
      - ./imports/realm/realm-export.json:/root/opt/jboss/keycloak/bin/custom-import.json
    environment:
      DB_VENDOR: MYSQL
      DB_ADDR: mysql
      DB_DATABASE: keycloak
      DB_USER: keycloak
      DB_PASSWORD: password
        #KEYCLOAK_USER: admin
        #KEYCLOAK_PASSWORD: Pa55w0rd
    ports:
      - 8080:8080
    depends_on:
      - mysql
  keycloak_installer:
    build:
      context: .
      dockerfile: Dockerfile_1
    volumes:
      - ./imports/scripts/import-realm.sh:/docker-entrypoint-initdb.d/init.sh
    ports:
      - 8090:8080
    depends_on: 
      - keycloak
</code></pre>
"
"46517241","""docker build"" requires exactly 1 argument(s)","<docker><dockerfile>","65932455","docker: specifying a Dockerfile - ""docker build"" requires exactly 1 argument","<docker>","<p>I am trying to build an image from a specific Dockerfile, and tag it at the same time; I am following the <a href=""https://docs.docker.com/engine/reference/commandline/build/#tag-an-image--t"" rel=""noreferrer"">online instructions</a> for<code>docker build</code>, but I get the following error:</p>
<blockquote>
<p>&quot;docker build&quot; requires exactly 1 argument(s)</p>
</blockquote>
<h3>My directory structure:</h3>
<pre><code>project/
    foo/
    MyDockerfile
</code></pre>
<p>This is the command I run:</p>
<p><code>docker build -f /full/path/to/MyDockerfile -t proj:myapp</code></p>
<p>I have tried various combinations of the above command, but the results are always the error message given above. Why is this happening - as I am following what the documentation says?</p>
","<p>The docs say:</p>
<pre><code>docker build [OPTIONS] PATH | URL | -
</code></pre>
<p>and</p>
<pre><code>--file , -f     Name of the Dockerfile (Default is 'PATH/Dockerfile')
</code></pre>
<p>so I'm using</p>
<pre><code>docker build -t &lt;my tag&gt; -f /path/to/my/Dockerfile&gt;
</code></pre>
<p>but I get:</p>
<pre><code>&quot;docker build&quot; requires exactly 1 argument.
See 'docker build --help'.

Usage:  docker build [OPTIONS] PATH | URL | -
</code></pre>
<p>How do the docs need correcting?</p>
"
"47580528","Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 192.168.65.1:53: no such host","<docker><docker-machine><docker-registry>","66117136","Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.171.30.4:53: no such host","<docker><build><gitlab><yaml>","<p>I am new to dockers. When I am running the docker pull sonarqube I am getting the following error.</p>

<p><strong>Error response from daemon: Get <a href=""https://registry-1.docker.io/v2/"" rel=""noreferrer"">https://registry-1.docker.io/v2/</a>: dial TCP: lookup registry-1.docker.io on 192.168.65.1:53: no such host</strong></p>

<p>Can you please let me know why I am getting the error and how can I rectify this.</p>
","<p>Docker image from a Dockerfile.
Part&quot; and potentially the &quot;test-job&quot;</p>
<p>variables:</p>
<h3>EDIT PART</h3>
<p><a href=""https://hub.docker.com/r/library/docker/"" rel=""nofollow noreferrer"">https://hub.docker.com/r/library/docker/</a>)
DOCKER_VERSION: &quot;20.10&quot;</p>
<p>(<a href=""https://docs.docker.com/engine/reference/builder/#healthcheck"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/builder/#healthcheck</a>)
DOCKER_HEALTHCHECK_TIMEOUT: &quot;60&quot;</p>
<p>REGISTRY_USER: gitlab-ci-token
REGISTRY_PASSWORD: $CI_JOB_TOKEN
REGISTRY_TEST_URL: $CI_REGISTRY
REGISTRY_RELEASE_URL: $CI_REGISTRY
CONTAINER_TEST_IMAGE: $CI_REGISTRY_IMAGE:latest
K8S_SERVER: <a href=""https://caas-cnp-sys.com.intraorange"" rel=""nofollow noreferrer"">https://caas-cnp-sys.com.intraorange</a>
K8S_GITLAB_USER: massive-migration</p>
<p>stages:</p>
<ul>
<li>build-docker</li>
<li>deploy</li>
</ul>
<p>.job_template_docker: &amp;job_definition_docker
image: dockerproxy-iva.si.francetelecom.fr/docker:$DOCKER_VERSION
services:
- name: dockerproxy-iva.si.francetelecom.fr/docker:$DOCKER_VERSION-dind
alias: docker
tags:
- rsc
- docker-privileged
- shared
before_script:
- env | grep ^DOCKER_
- env | grep ^CI_
- docker info
- '[ -z &quot;$REGISTRY_PASSWORD&quot; ] &amp;&amp; echo &quot;Registry Password is not set or empty (or protected) in Secret Variables&quot; &amp;&amp; exit 1'
- '[ -z &quot;$REGISTRY_USER&quot; ] &amp;&amp; echo &quot;Registry User is not set or empty (or protected) in Secret Variables&quot; &amp;&amp; exit 1'
- echo $REGISTRY_PASSWORD | docker login $REGISTRY_TEST_URL --username $REGISTRY_USER --password-stdin</p>
<p>docker-build-job:
&lt;&lt;: *job_definition_docker
stage: build-docker
environment:
name: prod
script:
- nslookup 10.171.30.4
- echo $CONTAINER_TEST_IMAGE
- docker build --pull --build-arg &quot;http_proxy=http://devwatt-proxy.si.fr.intraorange:8080&quot; --build-arg &quot;https_proxy=http://devwatt-proxy.si.fr.intraorange:8080&quot; -t $CONTAINER_TEST_IMAGE .
- docker push $CONTAINER_TEST_IMAGE
- docker history $CONTAINER_TEST_IMAGE
- docker images $CONTAINER_TEST_IMAGE</p>
<p>deploy-job:
tags:
- rsc
- docker
image: dockerfactory-iva.si.francetelecom.fr/kubectl-client:1.8.11
stage: deploy
environment:
name: prod
before_script:
- echo &quot;$K8S_CA_CRT&quot; &gt; ca.crt
- kubectl config set-cluster OBSIT-cluster --server=$K8S_SERVER --certificate-authority=ca.crt
- kubectl config set-credentials $K8S_GITLAB_USER --certificate-authority=ca.crt --token=$K8S_GITLAB_USER_TOKEN
- kubectl config set-context OBSIT-context --cluster=OBSIT-cluster --user=$K8S_GITLAB_USER
- kubectl config use-context OBSIT-context
script:
- kubectl apply -f deployment.yaml -n sandbox-obsit --validate=false
- kubectl delete pods -l app=massive-migration-portal -n sandbox-obsit</p>
"
"47979270","django cannot connect mysql in docker-compose","<mysql><django><docker><docker-compose><mariadb>","66092813","Is the server running on host ""localhost"" (::1) and accepting TCP/IP connections on port 5432? in Django","<python><django><postgresql><docker>","<p>I'm very new for docker, now I am trying to run django with mariadb in docker through docker-compose, but I always get this error:</p>

<p>I use <code>Docker version 17.09.1-ce, build 19e2cf6</code>, <code>docker-compose version 1.18.0, build 8dd22a9</code></p>

<blockquote>
  <p>django.db.utils.OperationalError: (2003, 'Can\'t connect to MySQL
  server on \'mariadb55\' (111 ""Connection refused"")')</p>
</blockquote>

<p>I can connect db correctly after run <code>docker-compose up db</code> in local or remote, and I even can run <code>python manage.py runserver 0.0.0.0:6001</code> correctly in <em>anaconda virtual environment</em> to connect <code>db</code> service in docker by setting parameters of <strong>settings.py</strong> file like below:</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'test',
        'USER': 'belter',
        # 'HOST': 'mariadb55',
        'HOST': '127.0.0.1',
        'PORT': '3302',
        'PASSWORD': 'belter_2017',
        'default-character-set': 'utf8',
        'OPTIONS': {
            'sql_mode': 'traditional',
        }
    }
}
</code></pre>

<p>This is my <strong>docker-compose.yml</strong> file</p>

<pre><code>version: '3'

services:
  db:
    image: mariadb:5.5
    restart: always
    environment:
      - MYSQL_HOST=localhost
      - MYSQL_PORT=3306
      - MYSQL_ROOT_HOST=%
      - MYSQL_DATABASE=test
      - MYSQL_USER=belter
      - MYSQL_PASSWORD=belter_2017
      - MYSQL_ROOT_PASSWORD=123456_abc
    volumes:
      - /home/belter/mdbdata/mdb55:/var/lib/mysql
    ports:
      - ""3302:3306""
  web:
    image: onlybelter/django_py35
    command: python3 manage.py runserver 0.0.0.0:6001
    volumes:
      - /mnt/data/www/mysite:/djcode
    ports:
      - ""6001:6001""
    depends_on:
      - db
    links:
      - db:mariadb55
</code></pre>

<p>I almost tried everything I can find, but still cannot figure it out, any help would be nice!</p>

<p>What I have tried:</p>

<p><a href=""https://stackoverflow.com/questions/42106613/docker-compose-mysql-connection-failing"">Docker compose mysql connection failing</a></p>

<p><a href=""https://stackoverflow.com/questions/31035887/linking-django-and-mysql-containers-using-docker-compose?rq=1"">Linking django and mysql containers using docker-compose</a></p>

<p><a href=""https://stackoverflow.com/questions/42811727/django-connection-to-postgres-by-docker-compose"">Django connection to postgres by docker-compose</a></p>
","<p>I'm getting this error while trying to run:</p>
<pre><code>docker-compose run app python manage.py migrate
</code></pre>
<p>with Django.</p>
<p>this is the docker-compose.yml:</p>
<pre><code>version: '3'

volumes:
  database: { }

services:
  app:
    build:
      context: .
      dockerfile: ./docker/app/dev/Dockerfile
    depends_on:
      - postgres
    volumes:
      - ./app:/app:z
    env_file:
      - ./.envs/.dev/.app
      - ./.envs/.dev/.postgres
    ports:
      - &quot;8000:8000&quot;
    command: &quot;python manage.py runserver 0.0.0.0:8000&quot;

  postgres:
    image: &quot;postgres:13.1&quot;
    volumes:
      - database:/var/lib/postgresql/data:Z
    env_file:
      - ./.envs/.dev/.postgres
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
    ports:
      - &quot;5432:5432&quot;
</code></pre>
<p>I don't know why I'm getting this error. PostgreSQL is running correctly and all the settings are appropiately configured.</p>
<p>This is the full traceback error:.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/app/manage.py&quot;, line 7, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/core/management/commands/migrate.py&quot;, line 92, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/executor.py&quot;, line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 53, in __init__
    self.build_graph()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/loader.py&quot;, line 216, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 77, in applied_migrations
    if self.has_table():
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/migrations/recorder.py&quot;, line 55, in has_table
    with self.connection.cursor() as cursor:
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 259, in cursor
    return self._cursor()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 235, in _cursor
    self.ensure_connection()
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
    self.connect()
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/utils.py&quot;, line 90, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 219, in ensure_connection
    self.connect()
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/base/base.py&quot;, line 200, in connect
    self.connection = self.get_new_connection(conn_params)
  File &quot;/usr/local/lib/python3.9/site-packages/django/utils/asyncio.py&quot;, line 26, in inner
    return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/django/db/backends/postgresql/base.py&quot;, line 187, in get_new_connection
    connection = Database.connect(**conn_params)
  File &quot;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py&quot;, line 127, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: could not connect to server: Connection refused
        Is the server running on host &quot;localhost&quot; (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
        Is the server running on host &quot;localhost&quot; (::1) and accepting
        TCP/IP connections on port 5432?
</code></pre>
<p>I've searched for multiple answers and tried multiple solutions but they are not working.</p>
<p>They have asked me to put the .postgres note so here it is:</p>
<pre><code>POSTGRES_USER=app
POSTGRES_DB=app
POSTGRES_PASSWORD=&lt;set_me&gt;
</code></pre>
<p>And also the database settings:</p>
<pre><code>DATABASES = {
    'default': {
        &quot;ENGINE&quot;: &quot;django.db.backends.postgresql&quot;,
        &quot;NAME&quot;: 'DjangoCRM',
        &quot;USER&quot;: 'postgres',
        &quot;PASSWORD&quot;: 'xxxx',
        &quot;HOST&quot;: 'localhost',
        &quot;PORT&quot;: '5432',
    }
}
</code></pre>
"
"48568172","docker.sock permission denied","<linux><docker>","65955193","How to run docker on ubuntu without the sudo","<docker><ubuntu>","<p>When I try to run simple docker commands like:</p>

<pre><code>$ docker ps -a
</code></pre>

<p>I get an error message:</p>

<blockquote>
  <p>Got permission denied ... /var/run/docker.sock: connect: permission denied</p>
</blockquote>

<p>When I check permissions with</p>

<pre><code>$ ls -al /var/run/
</code></pre>

<p>I see this line:</p>

<pre><code>srw-rw---- root docker docker.sock
</code></pre>

<p>So, I follow an advice from many forums and add local user to docker group:</p>

<pre><code>$ sudo usermod -aG docker $USER
</code></pre>

<p>But it does not help. I still get the very same error message. How can I fix it?</p>
","<p>I want to run my docker by running this command:</p>
<pre><code>docker build -t getting-started .
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=getting-started&amp;target=&amp;ulimits=null&amp;version=1: dial unix /var/run/docker.sock: connect: permission denied</p>
</blockquote>
<p>My question is: how can I run docker on ubuntu without adding &quot;sudo&quot;?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65905718","AWS EC2 Failed to execute script docker-compose","<python><docker><ubuntu><amazon-ec2><docker-compose>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I can run my app with <code>sudo docker-compose up</code>  but when I just do <code>docker-compose up</code> I get</p>
<pre><code>$ docker-compose up
Traceback (most recent call last):
  File &quot;urllib3/connectionpool.py&quot;, line 670, in urlopen
  File &quot;urllib3/connectionpool.py&quot;, line 392, in _make_request
  File &quot;http/client.py&quot;, line 1255, in request
  File &quot;http/client.py&quot;, line 1301, in _send_request
  File &quot;http/client.py&quot;, line 1250, in endheaders
  File &quot;http/client.py&quot;, line 1010, in _send_output
  File &quot;http/client.py&quot;, line 950, in send
  File &quot;docker/transport/unixconn.py&quot;, line 43, in connect
PermissionError: [Errno 13] Permission denied

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;requests/adapters.py&quot;, line 439, in send
  File &quot;urllib3/connectionpool.py&quot;, line 726, in urlopen
  File &quot;urllib3/util/retry.py&quot;, line 410, in increment
  File &quot;urllib3/packages/six.py&quot;, line 734, in reraise
  File &quot;urllib3/connectionpool.py&quot;, line 670, in urlopen
  File &quot;urllib3/connectionpool.py&quot;, line 392, in _make_request
  File &quot;http/client.py&quot;, line 1255, in request
  File &quot;http/client.py&quot;, line 1301, in _send_request
  File &quot;http/client.py&quot;, line 1250, in endheaders
  File &quot;http/client.py&quot;, line 1010, in _send_output
  File &quot;http/client.py&quot;, line 950, in send
  File &quot;docker/transport/unixconn.py&quot;, line 43, in connect
urllib3.exceptions.ProtocolError: ('Connection aborted.', PermissionError(13, 'Permission denied'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;docker/api/client.py&quot;, line 214, in _retrieve_server_version
  File &quot;docker/api/daemon.py&quot;, line 181, in version
  File &quot;docker/utils/decorators.py&quot;, line 46, in inner
  File &quot;docker/api/client.py&quot;, line 237, in _get
  File &quot;requests/sessions.py&quot;, line 543, in get
  File &quot;requests/sessions.py&quot;, line 530, in request
  File &quot;requests/sessions.py&quot;, line 643, in send
  File &quot;requests/adapters.py&quot;, line 498, in send
requests.exceptions.ConnectionError: ('Connection aborted.', PermissionError(13, 'Permission denied'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;docker-compose&quot;, line 3, in &lt;module&gt;
  File &quot;compose/cli/main.py&quot;, line 80, in main
  File &quot;compose/cli/main.py&quot;, line 189, in perform_command
  File &quot;compose/cli/command.py&quot;, line 60, in project_from_options
  File &quot;compose/cli/command.py&quot;, line 152, in get_project
  File &quot;compose/cli/docker_client.py&quot;, line 41, in get_client
  File &quot;compose/cli/docker_client.py&quot;, line 170, in docker_client
  File &quot;docker/api/client.py&quot;, line 197, in __init__
  File &quot;docker/api/client.py&quot;, line 221, in _retrieve_server_version
docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))
[48420] Failed to execute script docker-compose
</code></pre>
<p>This is my Docker file</p>
<pre><code>FROM golang:alpine AS builder

ENV GO111MODULE=on
RUN mkdir /app
ADD . /app/
WORKDIR /app
COPY ./data.go .
COPY ./handlers.go .
COPY ./main.go .
COPY ./static /static
COPY ./ssl /ssl
COPY ./views /views
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build $(ls -1 *.go)
RUN useradd -u ubuntu
USER ubuntu
EXPOSE 3000
CMD [&quot;go&quot;, &quot;run&quot;, &quot;.&quot;]
</code></pre>
"
"51011552","MongoDB on with Docker ""failed to connect to server [localhost:27017] on first connect ""","<node.js><mongodb><docker><docker-compose>","66076158","Connecting database (MongoDB) and backend (nodejs) running in docker containers","<node.js><mongodb><docker>","<p>I am using mongoDB with and NodeJS backend. The Problem is I am getting the following error</p>

<blockquote>
  <p>node:16) UnhandledPromiseRejectionWarning: MongoNetworkError: failed
  to connect to server [localhost:27017] on first connect
  [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]</p>
</blockquote>

<p>This is my docker-compose</p>

<pre><code>version: '3.4'

services:
  db:
    image: mongo:latest
    ports:
      - '27017:27017'

  rest-api-node:
    build: .
    ports:
      - '5000:5000'
    links:
      - db
    restart: on-failure
</code></pre>

<p>I have tried with <code>depends_on</code> as well , was not working.</p>

<p>On backend I am mongoose as a middleware to communicate with DB. this is the part of my <code>index.js</code></p>

<pre><code>mongoose.Promise = global.Promise
mongoose.connect('mongodb://localhost/demo')
app.listen(port, () =&gt; console.log(""live""))
</code></pre>

<p>I have tried using promise as well , no change though. Please Help me out.
Thanks in advance </p>

<p>complete error log</p>

<blockquote>
  <p>at Pool.
  (/app/node_modules/mongodb-core/lib/topologies/server.js:505:11)
  rest-api-node_1  |     at Pool.emit (events.js:180:13) rest-api-node_1
  |     at Connection.
  (/app/node_modules/mongodb-core/lib/connection/pool.js:329:12)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Connection.emit (events.js:180:13)
  rest-api-node_1  |     at Socket.
  (/app/node_modules/mongodb-core/lib/connection/connection.js:245:50)
  rest-api-node_1  |     at Object.onceWrapper (events.js:272:13)
  rest-api-node_1  |     at Socket.emit (events.js:180:13)
  rest-api-node_1  |     at emitErrorNT
  (internal/streams/destroy.js:64:8) rest-api-node_1  |     at
  process._tickCallback (internal/process/next_tick.js:178:19)
  rest-api-node_1  |   name: 'MongoNetworkError', rest-api-node_1  |<br>
  message: 'failed to connect to server [localhost:27017] on first
  connect [MongoNetworkError: connect ECONNREFUSED 127.0.0.1:27017]' }</p>
</blockquote>
","<p>First, I looked through several discussion with similar problems and it still did not work.</p>
<p>I have a mongodb docker container running, I did port forwarding with -p command
to be exact this is the command I ran:</p>
<pre><code>sudo docker run -t -d -p 27017:27017 --name mongo mongo-0000
</code></pre>
<p>docker ps shows container running</p>
<pre><code>9d9040a7bd66   mongo-0000   &quot;docker-entrypoint.s…&quot;   4 minutes ago   Up 4 minutes   0.0.0.0:27017-&gt;27017/tcp   mongo
</code></pre>
<p>as in another thread it was suggested to change mongodb bindip from 127.0.0.1 to 0.0.0.0 , which I also did (I tried both ways).</p>
<p>then I am trying to start up a backend app container with nodejs express backend app, I have had them working fine together on a VM, not on docker jet.</p>
<p>And i get following error</p>
<pre><code>    sudo docker run conduit-backend
Listening on port 3000

/ConduitReactApp/src/node_modules/mongodb/lib/server.js:261
        process.nextTick(function() { throw err; })
                                      ^
Error [MongoError]: failed to connect to server [localhost:27017] on first connect
    at Pool.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/topologies/server.js:313:35)
    at Pool.emit (node:events:378:20)
    at Connection.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/connection/pool.js:260:12)
    at Object.onceWrapper (node:events:485:26)
    at Connection.emit (node:events:378:20)
    at Socket.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/connection/connection.js:162:49)
    at Object.onceWrapper (node:events:485:26)
    at Socket.emit (node:events:378:20)
    at emitErrorNT (node:internal/streams/destroy:188:8)
    at emitErrorCloseNT (node:internal/streams/destroy:153:3)
    at processTicksAndRejections (node:internal/process/task_queues:81:21)
Emitted 'error' event on NativeConnection instance at:
    at /ConduitReactApp/src/node_modules/mongoose/lib/connection.js:288:17
    at NativeConnection.Connection.error (/ConduitReactApp/src/node_modules/mongoose/lib/connection.js:489:12)
    at /ConduitReactApp/src/node_modules/mongoose/lib/connection.js:520:15
    at /ConduitReactApp/src/node_modules/mongoose/lib/drivers/node-mongodb-native/connection.js:69:21
    at /ConduitReactApp/src/node_modules/mongodb/lib/db.js:229:14
    at Server.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb/lib/server.js:259:9)
    at Object.onceWrapper (node:events:485:26)
    at Server.emit (node:events:378:20)
    at Pool.&lt;anonymous&gt; (/ConduitReactApp/src/node_modules/mongodb-core/lib/topologies/server.js:313:21)
    at Pool.emit (node:events:378:20)
    [... lines matching original stack trace ...]
    at Socket.emit (node:events:378:20)
</code></pre>
<p>Also, inside app.js (in the backend app) for connecting to mongoDB it reads so</p>
<pre><code>if(isProduction){
  mongoose.connect(process.env.MONGODB_URI);
} else {
  mongoose.connect('mongodb://localhost/conduit');
  mongoose.set('debug', true);
}
</code></pre>
<p>What is still wrong here ?</p>
"
"51948084","docker-compose.yml content - How can i avoid ""must be a mapping not a string"" error message?","<docker><docker-compose>","66075689","ERROR: In file './docker-compose.yml', network must be a mapping, not a string","<docker><jenkins>","<p>Problem: the below content returns ""service 'image' must be a mapping not a string."" 
I tried using YAML Parser(<a href=""http://yaml-online-parser.appspot.com/"" rel=""nofollow noreferrer"">http://yaml-online-parser.appspot.com/</a>), but it returned no error. </p>

<pre><code>version: 
     ""2.0""

services:

 blog:

 image: 
  abc/defg
 environment:
  APPLICATION_SECRET:
   82xxxxxxx

  ports: -""9000:9000""
</code></pre>

<p>working version:</p>

<pre><code>version: ""2.1""
services:
 blog:
  image:  abc/defg
  environment:
   APPLICATION_SECRET:
    82xxx
  ports: 
   - ""9000:9000""
networks:
  default:
    external:
      name: nat
</code></pre>
","<p>Geeting below error upon running the docker compose file/jenkins container.</p>
<pre><code>ERROR: In file './docker-compose.yml', network must be a mapping, not a string.
</code></pre>
<p>Below is the yml file:</p>
<pre><code>version: '3'
services:
  jenkins:
    container_name: jenkins
    image: jenkins/jenkins
    ports:
      -&quot;8080&quot;:&quot;8080&quot;
    volume:
      -&quot;$PWD/jenkins_home:/var/jenkins_home&quot;
  networks:
      -net
networks:
   net
</code></pre>
<p>Docker compose verion:</p>
<pre><code>[jenkins@localhost ~]$ docker compose -v
Docker version 20.10.3, build 48d30b5
</code></pre>
"
"53422407","Different CUDA versions shown by nvcc and NVIDIA-smi","<cuda>","66062569","What decides the CUDA version inside the docker container?","<docker><tensorflow><cuda><azure-aks>","<p>I am very confused by the different CUDA versions shown by running <code>which nvcc</code> and <code>nvidia-smi</code>.</p>

<p>I have both cuda9.2 and cuda10 installed on my ubuntu 16.04. Now I set the PATH to point to cuda9.2. So when I run :</p>

<pre><code> $ which nvcc
 /usr/local/cuda-9.2/bin/nvcc
</code></pre>

<p>However, when I run </p>

<pre><code>$ nvidia-smi
Wed Nov 21 19:41:32 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   53C    P0    26W /  N/A |    379MiB /  6078MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1324      G   /usr/lib/xorg/Xorg                           225MiB |
|    0      2844      G   compiz                                       146MiB |
|    0     15550      G   /usr/lib/firefox/firefox                       1MiB |
|    0     19992      G   /usr/lib/firefox/firefox                       1MiB |
|    0     23605      G   /usr/lib/firefox/firefox                       1MiB |
</code></pre>

<p>So am I using cuda9.2 as <code>which nvcc</code> suggests, or am I using cuda10 as <code>nvidia-smi</code> suggests? </p>

<p>I <a href=""https://stackoverflow.com/questions/34319877/nvcc-has-different-version-than-cuda"">saw this answer</a> but it does not provide direct answer to the confusion, it just asks us to reinstall the cudatoolkit, which I already did.</p>
","<p>I'm trying to use the GPU in AKS based on this <a href=""https://docs.microsoft.com/en-us/azure/aks/gpu-cluster"" rel=""nofollow noreferrer"">tutorial</a>. Since in my program I'm using <code>tensorflow-1.15</code> and the supported CUDA version should be 10.0 according to this <a href=""https://www.tensorflow.org/install/source#gpu"" rel=""nofollow noreferrer"">table</a>. I used the <code>nvidia/cuda:10.0-base-ubuntu18.04</code> as the base image. But after the container launched, I run <code>nvida-smi</code> inside it and I get the following results:</p>
<pre><code>root@test-retrainer-swl2d:/app# nvidia-smi
Fri Feb  5 12:45:28 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000001:00:00.0 Off |                    0 |
| N/A   48C    P0    58W / 149W |     61MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
root@test-retrainer-swl2d:/app# cat /usr/local/cuda/version.txt
CUDA Version 10.0.130
</code></pre>
<p>The CUDA version showed in the table is <code>11.0</code> but the CUDA version in the version.txt is <code>10.0.130</code> instead. I'm wondering what decides the CUDA version inside the container?</p>
"
"53610385","Docker - Postgres and pgAdmin 4 : Connection refused","<postgresql><docker>","66006126","Docker - cannot connect to posgres from pgadmin","<postgresql><docker><docker-compose><pgadmin-4>","<p>Newbie with docker, I am trying to connect throught localhost my pgAdmin container to the postgres one.</p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
0b00555238ba        dpage/pgadmin4      ""/entrypoint.sh""         43 minutes ago      Up 43 minutes       0.0.0.0:80-&gt;80/tcp, 443/tcp   pedantic_turing
e79fb6440a95        postgres            ""docker-entrypoint.s…""   About an hour ago   Up About an hour    0.0.0.0:5432-&gt;5432/tcp        pg-docker
</code></pre>

<p>I succeed connecting with psql command.</p>

<pre><code>psql -h localhost -U postgres -d postgres
</code></pre>

<p>But when I create the server on pgAdmin with the same parameters as psql I got the following error.</p>

<blockquote>
  <p>Unable to connect to server:</p>
  
  <p>could not connect to server: Connection refused Is the server running
  on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on
  port 5432? could not connect to server: Address not available Is the
  server running on host ""localhost"" (::1) and accepting TCP/IP
  connections on port 5432?</p>
</blockquote>

<p>I succeed to connect throught the IPAddress given by <code>docker inspect</code> on the container.</p>

<p>By the way, I checked postgresql.conf and assert that <code>listen_addresses = '*'</code> and also that pg_hba.conf contain <code>host all all all md5</code>.</p>

<p>But I don't get it, why shouldn't I be able to use the localhost address ? And why does docker even give me an address that is not local ?</p>
","<p>I ran a docker-compose to run Postgres and pgadmin, with the following docker-compose file.
I can login to pgadmin but pgadmin cannot connect to the Postgres. However, Postgres is running smoothly and accepting requests.</p>
<p>docker-compose.yaml</p>
<pre><code>version: &quot;3&quot;
services:
  postgres:
    image: postgres
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres-data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - 5050:80
    environment:
      - PGADMIN_DEFAULT_EMAIL=user@hotmail.com
      - PGADMIN_DEFAULT_PASSWORD=password

volumes:
  postgres-data:
    driver: local
</code></pre>
<p>This is the error while connecting the Postgres from pgadmin view page.</p>
<blockquote>
<p>Unable to connect to server:</p>
<p>could not connect to server: Connection refused Is the server running
on host &quot;localhost&quot; (127.0.0.1) and accepting TCP/IP connections on
port 5432? could not connect to server: Address not available Is the
server running on host &quot;localhost&quot; (::1) and accepting TCP/IP
connections on port 5432?</p>
</blockquote>
<p>or</p>
<p><a href=""https://i.stack.imgur.com/lBcaW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lBcaW.png"" alt=""enter image description here"" /></a></p>
<p>This is the docker-compose log.</p>
<pre><code>db_1            | PostgreSQL init process complete; ready for start up.
db_1            | 
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  starting PostgreSQL 12.5 (Debian 12.5-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  listening on IPv4 address &quot;0.0.0.0&quot;, port 5432
db_1            | 2021-02-02 04:57:59.825 UTC [1] LOG:  listening on IPv6 address &quot;::&quot;, port 5432
db_1            | 2021-02-02 04:57:59.842 UTC [1] LOG:  listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
db_1            | 2021-02-02 04:57:59.867 UTC [76] LOG:  database system was shut down at 2021-02-02 04:57:59 UTC
db_1            | 2021-02-02 04:57:59.877 UTC [1] LOG:  database system is ready to accept connections
pgadmin_1       | NOTE: Configuring authentication for SERVER mode.
pgadmin_1       | 
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Starting gunicorn 19.9.0
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Listening at: http://[::]:5050 (1)
pgadmin_1       | [2021-02-02 04:58:10 +0000] [1] [INFO] Using worker: threads
pgadmin_1       | /usr/local/lib/python3.9/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used
pgadmin_1       |   return io.open(fd, *args, **kwargs)
pgadmin_1       | [2021-02-02 04:58:10 +0000] [87] [INFO] Booting worker with pid: 87
pgadmin_1       | ::ffff:192.168.80.1 - - [02/Feb/2021:05:34:52 +0000] &quot;GET /browser/ HTTP/1.1&quot; 302 257 &quot;http://localhost:5050/login?next=%2Fbrowser%2F&quot; &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&quot;
pgadmin_1       | ::ffff:192.168.80.1 - - [02/Feb/2021:05:34:52 +0000] &quot;GET /login?next=%2Fbrowser%2F HTTP/1.1&quot; 200 1707 &quot;http://localhost:5050/login?next=%2Fbrowser%2F&quot; &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36&quot;
pgadmin_1       | 2021-02-02 05:34:52,568: ERROR    flask.app:  404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.
pgadmin_1       | Traceback (most recent call last):
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1813, in full_dispatch_request
pgadmin_1       |     rv = self.dispatch_request()
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1791, in dispatch_request
pgadmin_1       |     self.raise_routing_exception(req)
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/app.py&quot;, line 1774, in raise_routing_exception
pgadmin_1       |     raise request.routing_exception
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/flask/ctx.py&quot;, line 336, in match_request
pgadmin_1       |     self.url_adapter.match(return_rule=True)
pgadmin_1       |   File &quot;/usr/local/lib/python3.9/site-packages/werkzeug/routing.py&quot;, line 1945, in match
pgadmin_1       |     raise NotFound()
pgadmin_1       | werkzeug.exceptions.NotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.
</code></pre>
<p>I don't know what could have been gone wrong.</p>
"
"53681522","Share variable in multi-stage Dockerfile: ARG before FROM not substituted","<docker><dockerfile><docker-multi-stage-build>","65940714","Not Able to use ARGUMENT for ADD command in Dockerfile","<java><docker><jar><dockerfile><build-automation>","<p>I'm writing a multi-stage Dockerfile for the <a href=""https://www.mcs.anl.gov/research/projects/darshan/"" rel=""noreferrer"">darshan utils</a>:</p>

<pre><code>ARG DARSHAN_VER=3.1.6

FROM fedora:29 as build
RUN dnf install -y \
        gcc \
        make \
        bzip2 bzip2-devel zlib zlib-devel
RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz"" \
    &amp;&amp; tar ...


FROM fedora:29
COPY --from=build ""/usr/local/darshan-${DARSHAN_VER}"" ""/usr/local/darshan-${DARSHAN_VER}""
...
</code></pre>

<p>I build it with <code>docker build -t darshan-util:3.6.1 .</code> and the error I get is:</p>

<pre><code>Step 5/10 : RUN curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...

 ---&gt; Running in 9943cce1669c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
...
curl: (78) RETR response: 550
The command '/bin/sh -c curl -O ""ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz""     &amp;&amp; tar ...' returned a non-zero code: 78
</code></pre>

<p>I'd like to reuse the same ARG in both stages, so that I can define a default build variable just once.
If I duplicate ARG in both stages, just below the two FROMs, it builds correctly.</p>

<p>What is the correct way to define a ""global"" multi-stage ARG variable with a default?</p>
","<p>i have a Dockerfile in which i need to provide build argument before running Dockerfile.</p>
<pre><code>ARG JAR_VERSION=${JAR_VERSION}
FROM openjdk:8-jre-slim
ADD App1-${JAR_VERSION}-SNAPSHOT.jar /opt/myapp/examples/App1-${JAR_VERSION}-SNAPSHOT.jar
</code></pre>
<p><strong>Not able to figure out why ADD command is not working as required, i have tried multiple ways, but no luck</strong></p>
<p>i am supplying build arg on while building image as
docker build -t myapp:1.0.20 --build-arg JAR_VERSION=1.0.20 .</p>
<p>but in logs it shows same error below
Step 16/20 : ADD App1-${JAR_VERSION}-SNAPSHOT.jar /opt/myapp/examples/App1-${JAR_VERSION}-SNAPSHOT.jar
20:55:12  ADD failed: stat /var/lib/docker/tmp/docker-builder164635490/App1-SNAPSHOT.jar: no such file or directory</p>
"
"55123637","Activate conda environment in docker","<python><docker><anaconda><conda>","66020955","How do you create a Docker container that includes a conda environment and all its dependencies?","<windows><docker><dockerfile><conda><environment>","<p>I need to activate environment in docker and run a command in this environment.
I create the environment, but then I try to activate this environment and run the command in this way:</p>

<pre><code>CMD [ ""source activate mro_env &amp;&amp; ipython kernel install --user --name=mro_env"" ]
</code></pre>

<p>but when I ran docker I get an error:</p>

<pre><code>[FATAL tini (8)] exec source activate mro_env &amp;&amp; ipython kernel install 
--user --name=mro_env failed: No such file or directory
</code></pre>

<p>This is the whole Dockerfile:</p>

<pre><code>FROM continuumio/miniconda3

ADD /src/mro_env.yml /src/mro_env.yml
RUN conda env create -f /src/mro_env.yml

# Pull the environment name out of the mro_env.yml
RUN echo ""source activate $(head -1 /src/mro_env.yml | cut -d' ' -f2)"" &gt; ~/.bashrc
ENV PATH /opt/conda/envs/$(head -1 /src/mro_env.yml | cut -d' ' -f2)/bin:$PATH

CMD [ ""source activate mro_env &amp;&amp; ipython kernel install --user --name=mro_env"" ]
</code></pre>
","<p>I'm attempting to create a Docker container for a data science project. The project uses Anaconda and conda environments and I'm working in Windows.</p>
<p>I'm completely new to Docker and have seen tutorials for creating simple containers, but nothing for conda environments that fully explains how the Dockerfile should be constructed. It seems like conda can be tricky due to the need to have conda on the path in order to activate the environment.</p>
<p>This is what I have attempted so far, but Docker doesn't even seem to recognize it as a legitimate Dockerfile:</p>
<pre><code>FROM continuumio/miniconda3

WORKDIR /app

COPY environment.yml ./
COPY . . 

RUN conda env create -f environment.yml


CMD [&quot;conda&quot;, &quot;activate&quot;, &quot;project-environment&quot;]
</code></pre>
<p>What is needed in the Dockerfile to ensure that the container will have all the environment attributes and dependencies and that it will activate the environment when it is run?</p>
"
"55756372","When using BuildKit with Docker, how do I see the output of RUN commands?","<docker><docker-buildkit>","65936425","Dockerfile: RUN ls -l","<docker><dockerfile>","<p>When building Docker images with <code>DOCKER_BUILDKIT=1</code>, there is a very cool progress indicator but no command output. How do I see the command output to debug my build?</p>
","<p>I have this simple Dockerfile</p>
<pre><code>FROM adoptopenjdk/openjdk8
RUN ls -l
RUN pwd
</code></pre>
<p>When I build it <code>docker build . --no-cache</code> I'm expecting to see the result of the <code>ls -l</code> and <code>pwd</code> but nothing is shown.</p>
<pre><code> =&gt; CACHED [1/3] FROM docker.io/adoptopenjdk/openjdk8@sha256:e8de192088d82fa93e1...
 =&gt; [2/3] RUN ls -l
 =&gt; [3/3] RUN pwd
 =&gt; exporting to image
 =&gt; =&gt; exporting layers
 =&gt; =&gt; writing image sha256:ea9d3d5490cea3d73321654a6... 
</code></pre>
<p>I'm using Big Sur with Docker 3.1.0 (just freshly reinstalled). My colleagues are able to see the result of the command.</p>
"
"55951014","Docker in MacOs is very slow","<php><symfony><docker><docker-compose>","66002476","Docker with WordPress Page Load is Painfully slow","<wordpress><docker>","<p>I have this docker-compose.yml:</p>

<pre><code>version: ""3.1""
services:

    memcached:
      image: memcached:alpine
      container_name: universal-memcached2

    redis:
      image: redis:alpine
      container_name: universal-redis2

    mariadb:
      image: mariadb:10.4
      container_name: universal-mariadb2
      working_dir: /application
      volumes:
        - .:/application
        - ""../data/db:/var/lib/mysql"" # skasowac
      environment:
        - MYSQL_ROOT_PASSWORD=Haslo
        - MYSQL_DATABASE=sample
        - MYSQL_USER=user
        - MYSQL_PASSWORD=Haslo
      ports:
        - ""8083:3306""


    webserver:
      image: nginx:alpine
      container_name: universal-webserver2
      working_dir: /application
      volumes:
          - .:/application
          - ./phpdocker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      ports:
       - ""8080:80""

    php-fpm:
      build: phpdocker/php-fpm
      container_name: universal-php-fpm2
      working_dir: /application
      volumes:
        - .:/application
        - ./phpdocker/php-fpm/php-ini-overrides.ini:/etc/php/7.3/fpm/conf.d/99-overrides.ini

    volumes:
        generated:
        var:
        static:

    phpmyadmin:
      image: phpmyadmin/phpmyadmin
      links:
        - mariadb
      environment:
        PMA_HOST: mariadb
        PMA_PORT: 3306
      ports:
        - '8081:80'
</code></pre>

<p>When I run my newest project in symfony 4 on it, it works very slowly... :(</p>

<p>I have new MacOs and Docker Desktop.
I'm currently learning the Symfony and Laravel framework, but this is very slow for Docker. It is not even working on it.</p>

<p>How can I repair it?</p>
","<p>I am using Docker to run acceptance tests. The website in production is fast and loads almost immediately. On my docker container, the a lot of time is just spent waiting:</p>
<p>The docker container Waiting (TTFB) time is about 5 seconds:</p>
<p><a href=""https://i.stack.imgur.com/KhNlr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KhNlr.jpg"" alt=""Docker Container Network"" /></a></p>
<p>The production website's (TFFB) time is 200 ms.</p>
<p><a href=""https://i.stack.imgur.com/ESMln.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ESMln.jpg"" alt=""Production Website Network"" /></a></p>
<p>I have tried to delegate the volumes in my DockerComposer file:</p>
<pre><code>    volumes:
      - ./config/php.conf.ini:/usr/local/etc/php/php.ini:delegated
      # Docker WordPress files will be placed at the root.
      - .:/var/www/html:delegated
</code></pre>
<p>And the speed is still awful. An acceptance test that should take 3 seconds to run is taking about 20 seconds with Docker.</p>
<p>Why is there such a long waiting time with Docker? I love it but if this continues I am better off running MAMP. I LOVE Docker, if there is a way to improve this massive wait time, please pitch in.</p>
"
"56518032","Windows 10 and Docker container logs / Docker Logging driver","<docker><windows-10>","66059094","Docker: ""Cannot access '/var/lib/docker/containers': No such file or directory"" error","<windows><docker><powershell><dockerfile>","<p>I'm using Windows 10 with native docker installation. </p>

<p>I'm looking for the location where docker save the containers logs.<br>
In Linux, the Docker containers log files are in this location:</p>

<pre><code>/var/lib/docker/containers/container-id/container-id-json.log
</code></pre>

<p>But where can I find it in windows 10 ?</p>
","<p>I am trying to copy a file from container to local drive, but I encountered  several errors during this procedure. Later, I realized that the problem is not related to the commands I use, it is related to permission. Because I cannot list the containers directory that I use when trying to copy.</p>
<p>I try to list the <code>/var/lib/docker/containers</code> using PowerShell on Windows:</p>
<pre><code>PS C:\Windows\system32&gt; docker exec 144625ac6917 ls /var/lib/docker/containers
</code></pre>
<p>Gives this error: <strong>ls: cannot access '/var/lib/docker/containers': No such file or directory</strong></p>
<p>How can I fix this problem?</p>
"
"57414286","What does ""locked at"" <tag> mean when running ""composer update <package>""?","<php><composer-php>","66062991","Your requirements could not be resolved to an installable set of packages. PHP 7.4 docker","<php><docker><composer-php>","<p>I am trying to perform a <code>composer update &lt;package&gt;</code> but getting the following error:</p>
<blockquote>
<p>The requested package &lt;package&gt; (locked at &lt;tag&gt;, required as
&lt;version&gt;) is satisfiable by &lt;package&gt;[&lt;tag&gt;] but these conflict
with your requirements or minimum-stability.</p>
</blockquote>
<p>Meanwhile, the tag <code>&lt;tag&gt;</code> exists as a string only in my composer.lock file, which I thought was only modified by <code>composer update</code>, not read back.</p>
<p>I tried running <code>composer why-not &lt;package&gt;</code>, but its output didn't really explain the issue:</p>
<blockquote>
<p>&lt;program&gt; &lt;other-version&gt; requires &lt;package&gt; (&lt;version&gt;)</p>
</blockquote>
<p>What does 'locked at' mean in this context and how do I solve the issue?</p>
","<p>I am trying to install the following package in my laravel app <code>composer require 64robots/nova-fields</code>, I am getting the error:</p>
<pre><code>Problem 1
    - phpunit/phpunit is locked to version 9.5.2 and an update of this package was not requested.
    - phpunit/phpunit 9.5.2 requires ext-dom * -&gt; it is missing from your system. Install or enable PHP's dom extension.
  Problem 2
    - tijsverkoyen/css-to-inline-styles 2.2.3 requires ext-dom * -&gt; it is missing from your system. Install or enable PHP's dom extension.
    - laravel/framework v8.26.1 requires tijsverkoyen/css-to-inline-styles ^2.2.2 -&gt; satisfiable by tijsverkoyen/css-to-inline-styles[2.2.3].
    - laravel/framework is locked to version v8.26.1 and an update of this package was not requested.

To enable extensions, verify that they are enabled in your .ini files:
    - /etc/php/7.4/cli/php.ini
    - /etc/php/7.4/cli/conf.d/10-opcache.ini
    - /etc/php/7.4/cli/conf.d/10-pdo.ini
    - /etc/php/7.4/cli/conf.d/20-calendar.ini
    - /etc/php/7.4/cli/conf.d/20-ctype.ini
    - /etc/php/7.4/cli/conf.d/20-exif.ini
    - /etc/php/7.4/cli/conf.d/20-ffi.ini
    - /etc/php/7.4/cli/conf.d/20-fileinfo.ini
    - /etc/php/7.4/cli/conf.d/20-ftp.ini
    - /etc/php/7.4/cli/conf.d/20-gettext.ini
    - /etc/php/7.4/cli/conf.d/20-iconv.ini
    - /etc/php/7.4/cli/conf.d/20-json.ini
    - /etc/php/7.4/cli/conf.d/20-mbstring.ini
    - /etc/php/7.4/cli/conf.d/20-phar.ini
    - /etc/php/7.4/cli/conf.d/20-posix.ini
    - /etc/php/7.4/cli/conf.d/20-readline.ini
    - /etc/php/7.4/cli/conf.d/20-shmop.ini
    - /etc/php/7.4/cli/conf.d/20-sockets.ini
    - /etc/php/7.4/cli/conf.d/20-sysvmsg.ini
    - /etc/php/7.4/cli/conf.d/20-sysvsem.ini
    - /etc/php/7.4/cli/conf.d/20-sysvshm.ini
    - /etc/php/7.4/cli/conf.d/20-tokenizer.ini
</code></pre>
<p>this is my Dockerfile:</p>
<pre><code>FROM php:7.4
WORKDIR /app
RUN ls -la /app
RUN apt-get update &amp;&amp; apt-get install -y git libpq-dev
RUN docker-php-ext-install pdo pdo_pgsql php-xml
</code></pre>
<p>here is my composer file:</p>
<pre><code> &quot;require&quot;: {
        &quot;php&quot;: &quot;^7.3|^8.0&quot;,
        &quot;64robots/nova-fields&quot;: &quot;^0.18.0&quot;,
        &quot;fideloper/proxy&quot;: &quot;^4.4&quot;,
        &quot;fruitcake/laravel-cors&quot;: &quot;^2.0&quot;,
        &quot;guzzlehttp/guzzle&quot;: &quot;^7.0.1&quot;,
        &quot;laravel/framework&quot;: &quot;^8.12&quot;,
        &quot;laravel/nova&quot;: &quot;*&quot;,
        &quot;laravel/tinker&quot;: &quot;^2.5&quot;
    },
    &quot;require-dev&quot;: {
        &quot;facade/ignition&quot;: &quot;^2.5&quot;,
        &quot;fakerphp/faker&quot;: &quot;^1.9.1&quot;,
        &quot;laravel/sail&quot;: &quot;^1.0.1&quot;,
        &quot;mockery/mockery&quot;: &quot;^1.4.2&quot;,
        &quot;nunomaduro/collision&quot;: &quot;^5.0&quot;,
        &quot;phpunit/phpunit&quot;: &quot;^9.3.3&quot;
    },
    &quot;config&quot;: {
        &quot;optimize-autoloader&quot;: true,
        &quot;preferred-install&quot;: &quot;dist&quot;,
        &quot;sort-packages&quot;: true
    },
</code></pre>
<p>Im new to PHP stack - so some details in the solution will be appreceated.</p>
<p>Thanks</p>
"
"62700271","Running karate tests on chrome with chromedriver inside docker","<automated-tests><karate>","66005226","Connexion Refuse after second execution driver/ui karate test in docker ptrthomas/karate-chrome","<docker><connection><karate>","<p>I would like to run my karate tests using the chromedriver and chrome present in a docker container. How can this be achieved ? I could only find the documentation on the native version of chrome <a href=""https://github.com/intuit/karate/wiki/Docker#examples"" rel=""nofollow noreferrer"">here</a></p>
","<p>I try to use ptrthomas/karate-chrome image docker.
I wish execute multiple driver/ui karate tests in the docker image.
Currently use, your image docker ptrthomas/karate-chrome, but only 1 scenario driver/ui is successful execute.</p>
<p>The scenario are basic:</p>
<pre><code>Scenario: 
Given driver 'https://github.com/login'
</code></pre>
<p>After execution, I have this error:</p>
<pre><code>[ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection leased: [id: 3][route: {}-&gt;http://localhost:9222][total available: 0; route allocated: 1 of 5; total allocated: 1 of 10]
 karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.execchain.MainClientExec - Opening connection {}-&gt;http://localhost:9222 
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.conn.DefaultHttpClientConnectionOperator - Connecting to localhost/127.0.0.1:9222 
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.conn.DefaultHttpClientConnectionOperator - Connection established 127.0.0.1:54604&lt;-&gt;127.0.0.1:9222
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.conn.DefaultManagedHttpClientConnection - http-outgoing-3: set socket timeout to 30000 
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG     org.apache.http.impl.execchain.MainClientExec - Executing request GET /json HTTP/1.1
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.execchain.MainClientExec - Target auth state: UNCHALLENGED 
karate-chrome-runner_docker | 16:07:50.206 [ForkJoinPool-2-worker-1] DEBUG org.apache.http.impl.execchain.MainClientExec - Proxy auth state: UNCHALLENGED
</code></pre>
<p>And I try to use 2 dockers (one to execute tests -maven:3.6-jdk-11 and one to target UI chrome -justinribeiro/chrome-headless:stable )
But already issues: connection failed
So, I try many configuration to drivers:</p>
<pre><code>karate.configure( 
    'driver', {
      type: 'chrome',
      executable: 'chrome',
     // port: 9222, //default value
     // host: 'localhost', //default value
      showDriverLog: true,
      showProcessLog: true,
      start: false,
      headless: true
    }
  )
</code></pre>
<p>Docker_compose :</p>
<pre><code>      version: '3.7'

networks:
  karate: {}

services:
  karate-chrome-runner:
    image: ptrthomas/karate-chrome
    container_name: &quot;karate-chrome-runner_docker&quot;
    volumes:
      - &quot;~/.m2:/var/maven/.m2&quot;
      - &quot;./:/usr/src/mymaven&quot;
    environment:
      - MAVEN_CONFIG=/var/maven/.m2
    working_dir: &quot;/usr/src/mymaven&quot;
    entrypoint: &quot;./entrypoint.sh&quot;
    networks:
      - karate
</code></pre>
<p>Please, would you help me to understand the correct operation?
Have you an example to project docker ?</p>
<p>Thanks for your help</p>
"
"63602750","How to create and add user with password in alpine Dockerfile?","<docker><alpine>","65985089","/bin/sh: useradd: not found when running RUN useradd app with Alpine","<node.js><docker><dockerfile><alpine>","<p>The following <code>Dockerfile</code> works fine for Ubuntu:</p>
<pre><code>FROM ubuntu:20.04
SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]
ARG user=hakond
ARG home=/home/$user
RUN useradd --create-home -s /bin/bash $user \
        &amp;&amp; echo $user:ubuntu | chpasswd \
        &amp;&amp; adduser $user sudo

WORKDIR $home
USER $user
COPY --chown=$user entrypoint.sh .
RUN chmod +x entrypoint.sh
ENTRYPOINT [&quot;./entrypoint.sh&quot;]
</code></pre>
<p>where <code>entrypoint.sh</code> is</p>
<pre><code>#! /bin/bash
exec bash
</code></pre>
<p>How can I do the same in Alpine? I tried:</p>
<pre><code>FROM alpine:3.12
SHELL [&quot;/bin/sh&quot;, &quot;-c&quot;]
RUN apk add --no-cache bash
ARG user=hakond
ARG home=/home/$user
RUN addgroup -S docker
RUN adduser \
    --disabled-password \
    --gecos &quot;&quot; \
    --home $home \
    --ingroup docker \
    $user
WORKDIR $home
USER $user
COPY chown=$user entrypoint.sh .
RUN chmod +x entrypoint.sh
ENTRYPOINT [&quot;./entrypoint.sh&quot;]
</code></pre>
<p>But this fails to build:</p>
<pre><code>$ docker build -t alpine-user .
Sending build context to Docker daemon   5.12kB
Step 1/12 : FROM alpine:3.12
 ---&gt; a24bb4013296
Step 2/12 : SHELL [&quot;/bin/sh&quot;, &quot;-c&quot;]
 ---&gt; Using cache
 ---&gt; ce9a303c96c8
Step 3/12 : RUN apk add --no-cache bash
 ---&gt; Running in e451a2481846
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
(1/4) Installing ncurses-terminfo-base (6.2_p20200523-r0)
(2/4) Installing ncurses-libs (6.2_p20200523-r0)
(3/4) Installing readline (8.0.4-r0)
(4/4) Installing bash (5.0.17-r0)
Executing bash-5.0.17-r0.post-install
Executing busybox-1.31.1-r16.trigger
OK: 8 MiB in 18 packages
Removing intermediate container e451a2481846
 ---&gt; 7b5f7f87bdf6
Step 4/12 : ARG user=hakond
 ---&gt; Running in 846b4b12856e
Removing intermediate container 846b4b12856e
 ---&gt; a0453cb6706e
Step 5/12 : ARG home=/home/$user
 ---&gt; Running in 06550ad3f550
Removing intermediate container 06550ad3f550
 ---&gt; 994d71fb0281
Step 6/12 : RUN addgroup -S docker
 ---&gt; Running in 70aaec6f40e0
Removing intermediate container 70aaec6f40e0
 ---&gt; 5188ed7b234c
Step 7/12 : RUN adduser     --disabled-password     --gecos &quot;&quot;     --home $home     --ingroup docker     $user
 ---&gt; Running in ff36a7f7e99b
Removing intermediate container ff36a7f7e99b
 ---&gt; 97f481916feb
Step 8/12 : WORKDIR $home
 ---&gt; Running in 8d7f0411d6e3
Removing intermediate container 8d7f0411d6e3
 ---&gt; 5de66f4b5d4e
Step 9/12 : USER $user
 ---&gt; Running in ac4abac7c3a8
Removing intermediate container ac4abac7c3a8
 ---&gt; dffd2185df1f
Step 10/12 : COPY chown=$user entrypoint.sh .
COPY failed: stat /var/snap/docker/common/var-lib-docker/tmp/docker-builder615220199/chown=hakond: no such file or directory
</code></pre>
","<p>Hey guys can you check what is wrong with my Dockerfile? I am getting</p>
<pre><code>/bin/sh: useradd: not found
The command '/bin/sh -c useradd app' returned a non-zero code: 127

</code></pre>
<pre><code># Dockerfile

FROM node:12.13.0-alpine
RUN mkdir -p /opt/app
WORKDIR /opt/app
RUN useradd app
COPY addressbook/ .
RUN npm install
RUN chown -R app:app /opt/app
USER app
EXPOSE 3000
CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;pm2&quot; ]
</code></pre>
"
"65825025","Docker and Kafka Connection Issue Kafka-Python","<docker><apache-kafka>","65902677","BrokerConnect Issue","<python><docker><apache-kafka>","<p>I have a kafka cluster running in docker which I create with the .yml file below:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: &quot;confluentinc/cp-zookeeper:5.4.0&quot;
    hostname: zookeeper
    ports:
      - '32181:32181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafka:
    image: &quot;confluentinc/cp-enterprise-kafka:5.4.0&quot;
    hostname: kafka
    ports:
      - '9092:9092'
      - '29092:29092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  schema-registry:
    image: &quot;confluentinc/cp-schema-registry:latest&quot;
    hostname: schema-registry
    depends_on:
      - zookeeper
      - kafka
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafdrop:
    image: &quot;obsidiandynamics/kafdrop&quot;
    ports:
      - '9000:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: &quot;-Xms32M -Xmx64M&quot;
      SERVER_SERVLET_CONTEXTPATH: &quot;/&quot;
    depends_on:
      - kafka

</code></pre>
<p>I then have a simple producer not in Docker just on my machine that I am trying to produce messages to the cluster. The producer file is:</p>
<pre><code>from kafka import KafkaProducer
import time


topicName = 'myTestTopic'

producer = KafkaProducer(bootstrap_servers=['localhost:9092', 'kafka:29092'])


while True:
    message = producer.send(topicName, value=b'Hello World')

    metadata = message.get()
    print(metadata.topic)
    print(metadata.partition)
    time.sleep(5)
</code></pre>
<p>When I run the file and the cluster is running on docker all I get is this error</p>
<pre><code>    raise Errors.NoBrokersAvailable()
kafka.errors.NoBrokersAvailable: NoBrokersAvailable
</code></pre>
<p>Any help to resolve this issue would be great</p>
","<p>I have a kafka producer written in python that I have added to docker-compose.yml</p>
<p>Producer:</p>
<pre><code>import os, csv, avro.schema, json
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter
from kafka import KafkaProducer
from collections import namedtuple

output_loc = '{}/avro.avro'.format(os.path.dirname(__file__))
CSV = '{}/oscar_age_male.csv'.format(os.path.dirname(__file__))
fields = (&quot;Index&quot;,&quot;Year&quot;, &quot;Age&quot;, &quot;Name&quot;, &quot;Movie&quot;)
csv_record = namedtuple('csv_record', fields)

p = KafkaProducer(bootstrap_servers = ['localhost:9092', 'kafka:9092'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))

def read_csv(path):
    with open(path, 'rU') as data:
        data.readline()
        reader = csv.reader(data, delimiter=&quot;,&quot;)
        for row in map(csv_record._make, reader):
            yield row

def parse_schema(path='{}/schema.avsc'.format(os.path.dirname(__file__))):
    with open(path, 'r') as data:
        return avro.schema.parse(data.read())

def serilialise_records_and_send(records, outpath=output_loc):
    schema = parse_schema()
    with open(outpath, 'w') as out:
        writer = DataFileWriter(out, DatumWriter(), schema)
        for record in records:
            record = dict((f, getattr(record, f)) for f in record._fields)
            writer.append(record)
            msg = p.send(topic='test', value=record)
            metadt = msg.get()
            print(metadt.topic)
            print(metadt.partition)

serilialise_records_and_send(read_csv(CSV))
</code></pre>
<p>when I run the docker-compose my producer image fails due to no brokers available.</p>
<p>can anyone enlighten me as to why the brokers aren't available?</p>
<p>when I run the producer locally from IDE I can connect so unsure what's missing</p>
<p>docker-compose.yml</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: &quot;confluentinc/cp-zookeeper:5.4.0&quot;
    hostname: zookeeper
    ports:
      - '32181:32181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  kafka:
    image: &quot;confluentinc/cp-enterprise-kafka:5.4.0&quot;
    hostname: kafka
    ports:
      - '9092:9092'
      - '29092:29092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;  
      
  schema-registry:
    image: &quot;confluentinc/cp-schema-registry:latest&quot;
    hostname: schema-registry
    depends_on:
      - zookeeper
      - kafka
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*' 
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

      
  kafdrop:
    image: &quot;obsidiandynamics/kafdrop&quot;
    ports:
      - '9000:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: &quot;-Xms32M -Xmx64M&quot;
      SERVER_SERVLET_CONTEXTPATH: &quot;/&quot;
    depends_on:
      - kafka
  
  producer:
    image: &quot;producer&quot;
    ports: 
      - '5000:5000'
    environment: 
      KAFKA_BROKERCONNECT: kafka:29092
    depends_on: 
      - kafka
</code></pre>
"
"19104847","How to generate a Dockerfile from an image?","<image><repository><docker>","64741013","Is it possible to create Dockerfile from the container/image?","<docker><dockerfile>","<p>Is it possible to generate a Dockerfile from an image?  I want to know for two reasons:</p>

<ol>
<li><p>I can download images from the repository but would like to see the recipe that generated them.</p></li>
<li><p>I like the idea of saving snapshots, but once I am done it would be nice to have a structured format to review what was done.</p></li>
</ol>
","<p>I have a container up and running. Is it a way to generate Dockerfile from the running container or from its image? Please advice.</p>
"
"19335444","How do I assign a port mapping to an existing Docker container?","<docker><port><lxc><linux-containers>","64441336","Docker: Expose running container to host","<mysql><docker>","<p>I'm not sure if I've misunderstood something here, but it seems like it's only possible to set port mappings by creating a new container from an image. Is there a way to assign a port mapping to an existing Docker container?</p>
","<p>I have a running MySQL container that I have import a lot of data in it (took me a few hours).</p>
<p>Now I want to query the data from this container using a GUI application (DBeaver) on my host. Is there a way that I can expose the network of the running container to host so I can connect to it?</p>
<p>I've been looking around and the most promising option so far is to run <code>docker run</code> a new container with <code>-p</code> tag <code>3306:3306</code> but when I do that, all the data imported is gone.</p>
<p>Thank you!</p>
"
"20096632","Limit memory on a Docker container doesn't work","<ubuntu><docker><cgroups>","64812804","Problem of limit memory on a Docker container","<docker>","<p>I am running the last version of Docker on top of Ubuntu&nbsp;13.04 (Raring Ringtail):</p>

<pre><code>root@docker:~# docker version
Client version: 0.6.6
Go version (client): go1.2rc3
Git commit (client): 6d42040
Server version: 0.6.6
Git commit (server): 6d42040
Go version (server): go1.2rc3
Last stable version: 0.6.6
</code></pre>

<p>But when I start the container,</p>

<pre><code>root@docker:~# docker run -m=1524288 -i  -t ubuntu /bin/bash
root@7b09f638871a:/# free -m
             total       used       free     shared    buffers     cached
Mem:          1992        608       1383          0         30        341
-/+ buffers/cache:        237       1755
Swap:         2047          0       2047
</code></pre>

<p>I don't see any limiting from any kind, and my kernel has the cgroups memory limit enabled:</p>

<pre><code>kernel /boot/vmlinuz-3.8.0-33-generic ro console=tty0 root=/dev/xvda1 cgroup_enable=memory swapaccount=1
</code></pre>

<p>What obvious thing am I missing here?</p>
","<p>What's the real technical issue preventing the container from displaying a limited amount of RAM instead of host ram?
I mean that if i run container with command</p>
<pre><code>docker run -m=1G -i  -t ubuntu /bin/bash
</code></pre>
<p>and run:</p>
<pre><code>free -g
</code></pre>
<p>i will get the physical amount of ram.
<strong>Why linux kernel don't provide opportunities for that?</strong></p>
"
"20813486","Exploring Docker container's file system","<linux><docker><filesystems>","64971053","How to inspect the resulting folder / file structure and contents of a Docker image/container?","<docker><dockerfile>","<p>I've noticed with docker that I need to understand what's happening inside a container or what files exist in there. One example is downloading images from the docker index - you don't have a clue what the image contains so it's impossible to start the application.</p>

<p>What would be ideal is to be able to ssh into them or equivalent. Is there a tool to do this, or is my conceptualisation of docker wrong in thinking I should be able to do this.</p>
","<p>Given this <code>Dockerfile</code>:</p>
<pre><code># Use small base image with nodejs 10
FROM node:10.13-alpine

# set current work dir
WORKDIR /

# Copy package.json, packge-lock.json into current dir
COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;./&quot;]

# install dependencies
RUN npm install

# copy sources
COPY . .

# open default port (CAN BE SET ON docker-compose)
EXPOSE 3000

# Run app (CAN BE SET ON docker-compose)
CMD [&quot;node&quot;, &quot;src/index.js&quot;]
</code></pre>
<p><strong>.dockerignore</strong></p>
<pre><code>node_modules
*/**.txt
</code></pre>
<p>The <code>WORKDIR</code> along with the <code>COPY</code> (maybe more than 1) and the <code>.dockerignore</code> file obviously result in a specific folder/file structure and content.</p>
<p>Is there a way to inspect it and see what was the result of that? I mean, I would like to know if my <code>.dockerignore</code> is working properly, for example. And see where my source files were copied to, etc.</p>
<p>Can I do that on the image? Or in a container that was run with that image? Maybe Using Docker Desktop or some interface where I could call <code>dir</code> and see what is going on inside my containerized code?</p>
"
"22937618","Reference - What does this regex mean?","<regex>","65004095","Regex for testing that a docker image name is prefixed with a registry?","<java><regex><docker><docker-registry>","<h2>What is this?</h2>
<p>This is a collection of common Q&amp;A. This is also a Community Wiki, so everyone is invited to participate in maintaining it.</p>
<h2>Why is this?</h2>
<p><a href=""/questions/tagged/regex"" class=""post-tag"" title=""show questions tagged &#39;regex&#39;"" rel=""tag"">regex</a> is suffering from <em>give me ze code</em> type of questions and poor answers with no explanation. This reference is meant to provide links to quality Q&amp;A.</p>
<h2>What's the scope?</h2>
<p>This reference is meant for the following languages: <a href=""/questions/tagged/php"" class=""post-tag"" title=""show questions tagged &#39;php&#39;"" rel=""tag"">php</a>, <a href=""/questions/tagged/perl"" class=""post-tag"" title=""show questions tagged &#39;perl&#39;"" rel=""tag"">perl</a>, <a href=""/questions/tagged/javascript"" class=""post-tag"" title=""show questions tagged &#39;javascript&#39;"" rel=""tag"">javascript</a>, <a href=""/questions/tagged/python"" class=""post-tag"" title=""show questions tagged &#39;python&#39;"" rel=""tag"">python</a>, <a href=""/questions/tagged/ruby"" class=""post-tag"" title=""show questions tagged &#39;ruby&#39;"" rel=""tag"">ruby</a>, <a href=""/questions/tagged/java"" class=""post-tag"" title=""show questions tagged &#39;java&#39;"" rel=""tag"">java</a>, <a href=""/questions/tagged/.net"" class=""post-tag"" title=""show questions tagged &#39;.net&#39;"" rel=""tag"">.net</a>.</p>
<p>This might be too broad, but these languages share the same syntax. For specific features there's the tag of the language behind it, example:</p>
<ul>
<li>What are regular expression Balancing Groups? <a href=""/questions/tagged/.net"" class=""post-tag"" title=""show questions tagged &#39;.net&#39;"" rel=""tag"">.net</a></li>
</ul>
","<p>I'm looking for a regex that will test whether a docker image is prefixed with a registry.</p>
<p>Here are what it should match:</p>
<pre><code>docker.io/library/busybox
docker.io/library/busybox:latest
docker.io/busybox
</code></pre>
<p>and what it shouldn't:</p>
<pre><code>busybox
busybox:latest
library/busybox
library/busybox:latest
</code></pre>
"
"22944631","How to get the IP address of the docker host from inside a docker container","<docker><ip>","64479931","In .net core in docker, how can I see the host's IP adress?","<docker><asp.net-core>","<p>As the title says. I need to be able to retrieve the IP address the docker hosts and the portmaps from the host to the container, and doing that inside of the container. </p>
","<p>The scenario is like this:</p>
<p>I have a .net Core docker image that runs on a laptop, where other laptops should be able to access it, ususally on an ad hoc local network with no internet access.</p>
<p>On the server laptop it is just:  http://localhost:23456</p>
<p>But when other laptops connects it is something like: <a href=""http://192.168.1.12:23456"" rel=""nofollow noreferrer"">http://192.168.1.12:23456</a></p>
<p>So I would like that when at app is started up on the server laptop on: http://localhost:23456
the app could resolve the hosts IP address and write:</p>
<p>&quot;For client laptops connecting to this server use this address: <a href=""http://192.168.1.12:23456%22"" rel=""nofollow noreferrer"">http://192.168.1.12:23456&quot;</a></p>
"
"23071214","Use environment variables in CMD","<environment-variables><docker>","64829758","How to assign property value with environment variable in docker file","<spring-boot><docker><dockerfile>","<p>Can I use environment variables in my CMD stanza in a Dockerfile?</p>

<p>I want to do something like this:</p>

<pre><code>CMD [""myserver"", ""--arg=$ARG"", ""--memcache=$MEMCACHE_11211_TCP_ADDR:$MEMCACHE_11211_TCP_PORT""]
</code></pre>

<p>Where $MEMCACHE_11211_TCP_* would be set automatically by the inclusion of the --link parameter of my <code>docker run</code> command.  And $ARG would be configurable by the user at runtime, maybe by the ""-e"" parameter?</p>

<p>This doesn't seem to be working for me, it seems to be literally passing through the string ""$ARG"" for example.</p>
","<p>I am working on docker file with springboot. This line in docker file</p>
<pre><code>ENTRYPOINT [&quot;java&quot;, &quot;-Dbootstrap.servers=$DB_PATH&quot;,&quot;org.springframework.boot.loader.JarLauncher&quot;]
</code></pre>
<p>when I run <code>&quot;docker run -p 9000:9000 -e DB_PATH=newvalue imaginename&quot;</code></p>
<p>I want bootstrap.servers to take value of variable $DB_PATH, however, it will not work and only takes variable name &quot;$DB_PATH&quot;</p>
<p>Or how can i let bootstrap.servers take variables in docker container ?
please help.</p>
<p>Thanks</p>
"
"23692470","Why can't I use Docker CMD multiple times to run multiple services?","<docker>","64911830","docker run only displays last CMD?","<docker>","<p>I have built a base image from Dockerfile named centos+ssh. In centos+ssh's Dockerfile, I use CMD to run ssh service.</p>

<p>Then I want to build a image run other service named rabbitmq,the Dockerfile:</p>

<pre><code>FROM centos+ssh
EXPOSE 22
EXPOSE 4149
CMD /opt/mq/sbin/rabbitmq-server start
</code></pre>

<p>To start rabbitmq container，run：</p>

<pre><code>docker run -d -p 222:22 -p 4149:4149 rabbitmq
</code></pre>

<p>but ssh service doesn't work, it sense rabbitmq's Dockerfile CMD override centos's CMD.</p>

<ol>
<li>How does CMD work inside docker image?</li>
<li>If I want to run multiple service, how to? Using supervisor?</li>
</ol>
","<p>I create a docker image with two echos but only the last one is displayed:</p>
<p><em>dockerfile</em>:</p>
<pre><code>FROM ubuntu:20.04
ENV DEBIAN_FRONTEND=noninteractive

CMD echo &quot;HELLO WORLD&quot;
CMD echo &quot;GOODBYE WORLD&quot;
</code></pre>
<p><em>build and run</em>:</p>
<pre><code>docker build -t junk:junk .
docker run -it junk:junk
</code></pre>
<p>The output is</p>
<pre><code>GOODBYE WORLD
</code></pre>
<p>How can I control the output of <code>docker run -it</code>?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64817110","How to access host machine's mongo db inside docker container","<php><laravel><mongodb><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am a bit new to docker. I am developing a laravel application, I have my app container running.
Inside app container I want to connect to my locally hosted mongo db server, which is generally <code>localhost:27017</code>.</p>
<p>This is my <strong>docker-compose.yml</strong> file</p>
<pre><code>version: '3'
services:

  #PHP Service
  app:
    build:
      context: .
      dockerfile: Dockerfile
    image: digitalocean.com/php
    container_name: bns_app
    restart: unless-stopped
    tty: true
    environment:
      SERVICE_NAME: app
      SERVICE_TAGS: dev
    working_dir: /var/www
    volumes:
      - ./:/var/www
      - ./docker_files/php/local.ini:/usr/local/etc/php/conf.d/local.ini
    networks:
      - app-network

  #Nginx Service
  webserver:
    image: nginx:alpine
    container_name: bns_web
    restart: unless-stopped
    tty: true
    ports:
      - &quot;80:80&quot;
      - &quot;443:443&quot;
    volumes:
      - ./:/var/www
      - ./docker_files/nginx/conf.d/:/etc/nginx/conf.d/
    networks:
      - app-network

  #MySQL Service
  db:
    image: mysql:5.7.22
    container_name: bns_db
    restart: unless-stopped
    tty: true
    ports:
      - &quot;3306:3306&quot;
    environment:
      MYSQL_DATABASE: laravel
      MYSQL_ROOT_PASSWORD: 12345
      SERVICE_TAGS: dev
      SERVICE_NAME: mysql
    volumes:
      - dbdata:/var/lib/mysql/
      - ./docker_files/mysql/my.cnf:/etc/mysql/my.cnf
    networks:
      - app-network

  #REDIS Service
  redis:
      build:
        context: ./docker_files/redis
        dockerfile: Dockerfile
      container_name: bns_redis
      volumes:
        - ./docker_files/redis:/data
      ports:
        - &quot;6379:6379&quot;
      networks:
        - app-network

  # Laravel Echo Server
  laravel-echo-server:
        build:
          context: ./docker_files/laravel-echo-server
          dockerfile: Dockerfile
        container_name: bns_echo_server

        volumes:
          - ./laravel-echo-server.json:/var/www/laravel-echo-server.json:ro
        ports:
          - &quot;6001:6001&quot;
        links:
          - redis
        networks:
          - app-network

  # PHP-WORKER
  php-worker:
    build:
      context: ./docker_files/php-worker
      dockerfile: Dockerfile
    container_name: bns_worker
    volumes:
      - ./php-worker/supervisord.d:/etc/supervisord.d
    depends_on:
      - app
    networks:
      - app-network


#Docker Networks
networks:
  app-network:
    driver: bridge
#Volumes
volumes:
  dbdata:
    driver: local
</code></pre>
<p>So inside my app container when I run artisan commands such as <code>docker-compose exec app php artisan migrate:fresh --seed</code> mysql tables are migrated successfully but mongo db (which is not in any container but it is in host machine only) documents are not migrate/seeded and I get error :</p>
<pre><code>No suitable servers found (`serverSelectionTryOnce` set): [connection refused calling ismaster on '172.18.0.3:27017']

  at /var/www/vendor/mongodb/mongodb/src/functions.php:431
    427|         // TODO: PHPLIB-476: Read transaction read preference once PHPC-1439 is implemented
    428|         $readPreference = new ReadPreference(ReadPreference::RP_PRIMARY);
    429|     }
    430| 
  &gt; 431|     return $manager-&gt;selectServer($readPreference);
    432| }
    433| 

  Exception trace:

  1   MongoDB\Driver\Manager::selectServer(Object(MongoDB\Driver\ReadPreference))
      /var/www/vendor/mongodb/mongodb/src/functions.php:431

  2   MongoDB\select_server(Object(MongoDB\Driver\Manager), [])
      /var/www/vendor/mongodb/mongodb/src/Database.php:419

  Please use the argument -v to see more details.
</code></pre>
<p>This is my ENV file:</p>
<pre><code>MONGO_DATABASE=exchange
MONGO_HOST=172.18.0.3 # &lt;&lt; This is the IP address of my app container  &lt;&lt;
MONGO_PORT=27017
MONGO_USERNAME=
MONGO_PASSWORD=
</code></pre>
<p>Please do help me. Where am I going wrong ?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64962558","connect kafka from host inside python app with docker","<python><docker><apache-kafka>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>i have python app that run with docker container that consume kafka, i also run kafka on my host machine, i run kafka manually not from docker.</p>
<p>when i run my python app container it won't connect to kafka from host, i run it with this command:</p>
<pre><code>docker run -d --network=&quot;host&quot; --name myptm-rating -p 8002:8002 myptm-rating-command
</code></pre>
<p>i set <code>--network</code> flag to connect my app with host.</p>
<p>this is my consumer configuration (i use confluent kafka package):</p>
<pre><code>consumer_conf = {
        'bootstrap.servers': 'localhost:9092',
    }
</code></pre>
<p>then i see container logs, it shows errors:</p>
<pre><code>[2020-11-23 04:09:44 +0000] [10] [INFO] Starting gunicorn 20.0.4
[2020-11-23 04:09:44 +0000] [10] [INFO] Listening at: http://0.0.0.0:8002 (10)
[2020-11-23 04:09:44 +0000] [10] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2020-11-23 04:09:44 +0000] [13] [INFO] Booting worker with pid: 13
[2020-11-23 04:09:44 +0000] [14] [INFO] Booting worker with pid: 14
[2020-11-23 04:09:44 +0000] [15] [INFO] Booting worker with pid: 15
%3|1606104584.818|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT)
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.826|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT)
%3|1606104584.827|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
[2020-11-23 04:09:44 +0000] [13] [INFO] Started server process [13]
[2020-11-23 04:09:44 +0000] [13] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [13] [INFO] Application startup complete.
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.841|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
%3|1606104584.845|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
INFO:src.utils.kafka_client:Start kafka consumer...
%3|1606104584.849|FAIL|rdkafka#consumer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)
[2020-11-23 04:09:44 +0000] [14] [INFO] Started server process [14]
[2020-11-23 04:09:44 +0000] [14] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [14] [INFO] Application startup complete.
[2020-11-23 04:09:44 +0000] [15] [INFO] Started server process [15]
[2020-11-23 04:09:44 +0000] [15] [INFO] Waiting for application startup.
[2020-11-23 04:09:44 +0000] [15] [INFO] Application startup complete.
%3|1606104585.819|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv6#[::1]:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)
</code></pre>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","64622713","docker access outer localhost (host) ip from inside container [windows]","<docker><nginx>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I am running <code>nginx</code> server inside a docker container and in <code>nginx.conf</code> file I need to access certain addresses (localhost for now) with different ports.</p>
<p>Currently I'm putting <code>127.0.0.1</code> address which is the nginx container's localhost address.</p>
<p>So I need to replace that.</p>
<p>Here's <code>nginx.conf</code>:</p>
<pre><code>http {
    upstream allbackend {
        #ip_hash;
        server 127.0.0.1:9000; &lt;- this is a container localhost
        server 127.0.0.1:9001;     
        server 127.0.0.1:9002;
        server 127.0.0.1:9003;
    }    

    server {
        listen 80;
        location / {
            proxy_pass http://allbackend/;
        }

        location /admin {
            return 403;
        }
    }
}
events {}
</code></pre>
<p>In <code>server</code> directive, how can I connect to the outer <code>localhost</code>?</p>
<p>I get this error now in container's log:</p>
<pre><code>2020/10/31 14:00:34 [error] 103#103: *11 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: &quot;GET / HTTP/1.1&quot;, upstream: &quot;http://127.0.0.1:9003/&quot;, host: &quot;localhost:4000&quot;
</code></pre>
"
"27068596","How to include files outside of Docker's build context?","<docker>","64410123","Dockerfile - How to copy files from a local folder?","<docker><dockerfile><docker-volume>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>Below is the <code>Dockerfile</code>:</p>
<pre><code>FROM golang:1.14.10
MAINTAINER xyz

COPY ~/go/bin/product-api /go/bin/product-api

COPY ~/go/bin/swagger /go/bin/swagger

ENTRYPOINT [&quot;/go/bin/product-api&quot;]
</code></pre>
<hr />
<p>on <code>docker build -t cloud-native-product-api:1.0.0 .</code>, gives error:</p>
<pre><code>Step 3/5 : COPY ~/go/bin/product-api /go/bin/product-api
COPY failed: stat /var/lib/docker/tmp/docker-builder398080099/~/go/bin/product-api: no such file or directory
</code></pre>
<hr />
<p>I think the image build process takes <code>/var/lib/docker/tmp/docker-builder398080099</code> as workspace and refer from that path.</p>
<p>How to copy files from specific folder of local machine into Docker image?</p>
"
"28996907","docker: ""build"" requires 1 argument. See 'docker build --help'","<docker><containers>","64502213","""docker build"" requires exactly 1 argument","<docker><dockerfile><docker-build><docker-buildkit>","<p>Trying to follow the instructions for building a docker image from the docker website.</p>

<p><a href=""https://docs.docker.com/examples/running_redis_service/"">https://docs.docker.com/examples/running_redis_service/</a></p>

<p>this is the error I get will following the instructions on the doc and using this Dockerfile</p>

<pre><code>FROM        ubuntu:14.04
RUN         apt-get update &amp;&amp; apt-get install -y redis-server
EXPOSE      6379
ENTRYPOINT  [""/usr/bin/redis-server""]


sudo docker build -t myrepo/redis
docker: ""build"" requires 1 argument. See 'docker build --help'.
</code></pre>

<p>How do  resolve?</p>
","<p>I just want to build a dockerfile from a different directory, I tried the following command</p>
<p><code>docker build -f C:/Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile</code></p>
<p>and</p>
<p><code>docker build -f C://Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile</code></p>
<p>Both of them yield the same error</p>
<p><code>&quot;docker build&quot; requires exactly 1 argument.</code></p>
<p>Am I missing something ?</p>
"
"29181032","Add a volume to Docker, but exclude a sub-folder","<docker><dockerfile>","64575774","Add a volume to Docker, but exclude a specific file","<docker><docker-compose><dockerfile><mounted-volumes>","<p>Supposed I have a Docker container and a folder on my host <code>/hostFolder</code>. Now if I want to add this folder to the Docker container as a volume, then I can do this either by using <code>ADD</code> in the <code>Dockerfile</code> or mounting it as a volume.</p>

<p>So far, so good.</p>

<p>Now <code>/hostFolder</code> contains a sub-folder, <code>/hostFolder/subFolder</code>.</p>

<p>I want to mount <code>/hostFolder</code> into the Docker container (whether as read-write or read-only does not matter, works both for me), but I do <strong>NOT</strong> want to have it included <code>/hostFolder/subFolder</code>. I want to exclude this, and I also want the Docker container be able to make changes to this sub-folder, without the consequence of having it changed on the host as well.</p>

<p>Is this possible? If so, how?</p>
","<p>Hi this might be flagged as a duplicate but none of the other solutions have really worked for me.</p>
<p>So I have a directory structure as (code is top most with the rest subfolders and subfiles)</p>
<ul>
<li>code/
<ul>
<li>task_queue/</li>
<li>tasks/</li>
<li>scheduler/</li>
<li>static/</li>
<li>core/</li>
<li>run_scheduler.sh</li>
<li>scheduler.cfg</li>
</ul>
</li>
</ul>
<p>scheduler.cfg is a config file in which secrets and variables are dynamically inserted to avoid  sensitive information from being compromised. Recently, we started mounting the code volume so that we can edit python code without rebuilding the container every time. That's all good and well but as a side affect any dynamic secrets inserted into the scheduler.cfg file now persist locally as well.</p>
<p>Is there a good way to exclude or avoid changing the local copy of this file? It cannot be gitignore I believe as we typically want to change scheduler.cfg to add new configs and commit to git. It is the dynamic secret insertion part that's being propagated locally that seems to be the problem.</p>
<p>Thank you and apologies if it is a duplicate. I'm not too handy with docker stuff.</p>
"
"32126003","Node.js document is not defined","<javascript><node.js>","64467986","ReferenceError: document is not defined ( JavaScript)","<javascript><docker>","<p>Why node.js does not recognize document.GetElementById? 
It says 'ReferenceError: document is not defined'.
What can I do?</p>

<pre><code>ReferenceError: document is not defined
at Object.&lt;anonymous&gt; (C:\Users\Desktop\main.js:9:18)
at Module._compile (module.js:460:26)
at Object.Module._extensions..js (module.js:478:10)
at Module.load (module.js:355:32)
at Function.Module._load (module.js:310:12)
at Function.Module.runMain (module.js:501:10)
at startup (node.js:129:16)
at node.js:814:3
</code></pre>
","<p>I've got a weird problem, where I can run my page locally, but not on Docker?</p>
<p>Here is the error snippet from docker:</p>
<pre><code>/app/script.js:1

(function (exports, require, module, __filename, __dirname) { const daysEl = window.document.getElementById(&quot;days&quot;);

^


ReferenceError: window is not defined

at Object.&lt;anonymous&gt; (/app/script.js:1:78)

at Module._compile (module.js:571:32)

at Object.Module._extensions..js (module.js:580:10)

at Module.load (module.js:488:32)

at tryModuleLoad (module.js:447:12)

at Function.Module._load (module.js:439:3)

at Module.runMain (module.js:605:10)

at run (bootstrap_node.js:427:7)

at startup (bootstrap_node.js:151:9)

at bootstrap_node.js:542:3
</code></pre>
<p>script.js goes as follow:</p>
<pre><code>const daysEl = window.document.getElementById(&quot;days&quot;);
const hoursEl = window.document.getElementById(&quot;hours&quot;);
const minsEl = window.document.getElementById(&quot;mins&quot;);
const secondsEl = window.document.getElementById(&quot;seconds&quot;);

const newYears = &quot;1 Jan 2021&quot;;

function countdown() {
    const newYearsDate = new Date(newYears);
    const currentDate = new Date();

    const totalSeconds = (newYearsDate - currentDate) / 1000;

    const days = Math.floor(totalSeconds / 3600 / 24);
    const hours = Math.floor(totalSeconds / 3600) % 24;
    const mins = Math.floor(totalSeconds / 60) % 60;
    const seconds = Math.floor(totalSeconds) % 60;

    daysEl.innerHTML = days;
    hoursEl.innerHTML = formatTime(hours);
    minsEl.innerHTML = formatTime(mins);
    secondsEl.innerHTML = formatTime(seconds);
}

function formatTime(time) {
    return time &lt; 10 ? `0${time}` : time;
}

// initial call
countdown();

setInterval(countdown, 1000);
</code></pre>
<p>The Docker file seems to be correct, as I managed to deploy rest api before, it's just html thatm seems to cause problems.</p>
<pre><code>FROM node:7
WORKDIR /app
COPY package.json /app
RUN npm install
COPY . /app
CMD node script.js
EXPOSE 8082
</code></pre>
<p>I will be grateful for any help :)</p>
"
"32163955","How to run shell script on host from docker container?","<docker>","64980331","Execute Host OS Command from Flask container","<python><linux><docker><kubernetes>","<p>How to control host from docker container?</p>

<p>For example, how to execute copied to host bash script?</p>
","<p>I have Flask application which is running docker container, I have to run host OS commands from Flask Application,</p>
<p>How to execute Host OS commands from Docker container?</p>
<p>My Docker image File:</p>
<pre><code>FROM python:3.8-slim
RUN mkdir /app
WORKDIR /app
ADD requirements.txt /app
RUN pip3 install -r requirements.txt
ADD . /app
RUN chmod +x ./entrypoint.sh
ENTRYPOINT [&quot;sh&quot;, &quot;entrypoint.sh&quot;]
</code></pre>
<p>app.py</p>
<pre><code>import subprocess
from flask import Flask
app = Flask(__name__)

app.config['DEBUG'] = True

# Run host command from container
def run_os_cmd():
    return subprocess.checkout([&quot;/usr/bin/some-cmd&quot;, &quot;few-options&quot;])

@app.route(&quot;/&quot;)
def hello_world():
    cmd_output = run_os_cmd()
    return cmd_output

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0')
</code></pre>
<p>Host is Linux</p>
<p>Note, I cannot install host command package inside docker container.</p>
"
"32163955","How to run shell script on host from docker container?","<docker>","64818493","How to callculate the free space on a host machine and get the information inside a docker container","<linux><docker><shell><webserver>","<p>How to control host from docker container?</p>

<p>For example, how to execute copied to host bash script?</p>
","<p>I want to run a shell script which calculates the free disc space on the host machine so that the docker container can use the information in one of its REST endpoints.</p>
<p>Is there a clean way to do this or should I try to solve the problem differently. Because I'm quite sure that it is not a good idea to create this coupling between host and container.</p>
"
"32723111","How to remove old and unused Docker images","<docker><docker-image>","64628988","How to remove multiple <none> docker images blocked by stopped containers quickly","<docker><containers>","<p>When running Docker for a long time, there are a lot of images in system. How can I remove all unused Docker images at once safety to free up the storage?</p>

<p>In addition, I also want to remove images pulled months ago, which have the correct <code>TAG</code>.</p>

<p>So, I'm not asking for removing untagged images only. I'm searching for a way to remove general unused images, which includes both untagged and other images such as pulled months ago with correct <code>TAG</code>.</p>
","<p>I was trying to remove all dangling images including the images which have <code>&lt;none&gt;</code> as repo name or tag while listing with <code>docker images</code> command.</p>
<p><a href=""https://i.stack.imgur.com/tc6ML.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tc6ML.png"" alt=""enter image description here"" /></a></p>
<p>Now, if you try to remove the images , you may have seen that the <code>docker rmi &lt;image_id&gt;</code> commands might fail due to presence of invalid docker containers which were Exited abruptly and persisted as below.</p>
<p><a href=""https://i.stack.imgur.com/OG1NI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OG1NI.jpg"" alt=""enter image description here"" /></a></p>
<p>It is tedious to remove all the containers and dependent images one by. Hence, is there a good way of removing such images quickly or in automated way ?</p>
"
"33322103","Multiple FROMs - what it means","<docker><dockerfile>","64721365","MultiStage dockerfile skips first step and only runs second one","<docker><dockerfile><python-3.7><google-cloud-sdk>","<p>I want to build a docker image for the <a href=""https://github.com/Linkurious/linkurious.js"" rel=""noreferrer"">Linkurious</a> project on github, which requires both the Neo4j database, and Node.js to run.</p>

<p>my first approach was to declare a base image for my image, containing Neo4j.  The reference docs do not define ""base image"" in any helpful manner:</p>

<blockquote>
  <p>Base image:
  An image that has no parent is a base image</p>
</blockquote>

<p>from which I read that I may only have a base image if that image has no base image itself.</p>

<p>but what is a base image? does it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?</p>

<p>reading the Docker reference (see: <a href=""https://docs.docker.com/reference/builder/#from"" rel=""noreferrer"">https://docs.docker.com/reference/builder/#from</a>) I see: </p>

<blockquote>
  <p>FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command.</p>
</blockquote>

<p>do I want to create multiple images? it would seem what I want is to have a single image that contains the contents of other images e.g. neo4j and node.js</p>

<p>I've found no directive to declare dependencies in the reference manual.  are there no dependencies like in RPM where in order to run my image the calling context must first install the images it needs?</p>

<p>I'm confused...</p>
","<p>Docker file that's supposed to install cloud sdk and python 3.7 is skipping the cloud sdk and only running the python3.7 step.</p>
<p>Dockerfile:</p>
<pre><code>FROM google/cloud-sdk:247.0.0

FROM python:3.7
WORKDIR /test
COPY . .
RUN python3 -m pip install -U pip
</code></pre>
<ul>
<li>build image: <code>docker build -t test -t test .</code>
check python3.7 installation: <code>docker run test python3 --version</code> .
output: <code>Python 3.7.9</code></li>
<li>check gcloudsdk installation: <code>docker run test gcloud version</code>. output:  <code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;gcloud\&quot;: executable file not found in $PATH&quot;: unknown. ERRO[0000] error waiting for container: context canceled</code></li>
</ul>
"
"33443912","Commit to jenkins docker image does not save changes","<jenkins><docker>","65074264","Creating Docker image from a running container with its volume","<docker><jenkins>","<p>I pull the official Jenkins docker image from <a href=""https://hub.docker.com/_/jenkins/"">here</a>.
From Jenkins UI I create a new job , install the github plugin and set the repo urls in the job configuration.</p>

<p>Finally I save the changes from Jenkins.</p>

<p>I want to create a new image as it is. I stop the container, and commit it to a new image.</p>

<p>Then I start a new container from the new image...and Jenkins does not contain any of my changes.</p>

<p>I use <code>Docker version 1.6.2, build 7c8fca2</code></p>
","<p>I am trying to crate an image from one of my Jenkins container where I have create a pipeline. Now I want to share that whole Jenkins with those pipeline jobs as an image to my other teammates.</p>
<p>I tried  using docker commit</p>
<pre><code>docker commit -m &quot;Jenkins with AWS pipeline setup&quot; -a &quot;User&quot; 2b01xdxx9d8f pipeline/jenkins:v1
</code></pre>
<p>then I did <code>docker save pipeline/jenkins:v1 &gt; jenkins.tar</code>
and shared the .tar file with my team but when they loaded the .tar file  <code>docker load &lt; jenkins.tar</code>
they got back to fresh Jenkins my pipeline my users where not there . is there any way to load the data too in the image .</p>
"
"33827342","How to connect mysql workbench to running mysql inside docker?","<mysql><docker><mysql-workbench>","64422760","Connect to MySQL in docker from Datagrip","<mysql><docker><datagrip>","<p>I am using mysql server inside docker container and able to access inside docker. How to create connection in mysql workbench running on my local(Host Machine).  </p>
","<p>I installed MYSQL in docker using this command</p>
<pre><code>docker pull mysql/mysql-server:latest
</code></pre>
<p>and then run docker image via this</p>
<pre><code>docker run --name=mysql -d mysql/mysql-server:latest
</code></pre>
<p>I try to connect to container like this</p>
<p><a href=""https://i.stack.imgur.com/hJgPv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hJgPv.png"" alt=""enter image description here"" /></a></p>
<p>I got an error</p>
<blockquote>
<p>[08S01] Communications link failure</p>
<p>The last packet sent successfully to the server was 0 milliseconds
ago. The driver has not received any packets from the server.
java.net.ConnectException: Connection refused: connect.</p>
</blockquote>
<p>Password and User are correct.</p>
<p>How I can solve this problem?</p>
"
"33933107","Change Docker machine location - Windows","<windows><docker><docker-machine><docker-toolbox>","64402436","How to change Docker Image location on Windows 10 Home","<windows><docker>","<p>I am using docker toolbox on Windows 7 to run docker. (docker version 1.9.1)</p>

<p>As it turns out, the docker machine creates its VM at C:\Users\username\.docker\machine\machines\default. As I commit the images, the size of VM at this location bloats up. Since it's Windows, I can't afford the luxury of space on the C drive. </p>

<p>Is there any way I can change the location of the default machine?</p>
","<p>I have searched everywhere and all solutions are mostly outdated. I want to either change the whole docker dir from C Drive to D drive or only image from C to D. Because I have less space on C drive.
I am currently using Windows 10 Home.</p>
"
"33992867","How do you perform Django database migrations when using Docker-Compose?","<django><docker><docker-compose>","64696850","Make docker automatically run collecstatic and migrate in Django","<django><docker>","<p>I have set up a Docker Django/PostgreSQL app closely following the <a href=""https://docs.docker.com/compose/django/"" rel=""noreferrer"">Django Quick Start instructions on the Docker site</a>. </p>

<p>The first time I run Django's manage.py migrate, using the command <code>sudo docker-compose run web python manage.py migrate</code>, it works as expected. The database is built inside the Docker PostgreSQL container just fine.</p>

<p>Changes made to the Django app itself are likewise reflected in the Docker Django container, the moment I save them. It's great!</p>

<p>But if I then change a model in Django, and try to update the Postgres database to match the model, no changes are detected so no migration happens no matter how many times I run <code>makemigrations</code> or <code>migrate</code> again.</p>

<p>Basically, every time I change the Django model, I have to delete the Docker containers (using <code>sudo docker-compose rm</code>) and start afresh with a new migration.</p>

<p>I'm still trying to get my head around Docker, and there's an awful lot I don't understand about how it works, but this one is driving me nuts. Why doesn't migrate see my changes? What am I doing wrong?</p>
","<p>I have a following question.</p>
<p>I have dockerized <code>Django</code> project and instead of manually run <code>collectstatic</code> and <code>migrate</code> each and every time I would like to docker do it for me. Is it possible to make <code>Docker</code> automatically run:</p>
<pre><code>python manage.py collectstatic --noinput

python manage.py migrate

</code></pre>
<p>every time when image is created? Or alternatively before container is started…</p>
<p>My <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.8.6-alpine

LABEL project=&quot;short_urls&quot;

ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

WORKDIR /code

COPY Pipfile Pipfile.lock /code/
RUN pip install pipenv &amp;&amp; pipenv install --system --ignore-pipfile

COPY short_urls /code/

</code></pre>
<pre><code>Docker-Compose.yaml
</code></pre>
<pre><code>version: '3.8'

volumes:
    redis_data:

services:
  web:
    build: .
    command: python /code/manage.py runserver 0.0.0.0:8000
    volumes:
      - .:/code
    ports:
      - target: 8000
        published: 8000
        protocol: tcp
        mode: host
    depends_on:
      - redis
    restart: always

  redis:
    image: redis:6.0.9-alpine
    command:
      redis-server
    ports:
      - target: 6379
        published: 6380
        protocol: tcp
        mode: host
    volumes:
        - redis_data:/data
    restart: always
    environment:
      - REDIS_REPLICATION_MODE=master

  celery:
    build: .
    command: celery -A short_urls worker --loglevel=INFO -E
    restart: always
    environment:
      - C_FORCE_ROOT=1
    volumes:
      - .:/code
    links:
      - redis
    depends_on:
      - web
      - redis
    hostname: celery-main

  celery-beat:
    build: .
    command: celery -A short_urls beat --loglevel=INFO --pidfile=
    restart: always
    volumes:
        - .:/code
    depends_on:
        - web
        - redis
    hostname: celery-beat

  flower:
    image: mher/flower
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/3
      - FLOWER_PORT=8888
    depends_on:
      - celery
      - celery-beat
      - redis
    restart: always
    ports:
      - target: 8888
        published: 8888
        protocol: tcp
        mode: host
</code></pre>
<p>Thank you.</p>
"
"34132237","Container NAMES when deploying with Docker","<docker><dokku>","64409647","NAMES column of ""docker ps"" output","<docker><docker-container>","<p>I've just done my first ever Docker deployment, when i run this command to see the status of recent processes...</p>

<p><code>docker ps -a</code></p>

<p>I get this output</p>

<p><a href=""https://i.stack.imgur.com/66qrY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66qrY.png"" alt=""docker output""></a></p>

<p>My question is; what are those name referring to?</p>
","<p>Below is the docker file:</p>
<pre><code>FROM golang:1.14.10

MAINTAINER xyz

ENV SOURCES /product-api

COPY . ${SOURCES}

WORKDIR /product-api

RUN make swagger

ENTRYPOINT product-api
</code></pre>
<hr />
<p>After running the below commands:</p>
<p><code>$ docker build -t cloud-native-product-api:1.0.0</code></p>
<p><code>$ docker run -it -p 8080:8080 cloud-native-product-api:1.0.0</code></p>
<hr />
<p><code>docker ps</code> gives below output:</p>
<pre><code>$ docker ps -a
CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                    NAMES
9aa144b7873c        cloud-native-product-api:1.0.0   &quot;/bin/sh -c product-…&quot;   3 minutes ago       Up 3 minutes        0.0.0.0:9090-&gt;9090/tcp   trusting_matsumoto
</code></pre>
<hr />
<p>Where does value <code>trusting_matsumoto</code> derive from in <code>NAMES</code> column?</p>
"
"34911622","Dockerfile - set ENV to result of command","<dockerfile><env>","64601093","Set Env Var to Content of Version File in a Dockerfile","<docker><docker-compose><dockerfile>","<p>Is it possible to set a docker ENV variable to the result of a command?
Like:</p>

<pre><code>ENV MY_VAR whoami
</code></pre>

<p>i want MY_VAR to get the value ""root"" or whatever whoami returns</p>
","<p>I'm working in a Dockerfile that's used in a Jenkins-Yeti pipeline. This pipeline uses the Dockerfile, but I can't control the docker build command that the pipeline uses (so I can't add flags).<br>
We have a <code>version</code> file that only contains the current version of the API (ex: <code>0.2.0</code>). We want to reference the contents of the <code>version</code> file (to get the current version) in the Dockerfile to set an environment variable. But I'm having trouble.</p>
<p>Here's the current release part of the Dockerfile:</p>
<pre><code>#release
FROM alpine:latest
WORKDIR /root
COPY version .
ARG CONFIG_VERSION=0.0.1
ENV CONFIG_VER=$CONFIG_VERSION
COPY --from=builder /my-api/app .
EXPOSE 8000
CMD [&quot;./app&quot;]
</code></pre>
<p>As can be seen, we are setting the <code>CONFIG_VER</code> env var to 0.1.0. Instead, we want to make a reference to the version that's inside the <code>version</code> file.</p>
<p>I've tried to do something like this:</p>
<pre><code>#release
FROM alpine:latest
WORKDIR /root
COPY version .
RUN CONFIG_VER=$(cat version)
RUN echo &quot;version from file:&quot; $(cat version) &quot; - CONFIG_VER: &quot; ${CONFIG_VER}
COPY --from=builder /um-correspondence-api/app .
EXPOSE 8000
CMD [&quot;./app&quot;]
</code></pre>
<p>Above I set <code>CONFIG_VER</code> to <code>$(cat version)</code>. You can also see an echo after it I'm using to validate the env var is being set.
The value is not being set, BUT the Dockerfile is able to perform <code>$(cat version)</code><br>
The echo will look like this (please note the contents of the <code>version</code> file is <code>0.2.0</code>):<br>
<code>version from file: 0.2.0 - CONFIG_VER: </code></p>
<p>So, it's able to run the <code>$(cat version)</code> command, but won't set the value to <code>CONFIG_VER</code>
I also tried using <code>ENV CONFIG_VER=$(cat version)</code> instead of <code>RUN CONFIG_VER=$(cat version)</code> (<code>ENV</code> instead of <code>RUN</code>) but <code>ENV</code> doesn't allow that.</p>
<p>Any idea on how to set the <code>CONFIG_VER</code> env var in the Dockerfile to reference the contents of the <code>version</code> file.<br>
I think I need to use a <code>ARG</code> to set an <code>ENV</code>, but I can't set an <code>ARG</code> to <code>$(cat version)</code></p>
"
"35762459","Add Jar to standalone pyspark","<python><apache-spark><pyspark>","64860911","How do i add spark packages to a spark cluster running on docker?","<docker><apache-spark><pyspark><apache-kafka><jupyter>","<p>I'm launching a pyspark program:</p>

<pre><code>$ export SPARK_HOME=
$ export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.9-src.zip
$ python
</code></pre>

<p>And the py code:</p>

<pre><code>from pyspark import SparkContext, SparkConf

SparkConf().setAppName(""Example"").setMaster(""local[2]"")
sc = SparkContext(conf=conf)
</code></pre>

<p>How do I add jar dependencies such as the Databricks csv jar? Using the command line, I can add the package like this:</p>

<pre><code>$ pyspark/spark-submit --packages com.databricks:spark-csv_2.10:1.3.0 
</code></pre>

<p><strong>But I'm not using any of these. The program is part of a larger workflow that is not using spark-submit</strong> I should be able to run my ./foo.py program and it should just work. </p>

<ul>
<li>I know you can set the spark properties for extraClassPath but you have to copy JAR files to each node? </li>
<li>Tried conf.set(""spark.jars"", ""jar1,jar2"") that didn't work too with a py4j CNF exception</li>
</ul>
","<p>I am new to docker and Spark and used this tutorial to setup a spark cluster with a jupyterlab interface on docker: <a href=""https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445"" rel=""nofollow noreferrer"">https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445</a></p>
<p>Additionally I added a wurstmeister/kafka setup to the docker-compose reulting in the following file:</p>
<pre><code>version: &quot;3.6&quot;

networks:
  spark_net:

volumes:
  shared-workspace:
    name: &quot;hadoop-distributed-file-system&quot;
    driver: local
services:
  jupyterlab:
    image: jupyterlab
    networks:
      - spark_net
    container_name: jupyterlab
    ports:
      - 8888:8888
    volumes:
      - shared-workspace:/opt/workspace
  spark-master:
    image: spark-master
    networks:
      - spark_net
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    volumes:
      - shared-workspace:/opt/workspace
  spark-worker-1:
    image: spark-worker
    networks:
      - spark_net
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8081:8081
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
  spark-worker-2:
    image: spark-worker
    networks:
      - spark_net
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8082:8081
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
      
  zookeeper:
    image: wurstmeister/zookeeper
    networks:
      - spark_net
    ports:
      - &quot;2181:2181&quot;
      
  kafka:
    image: wurstmeister/kafka
    networks:
      - spark_net
    ports:
      - &quot;7575&quot;
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - ./var/run/docker.sock
</code></pre>
<p>Now I have multiple problems:</p>
<ul>
<li>I can't use Graphx in my current setup and I guess its because I have to run the following command:
--packages graphframes:graphframes:0.6.0-spark2.3-s_2.11</li>
<li>I also can't connect from pyspark to my kafka stream even though the kafka stream works. I guess thats also because I have to run a --packages command.</li>
</ul>
<p>I guess my main problem is that I don't know where to run these --package commands. Do I have to add them into my dockerfiles? If yes: into the master and worker dockerfiles? How should I change them then?</p>
"
"36756751","View logs for all docker containers simultaneously","<docker><docker-compose>","64783523","How do I view output of python script when running from docker on Amazon Linux","<python><linux><docker><logging><stdout>","<p>I currently use docker for my backend, and when I first start them up with</p>

<pre><code>docker-compose up
</code></pre>

<p>I get log outputs of all 4 dockers at once, so I can see how they are interacting with each other when a request comes in. Looking like this, one request going from nginx to couchdb</p>

<p><img src=""https://i.stack.imgur.com/S6Ldf.png"" alt=""http://i.imgur.com/E4GQ43F.png""></p>

<p>The issue is now that I am running on GCE with load balancing, when a new VM spins up, it auto starts the dockers and runs normally, I would like to be able to access a load balanced VM and view the live logs, but I can not get docker to allow me this style, when I use logs, it gives me normal all white font with no label of where it came from.</p>

<p>Using</p>

<pre><code>docker events  
</code></pre>

<p>does nothing, it won't return any info. </p>

<p>tldr; what is the best way to obtain a view, same as the log output you get when running ""docker-compose up""</p>
","<p>I'm currently trying to setup a flask application hosted by an nginx server through docker on a machine running Amazon Linux 2.</p>
<p>Basically I'm just trying to see the simple output of my python script when I trigger it through nginx on my running docker image.</p>
<p>I've previously worked on an Ubuntu machine and I could simply see the logs live with the command:</p>
<p>tail -f /var/log/syslog</p>
<p>but with Amazon linux or Linux distros in general I believe all logging is done through journalctl, and I can't seem to see any of my python script output when I search the main journalctl logs with the command:</p>
<p>sudo journalctl -f</p>
<p>Could anyone point me in the right direction on where my python script's output is going?</p>
"
"37599263","What's the reason Docker Ubuntu official image would exit immediately when you run/start it?","<linux><bash><ubuntu><docker>","64970769","docker container does not seem to stay running","<docker>","<p>I understand that a container will exit when the main process exit. My question is about the reason behind it, not how to get it to work. I of course know that I could pass the parameter <code>-it</code> to start with interactive mode. </p>

<p>The Ubuntu image will run <code>/bin/bash</code> when it starts according to the image Dockerfile. Shouldn't bash process wait for user input commands and not exit? (just like when you run <code>/bin/bash</code> in the host machine, it would start an interactive shell and wait for user inputs and not exit) Why would the Docker Ubuntu's bash exit right away?</p>
","<p>Dockerfile:</p>
<pre><code>FROM ubuntu
CMD [&quot;bash&quot;]

</code></pre>
<p>I ran</p>
<pre><code>docker build -t myimage:mytag .

docker run myimage:mytag 

docker ps
</code></pre>
<p>No images are seen running. Why not?  How do I find more information on this, such as logs of why it exited?</p>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","64932302","docker: Error response from daemon: driver failed programming external connectivity on endpoint funny_golick","<sql-server><docker>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I am trying to run a docker to run Microsoft SQL Server 2019 in my ubuntu 20.04 OS.
For that reason I write my terminal:</p>
<pre><code>docker pull mcr.microsoft.com/mssql/server
</code></pre>
<p>then I write</p>
<pre><code>sudo docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=Zima1000' -p 1433:1433 -d mcr.microsoft.com/mssql/server:2019-latest
</code></pre>
<p>then it send this messege</p>
<pre><code>7faf65528d4d0fe12567afb6b0b39a07cd060c3974a0b77ac6c0e66f19c23e57
</code></pre>
<p>docker: Error response from daemon: driver failed programming external connectivity on endpoint funny_golick (8946bfd95b1e65fb700ffa4fe457d3de30c8d11a9c8a8f286ebebc70af096d19): Error starting userland proxy: listen tcp 0.0.0.0:1433: bind: address already in use.</p>
<p>plese help how I can fix it.</p>
"
"37971961","Docker Error bind: address already in use","<ubuntu><docker><ubuntu-14.04><bind><docker-compose>","64558374","curl: (56) Recv failure: Connection reset by peer","<linux><docker><flask>","<p>When I run <code>docker-compose up</code> in my Docker project it failes with the following message:</p>

<pre><code>Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use
</code></pre>

<p><code>netstat -pna | grep 3000</code>
shows this:</p>

<pre><code>tcp        0      0 0.0.0.0:3000            0.0.0.0:*               LISTEN      -  
</code></pre>

<p>I've already tried <code>docker-compose down</code>, but it doesn't help.</p>
","<p>I have a flask application running on Linux machine (Ubuntu 18.04) and it is containerized inside a docker. The host and port on which the docker is running is on <code>0.0.0.0:3000</code> but I am not able to see the logs and moreover when I try to access the port via browser it is showing <code>curl: (56) Recv failure: Connection reset by peer</code>. I even tried to expose the port by <code>docker run -p 3000:3000 docker-app:6.0</code> But is giving error which reads:</p>
<pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint reverent_booth (46381f8b4b3e1c7853f1bb79c447f430de18ca81079ff4263e552d38fada675d): Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use.
ERRO[0001] error waiting for container: context canceled
</code></pre>
<p>Any suggestions would be really helpful.</p>
"
"38088279","Communication between multiple docker-compose projects","<networking><docker><docker-compose>","64956328","Docker-Compose connect service to another network","<docker><networking><docker-compose>","<p>I have two separate <code>docker-compose.yml</code> files in two different folders:</p>

<ul>
<li><code>~/front/docker-compose.yml</code></li>
<li><code>~/api/docker-compose.yml</code></li>
</ul>

<p>How can I make sure that a container in <code>front</code> can send requests to a container in <code>api</code>?</p>

<p>I know that <code>--default-gateway</code> option can be set using <code>docker run</code> for an individual container, so that a specific IP address can be assigned to this container, but it seems that this option is not available when using <code>docker-compose</code>.</p>

<p>Currently I end up doing a <code>docker inspect my_api_container_id</code> and look at the gateway in the output. It works but the problem is that this IP is randomly attributed, so I can't rely on it.</p>

<p>Another form of this question might thus be:</p>

<ul>
<li>Can I attribute a fixed IP address to a particular container using docker-compose?</li>
</ul>

<p>But in the end what I'm looking after is:</p>

<ul>
<li>How can two different docker-compose projects communicate with each other?</li>
</ul>
","<p>I have multiple <code>docker-compose.yml</code> which each defines a network.</p>
<p>I want to be able to link a service from one network to a service in another network.</p>
<p><strong>service1/docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.3'

services:
  service_1-mongo:
    image: mongo
    container_name: service_1-mongo
    restart: always
    environment:
      MONGO_INITDB_DATABASE: service_1-service
      MONGO_INITDB_ROOT_USERNAME: service_1
      MONGO_INITDB_ROOT_PASSWORD: service_1
    ports:
      - &quot;27017-27019:27017-27019&quot;
    networks:
      - service_1

  service_1-service:
    image: '...'
    container_name: service_1-service
    restart: always
    environment:
      MONGO_HOST: service_1-mongo
      MONGO_BASE: service_1-service
      MONGO_USER: service_1
      MONGO_PASS: service_1
    ports:
      - '4000:4000'
    networks:
      - network_1
    depends_on:
      - service_1-mongo
</code></pre>
<p><strong>service2/docker-compose.yml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.3'

services:
  service_2-mongo:
    image: mongo
    container_name: service_2-mongo
    restart: always
    environment:
      MONGO_INITDB_DATABASE: service_2-service
      MONGO_INITDB_ROOT_USERNAME: service_2
      MONGO_INITDB_ROOT_PASSWORD: service_2
    ports:
      - &quot;27027-27029:27027-27029&quot;
    networks:
      - service_2

  service_2-service:
    image: '...'
    container_name: service_2-service
    restart: always
    environment:
      MONGO_HOST: service_2-mongo
      MONGO_BASE: service_2-service
      MONGO_USER: service_2
      MONGO_PASS: service_2
      SERVICE_1_URL: **SOMETHING**
    ports:
      - '4100:4000'
    networks:
      - network_2
    depends_on:
      - service_2-mongo
</code></pre>
<p>I need <code>network_1:service_1-service</code> to be able to connect to <code>network_2:service_2-service</code>.</p>
<p>I don't know how to do it.</p>
"
"38133849","Can't use yum inside Docker container running on CentOS","<docker><centos><containers><sudo><yum>","64990005","Dockerfile Permission Denied when using yum install","<docker><dockerfile><centos7><rpm><yum>","<p>I am unable to run any yum commands inside my Docker container without getting the following error:</p>

<pre><code>ovl: Error while doing RPMdb copy-up:
[Errno 13] Permission denied: '/var/lib/rpm/Group'
You need to be root to perform this command.
</code></pre>

<p>I'm confused because I'm pretty sure docker containers are run with the default user root.  Still, if I try putting sudo in front of a <code>yum install -y &lt;package&gt;</code> or <code>yum update -y</code> command I get:</p>

<pre><code>/bin/sh: sudo: command not found
</code></pre>

<p>I'm using the following base image so I can easily run a Java Wildfly application inside Docker: <a href=""https://hub.docker.com/r/jboss/wildfly/"" rel=""noreferrer"">https://hub.docker.com/r/jboss/wildfly/</a></p>

<p>The underlying distro is <code>CentOS Linux release 7.2.1511 (Core)</code></p>
","<p>I'm trying to create Dockerfile from this Image: <a href=""https://hub.docker.com/r/centos/python-27-centos7"" rel=""nofollow noreferrer"">https://hub.docker.com/r/centos/python-27-centos7</a></p>
<p>My Dockerfile:</p>
<pre><code>FROM centos/python-27-centos7:latest

RUN yum install rpm-build -y

WORKDIR /usr/src/app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt


CMD [&quot;bash&quot;]
</code></pre>
<p>But I got next error:</p>
<pre><code> =&gt; ERROR [2/9] RUN yum install rpm-build -y                                                                                                                                0.6s
------                                                                                                                                                                           
 &gt; [2/9] RUN yum install rpm-build -y:                                                                                                                                           
#5 0.480 Loaded plugins: fastestmirror, ovl                                                                                                                                      
#5 0.485 ovl: Error while doing RPMdb copy-up:
#5 0.485 [Errno 13] Permission denied: '/var/lib/rpm/Dirnames'
#5 0.579 You need to be root to perform this command.
------
</code></pre>
<p>I tried add sudo but still doesn't work.
What is wrong here?</p>
"
"38485607","Mount host directory with a symbolic link inside in docker container","<docker>","64805889","How to properly include symlink in Docker volumes?","<wordpress><docker><docker-compose><symlink>","<p>I mounted the container with this parameter:</p>

<blockquote>
  <p>-v /home/test/:/home/test</p>
</blockquote>

<p>Inside /home/test in the host there is a symbolic link pointing to a /mnt/ folder.</p>

<p>But that link, although can be seen where is pointing to, seems broken inside the container:</p>

<pre><code>root@f93f72b45013:/var/www/html# cd /home/test/ 
root@f93f72b45013:/home/test# ls -lrt 
total 11956 
lrwxrwxrwx. 1 root root 40 Jul 20 15:55 file -&gt; /mnt/mountedfile/
root@f93f72b45013:/home/test# ls -lrt file/*
ls: cannot access file/*: No such file or directory
</code></pre>

<p>Is that even possible to be done in docker? I am not sure if is there a way to do it.</p>

<p>I know I can just directly mount where the symbolic link is pointing at but I was just wondering if this is possibe.</p>
","<p>I'm having an issue with symlinks in Docker volumes.</p>
<p>I'm new to Docker and for now, I tend to use it only for local WordPress plugin development.
My goal is to set up a WordPress container and create a symlink in plugins directory.</p>
<p>In docker-compose file I mapped wp volume to root dir.
Directory structure is like this:</p>
<pre><code>- some-wp-plugins
    - plugin-1
    - plugin-2
    - plugin-3

- some-wp-projects   
    - wp-project-1
        - docker-compose.yml
        - wp files...
        - wp-content
            - plugins
                - symlink to the plugin-1

    - wp-project-2
    - wp-project-3
</code></pre>
<p>The symlink is visible on the drive, but not inside a WordPress. If I copy plugin files instead of symlinking them, then everything is ok and I can activate plugin in wp.</p>
<p>Can symlinks be used in this scenario?</p>
<p>I have the latest Docker version (2.5.0.1) installed on mac machine.</p>
"
"39711924","Merge two docker images","<docker><dockerfile>","65019512","How to merge Docker Compose in one image?","<docker><google-cloud-platform><docker-compose><dockerfile><google-cloud-run>","<p>I'm hoping to use docker to set up some bioinformatic analysis.</p>

<p>I have found two docker images that I would like to use:</p>

<ol>
<li>jupyter/datascience-notebook</li>
<li>bioconductor/devel_base</li>
</ol>

<p>I have been successful in running each of these images independently, however I don't know how to merge them together.</p>

<p>Is merging two docker containers possible? Or do you start with one, and then manually install the features of the other?</p>
","<p>I'm trying to host a Docker Compose file via Google Cloud Run but for that I can only choose one image. So my idea was to merge it into one image. I know it's not exactly the best way to do it but I don't have time to learn Kubernetes.
Thank you!</p>
"
"40944479","Docker: How to use bash with an Alpine based docker image?","<bash><docker><dockerfile><alpine>","64998374","Docker run won't find shell script","<docker><sh>","<p>I created a docker image from openjdk:8-jdk-alpine but when I try to execute simple commands I get the following errors:</p>

<pre><code>RUN bash
/bin/sh: bash: not found

RUN ./gradlew build
env: can't execute 'bash': No such file or directory
</code></pre>
","<p>I've created a docker image, and attempted to run it like this:</p>
<pre><code>IMAGE=$(docker build --network=host -q .)

docker run \
        --network=host \
        --rm $IMAGE sh -c &quot;cd bamboo &amp;&amp; ./publish.sh&quot;
</code></pre>
<p>The output is:</p>
<pre><code>sh: ./publish.sh: not found
</code></pre>
<p>This variation of the command</p>
<pre><code>IMAGE=$(docker build --network=host -q .)

docker run \
        --network=host \
        --rm $IMAGE sh -c &quot;cd bamboo &amp;&amp; ls -l&quot;
</code></pre>
<p>shows:</p>
<pre><code>-rwxr-xr-x    1 node     node           646 Nov 25 01:40 publish.sh
</code></pre>
<p>Outside the container (whose content is copied into the container via the docker file), while in same parent folder as the above script, running</p>
<pre><code>cd bamboo &amp; ./publish.sh
</code></pre>
<p>works as expected.  Any ideas what I can be doing wrong?</p>
<p>I found this similar question:
<a href=""https://stackoverflow.com/questions/37419042/container-command-start-sh-not-found-or-does-not-exist-entrypoint-to-contain"">Container command &#39;/start.sh&#39; not found or does not exist, entrypoint to container is shell script</a></p>
<p>and it mentioned line ending on Windows (where I am running Docker) can be problematic.  I confirmed my line ending are LF rather than CRLF.</p>
<p>EDIT: The first few lines of the called script is this:</p>
<pre><code>#!/bin/bash

set -ex

SCRIPTDIR=&quot;$( cd &quot;$( dirname &quot;${BASH_SOURCE[0]}&quot; )&quot; &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )&quot;
</code></pre>
<p>and my dockerfile looks like this:</p>
<pre><code>FROM node:12.16.2-alpine3.11

WORKDIR /app

# node image creates node user for security purposes
RUN chown -R node:node /app

USER node

COPY --chown=node:node . .

RUN yarn install --no-progress --pure-lockfile

RUN yarn build
</code></pre>
"
"41208870","Docker compose named volume: find volume on host machine","<docker><docker-compose><volume>","65084035","docker-compose named volume not showing on host","<docker><docker-compose>","<p>I have a docker-compose.yml:</p>

<pre><code>version: '2'

services:
   db:
     image: mysql:5.7
     volumes:
       - db_data:/var/lib/mysql
     restart: always
     environment:
       MYSQL_ROOT_PASSWORD: wordpress
       MYSQL_DATABASE: wordpress
       MYSQL_USER: wordpress
       MYSQL_PASSWORD: wordpress

   wordpress:
     depends_on:
       - db
     image: wordpress:latest
     ports:
       - ""8000:80""
     restart: always
     environment:
       WORDPRESS_DB_HOST: db:3306
       WORDPRESS_DB_PASSWORD: wordpress
volumes:
    db_data:
</code></pre>

<p>When I run this <code>docker-compose up -d</code> and then do <code>docker inspect -f '{{ (index .Mounts 0).Source }}' ef8be254a08b</code> for the running db container to get the Source which is specifying the volume location on the host, I always get ""No such file or directory"" if I <code>ls</code> the directory (<code>ls /var/lib/docker/volumes/test_db_data/_data: No such file or directory</code>).
How is that possible to get the real location for the volume?</p>
","<p>I am learning docker-compose, I have a simple docker-compose, which is working, e.g I can use mongo from my nodejs api:</p>
<pre><code>version: '3'    
services:
  main:
    build: ./api
    container_name: main
    command: npm run start
    restart: unless-stopped
    ports:
      - &quot;3001:3001&quot;
    environment:
      - PORT=3001
      - MONGO_URL=mongodb://api_db:27017/api
    depends_on:
      - api_db
    networks:
      - realworld-docker-network
  api_db:
      image: mongo:latest
      container_name: mongo
      volumes:
        - mongodb_auth:/data/db
      networks:
        - realworld-docker-network
networks:
  realworld-docker-network:
    driver: bridge
volumes:
  mongodb_auth:
    driver: local
</code></pre>
<p>What i can not understand is where mongodb is mapped to the host system directory, because there is no <code>/data/db</code> on my host ubuntu machine, when all containers are up.</p>
<p>I tried to search on net, but cant figure this out. Please help.</p>
"
"42919181","Why does pip install requirements.txt install extra packages?","<python><pip><requirements.txt>","64854380","Docker pip install odd with requirements.txt","<python><docker><docker-compose><pip><dockerfile>","<p>I have a repository with an inflated requirements.tx that I'd like to clean up. Using <a href=""https://github.com/bndr/pipreqs"" rel=""nofollow noreferrer"">pipreqs</a> I've set my requirements.txt to be a minimal set of packages need for my repository. To test this, I setup a virtualenv to install the packages and then run all my unit tests to make sure they're satisfactory.</p>

<pre><code>virtualenv temp_venv --no-site-packages
source temp_venv/bin/activate
pip install -r requirements.txt
</code></pre>

<p>Which runs fine, but I see that a whole bunch of extra packages are collected and installed. Why? Are these identified as needed by required packages, and thus installed? If so, should I then include them in the requirements.txt?</p>
","<p>Firstly, I add a lots of package in requirements.txt, docker build it and works better.
and then I changed my requirements.txt like below.</p>
<pre><code>Django==2.2.6 
django-filter==2.4.0
djangorestframework==3.12.1
djongo==1.3.3
Pillow==8.0.0
pylint==2.6.0
requests==2.24.0
sqlparse==0.2.4
urllib3==1.25.11
</code></pre>
<p>but when I try docker-compose up again, pip still install other pacakge that I removed.</p>
<pre><code>Step 8/22 : RUN pip install --no-cache-dir -r /requirements.txt
 ---&gt; Running in 55fcc339ce74
Collecting Django==2.2.6
  Downloading Django-2.2.6-py3-none-any.whl (7.5 MB)
Collecting django-filter==2.4.0
  Downloading django_filter-2.4.0-py3-none-any.whl (73 kB)
Collecting djangorestframework==3.12.1
  Downloading djangorestframework-3.12.1-py3-none-any.whl (913 kB)
Collecting djongo==1.3.3
  Downloading djongo-1.3.3.tar.gz (334 kB)
Collecting Pillow==8.0.0
  Downloading Pillow-8.0.0.tar.gz (44.6 MB)
Collecting pylint==2.6.0
  Downloading pylint-2.6.0-py3-none-any.whl (325 kB)
Collecting requests==2.24.0
  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)
Collecting sqlparse==0.2.4
  Downloading sqlparse-0.2.4-py2.py3-none-any.whl (38 kB)
Collecting urllib3==1.25.11
  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)
Collecting pytz
  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)
Collecting pymongo&gt;=3.2.0
  Downloading pymongo-3.11.0.tar.gz (771 kB)
Collecting astroid&lt;=2.5,&gt;=2.4.0
  Downloading astroid-2.4.2-py3-none-any.whl (213 kB)
Collecting toml&gt;=0.7.1
  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting mccabe&lt;0.7,&gt;=0.6
  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)
Collecting isort&lt;6,&gt;=4.2.5
  Downloading isort-5.6.4-py3-none-any.whl (98 kB)
Collecting idna&lt;3,&gt;=2.5
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting chardet&lt;4,&gt;=3.0.2
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting certifi&gt;=2017.4.17
  Downloading certifi-2020.11.8-py2.py3-none-any.whl (155 kB)
Collecting wrapt~=1.11
  Downloading wrapt-1.12.1.tar.gz (27 kB)
</code></pre>
<p>I don't know why this happen again and again.
I thought something cache in my docker system.
I already tried remove all the docker images, docker cache deleted... etc...</p>
<p>How can I fix it?</p>
<pre><code>FROM python:3.8-alpine

ENV PATH=&quot;/scripts:${PATH}&quot;
ENV LIBRARY_PATH=/lib:/usr/lib

COPY ./requirements.txt /requirements.txt
RUN apk add --update --no-cache --virtual .tmp gcc libc-dev linux-headers jpeg-dev libjpeg-turbo
RUN apk add build-base python3-dev zlib-dev
RUN pip install --upgrade pip
RUN pip install --no-cache-dir -r /requirements.txt
RUN apk add libjpeg
RUN apk del .tmp

RUN mkdir /webserver
COPY ./webserver /webserver
WORKDIR /webserver
COPY ./scripts /scripts

RUN chmod +x /scripts/*

RUN mkdir -p /vol/web/media
RUN mkdir -p /vol/web/

RUN adduser -D user
RUN chown -R user:user /vol
RUN chmod -R 755 /vol/web
USER user

CMD [&quot;entrypoint.sh&quot;]
</code></pre>
<p>This is my docker file. pip install --no-cache-dir doesn't work either...</p>
"
"44635352","difference between localhost and postgres for host in docker","<django><postgresql><docker><docker-compose>","65010985","Database Postgres in Docker with Django?","<django><database><postgresql><docker>","<p>I am developing a django app and trying to run it inside docker. I have an issue that I could not understand so far. while running the app with <code>docker-compose</code>, it seems that the <code>web</code> app cannot connect to the database when i use these configurations:</p>

<pre><code>DATABASES = {
    'default': {
       'ENGINE': 'django.db.backends.postgresql_psycopg2',
       'NAME': 'my_db',
       'USER': 'my_user',
       'PASSWORD': '',
       'HOST': 'localhost',
       'PORT': '5432',
    }
</code></pre>

<p>but once I change the host to <code>postgres</code>, it works. like this</p>

<pre><code>DATABASES = {
    'default': {
       'ENGINE': 'django.db.backends.postgresql_psycopg2',
       'NAME': 'my_db',
       'USER': 'my_user',
       'PASSWORD': '',
       'HOST': 'postgres',
       'PORT': '5432',
    }
</code></pre>

<p>what is the difference between <code>postgres</code> and <code>localhost</code>. One is running without and issue inside docker and not in development environment in my mac and the other one is the opposite.</p>

<pre><code># docker-compose.yml    
version: '3'

    services:
      db:
        image: postgres
        expose: 
          - ""5432""
      web:
        build: .
        command: python3 manage.py runserver 0.0.0.0:8000
        volumes:
          - .:/code
        ports:
          - ""8000:8000""
        depends_on:
          - db
</code></pre>
","<p>For example I Use  defoult settings in Django</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'database',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': '127.0.0.1',
        'PORT': '5432',
    }
</code></pre>
<p>When dockerize my db with</p>
<pre><code>depends_on:
          - db
</code></pre>
<p>Should I change change my <code>'HOST'</code> parameter <code> '127.0.0.1'</code>  for <code>'db'</code></p>
<p>could you explain this process</p>
<p>thank you in advance</p>
"
"47303141","How to use the host network, and any other user-defined network together in Docker-Compose?","<docker><networking><service><docker-compose><ethernet>","65009367","Is it possible for a container with the host network to receive the network bridge to the network?","<docker>","<p>I want to connect two Docker containers, defined in a Docker-Compose file to each other (<code>app</code> and <code>db</code>). And one of them (<code>app</code>) should also be connected to the <code>host</code> network.</p>

<p>The containers should be connected to a common user-defined network (<code>appnet</code> or <code>default</code>) to use the <em>embedded DNS</em> capabilities from docker networking.</p>

<p><code>app</code> needs also to be directly connected to the host network to receive ethernet broadcasts (network layer 2) in the physical network of the docker host.</p>

<p>If I use both directives <code>network_mode: host</code> and <code>networks</code> in compose together, I get the following error.</p>

<p><code>ERROR: 'network_mode' and 'networks' cannot be combined</code></p>

<p>So I have to do this with <code>networks</code> only!?</p>

<pre><code>version: ""3.3""

services:

  app:
    build: .
    image: app
    container_name: app
    environment:
      - MONGODB_HOST=db
    depends_on:
      - db
    networks:
      - appnet
      - hostnet

  db:
    image: mongo:latest
    container_name: db
    networks:
      - appnet

networks:
  appnet:
  hostnet:
    external:
      name: host
</code></pre>

<p>The foregoing compose file produces a error.
<code>ERROR: for app  network-scoped alias is supported only for containers in user defined networks</code></p>

<p>If I use the network name <code>host</code> in the service without defining it in networks (because it already exists), it says.
<code>ERROR: Service ""app"" uses an undefined network ""host""</code></p>

<p>How to use the <code>host</code> network, and any other user-defined network (or the default) together in Docker-Compose?</p>

<p>I'm using <code>Docker version 17.11.0-ce-rc3, build 5b4af4f</code> and <code>docker-compose version 1.17.1, build 6d101fb</code></p>
","<p>Let's assume that I have docker compose as bellow</p>
<p>version: '2.1'</p>
<pre><code>services:

    example-first:
      image: example-first:latest
      container_name: example-first
      ports:
        - &quot;6161:6161&quot;   
      restart: always
      networks:
      - default


        
    example-second:
      image: example-second:latest
      container_name: example-second
      ports:
        - &quot;6767:6767&quot;  
      restart: always
      network_mode: host


networks:
  default:
      driver: bridge
</code></pre>
<p>How the second container can receive (send request) the first container? Is it possible?</p>
<p>Thanks in advance!</p>
"
"48915458","windows run docker with --network=host and access with 127.0.0.1","<windows><docker><hyper-v>","64930840","How to connect to container from windows?","<windows><docker>","<p>I have windows 10 pro and I'm trying to run a docker with network mode host.</p>

<p>my issue is that I can't run a docker and access it using the host ip not 127.0.0.1 and not the ip (in linux it works differently).</p>

<p>looks like the hyper v has it's own network that not accessible using the host ip </p>

<pre><code>docker run -d --network=host nginx
</code></pre>

<p>output:</p>

<pre><code>CONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS                          PORTS               NAMES
8edd86bf292b        nginx                                             ""nginx -g 'daemon of…""   3 seconds ago       Up 2 seconds                                        happy_curie
</code></pre>

<p>so there is no ports as expected but and no errors.
When I'm trying to open the browser using 127.0.0.1 I'm getting <code>ERR_CONNECTION_REFUSED</code> 
if I set ports to instead of network mode host it is working</p>

<pre><code>docker run -d -p 80:80   nginx
</code></pre>

<p>Hyper v  Ethernet adapter vEthernet (DockerNAT):</p>

<pre><code>   Connection-specific DNS Suffix  . :
   IPv4 Address. . . . . . . . . . . : 10.0.75.1
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . 
</code></pre>

<p>Remarks:</p>

<ul>
<li>changing in the hyper v virtual switch manager the network to be external - not helping

<ul>
<li>firewall is disabled</li>
</ul></li>
</ul>

<p>any idea how to work with network mode host in windows? </p>
","<p>I'm on Windows 10, Docker version 19.03.8
I have the following docker-compose.yml:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;3.2&quot;
services:
  mongo:
    image: mongo
    #restart: always
    network_mode: &quot;host&quot;
    ports:
      - 27017:27017
</code></pre>
<p>I start it with <strong>docker-compose up mongo</strong></p>
<p>Now I want to connect to it from my console by calling mongo.exe
I got error:</p>
<pre><code>Error: couldn't connect to server 127.0.0.1:27017, connection attempt failed:
</code></pre>
<p>Why can't I connect to my docker container? How do I connect to it?</p>
"
"49638532","Docker copy file to host from within dockerfile","<docker>","64743398","How to get artifact out of docker build?","<docker><docker-build>","<p>I am writing a <code>Dockerfile</code> to build an application.  I know that you can copy files out of the container manually with </p>

<pre><code>docker cp &lt;containerId&gt;:/file/path/within/container /host/path/target
</code></pre>

<p>(see <a href=""https://stackoverflow.com/questions/22049212/copying-files-from-docker-container-to-host"">Copying files from Docker container to host</a>), but I would like to just run </p>

<pre><code>docker build
</code></pre>

<p>and have it dump the build artifacts created by my <code>Dockerfile</code> somewhere on my host file system.</p>

<p>Is there a command I can use within the Dockerfile to copy out of the container and into the host?  Like the opposite of <code>COPY</code>?</p>
","<p>Is there a way to get artifacts from <code>docker build</code> out to the host machine?</p>
<p>As an example, given the following <code>Dockerfile</code>, is there a way to get the file <code>log.log</code>?</p>
<pre><code>FROM alpine:3.7 as base
RUN mkdir /log/
RUN touch /log/log.log
</code></pre>
<hr />
<p>My attempt at using <code>COPY</code> seems to only work copying <em>from</em> host <em>to</em> docker image:</p>
<pre><code>FROM alpine:3.7 as base
RUN mkdir /log/
RUN touch /log/log.log
COPY ./foo.bar /log/
RUN ls -l /log/
COPY /log/log.log ./
</code></pre>
<pre><code>$ touch ./foo.bar &amp;&amp; docker build -t demo --no-cache .
Sending build context to Docker daemon  15.36kB
Step 1/6 : FROM alpine:3.7 as base
 ---&gt; 6d1ef012b567
Step 2/6 : RUN mkdir /log/
 ---&gt; Running in 4b6df3797ee3
Removing intermediate container 4b6df3797ee3
 ---&gt; 827e6001d34a
Step 3/6 : RUN touch /log/log.log
 ---&gt; Running in d93d50d61b69
Removing intermediate container d93d50d61b69
 ---&gt; c44620d4f9c4
Step 4/6 : COPY ./foo.bar /log/
 ---&gt; 6996718d44da
Step 5/6 : RUN ls -l /log/
 ---&gt; Running in 84e997af182b
total 0
-rw-r--r--    1 root     root             0 Nov  8 21:44 foo.bar
-rw-r--r--    1 root     root             0 Nov  8 21:44 log.log
Removing intermediate container 84e997af182b
 ---&gt; 5a440f258772
Step 6/6 : COPY /log/log.log ./
COPY failed: stat /var/lib/docker/tmp/docker-builder677155266/log/log.log: no such file or directory
</code></pre>
<hr />
<p>I'm aware of the <code>-v</code> (volume mount) argument to <code>docker run</code> - but that's not what I'm looking for: I'm looking to learn if there's a way to get artifacts from out of the <code>docker build</code> process specifically.</p>
<hr />
<p><strong>Update:</strong> RE: @ChristianFosli's suggestion to use <code>docker cp</code>: that solution requires a docker <em>container</em>. However, in my case, the reason I am looking to extract files specifically during the <code>docker build</code> process is that my <code>Dockerfile</code> runs an executable that fails, therefore I have no image that I can run as a container on which I can perform the <code>docker cp</code>. I.e. the file that I would like to extract from the <code>docker build</code> process is a metadata file that contains debugging information about the failure, that I'd like to inspect.</p>
"
"49718431","docker-compose.yml file naming convention","<docker><docker-compose>","65060070","How to run docker-compose with custom file name","<docker><docker-compose>","<p>What is the naming convention for the <code>docker-compose.yml</code> file? Should it always be named like this or am I free to rename it to, for instance, <code>docker-compose-jenkins.yml</code>?</p>
","<p>I have this file: <code>docker-compose.test.yml</code></p>
<p>But when I try to run <code>docker-compose</code>, this happens:</p>
<pre class=""lang-sh prettyprint-override""><code>$ docker-compose up
ERROR:
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml

</code></pre>
<p><strong>Is there some option to specify which file to use?</strong></p>
"
"51552706","Is distributing python source code in Docker secure?","<python><security><docker><source-code-protection>","64650906","How to prevent passwordless login from docker exec?","<docker>","<p>I am about to decide on programming language for the project.
The requirements are that some of customers want to run application on isolated servers without external internet access.</p>

<p>To do that I need to distribute application to them and cannot use SaaS approach running on, for example, my cloud (what I'd prefer to do...).</p>

<p>The problem is that if I decide to use Python for developing this, I would need to provide customer with easy readable code which is not really what I'd like to do (of course, I know about all that ""do you really need to protect your source code"" kind of questions but it's out of scope for now).</p>

<p>One of my colleagues told me about Docker. I can find dozen of answers about Docker container security. Problem is all that is about protecting (isolating) host from code running in container.</p>

<p>What I need is to know if the Python source code in the Docker Image and running in Docker Container is secured from access - can user in some way (doesn't need to be easy) access that Python code?</p>

<p>I know I can't protect everything, I know it is possible to decompile/crack everything. I just want to know the answer just to decide whether the way to access my code inside Docker is hard enough that I can take the risk.</p>
","<p>I have source code in docker images which I have to protect from the copy.</p>
<p>I have set up a password in the image but when I run command
<code>docker exec -it container-name /bin/bash </code> then it login without password.</p>
<p>How to set any protect so when anyone run above command then it will ask password for that so I can protect my source from the copy.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","64512218","Docker and VS Code Remote Containers : How to connect remote Python dev container with Kafka container","<python><docker><visual-studio-code><apache-kafka><remote-access>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I use VS Code with the remote-containers extension.</p>
<p>I have 2 project folders for with each has it's own docker-compose file.</p>
<ol>
<li>zookeeper/kafka (listening to port 9092)</li>
<li>python dev container (running as a remote-container)</li>
</ol>
<p>In the python container I have a producer that sends messages to the kafka container. Unfortunately I cannot reach the kafka container from within the python container. I get the following error:<br />
<code>kafka.errors.NoBrokersAvailable: NoBrokersAvailable</code><br />
(If I run the producer directly from the host then it works like a charm!)</p>
<p><strong>How can I connect from the python remote-container to the kafka container?</strong></p>
<p>I tried port forwarding and publishing in the devcontainer.json without success:<br />
&quot;forwardPorts&quot;: [9092]<br />
&quot;appPort&quot;: [9092]
I even tried temporary port forwarding with
Forward a port &gt;&gt; 9092</p>
<p>I'm a docker beginner so I guess I have a misconception here, I would highly appreciate if someone could help me out.</p>
<p>Thanks!<br />
carlo</p>
<p><em><strong>devcontainer.json</strong></em></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;python&quot;,
  &quot;dockerComposeFile&quot;: [&quot;../docker-compose.yml&quot;],
  &quot;service&quot;: &quot;python&quot;,
  &quot;workspaceFolder&quot;: &quot;/workspace&quot;,
  &quot;settings&quot;: {
    &quot;terminal.integrated.shell.linux&quot;: &quot;/bin/bash&quot;,
    &quot;python.pythonPath&quot;: &quot;/usr/local/bin/python&quot;,
    &quot;python.linting.enabled&quot;: true,
    &quot;python.linting.pylintEnabled&quot;: true,
    &quot;python.linting.pylintPath&quot;: &quot;/usr/local/bin/pylint&quot;,
    &quot;python.formatting.autopep8Path&quot;: &quot;/usr/local/bin/autopep8&quot;,
    &quot;python.formatting.blackPath&quot;: &quot;/usr/local/bin/black&quot;
  },
  &quot;extensions&quot;: [
    &quot;ms-python.python&quot;,
    &quot;coenraads.bracket-pair-colorizer&quot;,
    &quot;wayou.vscode-todo-highlight&quot;
  ],
  // forward ports
  &quot;forwardPorts&quot;: [9092],
  // publish ports to the host
  &quot;appPort&quot;: [9092]
}
</code></pre>
<p><em><strong>producer.py</strong></em></p>
<pre class=""lang-py prettyprint-override""><code>from kafka import KafkaProducer
import json
from data import get_registered_user
import time

def json_serializer(data):
    return json.dumps(data).encode('utf-8')

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=json_serializer,
)

if __name__ == &quot;__main__&quot;:
    while True:
        registered_user = get_registered_user()
        producer.send('registered_user', registered_user)
        print(registered_user)
        time.sleep(3)
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","64956537","Spring Boot connect to Kafka Brokers","<spring-boot><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I have a docker-compose file that creates 3 Kafka nodes and 1 topic:</p>
<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - &quot;2181:2181&quot;
  kafka:
    build: .
    ports:
      - &quot;9092&quot;
    environment:
      HOSTNAME_COMMAND: &quot;docker info | grep ^Name: | cut -d' ' -f 2&quot; # Normal instances
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_HOST_NAME: ${IPADDRESS}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE  
      KAFKA_CREATE_TOPICS: &quot;Upload_Kafka_Topic:1:3&quot; # 1 partition and 3 replicas
    volumes:
      - /docker-volumes/run/docker.sock:/var/run/docker.sock
</code></pre>
<p>Now I have an application.yml file as part of my Spring Project that gets the Broker IP Adresses injected:</p>
<pre><code>spring:
  kafka:
    consumer:
      group-id: upload-group
      auto-offset-reset: earliest
    bootstrap-servers: ${BROKERS_IP_ADDRESSES}
</code></pre>
<p>when I inject BROKERS_IP_ADDRESSES with for instance localhost:9092 or anything alike I get a connection error saying:</p>
<blockquote>
<p>[Consumer clientId=json-0, groupId=upload-group] Connection to node -1 (/localhost:9092) could not be established. Broker may not be available</p>
</blockquote>
<p>But when I use a custom script to insert each individual broker (or even just manually a single one!) into the BROKERS_IP_ADDRESSES,
I also get an error, but a java.net.UnknownHostException with exactly 3 different hashes something like: 8eedde00f315 (matching the count of my broker addresses)</p>
<p>I am <em><strong>assuming</strong></em> my second approach works and is delegated to Kafka, which uses a different internal handling of the brokers, but exposes these hashes to my application, which fails to make the correct look up of brokers in return.</p>
<p>Is there some additional configuration or a configuration of my Kafka environment that would resolve this look up error?</p>
"
"52148404","Is there a Docker-ce version compatible for Ubuntu 16.04(i386 arch)? I find nothing on Web, only amd64, arm64 and other archs etc","<docker>","64428557","Docker and mysql problems?","<mysql><docker><mariadb>","<p>According to the official tutorial, it says ""Go to <a href=""https://download.docker.com/linux/ubuntu/dists/"" rel=""nofollow noreferrer"">https://download.docker.com/linux/ubuntu/dists/</a>, choose your Ubuntu version, browse to pool/stable/ and choose amd64, armhf, ppc64el, or s390x. Download the .deb file for the Docker version you want to install."". I did check that url, no deb for i386.</p>
","<p>I like to set up a LAMP on a docker.
I work on an i386 architecture.
I have some issues regarding the sql part.</p>
<p>I tried to do my own docker-compose.yml, and tried serveral other source but i encounter always the following problems :
When I do</p>
<blockquote>
<p>docker-compose up -d</p>
</blockquote>
<p>I always have an ERROR on the sql part :</p>
<ul>
<li><p>MARIADB<br />
Building database Step 1/1 : FROM mariadb:10.3<br />
10.3: Pulling from library/mariadb ERROR: Service 'database' failed to<br />
<strong>build: no matching manifest for unknown in the manifest list entries</strong></p>
</li>
<li><p>MYSQL<br />
8.0: Pulling from library/mysql<br />
<strong>ERROR: no matching manifest for unknown in the manifest list entries</strong></p>
</li>
</ul>
<p>I understand that there is no mariadb/sql on manifst...</p>
<p>Can you please tell me what goes wrong, or/and how can i setup a php-mysql-phpmyadmin on docker with an i386 under Debian?</p>
<p>EXEMPLE :<br />
The last source code i've tried is
<a href=""https://github.com/jcavat/docker-lamp"" rel=""nofollow noreferrer"">https://github.com/jcavat/docker-lamp</a></p>
<p>I also tried to use</p>
<pre><code>Example stack.yml for mariadb:

# Use root/example as user/password credentials
version: '3.1'

services:

  db:
    image: mariadb
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: example

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
</code></pre>
<p>then</p>
<pre><code>docker-compose -f stack.yml up
</code></pre>
<p>It result by the same error</p>
<pre><code>Pulling db (mariadb:)...
latest: Pulling from library/mariadb
ERROR: no matching manifest for unknown in the manifest list entries
</code></pre>
"
"52787329","PHP extension unavailable to composer container in docker-compose","<php><docker><docker-compose><composer-php>","64783325","ext-gd missing in docker container with composer","<php><docker><composer-php><gd>","<p>I use docker-compose and have a number of containers in one project: Nginx, PHP, Composer and nginx. All works well except for one thing: composer does not work. I am trying to install a composer project that uses the GD extension, which is installed in PHP (confirmed using php -m inside the PHP container). However, the composer container does not ""see"" this extension and complaints it does not exist. How can I link those two?</p>

<p>docker-compose.yml:</p>

<pre><code>version: '2'
services:
    web:
        image: nginx:1.15.1
        volumes:
            - ./.docker/conf/nginx/default.conf:/etc/nginx/conf.d/default.conf
            - ./html:/var/www/html
        ports:
            - 8888:80
        depends_on:
            - php
            - db
    php:
        build: .docker
        volumes:
            - ./.docker/conf/php/php.ini:/usr/local/etc/php/conf.d/php.ini
            - ./.docker/conf/php/xdebug.ini:/usr/local/etc/php/conf.d/xdebug.ini
            - ./html:/var/www/html
    composer:
        image: composer
        volumes:
            - ./html:/app
        command: install
        depends_on:
            - php
    db:
        image: postgres:10.4
        environment:
            - POSTGRES_DB=test
            - POSTGRES_USER=test
            - POSTGRES_PASSWORD=test
        ports:
            - 5432:5432
        volumes:
            - ./.docker/conf/postgres/:/docker-entrypoint-initdb.d/

    adminer:
        image: adminer
        ports:
        - 8080:8080
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM php:7.2-fpm

RUN apt-get update &amp;&amp; apt-get install -y \
        libfreetype6-dev \
        libjpeg62-turbo-dev \
        libmcrypt-dev \
        libpng-dev \
        libicu-dev \
        libpq-dev \
        libxpm-dev \
libvpx-dev \
    &amp;&amp; pecl install xdebug \
    &amp;&amp; docker-php-ext-enable xdebug \
    &amp;&amp; docker-php-ext-install -j$(nproc) gd \
    &amp;&amp; docker-php-ext-install -j$(nproc) intl \
    &amp;&amp; docker-php-ext-install -j$(nproc) zip \
    &amp;&amp; docker-php-ext-install -j$(nproc) pgsql \
    &amp;&amp; docker-php-ext-install -j$(nproc) pdo_pgsql \
    &amp;&amp; docker-php-ext-install -j$(nproc) exif \
    &amp;&amp; docker-php-ext-configure gd \
        --with-freetype-dir=/usr/include/ \
        --with-jpeg-dir=/usr/include/ \
        --with-xpm-dir=/usr/lib/x86_64-linux-gnu/ \
        --with-vpx-dir=/usr/lib/x86_64-linux-gnu/ \
</code></pre>

<p>Error:</p>

<pre><code>Starting test_app_db_1      ... done
Starting test_app_php_1     ... done
Starting test_app_adminer_1 ... done
Recreating test_app_composer_1 ... done
Starting test_app_web_1        ... done
Attaching to test_app_adminer_1, test_app_php_1, test_app_db_1, test_app_web_1, test_app_composer_1
php_1       | [12-Oct-2018 21:26:47] NOTICE: fpm is running, pid 1
php_1       | [12-Oct-2018 21:26:47] NOTICE: ready to handle connections
adminer_1   | PHP 7.2.1 Development Server started at Fri Oct 12 21:26:47 2018
db_1        | 2018-10-12 21:26:47.716 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
db_1        | 2018-10-12 21:26:47.716 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
db_1        | 2018-10-12 21:26:47.736 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
db_1        | 2018-10-12 21:26:47.827 UTC [21] LOG:  database system was shut down at 2018-10-12 21:11:42 UTC
db_1        | 2018-10-12 21:26:47.845 UTC [1] LOG:  database system is ready to accept connections
composer_1  | Loading composer repositories with package information
composer_1  | Updating dependencies (including require-dev)
composer_1  | Your requirements could not be resolved to an installable set of packages.
composer_1  | 
composer_1  |   Problem 1
composer_1  |     - gumlet/php-image-resize 1.9.1 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
composer_1  |     - gumlet/php-image-resize 1.9.0 requires ext-gd * -&gt; the requested PHP extension gd is missing from your system.
composer_1  |     - Installation request for gumlet/php-image-resize 1.9.* -&gt; satisfiable by gumlet/php-image-resize[1.9.0, 1.9.1].
composer_1  | 
composer_1  |   To enable extensions, verify that they are enabled in your .ini files:
composer_1  |     - 
composer_1  |     - /usr/local/etc/php/conf.d/date_timezone.ini
composer_1  |     - /usr/local/etc/php/conf.d/docker-php-ext-sodium.ini
composer_1  |     - /usr/local/etc/php/conf.d/docker-php-ext-zip.ini
composer_1  |     - /usr/local/etc/php/conf.d/memory-limit.ini
composer_1  |   You can also run `php --ini` inside terminal to see which files are used by PHP in CLI mode.
test_app_composer_1 exited with code 2
</code></pre>
","<p>ive a docker container where i installed composer. My docker-composer.yml looks like this:</p>
<pre><code>webserver:
  build: ./docker/php
  ports:
    - 80:80
  links:
    - mysql
  volumes_from:
    - app

composer:
    image: composer:2.0
    volumes_from:
      - webserver
    working_dir: /var/www/html

mysql:
    image: mysql:5.7
    environment:
      MYSQL_DATABASE: &quot;${DB_NAME}&quot;
      MYSQL_USER: &quot;${DB_USER}&quot;
      MYSQL_ROOT_PASSWORD: &quot;${DB_ROOT_PW}&quot;
      MYSQL_PASSWORD: &quot;${DB_PW}&quot;
    volumes_from:
      - data

data:
  image: mysql:5.7.31
  volumes:
    - /var/lib/mysql
  command: &quot;true&quot;
    
phpmyadmin:
  image: phpmyadmin/phpmyadmin
  ports:
    - 8080:80
  links:
    - mysql
  environment:
    PMA_HOST: mysql
  volumes:
    - ./docker/php/uploads.ini:/usr/local/etc/php/conf.d/uploads.ini

app:
  image: tianon/true
  volumes:
    - ./src/oxid:/var/www/html:delegated
</code></pre>
<p>My dockerfile:</p>
<pre><code>FROM php:7.4.8-apache

COPY php.ini /usr/local/etc/php/
COPY 000-default.conf /etc/apache2/sites-available/
COPY ./ssl/ssl.crt /etc/apache2/ssl/ssl.crt
COPY ./ssl/ssl.key /etc/apache2/ssl/ssl.key

RUN a2enmod rewrite
RUN a2enmod expires
RUN a2enmod headers


RUN apt-get update
RUN apt-get install -y zlib1g-dev libxml2-dev libfreetype6-dev libjpeg62-turbo-dev libmcrypt-dev libpng12.0 imagemagick
RUN docker-php-ext-install mysqli soap pdo_mysql bcmath pdo
RUN docker-php-ext-configure gd --with-freetype --with-jpeg
RUN docker-php-ext-install gd
RUN docker-php-ext-configure intl
RUN docker-php-ext-install intl

# Install opcache
RUN docker-php-ext-install opcache

RUN a2enmod ssl
EXPOSE 80
EXPOSE 443

# Install APCu
RUN pecl install apcu
RUN echo &quot;extension=apcu.so&quot; &gt; /usr/local/etc/php/conf.d/apcu.ini
</code></pre>
<p>Now when i open the <code>index.php</code> with <code>phpinfo();</code> it says, that GD is installed and enabled.</p>
<p>But <code>docker-compose run composer install</code> gives me this error and i'm not really understanding why.</p>
<blockquote>
<ul>
<li>shopware/core[6.3.0.0, ..., 6.3.3.1] require ext-gd * -&gt; it is missing from your system. Install or enable PHP's gd extension.</li>
</ul>
</blockquote>
<p>Can someone please tell me how i can enable GD for this composer process.</p>
<p>Thank you!</p>
"
"53276839","use docker's remote API in a secure manner","<docker><authentication><encryption><tls1.2>","64648259","How to add an authentication layer to Docker REST API?","<docker><rest><oauth-2.0><docker-compose><docker-machine>","<p>I am trying to find an effective way to use the docker remote API in a secure way.
I have a docker daemon running in a remote host, and a docker client on a different machine. I need my solution to not be client/server OS dependent, so that it would be relevant to any machine with a docker client/daemon etc. </p>

<p>So far, the only way I found to do such a thing is to create certs on a Linux machine with openssl and copy the certs to the client/server manually, as in this example:</p>

<p><a href=""https://docs.docker.com/engine/security/https/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/security/https/</a></p>

<p>and then configure docker on both sides to use the certificates for encryption and authentication.</p>

<p>This method is rather clunky in my opinion, because some times it's a problem to copy files and put them on each machine I want to use remote API from.</p>

<p>I am looking for something more elegant. </p>

<p>Another solution I've found is using a proxy for basic HTTP authentication, but in this method the traffic is not encrypted and it is not really secure that way. </p>

<p>Does anyone have a suggestion for a different solution or for a way to improve one of the above?</p>
","<p>I am trying to connect to docker daemon through a tcp port using REST API remotely. Currently anyone can access it through the endpoint. How can I make it secured with Oauth or ID/Password?</p>
"
"53509236","Mongo authentication inside Docker","<mongodb><docker><docker-compose>","64734574","Docker-compose caches my password for mongodb","<mongodb><docker><caching><docker-compose>","<p>I am trying to run the mongo docker image with authentication. Following the most simple example from the <a href=""https://docs.docker.com/samples/library/mongo/#start-a-mongo-server-instance"" rel=""noreferrer"">documentation</a> I ran the mongo and the mongo-express images by the <code>docker-compose up</code> command. My <code>docker-compose.yml</code> at this stage:</p>

<pre><code>version: '3.1'

services:

  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example

  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
</code></pre>

<p>This runs, both containers start ok and I can browse the contents of mongo from the mongo-express website. However, whenever I change the username or the password in the <code>docker-compose.yml</code> file, for example to this:</p>

<pre><code>version: '3.1'

services:

  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example123

  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example123
</code></pre>

<p>the mongo-express throws an unauthotrized error message:</p>

<pre><code>mongo-express_1  | Admin Database connected
mongo-express_1  | { MongoError: Authentication failed.
mongo-express_1  |     at Function.MongoError.create (/node_modules/mongodb-core/lib/error.js:31:11)
mongo-express_1  |     at /node_modules/mongodb-core/lib/connection/pool.js:483:72
mongo-express_1  |     at authenticateStragglers (/node_modules/mongodb-core/lib/connection/pool.js:429:16)
mongo-express_1  |     at Connection.messageHandler (/node_modules/mongodb-core/lib/connection/pool.js:463:5)
mongo-express_1  |     at Socket.&lt;anonymous&gt; (/node_modules/mongodb-core/lib/connection/connection.js:319:22)
mongo-express_1  |     at emitOne (events.js:116:13)
mongo-express_1  |     at Socket.emit (events.js:211:7)
mongo-express_1  |     at addChunk (_stream_readable.js:263:12)
mongo-express_1  |     at readableAddChunk (_stream_readable.js:250:11)
mongo-express_1  |     at Socket.Readable.push (_stream_readable.js:208:10)
mongo-express_1  |   name: 'MongoError',
mongo-express_1  |   message: 'Authentication failed.',
mongo-express_1  |   ok: 0,
mongo-express_1  |   errmsg: 'Authentication failed.',
mongo-express_1  |   code: 18,
mongo-express_1  |   codeName: 'AuthenticationFailed' }
mongo-express_1  | unable to list databases
mongo-express_1  | { MongoError: command listDatabases requires authentication
mongo-express_1  |     at Function.MongoError.create (/node_modules/mongodb-core/lib/error.js:31:11)
mongo-express_1  |     at /node_modules/mongodb-core/lib/connection/pool.js:483:72
mongo-express_1  |     at authenticateStragglers (/node_modules/mongodb-core/lib/connection/pool.js:429:16)
mongo-express_1  |     at Connection.messageHandler (/node_modules/mongodb-core/lib/connection/pool.js:463:5)
mongo-express_1  |     at Socket.&lt;anonymous&gt; (/node_modules/mongodb-core/lib/connection/connection.js:319:22)
mongo-express_1  |     at emitOne (events.js:116:13)
mongo-express_1  |     at Socket.emit (events.js:211:7)
mongo-express_1  |     at addChunk (_stream_readable.js:263:12)
mongo-express_1  |     at readableAddChunk (_stream_readable.js:250:11)
mongo-express_1  |     at Socket.Readable.push (_stream_readable.js:208:10)
mongo-express_1  |   name: 'MongoError',
mongo-express_1  |   message: 'command listDatabases requires authentication',
mongo-express_1  |   ok: 0,
mongo-express_1  |   errmsg: 'command listDatabases requires authentication',
mongo-express_1  |   code: 13,
mongo-express_1  |   codeName: 'Unauthorized' }
</code></pre>

<p>No matter what username or password I enter in <code>docker-compose.yml</code>, I cannot make mongo-express connect to mongo, only if I use the original <code>root</code> and <code>example</code> pair. </p>

<p>Note, that I am not getting the username and password as environment variables, but they are directly typed into the <code>docker-compose.yml</code> file as you can see here.</p>

<p>Also note, that when I change the <code>MONGO_INITDB_ROOT_USERNAME</code> and <code>MONGO_INITDB_ROOT_PASSWORD</code> (mongo's) variables to anything, they don't seem to have an effect, I can still connect with mongo-express using the original root and example credentials.</p>

<p>What causes this behaviour? How can I make this work? </p>
","<p>I'm trying to setup my mongodb and with docker. I created a docker-compose file</p>
<pre><code>services:
  mongo:
    image: mongo:latest
    container_name: mongo
    ports: [&quot;27017:27017&quot;]
    env_file: [&quot;.env&quot;]
    environment:
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    networks: [&quot;some_network&quot;]
    volumes: [./mongo-volume:/data/db]

volumes: 
  mongo-volume:

networks:
  some_network:
    driver: bridge
</code></pre>
<p>In my .env I did define my variables:</p>
<pre><code>MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=hMSZ7!dM!@HA&amp;EF6
</code></pre>
<p>Because I'd like to use MongoCompass and had some problems with the '@' I had to change the password, simply removed the '@'.</p>
<p>But I still can not got a connection. Instead I try to use mongo-client and used the old password, well it worked. But I'll like to change my password. I already used docker-compose down and remove the volume and the image. But it is still cached somewhere. Also I logged in to the container and echo the password, this will be the new one.</p>
<p>Additional information:
I use docker on windows with wsl (Ubuntu)
With this I watched through the directory /var/lib/docker/volumes/ what I found was nothing. I guess. on wsl the volumes are mount somewhere else...
So I changed the mount point, and my new password would be accepted.
But where are my volume mounted?</p>
"
"53539807","Why docker in docker (dind) containers mount volumes with host path?","<docker><dind>","64605789","Docker inside Docker via -v /var/run/docker.sock:/var/run/docker.sock and volume inside the Docker","<docker><docker-compose>","<p>I have a setup with docker in docker and try to mount folders.</p>

<p>Let's say I have those folders that I wish to share with his parent. On the host, I created a file in /tmp/dind called <code>foo</code>. Host starts container 1, which starts container 2. This is the result I want to have.</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;     &lt;-------&gt;
</code></pre>

<p>Instead, I get</p>

<pre><code>Host      | Container 1 | Container 2

/tmp/dind |  /tmp/dind2 | /tmp/dind3
      &lt;-------&gt;
      &lt;-----------------------&gt;
</code></pre>

<p>Code here:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind2:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>This outputs nothing, while the next command gives foo as result. I changed the mounted volume:</p>

<pre><code>docker run --rm -it \
  -v /tmp/dind:/tmp/dind2 \
  -v /var/run/docker.sock:/var/run/docker.sock docker sh -c \
    ""docker run --rm -it \
      -v /tmp/dind:/tmp/dind3 \
      -v /var/run/docker.sock:/var/run/docker.sock \
      docker ls /tmp/dind3""
</code></pre>

<p>The question is, what do I need to do in order to use Container 1 path and not host? Or do I misunderstand something about docker here?</p>
","<p>I have a docker container which image is based on <code>docker/compose</code> which I run with an option <code>-v /var/run/docker.sock:/var/run/docker.sock</code> on a Ubuntu 18.04 docker host. Inside that docker image I start <code>docker-compose up --abort-on-container-exit --build</code>. In my docker-compose.yml one of the services is specified this way</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3'
services:
  test:
    build: .
    depends_on:
      - hub
      - chrome
    volumes:
      - './TestResults:/tests/TestResults'      
    networks:
      - selenium-test 
</code></pre>
<p>What happens is that docker image runs the tests and test results from the run go into <code>/tests/TestResults/results.trx</code> file. When I was running <code>docker-compose up --abort-on-container-exit --build</code> on a Ubuntu 18.04 docker host all was working perfectly fine, after tests completed I have tests results in <code>TestResults/results.trx</code> on a host, because volume worked fine.
Now I need to run <code>docker-compose up --abort-on-container-exit --build</code> in Azure DevOps agent which is runnning inside the docker container on the same Ubuntu 18.04 docker host. All works fine, but I have a problem with volume which is in docker-compose specified:</p>
<pre class=""lang-yaml prettyprint-override""><code>    volumes:
      - './TestResults:/tests/TestResults'      
</code></pre>
<p>As I specify <code>-v /var/run/docker.sock:/var/run/docker.sock</code> when I run agent in a docker, so all docker containers which created inside of a agent container in fact are created on docker host of a host machine, but not as docker inside of a docker. The problem appears with volume. After <code>docker-compose up --abort-on-container-exit --build</code> finishes its work agent which runs in a docker wants to get test results from <code>./TestResults</code> folder, but for some reason nothing is in there as it seems it mounted host machine folder and I found the results on a host machine in a folder <code>/azp/agent/_work/1/s/TestResults</code>, while agent which runs in docker needs those result inside the agent container in the same path <code>/azp/agent/_work/1/s/TestResults</code>.</p>
<p>What do I do wrong and how do I fix that a container which I start inside of a container works with volumes correctly.</p>
"
"53559545","Docker unknown shorthand flag: 'a' in -aq)","<docker>","64504751","What is wrong with the Docker command to remove all containers?","<docker><command-prompt><cmder>","<p>I have multiple docker (version 18.09.0, build 4d60db4) containers running and I wish to stop them all at once. <a href=""http://blog.baudson.de/blog/stop-and-remove-all-docker-containers-and-images"" rel=""noreferrer"">This blog post</a> shows concisely exactly how to achieve this, great! </p>

<p>I can list all containers using <code>docker ps -aq</code> and have no issues. </p>

<p>However, when trying to stop all containers using the output of <code>docker ps -aq</code>, i.e. <code>docker stop $(docker ps -aq)</code>, I receive the following error:</p>

<blockquote>
  <p><code>unknown shorthand flag: 'a' in -aq)</code></p>
</blockquote>

<p><strong>EDIT:</strong> I'm running Windows 10 Version 10.0.17134.407 in a standard (elevated) command prompt.</p>

<p>Can anyone shed any insight into this?</p>

<p>Thanks.</p>
","<p>I tried removing all the container that have stopped in Docker using</p>
<pre><code>docker rm $(docker ps -aq)
</code></pre>
<p>It gives an error saying <em>unknown shorthand flag: 'a' in -aq)</em></p>
<p>Using <code>docker rm Container_id</code> works correctly .</p>
<p>Is there anything else I should use?</p>
<p>I am using Docker version :</p>
<p>Client: Docker Engine - Community
Cloud integration  0.1.18
Version:           19.03.13
API version:       1.40
Go version:        go1.13.15
Git commit:        4484c46d9d
Built:             Wed Sep 16 17:00:27 2020
OS/Arch:           windows/amd64
Experimental:      false</p>
<p>Server: Docker Engine - Community
Engine:
Version:          19.03.13
API version:      1.40 (minimum version 1.12)
Go version:       go1.13.15
Git commit:       4484c46d9d
Built:            Wed Sep 16 17:07:04 2020
OS/Arch:          linux/amd64
Experimental:     false
containerd:
Version:          v1.3.7
GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175
runc:
Version:          1.0.0-rc10
GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
docker-init:
Version:          0.18.0
GitCommit:        fec3683</p>
<p>I use <strong>cmder</strong> mainly but the problem persists in <strong>windows command prompt</strong> too.</p>
<p>EDIT:
I tried most of the methods.</p>
<p>First -</p>
<pre><code>λ docker rm $(docker ps -aq)
unknown shorthand flag: 'a' in -aq)
See 'docker rm --help'.
</code></pre>
<p>Second -</p>
<pre><code>λ docker rm $(docker ps -a -q)
unknown shorthand flag: 'a' in -a
See 'docker rm --help'.
</code></pre>
"
"58879058","Remove containers by image name","<bash><docker><unix>","64797449","docker remove containers by image","<docker>","<p>I mistakenly created a bunch of containers which I now want to remove. I can list them with:</p>

<pre><code>docker container ls -aq -f ""ancestor=portainer/portainer""
</code></pre>

<p>How can I ""pipe"" these container IDs to <code>docker container rm</code>?</p>

<p>What doesn't work:</p>

<ul>
<li><code>docker ls -aq -f ""ancestor=portainer/portainer"" | docker container rm</code></li>
<li><code>docker container rm $(docker ls -aq -f ""ancestor=portainer/portainer"")</code></li>
<li><code>docker rm `docker ls -aq -f ""ancestor=portainer/portainer""`</code></li>
</ul>
","<p>I need the smallest code possible to remove all containers that were run from a specific image. For example, to run the container:</p>
<pre><code>docker run -p 8888:80 fakeuser/my-image-name
</code></pre>
<p>And to remove the container (or containers):</p>
<pre><code>docker rm -f &quot;all containers that were run from the image fakeuser/my-image-name&quot;
</code></pre>
<p>Is this possible? If so, whats the syntax? A one liner would be great if possible.</p>
<p>Thanks in advance</p>
"
"59035543","How to execute command from one docker container to another","<docker><docker-container><docker-networking><docker-command>","64893034","Calling code in one docker container from another, using docker-compose (without using ports)?","<docker><docker-compose>","<p>I'm creating an application that will allow users to upload video files that will then be put through some processing.</p>

<p>I have two containers.</p>

<ol>
<li><code>Nginx</code> container that serves the website where users can upload their video files. </li>
<li>Video processing container that has <code>FFmpeg</code> and some other processing stuff installed.</li>
</ol>

<p>What I want to achieve. I need container 1 to be able to run a bash script on container 2.</p>

<p>One possibility as far as I can see is to make them communicate over HTTP via an API. But then I would need to install a web server in container 2 and write an API which seems a bit overkill. 
I just want to execute a bash script. </p>

<p>Any suggestions?  </p>
","<p>Trying to get one docker-compose service to run a shell script that's in another container. Is there a way of doing this without having to install docker in docker?</p>
<p>In the example below, ideally I'd like to be able to run something like <code>shell-svc /bin/bash helloworld.sh</code> from <code>helloworld.py</code> in the <code>mgr-svc</code>  python container.</p>
<p>I can run <code>helloworld.sh</code> independently using docker-compose like this:</p>
<pre class=""lang-sh prettyprint-override""><code>docker-compose run shell-svc /bin/bash helloworld.sh
</code></pre>
<p>and <code>helloworld.py</code> using</p>
<pre><code>docker-compose run mgr-svc python helloworld.py
</code></pre>
<p>Example files:</p>
<p><code>docker-compose.yml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>services :

  shell-svc:
    build:
      context: .
      dockerfile: Dockerfile-shell
    image: shell-svc:1.0
    networks:
      - test-net
    expose:
      - &quot;22&quot;
      - &quot;8088&quot;
      - &quot;50070&quot;     

  mgr-svc:
    build:
       context: .
       dockerfile: Dockerfile-python
    image: mrg-svc:1.0
    networks:
      - test-net

networks:
 test-net:
version: &quot;3.5&quot;

</code></pre>
<p><code>Dockerfile-python</code></p>
<pre><code>FROM python:3.7
RUN pip install paramiko

COPY helloworld.py helloworld.py
</code></pre>
<p><code>Dockerfile-shell</code></p>
<pre><code>FROM debian:stretch-slim

USER root

RUN apt-get update &amp;&amp; \
    apt-get install -y openssh-server


RUN eval `ssh-agent -s` &amp;&amp; \
    mkdir ~/.ssh &amp;&amp; \
    echo &quot;StrictHostKeyChecking no&quot; &gt;&gt; /etc/ssh/ssh_config &amp;&amp; \
    cat /etc/ssh/ssh_config &amp;&amp; \
    chmod go-w /root &amp;&amp; \
    chmod 700 /root/.ssh 


EXPOSE 22 8088 50070

COPY helloworld.sh ./helloworld.sh

CMD service ssh start &amp;&amp; while true; do sleep 3000; done
</code></pre>
<p><code>helloworld.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import paramiko

if __name__ == '__main__':
    print (&quot;hello from python&quot;)

    ssh = paramiko.SSHClient()
    ssh.connect(&quot;shell-svc&quot;, username=&quot;root&quot;, password=&quot;password&quot;)
    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(helloworld.sh)
</code></pre>
<p><code>helloworld.sh</code></p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh

echo &quot;hello from shell&quot;
</code></pre>
"
"60183313","""The connection was reset"" in localhost:8000 using django and docker","<django><docker><docker-compose>","64841205","Django not running on localhost when using with Docker","<javascript><python><django><docker><web>","<p>After running <code>docker-compose up</code>,</p>
<blockquote>
<p>Starting docker_django ... done</p>
<p>Attaching to docker_django</p>
<p>docker_django | Watching for file changes with StatReloader</p>
<p>docker_django | Performing system checks...</p>
<p>docker_django | System check identified no issues (0 silenced).</p>
<p>docker_django | February 12, 2020 - 07:26:35</p>
<p>docker_django | Django version 3.0.3, using settings 'backend.settings'</p>
<p>docker_django | Starting development server at <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a></p>
<p>docker_django | Quit the server with CONTROL-C.</p>
</blockquote>
<p>when i connect to <a href=""http://127.0.0.1:8000/"" rel=""nofollow noreferrer"">http://127.0.0.1:8000/</a> it shows this
<a href=""https://i.stack.imgur.com/7Jidk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Jidk.png"" alt=""enter image description here"" /></a>
There is no problem / error running the docker-compose command but only when visiting the site.</p>
<p>Im using ubuntu 19.10 and the project has django v3.</p>
<p>Dockerfile</p>
<pre><code>FROM python:3

ENV PYTHONUNBUFFERED 1

RUN mkdir /code
WORKDIR /code

COPY . /code/

RUN pip install -r requirements.txt

</code></pre>
<p>docker-compose.yml</p>
<pre><code>version: '3'

services:
    web:
        build: .
        container_name: docker_django
        command: python manage.py runserver
        volumes:
            - .:/code
        ports:
            - &quot;8000:8000&quot;
</code></pre>
","<p>I am a beginner to web development with Django. I have been trying to use docker-compose as a part of the lectures but due to some issues, I am not able to run the server.</p>
<p>This is my docker-compose.yml file:</p>
<pre><code>version: '3'



services:

db:

image: postgres

environment:

- POSTGRES_DB=postgres

- POSTGRES_USER=postgres

- POSTGRES_PASSWORD=1029384756Vithayathil


ports:

- &quot;5432:5432&quot;

migration:

build: .

command: python3 manage.py migrate

volumes:

- .:/usr/src/app

depends_on:

- db

web:

build: .

command: python3 manage.py runserver

volumes:

- .:/usr/src/app

ports:

- &quot;8000:8000&quot;

depends_on:

- db

- migration
</code></pre>
<p>This is my DockerFile:</p>
<pre><code>FROM python:3

WORKDIR /usr/src/app

ADD requirements.txt /usr/src/app

RUN pip install -r requirements.txt

ADD . /usr/src/app
</code></pre>
<p>This is my database in settings.py:</p>
<pre><code>DATABASES = {

'default': {

'ENGINE': 'django.db.backends.postgresql',

'NAME': 'postgres',

'USER': 'postgres',

'HOST': 'db',

'PASSWORD':'**********',

'PORT': 5432,

}

}
</code></pre>
<p>On doing docker-compose up command, these things happen:</p>
<pre><code>Starting airline4_db_1 ... done Starting airline4_migration_1 ... done Starting airline4_web_1 ... done Attaching to airline4_db_1, airline4_migration_1, airline4_web_1

db_1 |

db_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization

db_1 |

db_1 | 2020-11-07 15:37:00.770 UTC [1] LOG: starting PostgreSQL 13.0 (Debian 13.0-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit

db_1 | 2020-11-07 15:37:00.789 UTC [1] LOG: listening on IPv4 address &quot;0.0.0.0&quot;, port 5432

db_1 | 2020-11-07 15:37:00.790 UTC [1] LOG: listening on IPv6 address &quot;::&quot;, port 5432

db_1 | 2020-11-07 15:37:01.179 UTC [1] LOG: listening on Unix socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;

db_1 | 2020-11-07 15:37:01.647 UTC [26] LOG: database system was shut down at 2020-11-07 10:55:06 UTC

db_1 | 2020-11-07 15:37:02.449 UTC [1] LOG: database system is ready to accept connections

web_1 | Watching for file changes with StatReloader

migration_1 | Operations to perform:

migration_1 | Apply all migrations: admin, auth, contenttypes, flights, sessions

migration_1 | Running migrations:

migration_1 | No migrations to apply.

airline4_migration_1 exited with code 0
</code></pre>
<p>But when I try going to 127.0.0.1:8000 on my browser, I see a message, 'This page isn't working'.</p>
<p>Any help will be great! Thanks in advance</p>
<pre><code>When I tried 'docker-compose ps' command, it says like this

    Name Command State Ports

---------------------------------------------------------------------

airline4_db_1 docker-entrypoint.sh postgres Exit 0

airline4_migration_1 python3 manage.py migrate Exit 0

airline4_web_1 python3 manage.py runserver Exit 0

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
88d8655af03a        airline4_web        &quot;python3 manage.py r…&quot;   24 seconds ago      Up 18 seconds       0.0.0.0:8000-&gt;8000/tcp   airline4_web_1
bec29de59ee1        postgres            &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 32 seconds       5432/tcp                 airline4_db_1
</code></pre>
<p>This is what I get now on docker ps command</p>
"
"60438128","Swarm mode routing mesh not working, instead is working like host mode by default","<docker><docker-swarm><docker-swarm-mode>","64372556","Docker Swarm Overlay Network - Can't Ping Other Hosts","<docker><networking><docker-swarm><docker-networking>","<p><strong>Description</strong></p>

<p>Swarm mode routing mesh not working, instead, it is working like using host mode by default.</p>

<p>We were deploying a swarm of 3 masters nodes and 8 worker nodes, each of them in a different instance of a cloud service <strong>OpenStack</strong> using Terraform and Ansible. The swarm and routing mesh was working perfectly since it stopped working and started working like in a host mode. We didn't change anything nor done any update or deploy new services. We tried to restart the swarm and re-deploy the swarm and all services, but nothing worked, we couldn't make it work in routing mesh mode again. So, we decided to destroy all instances and start from scratch (the issue reported below). We did a clean installation of <strong>Ubuntu 18.04 LTS</strong> and docker as we did before. Then we set 1 master node and 2 workers (this time manually) and deploy one service, but the swarm is still working like in host mode.</p>

<p>The only way to access the services is by the IP address of the node where it is running, otherwise, there is no answer (time out). We tried to access using the IP of the manager or the other worker instances, but it is not possible to access to the service. That is why we supposed that the swarm is using host mode by default instead of the ingress network and routing mesh.</p>

<p>We also tried with different services like Mongo or Cassandra but the behaviour is the same, the swarm looks like working using host mode. You can only access the service by using the instance IP address where the service is running.</p>

<p>Any ideas to how to bypass the host most and go back to the routing mesh?
We want to access to any service only by using the IP address of the manager nodes which are supossed to be in Drain mode.</p>

<p><strong>Steps to reproduce the issue:</strong></p>

<ol>
<li>[<strong>manager</strong>] <code>sudo docker swarm init --advertise-addr 158.39.201.14</code></li>
<li>[<strong>worker-0</strong>] <code>sudo docker swarm join --token SWMTKN-1-3np0cy0msnfurecckl4863hkftykuqkgeq998s1hix6jsoiarq-758o52hyma
iyzv74w3u1yzltt 158.39.201.14:2377</code></li>
<li>[<strong>worker-1</strong>] <code>sudo docker swarm join --token SWMTKN-1-3np0cy0msnfurecckl4863hkftykuqkgeq998s1hix6jsoiarq-758o52hyma
iyzv74w3u1yzltt 158.39.201.14:2377</code></li>
<li>[<strong>manger</strong>] sudo docker stack deploy -c docker-compose.yml nh</li>
</ol>

<p><strong>Describe the results you received:</strong></p>

<p>curl <a href=""http://[worker-0-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-0-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p>curl <a href=""http://[worker-1-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-1-ip]:8089/bigdata</a> <strong>FAIL TIME OUT</strong></p>

<p><strong>Describe the results you expected:</strong></p>

<p>curl <a href=""http://[worker-0-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-0-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p>curl <a href=""http://[worker-1-ip]:8089/bigdata"" rel=""nofollow noreferrer"">http://[worker-1-ip]:8089/bigdata</a> <strong>200 OK</strong></p>

<p><strong>Additional information you deem important (e.g. issue happens only occasionally):</strong></p>

<p>This issue was not happening 2 days ago and suddently it started happening. We didn't made any modification nor touch the servers.</p>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3.7'

networks:
  news-hunter:
    name: &amp;network news-hunter

x-network: &amp;network-base
  networks:
    - *network

services:
   blazegraph:
    &lt;&lt;: *network-base
    image: lyrasis/blazegraph:2.1.5
    ports:
      - published: 8089
        target: 8080
    deploy:
      placement:
        constraints:
          - node.role == worker 
</code></pre>

<p><strong>IP tables of manager, worker-1 and worker-2</strong> (all are the same): <code>sudo iptables -L</code></p>

<pre><code>Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy DROP)
target     prot opt source               destination
DOCKER-USER  all  --  anywhere             anywhere
DOCKER-INGRESS  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
DROP       all  --  anywhere             anywhere

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain DOCKER (2 references)
target     prot opt source               destination

Chain DOCKER-INGRESS (1 references)
target     prot opt source               destination
ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:8089
ACCEPT     tcp  --  anywhere             anywhere             state RELATED,ESTABLISHED tcp spt:8089
RETURN     all  --  anywhere             anywhere

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
target     prot opt source               destination
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-ISOLATION-STAGE-2 (2 references)
target     prot opt source               destination
DROP       all  --  anywhere             anywhere
DROP       all  --  anywhere             anywhere
RETURN     all  --  anywhere             anywhere

Chain DOCKER-USER (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
</code></pre>

<p><strong>Manager ports</strong>: <code>sudo netstat -tuplen</code></p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        46731      14980/systemd-resol
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          17752      865/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      0          17757      865/sshd
tcp6       0      0 :::8089                 :::*                    LISTEN      0          306971     24992/dockerd
tcp6       0      0 :::2377                 :::*                    LISTEN      0          301970     24992/dockerd
tcp6       0      0 :::7946                 :::*                    LISTEN      0          301986     24992/dockerd
udp        0      0 127.0.0.53:53           0.0.0.0:*                           101        46730      14980/systemd-resol
udp        0      0 158.39.201.14:68        0.0.0.0:*                           100        46591      14964/systemd-netwo
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          302125     -
udp6       0      0 fe80::f816:3eff:fef:546 :::*                                100        46586      14964/systemd-netwo
udp6       0      0 :::7946                 :::*                                0          301987     24992/dockerd
</code></pre>

<p><strong>Worker ports</strong>: <code>sudo netstat -tuplen</code></p>

<pre><code>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        44998      15283/systemd-resol
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          15724      1010/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      0          15726      1010/sshd
tcp6       0      0 :::8089                 :::*                    LISTEN      0          300227     25355/dockerd
tcp6       0      0 :::7946                 :::*                    LISTEN      0          283636     25355/dockerd
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          285465     -
udp        0      0 127.0.0.53:53           0.0.0.0:*                           101        44997      15283/systemd-resol
udp        0      0 158.39.201.15:68        0.0.0.0:*                           100        233705     15247/systemd-netwo
udp6       0      0 :::7946                 :::*                                0          283637     25355/dockerd
udp6       0      0 fe80::f816:3eff:fee:546 :::*                                100        48229      15247/systemd-netwo
</code></pre>

<p><strong>Services running</strong>: <code>sudo docker service ls</code></p>

<pre><code>ID                  NAME                MODE                REPLICAS            IMAGE                      PORTS
m7eha88ff4wm        nh_blazegraph       replicated          1/1                 lyrasis/blazegraph:2.1.5   *:8089-&gt;8080/tcp
</code></pre>

<p><strong>Stack</strong>: <code>sudo docker stack ps nh</code></p>

<pre><code>ID                  NAME                IMAGE                      NODE                DESIRED STATE       CURRENT STATE         ERROR               PORTS
tqkd9t4i03yt        nh_blazegraph.1     lyrasis/blazegraph:2.1.5   nh-worker-0         Running             Running 3 hours ago
</code></pre>

<p><strong>Output of <code>docker version</code>:</strong></p>

<pre><code>Client: Docker Engine - Community
 Version:           19.03.6
 API version:       1.40
 Go version:        go1.12.16
 Git commit:        369ce74a3c
 Built:             Thu Feb 13 01:27:49 2020
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.6
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.16
  Git commit:       369ce74a3c
  Built:            Thu Feb 13 01:26:21 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.2.10
  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:          1.0.0-rc8+dev
  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
</code></pre>

<p><strong>Output of <code>docker info</code>:</strong></p>

<pre><code>Client:
 Debug Mode: false

Server:
 Containers: 1
  Running: 0
  Paused: 0
  Stopped: 1
 Images: 1
 Server Version: 19.03.6
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Native Overlay Diff: true
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: active
  NodeID: hpcm67vxrmkm1wvlhfqbjevox
  Is Manager: true
  ClusterID: gnl96swlf7o3a976oarvjrazt
  Managers: 1
  Nodes: 3
  Default Address Pool: 10.0.0.0/8
  SubnetSize: 24
  Data Path Port: 4789
  Orchestration:
   Task History Retention Limit: 5
  Raft:
   Snapshot Interval: 10000
   Number of Old Snapshots to Retain: 0
   Heartbeat Tick: 1
   Election Tick: 10
  Dispatcher:
   Heartbeat Period: 5 seconds
  CA Configuration:
   Expiry Duration: 3 months
   Force Rotate: 0
  Autolock Managers: false
  Root Rotation In Progress: false
  Node Address: 158.39.201.14
  Manager Addresses:
   158.39.201.14:2377
 Runtimes: runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339
 runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657
 init version: fec3683
 Security Options:
  apparmor
  seccomp
   Profile: default
 Kernel Version: 4.15.0-74-generic
 Operating System: Ubuntu 18.04.4 LTS
 OSType: linux
 Architecture: x86_64
 CPUs: 1
 Total Memory: 3.852GiB
 Name: nh-manager-0
 ID: PHBO:E6UZ:RNJL:5LVU:OZXW:FM5M:GQVW:SCAQ:EEQW:7IIW:GARL:AUHI
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false
</code></pre>

<p><strong>Service inspect</strong>:<code>sudo docker service inspect --pretty nh_blazegraph</code></p>

<pre><code>ID:             ef9s5lesysovh5x2653qc6dk9
Name:           nh_blazegraph
Labels:
 com.docker.stack.image=lyrasis/blazegraph:2.1.5
 com.docker.stack.namespace=nh
Service Mode:   Replicated
 Replicas:      1
Placement:
 Constraints:   [node.role == worker]
UpdateConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:         lyrasis/blazegraph:2.1.5@sha256:e9fb46c9d7b2fc23202945a3d71b99ad8df2d7a18dcbcccc04cfc4f791b569e9
Resources:
Networks: news-hunter
Endpoint Mode:  vip
Ports:
 PublishedPort = 8089
  Protocol = tcp
  TargetPort = 8080
  PublishMode = ingress
</code></pre>

<p><strong>Additional environment details (AWS, VirtualBox, physical, etc.):</strong></p>

<p>We are working on a OpenStack IaaS cloud provider.
Out workload can expect more than 1000 http request per minute from external sources and more than 5000 requests between nodes.</p>

<p><em>Cross-posted:</em></p>

<p><a href=""https://forums.docker.com/t/swarm-mode-routing-mesh-not-working-instead-is-using-host-mode-by-default/89731"" rel=""nofollow noreferrer"">https://forums.docker.com/t/swarm-mode-routing-mesh-not-working-instead-is-using-host-mode-by-default/89731</a>
<a href=""https://github.com/moby/moby/issues/40590"" rel=""nofollow noreferrer"">https://github.com/moby/moby/issues/40590</a></p>
","<p>I have a docker swarm overlay network with 2 nodes, my localhost and a VM on my PC.</p>
<p><code>docker network list</code> output:</p>
<pre><code>NETWORK ID          NAME                   DRIVER              SCOPE
26ca272f0118        bridge                 bridge              local
48f94e0d47b8        docker_gwbridge        bridge              local
6ffc94e5474c        host                   host                local
o4ydpduwyfc9        ingress                overlay             swarm
3a177ebcdaae        none                   null                local
a3807498a383        source_default         bridge              local
igtysi5waw6s        test-overlay-network   overlay             swarm
</code></pre>
<p><code>docker node list</code> output:</p>
<pre><code>ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGE  STATUS      ENGINE VERSION
qex25m1yhjm28ojb18yrd3my3 *   node1              Ready               Active              Leader              19.03.13
64lsy02tvk2h6gqjdhejwvcvp     node2              Ready               Active              Reachable           19.03.13
</code></pre>
<p>Outputs are same for both hosts. I have 2 containers running on localhost and 2 containers running on the VM. What I am trying to do is to ping one of them from another.</p>
<p><code>netstat -tuplen</code> command shows me that following ports which are necessary for docker, are running for both hosts.</p>
<pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name    
tcp6       0      0 :::2377                 :::*                    LISTEN      0          616456     -                   
tcp6       0      0 :::7946                 :::*                    LISTEN      0          612282     -                   
udp        0      0 0.0.0.0:4789            0.0.0.0:*                           0          613172     -                   
udp6       0      0 :::7946                 :::*                                0          612283 
</code></pre>
<p>So I run a <code>ifconfig</code> in all the containers and get their ips, then try to ping each other. List of IPs on eth1 interface:</p>
<ul>
<li>10.0.3.102 -&gt; localhost</li>
<li>10.0.3.105 -&gt; localhost</li>
<li>10.0.3.107 -&gt; VM</li>
<li>10.0.3.110 -&gt; VM</li>
</ul>
<p>I can ping VM from the localhost, and also containers running on the same host. But I can not ping the containers from different hosts. What can be the reason?</p>
"
"61234588","Docker: Multistage builds result in multiple images","<docker><multistage>","64524104","Docker multi-stage building saves intermediate image","<docker>","<p>Given this small example of a multistage build</p>

<pre><code>FROM node:10 AS ui-build
WORKDIR /usr/src/app

FROM node:10 AS server-build
WORKDIR /root/

EXPOSE 3070

ENTRYPOINT [""node""]
CMD [""index.js""]
</code></pre>

<p>why does this result in 3 images on my local file system?</p>

<pre><code>""&lt;none&gt;"";""&lt;none&gt;"";""58d63982fbef"";""2020-04-15 17:53:14"";""912MB""
""node"";""10"";""bd83fcefc19d"";""2020-04-14 01:32:21"";""912MB""
""test"";""latest"";""3913dd4d03b6"";""2020-04-15 17:53:15"";""912MB""
</code></pre>

<p>I expected two images, the base image and the server-build image. I used the standard docker build command, i.e. </p>

<pre><code>docker build -t test . 
</code></pre>

<p>so which of the parts of the image is none and which is test?</p>

<p>I am confused</p>
","<p>I've recently started to learn Docker and have been trying to learn how to use multi-stage build.
The documentation here: <a href=""https://docs.docker.com/develop/develop-images/multistage-build/"" rel=""nofollow noreferrer"">https://docs.docker.com/develop/develop-images/multistage-build/</a>
Says</p>
<blockquote>
<p>The end result is the same tiny production image as before, with a
significant reduction in complexity. You don’t need to create any
intermediate images and you don’t need to extract any artifacts to
your local system at all.</p>
</blockquote>
<p>There's also an example I don't think is necessary to copy here, however I've also started a small Spring Boot project with Gradle that I want to containerize. Here's my Dockerfile:</p>
<pre><code># using multi-stage to avoid manual ./gradlew build
FROM openjdk:11 as build

COPY gradle gradle
COPY build.gradle build.gradle
COPY gradlew gradlew
COPY gradlew.bat gradlew.bat
COPY settings.gradle settings.gradle
COPY src src

RUN ./gradlew build

# final image
FROM openjdk:11
WORKDIR /service
COPY --from=build /build/libs/*.jar /service/app.jar
ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/service/app.jar&quot;]
</code></pre>
<p>This is <code>docker images</code> command output before the build:</p>
<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
openjdk             11                  5c6e71a989bc        12 days ago         627MB
</code></pre>
<p>Build command: <code>docker build . -t sdemo:1.2.1</code><br/>
And the <code>docker images</code> after it's finished:</p>
<pre><code>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
sdemo               1.2.1               24ea50770b2a        13 seconds ago      672MB
&lt;none&gt;              &lt;none&gt;              a1f8e74e1d81        20 seconds ago      960MB
openjdk             11                  5c6e71a989bc        12 days ago         627MB
</code></pre>
<p>As you can clearly see, I've got 2 images more instead of one, and the unnamed one is of ridiculous size of 960MB. What is causing this problem? Also, if it's relevant, I'm on a virtual box machine running Ubuntu.</p>
<p>The whole project is located under <a href=""https://github.com/vincent2704/demoSpringBootApp"" rel=""nofollow noreferrer"">https://github.com/vincent2704/demoSpringBootApp</a>, version 1.2.1 at the moment of posting this question.</p>
"
"62260186","pip install google-cloud-pubsub fails install in docker container","<python><docker><google-cloud-pubsub>","64461910","While running Dockerfile , python: can't open file 'setup.py': [Errno 2] No such file or directory","<python><docker><flask>","<p>I am trying to use a pupsub emulator. It starts but when I try to use my python script I get the following error</p>

<pre><code>ModuleNotFoundError: No module named 'google'
</code></pre>

<p>So i try to install the module.</p>

<pre><code>RUN pip install google-cloud-pubsub
</code></pre>

<p>error</p>

<pre><code>ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3.6 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-2hyoy1ly/grpcio/setup.py'""'""'; __file__='""'""'/tmp/pip-install-2hyoy1ly/grpcio/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-m25l52fe
         cwd: /tmp/pip-install-2hyoy1ly/grpcio/
    Complete output (11 lines):
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-2hyoy1ly/grpcio/setup.py"", line 191, in &lt;module&gt;
        if check_linker_need_libatomic():
      File ""/tmp/pip-install-2hyoy1ly/grpcio/setup.py"", line 152, in check_linker_need_libatomic
        stderr=PIPE)
      File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__
        restore_signals, start_new_session)
      File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'cc': 'cc'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
ERROR: Service 'praise-pubsub' failed to build: The command '/bin/sh -c pip install google-cloud-pubsub==0.24.0' returned a non-zero code: 1
</code></pre>

<p>full Dockerfile</p>

<pre><code>FROM google/cloud-sdk:alpine
RUN gcloud components install pubsub-emulator

FROM openjdk:jre-alpine


ENV PYTHONUNBUFFERED=1

RUN echo ""**** install Python ****"" &amp;&amp; \
    apk add --no-cache python3 &amp;&amp; \
    if [ ! -e /usr/bin/python ]; then ln -sf python3 /usr/bin/python ; fi &amp;&amp; \
    \
    echo ""**** install pip ****"" &amp;&amp; \
    python3 -m ensurepip &amp;&amp; \
    rm -r /usr/lib/python*/ensurepip &amp;&amp; \
    pip3 install --no-cache --upgrade pip setuptools wheel &amp;&amp; \
    if [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi

#RUN pip install google-cloud &lt;--- still fails when this is here
#RUN pip install Cython --install-option=""--no-cython-compile"" &lt;--- still fails
RUN pip install google-cloud-pubsub
COPY --from=0 /google-cloud-sdk/platform/pubsub-emulator /pubsub-emulator
</code></pre>
","<p>I am trying to build an image using Dockerfile using : <strong>sudo docker build . -f Dockerfile</strong></p>
<p>Dockerfile i am using :</p>
<pre><code>FROM python:3.6.8-alpine
WORKDIR /usr/app

COPY . ./

RUN pip install --upgrade pip
RUN pip install --upgrade setuptools
RUN pip install -r requirements.txt


RUN export GOOGLE_APPLICATION_CREDENTIALS=&quot;/app/static/*******.json&quot;
EXPOSE 5000
ENTRYPOINT [ &quot;python&quot; ]
CMD [ &quot;app.py&quot; ]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>Flask-RESTful==0.3.8
Flask==1.1.2
google-cloud-documentai==0.3.0
</code></pre>
<p><strong>Facing issues as:</strong></p>
<pre><code>  ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-v7prcez8/grpcio/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-v7prcez8/grpcio/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-x0mxi6mn
         cwd: /tmp/pip-install-v7prcez8/grpcio/
    Complete output (11 lines):
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-install-v7prcez8/grpcio/setup.py&quot;, line 212, in &lt;module&gt;
        if check_linker_need_libatomic():
      File &quot;/tmp/pip-install-v7prcez8/grpcio/setup.py&quot;, line 172, in check_linker_need_libatomic
        stderr=PIPE)
      File &quot;/usr/local/lib/python3.6/subprocess.py&quot;, line 729, in __init__
        restore_signals, start_new_session)
      File &quot;/usr/local/lib/python3.6/subprocess.py&quot;, line 1364, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'cc': 'cc'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 
</code></pre>
<p><strong>Apart from that :  python setup.py egg_info  giving me bellow log</strong></p>
<pre><code>python: can't open file 'setup.py': [Errno 2] No such file or directory
</code></pre>
<p>Any fix or clue will be helpful for me.</p>
"
"63400807","Docker input file and save in output","<python><docker><dockerfile>","64698272","Ho to run python script inside docker and save results locally?","<python><docker>","<p>I built a docker image that inputs a local file, does some stuff to it, and returns an output file saved locally, but it does not work. How do I allow local, user input for files and then saving the output on the local machine?</p>
<p>My Dockerfile looks like this:</p>
<pre><code>FROM python:3
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
EXPOSE 5000
CMD [ &quot;python&quot;, &quot;process.py&quot; ]
</code></pre>
<p>Ideally, the terminal command would be something like this:</p>
<pre><code>docker run -p 5000:5000 [name of docker] [local path to input file] [local path to save output file]
</code></pre>
<p>When I run, I get this error:</p>
<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;../test.flac\&quot;: stat ../test.flac: no such file or directory&quot;: unknown.
</code></pre>
<p>How can I do this?</p>
","<p>I'm trying to run a python script but before my script I have to do <code>docker run -it --rm psycopg2 python</code>.
Inside my script I want to calculate some results and then store them locally in files.</p>
<p>I currenty do all the above by hand. First I run <code>docker run -it --rm psycopg2 python</code> and then I type one by one the python commands to the shell but I still cannot save anything to a file locally.</p>
<p>Any ideas?</p>
"
"63400807","Docker input file and save in output","<python><docker><dockerfile>","64698860","Get files output after Docker container running","<docker><dockerfile><containers>","<p>I built a docker image that inputs a local file, does some stuff to it, and returns an output file saved locally, but it does not work. How do I allow local, user input for files and then saving the output on the local machine?</p>
<p>My Dockerfile looks like this:</p>
<pre><code>FROM python:3
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
EXPOSE 5000
CMD [ &quot;python&quot;, &quot;process.py&quot; ]
</code></pre>
<p>Ideally, the terminal command would be something like this:</p>
<pre><code>docker run -p 5000:5000 [name of docker] [local path to input file] [local path to save output file]
</code></pre>
<p>When I run, I get this error:</p>
<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused &quot;exec: \&quot;../test.flac\&quot;: stat ../test.flac: no such file or directory&quot;: unknown.
</code></pre>
<p>How can I do this?</p>
","<p>I have a Python program that generates some output files.</p>
<p>I have successfully created an image using Dockerfile:</p>
<pre><code>FROM python:3.8.3
WORKDIR /main
COPY main.py /main
COPY /res /main/res/
RUN pip install --upgrade pip
RUN pip install elementpath
RUN pip install loguru
CMD [&quot;python&quot;, &quot;main.py&quot;]
</code></pre>
<p>How can I find my output files after Docker container running?</p>
<p>I have tried <code>docker exec -t -i mycontainer /bin/bash</code>, but this command shows image's files, not generated by my Python script.</p>
"
"63448467","Installing OpenCV fails because it cannot find ""skbuild""","<python><opencv><pip>","64549453","Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-q4jjryc_/opencv-python/","<python><amazon-web-services><docker><opencv>","<p>I have a docker image that I need to install openCV in it and from yesterday it started to fail because it cannot find the &quot;skbuild&quot; module:</p>
<pre class=""lang-sh prettyprint-override""><code>Step 12/24 : RUN pip install opencv-python opencv-contrib-python
 ---&gt; Running in a0f746a23aed
Collecting opencv-python
  Downloading https://files.pythonhosted.org/packages/77/f5/49f034f8d109efcf9b7e98fbc051878b83b2f02a1c73f92bbd37f317288e/opencv-python-4.4.0.42.tar.gz (88.9MB)
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-build-cciracwm/opencv-python/setup.py&quot;, line 9, in &lt;module&gt;
        import skbuild
    ModuleNotFoundError: No module named 'skbuild'
</code></pre>
<p>And on the host also I cannot find that module and <code>pip search</code> is either return nothing or returns a server error:</p>
<pre class=""lang-sh prettyprint-override""><code>sudo pip3 search skbuild
WARNING: The directory '/home/ali/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
ERROR: Exception:
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py&quot;, line 216, in _main
    status = self.run(options, args)
  File &quot;/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/search.py&quot;, line 60, in run
    pypi_hits = self.search(query, options)
  File &quot;/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/search.py&quot;, line 80, in search
    hits = pypi.search({'name': query, 'summary': query}, 'or')
  File &quot;/usr/lib/python3.6/xmlrpc/client.py&quot;, line 1112, in __call__
    return self.__send(self.__name, args)
  File &quot;/usr/lib/python3.6/xmlrpc/client.py&quot;, line 1452, in __request
    verbose=self.__verbose
  File &quot;/usr/local/lib/python3.6/dist-packages/pip/_internal/network/xmlrpc.py&quot;, line 45, in request
    return self.parse_response(response.raw)
  File &quot;/usr/lib/python3.6/xmlrpc/client.py&quot;, line 1342, in parse_response
    return u.close()
  File &quot;/usr/lib/python3.6/xmlrpc/client.py&quot;, line 656, in close
    raise Fault(**self._stack[0])
xmlrpc.client.Fault: &lt;Fault -32403: 'server error; service unavailable'&gt;
</code></pre>
<p>Is this because their server has some problems or it is related to my pip?</p>
<h2>Update</h2>
<p>I tried it on different python and pip versions, python versions 3.6 and 3.7 and pip version 19.12 and 20.2.2 and 9.0.1 (the one inside image).</p>
<h3>Update 2</h3>
<p>I added <code>pip install scikit-build</code> one step before opencv installation but I get another error related to cmake:</p>
<pre class=""lang-sh prettyprint-override""><code>Step 12/25 : RUN pip install scikit-build
 ---&gt; Running in afe0c5c0fca0
Collecting scikit-build
  Downloading https://files.pythonhosted.org/packages/78/c9/7c2c7397ea64e36ebb292446896edcdecbb8c1aa6b9a1a32f6f67984c3df/scikit_build-0.11.1-py2.py3-none-any.whl (72kB)
Collecting packaging (from scikit-build)
  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl
Requirement already satisfied: wheel&gt;=0.29.0 in /usr/lib/python3/dist-packages (from scikit-build)
Collecting distro (from scikit-build)
  Downloading https://files.pythonhosted.org/packages/25/b7/b3c4270a11414cb22c6352ebc7a83aaa3712043be29daa05018fd5a5c956/distro-1.5.0-py2.py3-none-any.whl
Requirement already satisfied: setuptools&gt;=28.0.0; python_version &gt;= &quot;3&quot; in /usr/lib/python3/dist-packages (from scikit-build)
Requirement already satisfied: six in /usr/lib/python3/dist-packages (from packaging-&gt;scikit-build)
Collecting pyparsing&gt;=2.0.2 (from packaging-&gt;scikit-build)
  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)
Installing collected packages: pyparsing, packaging, distro, scikit-build
Successfully installed distro-1.5.0 packaging-20.4 pyparsing-2.4.7 scikit-build-0.11.1
Removing intermediate container afe0c5c0fca0
 ---&gt; 533658ddf26d
Step 13/25 : RUN pip install opencv-python opencv-contrib-python
 ---&gt; Running in 0f2c03bc042a
Collecting opencv-python
  Downloading https://files.pythonhosted.org/packages/77/f5/49f034f8d109efcf9b7e98fbc051878b83b2f02a1c73f92bbd37f317288e/opencv-python-4.4.0.42.tar.gz (88.9MB)
Collecting opencv-contrib-python
  Downloading https://files.pythonhosted.org/packages/48/98/05bd8e00c71b66c4e7847cc051cac404191d904df58b51a7dcc3767ff747/opencv_contrib_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (34.2MB)
Collecting numpy&gt;=1.14.5 (from opencv-python)
  Downloading https://files.pythonhosted.org/packages/3d/d1/90cd7e0b27ee86d77f5386d38b74520486100286d50772377791b6ef22ff/numpy-1.19.1-cp37-cp37m-manylinux1_x86_64.whl (13.4MB)
Building wheels for collected packages: opencv-python
  Running setup.py bdist_wheel for opencv-python: started
  Running setup.py bdist_wheel for opencv-python: finished with status 'error'
  Complete output from command /usr/bin/python -u -c &quot;import setuptools, tokenize;__file__='/tmp/pip-build-w49xv99a/opencv-python/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))&quot; bdist_wheel -d /tmp/tmpycj03zlwpip-wheel- --python-tag cp37:
  /usr/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'
    warnings.warn(msg)
  Traceback (most recent call last):
    File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/setuptools_wrap.py&quot;, line 560, in setup
      cmkr = cmaker.CMaker(cmake_executable)
    File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/cmaker.py&quot;, line 95, in __init__
      self.cmake_version = get_cmake_version(self.cmake_executable)
    File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/cmaker.py&quot;, line 82, in get_cmake_version
      &quot;Problem with the CMake installation, aborting build. CMake executable is %s&quot; % cmake_executable)
  
  Problem with the CMake installation, aborting build. CMake executable is cmake
  
  ----------------------------------------
  Failed building wheel for opencv-python
  Running setup.py clean for opencv-python
Failed to build opencv-python
Installing collected packages: numpy, opencv-python, opencv-contrib-python
  Found existing installation: numpy 1.13.3
    Not uninstalling numpy at /usr/lib/python3/dist-packages, outside environment /usr
  Running setup.py install for opencv-python: started
    Running setup.py install for opencv-python: finished with status 'error'
    Complete output from command /usr/bin/python -u -c &quot;import setuptools, tokenize;__file__='/tmp/pip-build-w49xv99a/opencv-python/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))&quot; install --record /tmp/pip-maky1uyw-record/install-record.txt --single-version-externally-managed --compile:
    /usr/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'long_description_content_type'
      warnings.warn(msg)
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/setuptools_wrap.py&quot;, line 560, in setup
        cmkr = cmaker.CMaker(cmake_executable)
      File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/cmaker.py&quot;, line 95, in __init__
        self.cmake_version = get_cmake_version(self.cmake_executable)
      File &quot;/usr/local/lib/python3.7/dist-packages/skbuild/cmaker.py&quot;, line 82, in get_cmake_version
        &quot;Problem with the CMake installation, aborting build. CMake executable is %s&quot; % cmake_executable)
    
    Problem with the CMake installation, aborting build. CMake executable is cmake
</code></pre>
","<p>Anyone can help for this errors, I no idea how to fix it?</p>
<p><a href=""https://i.stack.imgur.com/pTS5H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pTS5H.png"" alt=""enter image description here"" /></a></p>
"
"64338203","how to do reverse proxy on docker","<docker><docker-compose><node-red><nginx-reverse-proxy><mosquitto>","64372533","edited how to make nginx pont to my mosquitto broker in docker","<docker><nginx><docker-compose><mqtt>","<p>I have a server and I am using Ubuntu 20.04, nginx , mosquitto and node-red and docker , let's call the website <code>http://mywebsite.com</code>. The problem that I am facing that I have created a client lets call it client1 in docker so the URL will be <code>http://mywebsite.com/client1</code>
and I want to establish an MQTT connection via mosquitto and I'm sending the data on topic <code>test</code></p>
<p>The problem that on node red node of MQTT when I write the IP address of my mosquitto container it works</p>
<p><img src=""https://i.stack.imgur.com/MU79C.png"" alt=""enter image description here"" /></p>
<p>But if I change the IP address 192.144.0.5 with mywebsite.com/client1 I can't connect to mosquitto and I can't send or receive any form of data</p>
<p><img src=""https://i.stack.imgur.com/QVbpS.png"" alt=""enter image description here"" /></p>
<p>any idea on how to solve this problem</p>
","<p>I have an web server with ubuntu 20.04 lets call it mywebsite.com that i've installed nginx and [2 nodered 2 mosquitto 2 grafana those are runing on docker, docker-compose called client1  that contain 1 grafana 1 mosquitto 1 nodered  ]
client_mosquitto ha a local ip address of 192.170.0.5 and use port 60007
here is my nginx.conf</p>
<pre><code>server {
     rewrite ^/client1/grafana(.*) $1 break;
            proxy_pass &quot;http://mywebsite.com:60007/grafana/&quot;;
    }

    location /client1/node-red/ {
            rewrite ^/node-red(.*) $1 break;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection &quot;upgrade&quot;;
            proxy_pass http://mywebsite.com:60008;
    }
   location /client1/mqtt/ {
            rewrite ^/node-red(.*) $1 break;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection &quot;upgrade&quot;;
            proxy_pass http://mywebsite.com:60009;
    }
   }
</code></pre>
<p>the problem that I am facing is that I can't access my mosquitto broker, so if i send data to my clien1_broker with &quot;mywebsite.com:60009&quot; i don't receive any thing</p>
"
"64644587","Containers not restarted after update with restart always in docker-compose.yml","<docker><docker-compose><coreos><flatcar-linux>","64686964","Can a docker container 'go to sleep'?","<docker><docker-compose><dockerfile>","<p>I have some containers which all of them have the always restart value in the docker-compose file like this:</p>
<pre><code>version: &quot;3.7&quot;
services:

  container:
    image: ghost:latest
    container_name: some_container
    restart: always
    depends_on:
       - ...
    ports:
       - ...
...
</code></pre>
<p>As soon as the OS (Flatcar Linux / CoreOS) has updated itself none of the containers restart. But if I just do <code>$ sudo docker ps</code> all of the containers starts at once. Whats up with that and how do I fix it so my containers automatically restarts after an update?</p>
<p>EDIT:</p>
<p>Not sure what is unclear about my question, <code>restart: always</code> is turned on. Unless I'm missing some vital thing in the documentation, this command should restart the container even if the docker daemon is restarted (after an os reboot).</p>
<p>Copy of one my comments from below:</p>
<blockquote>
<p>Ok, so help me out here. As you can see in my question, I have
restart: always turned on. All these containers are started
successfully and are running well. Then the OS updates itself
automatically and restarts itself. After this restart the docker
daemon is restarted. But for some reasons the containers I had running
WITH <code>RESTART: ALWAYS</code> turned on DOES NOT START. If I enter my server
at this moment, type <code>sudo docker ps</code> to list my running containers,
suddenly all containers are booted up and I see the list. So why
wasn't the containers started, even though the daemon is running?</p>
</blockquote>
","<p>I am running a server inside a docker container. I start it by using <code>docker-compose up -d</code> which works fine, i can access the Server. After some time however I receive a 503 - service unavailable error.</p>
<p>When I then perform any docker command e. g. <code>docker ps</code>  or <code>docker logs</code> the server runs perfectly again. <code>docker ps</code> shows that the container was created days ago, but  Status says <code>Up 1 second</code>. Is there any way for me to know what state my docker was in before? How does <code>docker ps</code> change the status?</p>
<p>docker-compose.yaml</p>
<pre><code>version: '3'
services:
  postgres:
    restart: always
    image: postgres:12.1
    environment:
      POSTGRES_PASSWORD: pass
      POSTGRES_USER: user
    networks:
      - mynetwork
    volumes:
      - /home/user/data/postgres:/var/lib/postgresql/data
    ports:
      - 5432:5432
  web:
    restart: always
    build: .
    environment:
      JPDA_ADDRESS: 8001
      JPDA_TRANSPORT: dt_socket
    networks:
      - mynetwork
    depends_on:
      - postgres
    ports:
      - 80:8080
      - 8001:8001
    volumes:
    - /home/user/data/images:/data/images
networks:
  mynetwork:
    driver: bridge
</code></pre>
<p>Dockerfile</p>
<pre><code>
FROM maven:3.6.3-jdk-8-slim as java-build-stage

WORKDIR /app/build
COPY . .

RUN mvn clean package -am -DskipTests -Dmaven.javadoc.skip=true -P deployment

# create the image to run the tomcat
FROM tomcat:9.0.30-jdk8-openjdk-slim as myname-build

ADD docker-resources/tomcat-users.xml /usr/local/tomcat/conf/tomcat-users.xml
ADD docker-resources/index.html /usr/local/tomcat/webapps/ROOT/index.jsp
COPY --from=java-build-stage /app/build/deployment/target/myname.war /usr/local/tomcat/webapps/myname.war

CMD [&quot;catalina.sh&quot;, &quot;jpda&quot; ,&quot;run&quot;]
</code></pre>
"
"64733707","front-end Vue.js app in Kubernetes docker container cannot connect to back-end","<docker><vue.js><kubernetes><axios>","64778030","Docker: VueJS can not resolve service name backend","<docker><vue.js><docker-networking>","<p>I have built a front-end Vue.js application, running on a docker container under kubernetes environment. the backend is also in the same kubernetes cluster (I am using Minikube for the project). When running it gets error <code>net::ERR_NAME_NOT_RESOLVED</code> when connecting to back-end containers:
<a href=""https://i.stack.imgur.com/RlAsQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RlAsQ.png"" alt=""enter image description here"" /></a></p>
<p>while inside the container, there is no problem connect to the back-end using curl:</p>
<pre><code>$ kubectl exec -it deployment/hpl-browser-deployment -- sh
/ # curl http://hpl-manager-service:2354
{
  &quot;message&quot;: &quot;Manager status&quot;, 
  &quot;state&quot;: &quot;IDLE&quot;
}
</code></pre>
<p>I used <code>axios</code> for the api service:</p>
<pre><code>import axios from 'axios';

export default class APIService {
  API_URL = '';

  constructor(apiAddress) {
    this.API_URL = apiAddress;
  }

  async get() {
    console.log('ApiService: get()');
    try {
      const response = await axios.get(this.API_URL);
      console.log(`ApiService: get result: ${response.data}`);
      return response.data;
    } catch (error) {
      console.error(error);
      return error;
    }
  }

  async postPlainText(data) {
    console.log(`ApiService: post() - data: ${data}`);
    try {
      const response = await axios.post(this.API_URL,
        data,
        {
          headers: {
            'Content-Type': 'text/plain',
            Accept: '*/*',
          },
        });
      console.log(`ApiService: post result: ${response.data}`);
      return response.data;
    } catch (error) {
      console.error(error);
      return error;
    }
  }
}
</code></pre>
<p>The application has no problem running on development environment, when I port-forward the back-end service, and connect to <code>http://localhost:2354</code>.</p>
<p>I would like to know what may cause this problem?</p>
","<p>I created two container <code>boleto_api</code> and <code>boleto_web</code> the first is written in ExpressJS and the last is a VueJS application.</p>
<p>Created docker network named boleto and from that I started the containers.. If I ping <code>:ping boleto_api</code> inside container boleto_web I have successful</p>
<pre><code>PING boleto_api (172.21.0.2) 56(84) bytes of data.
64 bytes from boleto_api.boleto (172.21.0.2): icmp_seq=1 ttl=64 time=0.105 ms
64 bytes from boleto_api.boleto (172.21.0.2): icmp_seq=2 ttl=64 time=0.091 ms
</code></pre>
<p>But in Fetch API inside VueJS</p>
<pre><code>GET http://boleto_api:3030/pessoas/buscar/11532324723 net::ERR_NAME_NOT_RESOLVED
</code></pre>
<p>Part of code:</p>
<pre><code>fetch('http://boleto_api:3030/pessoas/buscar/'+this.$store.state.cpf)
             .then(response =&gt; response.json())
             .then(data =&gt; {
                this.dadosCliente = data
                this.show = false;
             });
</code></pre>
<p>I dont have sure if the frontend (VueJs) will be successful to do it, if not, how to proceeded ?</p>
"
"65036374","are VOLUME in Dockerfile persistent in kubernetes","<docker><kubernetes><dockerfile><docker-volume><persistent-volumes>","65074472","List Kubernetes containers that use docker local volumes","<docker><kubernetes><docker-volume><kubernetes-statefulset>","<p>Some Dockerfile have a <a href=""https://docs.docker.com/engine/reference/builder/#volume"" rel=""nofollow noreferrer"">VOLUME</a> command.</p>
<p>What happens when such containers are deployed in Kubernetes, but no kubernetes volume are provided: no persistent volume (PV), nor persistent volume claim (PVC) ?</p>
<p>Where are the file stored ?</p>
<p>Is the volume persistent ?</p>
<hr />
<p>For exemple, Dockerfile image for Docker's <a href=""https://github.com/docker-library/postgres/blob/master/13/Dockerfile"" rel=""nofollow noreferrer"">library/postgreSQL</a> container image has:</p>
<pre><code>    VOLUME /var/lib/postgresql/data
</code></pre>
<p>The <a href=""https://github.com/helm/charts/blob/master/stable/postgresql/templates/statefulset.yaml"" rel=""nofollow noreferrer"">stable/postgresql</a> helm charts won't always create a PV:</p>
<pre class=""lang-yaml prettyprint-override""><code>kind: StatefulSet
### SNIP SNIP ###
      containers:
        - name: {{ template &quot;postgresql.fullname&quot; . }}
          image: {{ template &quot;postgresql.image&quot; . }}
### SNIP SNIP ###
          volumeMounts:
            {{ if .Values.persistence.enabled }}
            - name: data
              mountPath: {{ .Values.persistence.mountPath }}
              subPath: {{ .Values.persistence.subPath }}
{{- end }}
### SNIP SNIP ###
{{- if and .Values.persistence.enabled .Values.persistence.existingClaim }}
        - name: data
          persistentVolumeClaim:
{{- with .Values.persistence.existingClaim }}
            claimName: {{ tpl . $ }}
{{- end }}
{{- else if not .Values.persistence.enabled }}
        - name: data
          emptyDir: {}
{{- else if and .Values.persistence.enabled (not .Values.persistence.existingClaim) }}
  volumeClaimTemplates:
    - metadata:
        name: data
      {{- with .Values.persistence.annotations }}
        annotations:
        {{- range $key, $value := . }}
          {{ $key }}: {{ $value }}
        {{- end }}
      {{- end }}
      spec:
        accessModes:
        {{- range .Values.persistence.accessModes }}
          - {{ . | quote }}
        {{- end }}
        resources:
          requests:
            storage: {{ .Values.persistence.size | quote }}
        {{ include &quot;postgresql.storageClass&quot; . }}
{{- end }}
</code></pre>
","<p>When deploying a workload that has a <code>VOLUME</code> in a Dockerfile, that volume may not be mapped to a persistent volume (PV/PVC) in Kubernetes.</p>
<p>Actually, unless a Kubernetes volume is attached to that workload, The docker daemon container will temporarily create a docker-volume when starting the container (<a href=""https://docs.docker.com/storage/volumes/#use-a-volume-driver"" rel=""nofollow noreferrer"">driver</a> type: <code>local</code>). Kubernetes won't be aware of it. See: <a href=""https://stackoverflow.com/q/65036374/1260896"">are VOLUME in Dockerfile persistent in kubernetes</a>. This docker volume will be destroyed when the pod is removed or redeployed.</p>
<p>It is certainly good practice to use a kubernetes volume, even <a href=""https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/"" rel=""nofollow noreferrer"">ephermeal volumes</a> (or <a href=""https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes"" rel=""nofollow noreferrer"">generic ephemeral volumes</a>... still in alpha in 1.19)</p>
<p><strong>Q:</strong> How to list pods/containers that use such local volumes?</p>
<p>This is really important since restarting the workload/deployment/stateful-set will cause disruption (lost of ephemeral volume).</p>
"
"3582552","What is the format for the PostgreSQL connection string / URL?","<postgresql><database-connection>","65111343","Docker - URL syntax","<docker><http><url>","<p>What is the format for the PostgreSQL connection string (URL <code>postgres://...</code>) when the host is not the localhost?</p>
","<p>I came accross a URL syntax in a config file, but I don't get the meaning of some of it:</p>
<pre><code>DATABASE_URL=postgresql://myuser:myuser@db:5432/db_prod
</code></pre>
<p>More specifically:</p>
<ol>
<li><p>the <code>//myuser:myuser</code> part. This is the url that Flask (a web service in my docker-compose file) uses to connect to the db (postgres). The username for the postgres db is indeed myuser as defined in POSTGRES_USER environment variable, however I don't get why this is repeated here.</p>
</li>
<li><p>The meaning of <code>=postgresql:</code>. That's not the name of my service, nor the name of the image (which is <code>postgres:13.1</code>).</p>
</li>
</ol>
<p>The rest is clear to me:</p>
<ul>
<li><code>db</code>: name of the service in the docker network</li>
<li><code>db_prod</code>: name of the database to connect to</li>
<li><code>5432</code>: the port on which the db services listen to on the local docker network</li>
</ul>
<p>thanks!</p>
"
"13944340","LLVM & Clang can't compile for a supported arch","<c++><architecture><arm><llvm><clang>","65246895","Failed to compile hello world with clang-11 built from source code","<c++><docker><clang><llvm>","<p>Under Ubuntu 64 bit I got</p>

<pre><code>llc --version
LLVM (http://llvm.org/):
  LLVM version 3.1
  Optimized build with assertions.
  Built Oct 15 2012 (18:15:59).
  Default target: x86_64-pc-linux-gnu
  Host CPU: btver1

  Registered Targets:
    arm      - ARM
    mips     - Mips
    mips64   - Mips64 [experimental]
    mips64el - Mips64el [experimental]
    mipsel   - Mipsel
    thumb    - Thumb
    x86      - 32-bit X86: Pentium-Pro and above
    x86-64   - 64-bit X86: EM64T and AMD64
</code></pre>

<p>I can't do this</p>

<pre><code>clang -march=arm -x c++ /tmp/cpp.cpp 
error: unknown target CPU 'arm'
</code></pre>

<p>I'm missing something here ? Why I can't compile for ARM ?</p>
","<p>I built llvm+clang+lld from source code in a docker container.</p>
<pre><code>% CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \
cmake …/llvm -G “Unix Makefiles” \
-DLLVM_ENABLE_PROJECTS=“clang;lld” \
-DCMAKE_BUILD_TYPE=MinSizeRel \
-DLLVM_TARGETS_TO_BUILD=“X86” \
 -DCMAKE_CXX_LINK_FLAGS=&quot;-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64&quot;

% make -j &amp;&amp; sudo make install
</code></pre>
<p>Then I try to compile a hello world program with clang, and I got the error message:</p>
<pre class=""lang-cpp prettyprint-override""><code>// test.c
#include &lt;stdio.h&gt;

int main(int argc, char** argv) {
  printf(“hello\n”);
  return 0;
}
</code></pre>
<pre><code>% clang test.c
error: unknown target CPU 'x86-64’
</code></pre>
<p>Are there something wrong with my building process?</p>
<p>The llvm version is 11.0.0 release.</p>
<p>The version of gcc under $HOME/toolchains is 5.2.0.</p>
<p>My docker version is:</p>
<pre><code>Client:
Version: 17.06.2-ce
API version: 1.30
Go version: go1.8.3
Git commit: e046fc5
Built: Tue Oct 17 02:06:56 2017
OS/Arch: linux/amd64

Server:
Version: 17.06.2-ce
API version: 1.30 (minimum version 1.12)
Go version: go1.8.3
Git commit: e046fc5
Built: Tue Oct 17 02:09:18 2017
OS/Arch: linux/amd64
Experimental: false
</code></pre>
"
"20096632","Limit memory on a Docker container doesn't work","<ubuntu><docker><cgroups>","65184514","Docker -memory --memory-swap don't take effect","<windows><docker><ubuntu>","<p>I am running the last version of Docker on top of Ubuntu&nbsp;13.04 (Raring Ringtail):</p>

<pre><code>root@docker:~# docker version
Client version: 0.6.6
Go version (client): go1.2rc3
Git commit (client): 6d42040
Server version: 0.6.6
Git commit (server): 6d42040
Go version (server): go1.2rc3
Last stable version: 0.6.6
</code></pre>

<p>But when I start the container,</p>

<pre><code>root@docker:~# docker run -m=1524288 -i  -t ubuntu /bin/bash
root@7b09f638871a:/# free -m
             total       used       free     shared    buffers     cached
Mem:          1992        608       1383          0         30        341
-/+ buffers/cache:        237       1755
Swap:         2047          0       2047
</code></pre>

<p>I don't see any limiting from any kind, and my kernel has the cgroups memory limit enabled:</p>

<pre><code>kernel /boot/vmlinuz-3.8.0-33-generic ro console=tty0 root=/dev/xvda1 cgroup_enable=memory swapaccount=1
</code></pre>

<p>What obvious thing am I missing here?</p>
","<p>I am running docker v19.03.13 on Windows. Essentially, my problem is that neither --memory=&quot;&quot; or --memory-swap=&quot;&quot; have any effect on any of the containers. I use the commands, they seem to be accepted, but when I examine the memory limits, there is no change. I want to increase my memory swap size so I can construct a large data base.</p>
<p>Default</p>
<p>C:\Users\MyPC&gt; docker ps</p>
<p>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f54e10ee70a8 staphb/kraken2:latest &quot;/bin/bash&quot; 10 days ago Up 14 minutes Kraken2</p>
<p>inside f54e10ee70a8
free - h</p>
<p>total used free shared buff/cache available
Mem: 24G 496M 22G 251M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>so the memory swap is limited to 7G. I have far more space than that available on the local disk for a swap. But when I try to modify it, the commands are accepted there is no change in the container.</p>
<p>C:\Users\MyPC&gt;docker run --memory=&quot;22g&quot; --memory-swap=&quot;600g&quot; ubuntu /bin/bash
free - h</p>
<p>total used free shared buff/cache available
Mem: 24G 496M 22G 251M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>or even when I specify the particular container</p>
<p>C:\Users\MyPC&gt;docker run -it --memory=&quot;21g&quot; --memory-swap=&quot;600g&quot; staphb/kraken2:latest
root@d6fb2f332834:/data# free -h
total used free shared buff/cache available
Mem: 24G 510M 22G 252M 1.5G 23G
Swap: 7.0G 0B 7.0G</p>
<p>Any help would be really appreciated. Thanks in advance!</p>
"
"21866477","nginx: use environment variables","<shell><nginx><proxy><environment-variables><docker>","65178527","How to pass variables from .env or docker-compose.yaml file to nginx server file","<docker><nginx>","<p>I have the following scenario: I have an env variable <code>$SOME_IP</code> defined and want to use it in a nginx block. Referring to the <a href=""http://wiki.nginx.org/CoreModule"" rel=""noreferrer"">nginx documentation</a> I use the <code>env</code> directive in the <code>nginx.conf</code> file like the following:</p>

<pre><code>user www-data;
worker_processes 4;
pid /run/nginx.pid;

env SOME_IP;
</code></pre>

<p>Now I want to use the variable for a <code>proxy_pass</code>. I tried it like the following:</p>

<pre><code>location / {
    proxy_pass http://$SOME_IP:8000;
}
</code></pre>

<p>But I end up with this error message: <code>nginx: [emerg] unknown ""some_ip"" variable</code></p>
","<p>I have a <code>docker-compose.yaml</code> file and a <code>.env</code> file. If I declare a variable in the env file, I can access it in docker-compose. So, this is good.</p>
<p>But, I would like to change nginx's <code>default.conf</code> properties, namely the <code>server_name = xxx</code> to be generated from the value I provide in the env.</p>
<p>So, my env looks like this (only sharing a portion)</p>
<pre><code>#.env
NGINX_SERVER_NAME=dev.example
</code></pre>
<p>In docker-compose I mention this variable here:</p>
<pre><code>nginx:
  container_name: ${APP_NAME}-server
  environment:
    - SERVER_NAME=${NGINX_SERVER_NAME}
</code></pre>
<p>And in <code>default.conf</code> I simple call the variable like this:</p>
<pre><code>server_name ${SERVER_NAME};
</code></pre>
<p>I am expecting the <code>default.conf</code> to contain <code>server_name dev.example</code> but it contains the token i.e. <code>${SERVER_NAME}</code></p>
<p>How can I achieve what I am looking for?
Thanks</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","65264773","Docker django connect non docker postgresql","<django><postgresql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have django project and  trying to dockerize the project.</p>
<p>Everything is working very well but the problem with postgresql.</p>
<p>I have installed my postgresql in my local machine, so postgresql host is localhost and my django app running in my local machine docker.</p>
<p>I dont want to install postgresql with docker, it's already installed without docker. so i tried to connect this with following infomrmatin:</p>
<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'mydbname',
        'USER': 'postgres',
        'PASSWORD': 'secret',
        'HOST': 'localhost',
        'PORT': '5432',
    }
}
</code></pre>
<p>and it is not working, it's firing error.</p>
<p>Can anyone help me what can i do in this case? I dont want to install postgres by docker, the datbase should be in my direct local machine.</p>
<p>and this is my dockerFile</p>
<pre><code>FROM python:3.8.1-slim-buster


RUN useradd wagtail


EXPOSE 8000


ENV PYTHONUNBUFFERED=1 \
    PORT=8000

# Install system packages required by Wagtail and Django.
RUN apt-get update --yes --quiet &amp;&amp; apt-get install --yes --quiet --no-install-recommends \
    build-essential \
    libpq-dev \
    libmariadbclient-dev \
    libjpeg62-turbo-dev \
    zlib1g-dev \
    libwebp-dev \
 &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN pip install &quot;gunicorn==20.0.4&quot;

# Install the project requirements.
COPY requirements.txt /
RUN pip install -r /requirements.txt

WORKDIR /app

RUN chown wagtail:wagtail /app


COPY --chown=wagtail:wagtail . .

USER wagtail


RUN python manage.py collectstatic --noinput --clear

CMD set -xe; python manage.py migrate --noinput; gunicorn projectile.wsgi:application
</code></pre>
"
"26135216","Why Isn't There A Git Clone Specific Commit Option?","<git><git-clone>","65279547","Clone a specific commit without downloading all other commits or entire repo","<git><docker><github><dockerfile>","<p>In light of a <a href=""https://stackoverflow.com/q/26091467/1144203"">recent question on SO</a>, I am wondering why isn't there an option in <a href=""http://git-scm.com/docs/git-clone"" rel=""noreferrer""><code>git clone</code></a> such that the <code>HEAD</code> pointer of the newly created branch will point to a specified commit? In say question above, OP is trying to provide instructions on the specific commit his users should clone.</p>

<p>Note that this question is not about <a href=""https://stackoverflow.com/q/3555107/1144203"">How To Clone To A Particular Version</a> using <code>reset</code>; but about <em>why isn't there</em>?</p>
","<p>I would like to git clone a specific commit or SHA which in my is &quot;6ba2b37ffb1ee374331683eac481e065ed76407e &quot;  from a project. When I run this command in my Dockerfile it doesnt seem to work:</p>
<pre><code>RUN git clone https://github.com/danpovey/pocolm --depth=1 --branch master 6ba2b37ffb1ee374331683eac481e065ed76407e &amp;&amp; \
    cd pocolm &amp;&amp; \
    make
</code></pre>
<p>This is the error that i get:</p>
<pre><code>Step 4/4 : RUN git clone https://github.com/danpovey/pocolm --depth=1 --branch master 6ba2b37ffb1ee374331683eac481e065ed76407e &amp;&amp;     cd pocolm &amp;&amp;     make
 ---&gt; Running in 897cd4a8cb95
Cloning into '6ba2b37ffb1ee374331683eac481e065ed76407e'...
/bin/sh: 1: cd: can't cd to pocolm
The command '/bin/sh -c git clone https://github.com/danpovey/pocolm --depth=1 --branch master 6ba2b37ffb1ee374331683eac481e065ed76407e &amp;&amp;     cd pocolm &amp;&amp;     make' returned a non-zero code: 2
</code></pre>
<p>One solution to this is to clone the repo and then checkout to the specific branch or SHA that i want. But what if I dont want to clone the entire repo and all the commits? I just want to get this specific SHA and thats it &quot;6ba2b37ffb1ee374331683eac481e065ed76407e &quot;.</p>
"
"27017715","Does virtualenv serve a purpose (in production) when using docker?","<python><virtualenv><docker>","65262769","Do we need to create virtual environment if we are using docker, in django project?","<python-3.x><django><docker>","<p>For development we use virtualenv to have an isolated development when it comes to dependencies. From <a href=""https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server"">this question</a> it seems deploying Python applications in a <a href=""/questions/tagged/virtualenv"" class=""post-tag"" title=""show questions tagged 'virtualenv'"" rel=""tag"">virtualenv</a> is recommended.</p>

<p>Now we're starting to use <a href=""/questions/tagged/docker"" class=""post-tag"" title=""show questions tagged 'docker'"" rel=""tag"">docker</a> for deployment. This provides a more isolated environment so I'm questioning the use of virtualenv inside a docker container. In the case of a single application I do not think virtualenv has a purpose as docker already provides isolation. In the case where multiple applications are deployed on a single docker container, I do think virtualenv has a purpose as the applications can have conflicting dependencies. </p>

<p>Should virtualenv be used when a single application is deployed in a docker container?</p>

<p>Should docker contain multiple applications or only one application per container?</p>

<p>If so, should virtualenv be used when deploying a container with multiple applications?</p>
","<p>Do we need virtual environment if we are using docker, in django project?</p>
"
"27068596","How to include files outside of Docker's build context?","<docker>","65297277","COPY command in Dockerfile. How to copy from one level up","<docker><copy>","<p>How can I include files from outside of Docker's build context using the ""ADD"" command in the Docker file?</p>

<p>From the Docker documentation:</p>

<blockquote>
  <p>The  path must be inside the context of the build; you cannot ADD
  ../something/something, because the first step of a docker build is to
  send the context directory (and subdirectories) to the docker daemon.</p>
</blockquote>

<p>I do not want to restructure my whole project just to accommodate Docker in this matter. I want to keep all my Docker files in the same sub-directory.</p>

<p>Also, it appears Docker does not yet (and may not ever) support symlinks: <a href=""https://github.com/docker/docker/issues/1676"" rel=""noreferrer"">Dockerfile ADD command does not follow symlinks on host #1676.</a> </p>

<p>The only other thing I can think of is to include a pre-build step to copy the files into the Docker build context (and configure my version control to ignore those files). Is there a better workaround for than that?</p>
","<p>I am trying to copy files in Dockerfile.</p>
<p>Here is how my folder looks like.</p>
<pre><code>main.R &lt;- my main executable file
app &lt;- folder with Dockerfile.
</code></pre>
<p>In Dockerfile I have the following commands:</p>
<pre><code>RUN mkdir -p ~/application
COPY &quot;../&quot; &quot;application/&quot;
WORKDIR &quot;application/&quot; 
ENTRYPOINT [&quot;Rscript&quot;, &quot;main.R&quot;]
</code></pre>
<p>This does not work, it gives me error `Fatal error: cannot open file 'main.R': No such file or directory</p>
<p>If I move <code>main.R</code> file inside <code>app</code> folder everything works fine. How can I make it work without moving files.</p>
"
"27989751","Mount SMB/CIFS share within a Docker container","<linux><windows><docker><mount><cifs>","65141704","Permission denied while mounting inside of container","<linux><docker><unix><permissions><docker-container>","<p>I have a web application running in a Docker container. This application needs to access some files on our corporate file server (Windows Server with an Active Directory domain controller). The files I'm trying to access are image files created for our clients and the web application displays them as part of the client's portfolio.</p>

<p>On my development machine I have the appropriate folders mounted via entries in <code>/etc/fstab</code> and the host mount points are mounted in the Docker container via the <code>--volume</code> argument. This works perfectly.</p>

<p>Now I'm trying to put together a production container which will be run on a different server and which doesn't rely on the CIFS share being mounted on the host. So I tried to add the appropriate entries to the <code>/etc/fstab</code> file in the container &amp; mounting them with <code>mount -a</code>. I get <code>mount error(13): Permission denied</code>.</p>

<p>A little research online led me to <a href=""http://opensource.com/business/14/9/security-for-docker"" rel=""noreferrer"">this article about Docker security</a>. If I'm reading this correctly, it appears that Docker explicitly denies the ability to mount filesystems within a container. I tried mounting the shares read-only, but this (unsurprisingly) also failed.</p>

<p>So, I have two questions:</p>

<ol>
<li><p>Am I correct in understanding that Docker prevents any use of <code>mount</code> inside containers?</p></li>
<li><p>Can anyone think of another way to accomplish this <strong>without</strong> mounting a CIFS share on the host and then mounting the host folder in the Docker container?</p></li>
</ol>
","<p>I've to mount external remote volume inside of container. To achieve this, I created a directory(where to mount) - <code>TMG</code> at root level with giving permission with command - <code>chmod -R 777 TMG</code>. Once entered in container, I hit command -</p>
<pre><code>sudo mount -t cifs '\\abcdomain\Data\Products\XYG\' /TMG -o username=username,dom=doamin,password=password
</code></pre>
<p>But this results into an error  - <code>mount: /TMG: permission denied.</code></p>
<p>Even though all permissions are given to the directory, I'm unable to get why this error is occurring. Please help to resolve this or any other way to achieve this..</p>
"
"28721699","Root password inside a Docker container","<docker>","65117674","Docker Image payara/server-full:latest: How is the root password of this image","<docker><dockerhub><payara>","<p>I'm using a Docker image which was built using the USER command to use a non-root user called <code>dev</code>.
Inside a container, I'm ""dev"", but I want to edit the <code>/etc/hosts</code> file.</p>

<p>So I need to be root. I'm trying the su command, but I'm asked to enter the root password.</p>

<p>What's the default root user's password inside a Docker container?</p>
","<p>when I open a bash with</p>
<p><strong><code>docker exec -it &lt;IMAGE-ID&gt; bash</code></strong></p>
<p>I have a bash shell without root rights. Does this image has a default root password?</p>
<p>How can I get SU rights?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65087636","""Docker-compose run"" shows backend running but can't access from host","<docker><flask><docker-compose><dockerfile><flask-sqlalchemy>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I was trying to use docker for my flask backend. I've written a docker file that uses <code>python:3.8 as build-python</code> then install all packages from <code>requirements.txt</code> file. I was  <code>PostgreSQL</code> database then written in <code>docker-compose.yml</code> file. When I put command in terminal</p>
<blockquote>
<p>sudo docker-compose run</p>
</blockquote>
<p>It shows <code>api_1  |  Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) </code> which looks good. But I can't access this from my host pc It shows <strong>This site can’t be reached</strong>
Here is my <code>docker-compose.yml</code> file</p>
<pre class=""lang-yaml prettyprint-override""><code>version: &quot;2&quot;

services:
  api:
    ports:
      - 5000:5000
    build:
      context: ./mymeds
      dockerfile: ./Dockerfile
    restart: unless-stopped
    networks:
      - mymeds-backend-tier
    depends_on:
      - db
    volumes:
      - ./mymeds/app/:/app/app:Z
    command: python manage.py run
    env_file: common.env

  db:
    image: library/postgres:11.1-alpine
    ports:
      - 5432:5432
    restart: unless-stopped
    networks:
      - mymeds-backend-tier
    volumes:
      - mymeds-db:/var/lib/postgresql
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=user123

volumes:
  mymeds-db:
    driver: local

networks:
  mymeds-backend-tier:
    driver: bridge

</code></pre>
<p>Here is my <code>Dockerfile</code> for <code>flask</code> backend</p>
<pre><code>FROM python:3.8 as build-python

RUN apt-get -y update \
  &amp;&amp; apt-get install -y gettext \
  &amp;&amp; apt-get clean \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY requirements.txt /app/
WORKDIR /app
RUN pip install -r requirements.txt

FROM python:3.8-slim


RUN apt-get update \
  &amp;&amp; apt-get install -y \
    libxml2 \
    libssl1.1 \
    libcairo2 \
    libpango-1.0-0 \
    libpangocairo-1.0-0 \
    libgdk-pixbuf2.0-0 \
    shared-mime-info \
    mime-support \
  &amp;&amp; apt-get clean \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY . /app
COPY --from=build-python /usr/local/lib/python3.8/site-packages/ /usr/local/lib/python3.8/site-packages/
COPY --from=build-python /usr/local/bin/ /usr/local/bin/
WORKDIR /app

EXPOSE 5000
ENV PORT 5000
</code></pre>
<p><a href=""https://i.stack.imgur.com/Auq3a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Auq3a.png"" alt=""enter image description here"" /></a></p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65271662","build docker image for datetime flask api in python","<python><docker><flask>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I make a docker image for datetime flask api in python. I cannot see the output on the specified port when I visit browser on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a></p>
<p>flask api python code <strong>app.py</strong></p>
<pre><code>from flask import Flask,jsonify
from datetime import datetime
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    now = datetime.now()
    return jsonify( {&quot;Date and Time: &quot;: now})

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM python:3.7.4
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
ENTRYPOINT [&quot;python&quot;]
CMD [&quot;app.py&quot;]
</code></pre>
<p><strong>requirements.txt</strong></p>
<pre><code>flask==1.1.2
</code></pre>
<p>The three files [ 'app.py', 'Dockerfile', 'requirements.txt'] are in 'my_docker_flask' folder. I run the following commands inside the folder</p>
<p>I build and run the docker</p>
<pre><code>docker build -t my_docker_flask:latest .
docker run  -p 5000:5000 my_docker_flask:latest
</code></pre>
<p>I cannot see the output on the specified port when I visit browser on <a href=""http://127.0.0.1:5000/"" rel=""nofollow noreferrer"">http://127.0.0.1:5000/</a>
What am I doing wrong?</p>
"
"30323224","Deploying a minimal flask app in docker - server connection issues","<python><deployment><flask><docker><dockerfile>","65273595","Flask API endpoint is not accessible from docker","<python><python-3.x><docker><flask><docker-compose>","<p>I have an app whose only dependency is flask, which runs fine outside docker and binds to the default port <code>5000</code>. Here is the full source:</p>
<pre><code>from flask import Flask
 
app = Flask(__name__)
app.debug = True
 
@app.route('/')
def main():
    return 'hi'
 
if __name__ == '__main__':
    app.run()
</code></pre>
<p>The problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.</p>
<p>Below is my Dockerfile. The image is ubuntu with flask installed. The tar just contains the <code>index.py</code> listed above;</p>
<pre><code># Dockerfile
FROM dreen/flask
MAINTAINER dreen
WORKDIR /srv

# Get source
RUN mkdir -p /srv
COPY perfektimprezy.tar.gz /srv/perfektimprezy.tar.gz
RUN tar x -f perfektimprezy.tar.gz
RUN rm perfektimprezy.tar.gz

# Run server
EXPOSE 5000
CMD [&quot;python&quot;, &quot;index.py&quot;]
</code></pre>
<p>Here are the steps I am doing to deploy</p>
<p><code>$&gt; sudo docker build -t perfektimprezy .</code></p>
<p>As far as I know the above runs fine, the image has the contents of the tar in <code>/srv</code>. Now, let's start the server in a container:</p>
<pre><code>$&gt; sudo docker run -i -p 5000:5000 -d perfektimprezy
1c50b67d45b1a4feade72276394811c8399b1b95692e0914ee72b103ff54c769
</code></pre>
<p>Is it actually running?</p>
<pre><code>$&gt; sudo docker ps
CONTAINER ID        IMAGE                   COMMAND             CREATED             STATUS              PORTS                    NAMES
1c50b67d45b1        perfektimprezy:latest   &quot;python index.py&quot;   5 seconds ago       Up 5 seconds        0.0.0.0:5000-&gt;5000/tcp   loving_wozniak

$&gt; sudo docker logs 1c50b67d45b1
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
</code></pre>
<p>Yep, seems like the flask server is running. Here is where it gets weird. Lets make a request to the server:</p>
<pre><code> $&gt; curl 127.0.0.1:5000 -v
 * Rebuilt URL to: 127.0.0.1:5000/
 * Hostname was NOT found in DNS cache
 *   Trying 127.0.0.1...
 * Connected to 127.0.0.1 (127.0.0.1) port 5000 (#0)
 &gt; GET / HTTP/1.1
 &gt; User-Agent: curl/7.35.0
 &gt; Host: 127.0.0.1:5000
 &gt; Accept: */*
 &gt;
 * Empty reply from server
 * Connection #0 to host 127.0.0.1 left intact
 curl: (52) Empty reply from server
</code></pre>
<p>Empty reply... But is the process running?</p>
<pre><code>$&gt; sudo docker top 1c50b67d45b1
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                2084                812                 0                   10:26               ?                   00:00:00            python index.py
root                2117                2084                0                   10:26               ?                   00:00:00            /usr/bin/python index.py
</code></pre>
<p>Now let's ssh into the server and check...</p>
<pre><code>$&gt; sudo docker exec -it 1c50b67d45b1 bash
root@1c50b67d45b1:/srv# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN
tcp        0      0 127.0.0.1:47677         127.0.0.1:5000          TIME_WAIT
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
root@1c50b67d45b1:/srv# curl -I 127.0.0.1:5000
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 5447
Server: Werkzeug/0.10.4 Python/2.7.6
Date: Tue, 19 May 2015 12:18:14 GMT
</code></pre>
<p>It's fine... But not from the outside.<br />
What am I doing wrong?</p>
","<p>I have simple flask app which uses SQLLite database.</p>
<p>Here is my <code>app.py</code> code</p>
<pre><code>from flask import Flask
from flask_restful import Api
from flask_jwt import JWT
from datetime import timedelta

from db import db
from security import authenticate, identity
from resources.user import UserRegister
from resources.product import Product, ProductList


app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///data.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.secret_key = '@hsgvTyb@3gd###123'
api = Api(app)


@app.before_first_request
def create_tables():
  db.create_all()


jwt = JWT(app, authenticate, identity)  # /auth
app.config['JWT_EXPIRATION_DELTA'] = timedelta(seconds=1800)

api.add_resource(Product, '/product/&lt;string:name&gt;')
api.add_resource(ProductList, '/products')
api.add_resource(UserRegister, '/register')


if __name__ == '__main__':
  db.init_app(app)
  app.run(port=5000, debug=False)
</code></pre>
<p>DockerFile as below:</p>
<pre><code>FROM python:3
ADD app/ /app
WORKDIR /app
RUN apt-get update
RUN pip install -r requirements.txt
CMD python app.py
</code></pre>
<p>docker-compose.yml as below:</p>
<pre><code>version: &quot;2&quot;
services:
  app:
    container_name: test-146957-app
    build:
      context: ../
      dockerfile: deployments/app/Dockerfile
    ports:
      - &quot;5000:5000&quot;
</code></pre>
<p>Now when I execute <code>docker-compose up</code> command it is showing container is running but I hit any endpoints from postman, it does not return any response.</p>
<p><a href=""https://i.stack.imgur.com/PTa9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PTa9c.png"" alt=""enter image description here"" /></a></p>
"
"32257830","What are the potential security problems running untrusted code in a Docker container as a non-root user?","<security><docker><multi-tenant>","65278153","run user supplied python code inside docker. Is it safe and also speed up the process","<python><docker><subprocess>","<p>I've seen plenty of ink spilled by now about how Docker is not sufficiently isolated to allow arbitrary containers to be run in a multi-tenant environment, and that makes sense. ""If it's root in Docker, consider it root in the host machine."" What about non-root though?</p>

<p>If I want to take some untrusted code and run it in a container, can it be done safely so long as the container is running as a non-root non-sudo user? What are the potential security pitfalls of doing something like that?</p>

<p>I'm fairly sure there are production applications doing this today (CI systems, runnable pastebins), but are they just lucky not to have had a determined attacker or is this a reasonable thing to do in a production system?</p>
","<p>I have a user supplied python code.</p>
<p>Intially i wanted to use exec, but later learning the security loopholes i dropped</p>
<p>Now I am thinking of using docker to run in an isolation, so nothing bad can happen outside it</p>
<p>But the problem is i have more than 100K custom codes to run</p>
<p>so i wanted to use multiprocessing.</p>
<p>So putting together multiprocesing and running code using docker container with python i have to do something like</p>
<pre><code>def getScoreForEachMethod(customCode):
    process = subprocess.Popen(docker pythonImage sh python -c customCode)
    process.wait
    return score (i.e capture the output from subprocess)

if __name__ == &quot;__main__&quot;:
    #pool = multiprocessing.Pool(joblib.cpu_count()) 
    pool = multiprocessing.Pool(4)
    results = pool.map(getScoreForEachMethod, [list of custom methods])
    pool.close()
    pool.join()

</code></pre>
<p>But inside the multiprocess calling subprocess will definitely make it slow.</p>
<p>the other option run the entire multiprocessing inside docker i.e</p>
<pre><code>def getScoreForEachMethod(customCode):
    score =  output from customcode
    return score 

if __name__ == &quot;__main__&quot;:
    #pool = multiprocessing.Pool(joblib.cpu_count()) 
    pool = multiprocessing.Pool(4)
    results = pool.map(getScoreForEachMethod, [list of custom methods])
    pool.close()
    pool.join()

</code></pre>
<p>but here any customcode is malacious (like delete some system files inside docker), it will effect the running of rest of the scirpts.</p>
<p>Can we run docker command using subprocess more faster so that we can get benefit of noninterruption and isolation, where in the later speed is gauranteed by interuption is not</p>
"
"32257830","What are the potential security problems running untrusted code in a Docker container as a non-root user?","<security><docker><multi-tenant>","65153042","Securing a Docker Container","<docker><security>","<p>I've seen plenty of ink spilled by now about how Docker is not sufficiently isolated to allow arbitrary containers to be run in a multi-tenant environment, and that makes sense. ""If it's root in Docker, consider it root in the host machine."" What about non-root though?</p>

<p>If I want to take some untrusted code and run it in a container, can it be done safely so long as the container is running as a non-root non-sudo user? What are the potential security pitfalls of doing something like that?</p>

<p>I'm fairly sure there are production applications doing this today (CI systems, runnable pastebins), but are they just lucky not to have had a determined attacker or is this a reasonable thing to do in a production system?</p>
","<p>Suppose I have a docker file which creates an image that may execute a possibly malicious python program.
The goal is to get the output of this program, but I don't want my host machine to become compromised. The
<code>Dockerfile</code> is as follows:</p>
<pre><code> FROM python3.7-alpine
 COPY possiblyMalicious.py .
 ENTRYPOINT [&quot;python3&quot;,&quot;possiblyMalicious.py&quot;]
</code></pre>
<p>So then I build an image from this docker file and run a respective container. My question is: Is there a way to configure my host system such that the effects of <code>possiblyMalicious.py</code> stays and dies within the container, and if so how?</p>
"
"37242217","Access docker container from host using containers name","<docker><docker-compose>","65094480","How do I make a Docker Compose alias available to the local machine?","<docker><docker-compose>","<p>I am developing a service and using there docker compose to spin services like postgres, redis, elasticsearch. I have a web application that is based on RubyOnRails and writes and reads from all those services.</p>

<p>Here is my <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      - frontapp

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      - frontapp

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      - frontapp

networks:
  frontapp:
    driver: bridge
</code></pre>

<p>And i can ping containers within this network</p>

<pre><code>$ docker-compose run redis /bin/bash
root@777501e06c03:/data# ping postgres
PING postgres (172.20.0.2): 56 data bytes
64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.346 ms
64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.047 ms
...
</code></pre>

<p>So far so good. Now I want to run ruby on rails application on my host machine but be able to access postgres instance with url like <code>postgresql://username:password@postgres/database</code> currently that is not possible</p>

<pre><code>$ ping postgres
ping: unknown host postgres
</code></pre>

<p>I can see my network in docker</p>

<pre><code>$ docker network ls
NETWORK ID          NAME                DRIVER
ac394b85ce09        bridge              bridge              
0189d7e86b33        elephant_default    bridge              
7e00c70bde3b        elephant_frontapp   bridge              
a648554a72fa        host                host                
4ad9f0f41b36        none                null 
</code></pre>

<p>And I can see an interface to it</p>

<pre><code>$ ifconfig 
br-0189d7e86b33 Link encap:Ethernet  HWaddr 02:42:76:72:bb:c2  
          inet addr:172.18.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:76ff:fe72:bbc2/64 Scope:Link
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:36 errors:0 dropped:0 overruns:0 frame:0
          TX packets:60 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2000 (2.0 KB)  TX bytes:8792 (8.7 KB)

br-7e00c70bde3b Link encap:Ethernet  HWaddr 02:42:e7:d1:fe:29  
          inet addr:172.20.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e7ff:fed1:fe29/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1584 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1597 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:407137 (407.1 KB)  TX bytes:292299 (292.2 KB)
...
</code></pre>

<p>But i am not sure what should I do next. I tried to play a bit with <code>/etc/resolv.conf</code>, mainly with <code>nameserver</code> directive, but that had no effect.</p>

<p>I would appreciate any help of suggestions how to configure this setup correctly.</p>

<p><strong>UPDATE</strong></p>

<p>After browsing through Internet resources I managed to assign static IP addresses to boxes. For now it is enough for me to continue development. Here is my current <code>docker-compose.yml</code></p>

<pre><code>version: '2'

services:
  redis:
    image: redis:2.8
    networks:
      frontapp:
        ipv4_address: 172.25.0.11

  elasticsearch:
    image: elasticsearch:2.2
    networks:
      frontapp:
        ipv4_address: 172.25.0.12

  postgres:  
    image: postgres:9.5
    environment:
      POSTGRES_USER: elephant
      POSTGRES_PASSWORD: smarty_pants
      POSTGRES_DB: elephant
    volumes:
      - /var/lib/postgresql/data
    networks:
      frontapp:
        ipv4_address: 172.25.0.10

networks:
  frontapp:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16
          gateway: 172.25.0.1
</code></pre>
","<p>Apologies if this is a duplicate. It's very possible I'm not familiar enough with Docker to even know the right terminology to know what to look for.</p>
<p>For the sake of the question, suppose that I have a web app that has a frontend that's a single-page app written in Vue, and then a backend written in Python. I'd like to run some end-to-end tests with Docker Compose by making an environment with the frontend and backend plus a third container running TestCafé. I'd also like to be able to use the app locally to poke around (e.g., go to <code>localhost:3000</code> on my computer to get the frontend with all the data hooked up).</p>
<p>I have this partly working. The Docker Compose file runs all the services, and I've created a DNS record inside the Docker Compose universe so that the frontend is at <code>frontend.local</code> and the backend is at <code>backend.local</code>, so TestCafé can get the resources when the tests runs.</p>
<p>But, I have to specify a URL for the frontend code to access the backend. In this case, that's <code>backend.local</code>. Which works in the TestCafé container browser, but doesn't work when I use the frontend at <code>localhost:3000</code>. Is there a way to map <code>backend.local</code> and <code>frontend.local</code> to my computer's local network as well as in the Docker Compose environment? If so, how do I do that?</p>
<p>For reference, here's a rough sketch of what I'm doing with the <code>docker-compose.yml</code>:</p>
<pre><code>version: &quot;3.7&quot;
services:

  frontend:
    image: frontend:latest
    networks:
      default:
        aliases:
        - frontend.local
    build:
      context: .
      dockerfile: Dockerfile
    ports:
     - &quot;3000:3000&quot;
    depends_on:
      - &quot;backend&quot;

  testcafe:
    image: testcafe:latest
    networks:
      default:
    build:
      context: testcafe
      dockerfile: Dockerfile
    depends_on:
      - &quot;frontend&quot;
    volumes:
      - ./testcafe/artifacts:/artifacts

  backend:
    image: backend:latest
    ports:
     - &quot;8100:8100&quot;
     - &quot;8000:8000&quot;
    networks:
      default:
        aliases:
        - backend.local

networks:
  default:
</code></pre>
"
"39136601","Changing /proc/sys/kernel/core_pattern file inside docker container","<linux><docker><linux-kernel><dockerfile><privileges>","65101860","Change /proc/sys/kernel/core_pattern inside docker container, but also changed host machine value","<linux><docker>","<p>How can i change <code>/proc/sys/kernel/core_pattern</code> file inside the docker container with out privileged mode? Are there any flags to be passed to <code>docker daemon</code> or <code>docker run</code> or anything related to <code>Dockerfile</code>?</p>
","<p>I just add</p>
<pre><code>echo &quot;${POOL_RUNTIME_PATH}/core.%e.signal_%s.unix_time_%t&quot; &gt;/proc/sys/kernel/core_pattern
</code></pre>
<p>to my Dockerfile.<br />
But after I run Docker container, The host /proc/sys/kernel/core_pattern value also changed.<br />
Why?</p>
"
"41685603","Reset Docker Machine to run Docker commands on my local machine","<docker><docker-machine>","65188558","docker-machine - how to restore previous ""machine"" or default machine","<docker><docker-machine>","<p>I'm learning docker right now and I have run into something that's confusing. I started using <code>docker-machine</code>, which was great for creating a local VM and spinning up a server on Digital Ocean. However, I now want to return to a state where the docker commands execue on my local machine so that I can add features, build, and push my docker repo. </p>

<p>I know how to change to other machines using <code>docker-machine env</code>, but how do I go back to having no active machine? If I restart docker this happens, but I don't want to be restarting docker all of the time. It seems like there should be a command for this?</p>

<p>And just to clarify, I am not trying to connect to my <code>default</code> machine. I want the docker commands to execute on my machine like they did before I set up Docker Machine. </p>
","<p><strong>Problem Description</strong></p>
<p>I'm not sure how to switch back to my local docker desktop machine.</p>
<p><strong>Background</strong></p>
<p>I'm running on a mac... and I downloaded Docker desktop, created a few containers and everything was hunky dory.</p>
<p>Then I discovered docker-machine and so I:</p>
<ul>
<li>downloaded oracle virtual box.</li>
<li>created a new docker machine. &quot;docker-machine create virtualbox dev&quot; or something like that</li>
<li>successfully created a new container on this dev host.</li>
<li>tested to make sure my dummy app runs properly on this new host.</li>
</ul>
<p>admins-Mac-mini-3:dev admin$ <strong>docker-machine ls</strong>
NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
dev    *        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.12<br />
admins-Mac-mini-3:dev admin$</p>
<pre><code>admins-Mac-mini-3:dev admin$ **docker ps**
        CONTAINER ID   IMAGE            COMMAND             CREATED             STATUS              PORTS                  NAMES
        69892fee42e7       cbrewsapp   &quot;npm start&quot;         About an hour ago   Up About an hour    0.0.0.0:80-&gt;3000/tcp   vigorous_stonebraker
       admins-Mac-mini-3:dev admin$ 


admins-Mac-mini-3:dev admin$ **curl http://192.168.99.100**
        {&quot;msg&quot;:&quot;hello world&quot;}admins-Mac-mini-3:dev admin$ 
</code></pre>
<p>Now I need to know how to switch back to the original host i had going when i downloaded / set up Docker Desktop.</p>
"
"44938344","Cannot connect to MongoDB via node.js in Docker","<node.js><mongodb><docker><docker-compose><dockerfile>","65190197","Issue connecting a Node.js app to a MongoDB using docker-compose","<node.js><mongodb><docker><docker-compose>","<p>My node.js express app cannot connect to the MongoDB in a Docker. <em>I'm not that familiar with Docker.</em></p>

<p>node.js connection:</p>

<pre><code>import mongodb from 'mongodb';
...
mongodb.MongoClient.connect('mongodb://localhost:27017', ... );
</code></pre>

<p>Dockerfile:</p>

<pre><code>FROM node:argon
RUN mkdir /app
WORKDIR /app
COPY package.json /app
RUN npm install
COPY . /app
EXPOSE 3000
CMD [""npm"", ""start""]
</code></pre>

<p>docker-compose.yml</p>

<pre><code>version: “2”
 services:
  web:
   build: .
   volumes:
     — ./:/app
   ports:
   — “3000:3000”
   links:
    — mongo
   mongo:
    image: mongo
    ports:
      — “27017:27017”
</code></pre>

<p>Build command: <code>docker build -t NAME .</code></p>

<p>Run command: <code>docker run -ti -p 3000:3000 NAME</code></p>

<p>Connection error:</p>

<pre><code>[MongoError: failed to connect to server [localhost:27017] on first connect [MongoError: connect ECONNREFUSED 127.0.0.1:27017]]
  name: 'MongoError',
  message: 'failed to connect to server [localhost:27017] on first connect [MongoError: connect ECONNREFUSED 127.0.0.1:27017]'
</code></pre>
","<p>Hy, I am pretty new to docker and I am having some issue understanding how containers interact with each other in the docker world created using a docker compose</p>
<p>I have a docker-compose.yaml file that contain 2 services:</p>
<ol>
<li>A MongoDB service which is suppose to create a MongoDB database</li>
<li>A Node.js app service that is trying to connect to that MongoDB database</li>
</ol>
<p>When i try to run the docker-compose up command I get an error letting me know there was a issue connecting to my MongoDB instance.</p>
<pre><code>info: Listening on port 3000...
error: uncaughtException: failed to connect to server [localhost:27017] on first connect [Error: connect ECONNREFUSED 127.0.0.1:27017
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1146:16) {
  name: 'MongoNetworkError'
</code></pre>
<p>This is my docker-compose.yaml file</p>
<pre><code>version: '3.7'
services:
  mongodb:
    container_name: mongodb_container
    image: mongo:latest
    ports:
      - 27017:27017
    volumes:
      - mongodb_data_container:/data/db

  my_app: 
    container_name: my_app_container
    image: my_app:1.0
    ports: 
      - 3000:3000
</code></pre>
<p>Can anyone help me out with this issue?</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65256027","docker.sock: connect: permission denied","<docker><permissions>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I'm trying to start playing with docker but unfortunately I faced an issue with socket permission</p>
<pre><code>&gt;&gt;&gt; whoami
tomasz

&gt;&gt;&gt; grep tomasz /etc/group
docker:x:462:tomasz

&gt;&gt;&gt; docker run --rm hello-world
docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/create&quot;: dial unix /var/run/docker.sock: connect: permission denied.

&gt;&gt;&gt; ls -l /var/run/docker.sock
srw-rw---- 1 root docker 0 Dec 11 18:20 /var/run/docker.sock=
</code></pre>
<p>I've tried restart docker several times but it does not help. As you can see I am in docker group but this fact did not change anything. :)
Do you have any idea why?</p>
<p>ps sorry for my english.</p>
"
"48957195","How to fix docker: Got permission denied issue","<docker><docker-compose>","65116462","Dockerfile fails on build - cannot connect to docker daemon","<docker><dockerfile>","<p>I installed Docker in my machine where I have Ubuntu OS.<br />
When I run:</p>
<pre><code>sudo docker run hello-world
</code></pre>
<p>All is ok, but I want to hide the word <code>sudo</code> to make more short the command.<br />
If I write the command without the word <code>sudo</code></p>
<pre><code>docker run hello-world
</code></pre>
<p>That displays the following:</p>
<blockquote>
<p>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. See 'docker run --help'.</p>
</blockquote>
<p>It has happened the same when I try to make:</p>
<pre><code>docker-compose up
</code></pre>
<p>How can I resolve this?</p>
","<p>I have this dockerfile:</p>
<pre><code>FROM ubuntu
RUN DEBIAN_FRONTEND=noninteractive apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common

#install freecad
RUN add-apt-repository ppa:freecad-maintainers/freecad-daily
RUN DEBIAN_FRONTEND=noninteractive apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y freecad-daily

#install utils
RUN DEBIAN_FRONTEND=noninteractive apt install -y vim
RUN DEBIAN_FRONTEND=noninteractive apt install -y nano
RUN DEBIAN_FRONTEND=noninteractive apt install -y git

#install protobuf
RUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/protocolbuffers/protobuf
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y autoconf autogen
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y --fix-missing autoconf automake pkg-config #missing: libgtk-3-dev
RUN DEBIAN_FRONTEND=noninteractive apt install -y build-essential #needed for g++?
RUN cd protobuf &amp;&amp; ./autogen.sh
RUN cd protobuf &amp;&amp; ./configure
RUN cd protobuf &amp;&amp; make
RUN cd protobuf &amp;&amp; make install

#install libarcus
#https://community.ultimaker.com/topic/20409-stuck-up-in-creating-a-docker-image-for-curaengine/
RUN DEBIAN_FRONTEND=noninteractive apt install -y cmake  #missing:  protobuf-compiler python3-dev python3-sip-dev libprotoc-dev libprotobuf-dev
RUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/Ultimaker/libArcus.git \
    &amp;&amp; cd libArcus \
    &amp;&amp; sed -i '/option(BUILD_PYTHON &quot;Build &quot; ON)/c\option(BUILD_PYTHON &quot;Build &quot; OFF)' ./CMakeLists.txt \
    &amp;&amp; mkdir build &amp;&amp; cd build \
    &amp;&amp; ls -la .. \
    &amp;&amp; cmake .. \
    &amp;&amp; make -j4 \
    &amp;&amp; make install

#install cura-engine and cura
RUN git clone https://github.com/Ultimaker/CuraEngine.git
RUN git clone https://github.com/Ultimaker/Cura.git
RUN cd CuraEngine &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make

#install node
RUN apt-get install git-core curl build-essential openssl libssl-dev \
 &amp;&amp; git clone https://github.com/nodejs/node.git \
 &amp;&amp; cd node \
 &amp;&amp; ./configure \
 &amp;&amp; make \
 &amp;&amp; sudo make install

CMD [&quot;node&quot;,&quot;server.js&quot;]
</code></pre>
<p>I open the terminal and navigate to the directory this is in and i type:</p>
<pre><code>docker build .
</code></pre>
<p>I get this error:</p>
<pre><code>ERRO[0000] failed to dial gRPC: cannot connect to the Docker daemon. Is 'docker daemon' running on this host?: dial unix /var/run/docker.sock: connect: permission denied 
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;session=y3on0l6lkezz43j468liw0907&amp;shmsize=0&amp;target=&amp;ulimits=null&amp;version=1: dial unix /var/run/docker.sock: connect: permission denied
</code></pre>
<p>I have no clue what is the problem.... I do everything by the book...</p>
"
"49019652","not able to connect to mysql docker from local","<mysql><docker>","65155723","Mysql Docker ERROR 1045 (28000): Plugin caching_sha2_password could not be loaded","<mysql><docker>","<p>I am trying to connect to mysql database from docker image. However it's throwing errors. </p>

<p>following is the docker image I am using. 
<a href=""https://hub.docker.com/_/mysql/"" rel=""noreferrer"">https://hub.docker.com/_/mysql/</a></p>

<p>And following is the command I have used to run the docker image. </p>

<pre><code>docker run -p 3306:3306 --name mysql_80 -e MYSQL_ROOT_PASSWORD=password -d mysql:8
</code></pre>

<p>Following is the output of <code>docker ps</code> command </p>

<pre><code>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                              NAMES
9f35d2e39476        mysql:8             ""docker-entrypoint.s…""   5 minutes ago       Up 5 minutes        0.0.0.0:3306-&gt;3306/tcp
</code></pre>

<p>if I check the IP using docker inspect and ping that IP, it shows IP is not reachable. </p>

<pre><code>docker inspect 9f35d2e39476 | grep -i ipaddress
</code></pre>

<p>And if i try to connect using <code>localhost</code> and <code>127.0.0.1</code> I am getting following error. </p>

<blockquote>
  <p>Unable to load authentication plugin 'caching_sha2_password'.</p>
</blockquote>
","<p>I have setup a mysql container in Docker and i start it up and when i want to connect via DBeaver of a CLI with mysql command i get this error :</p>
<pre><code>ERROR 1045 (28000): Plugin caching_sha2_password could not be loaded: /usr/lib/x86_64-linux-gnu/mariadb19/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory

</code></pre>
<p>I managed to solve this partially with an answer from this <a href=""https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded"">github post</a> like that:</p>
<blockquote>
<p>I loged in the container</p>
<p>docker exec -it CONTAINER_ID bash then log into mysql as root</p>
<p>mysql --user=root --password Enter the password for root (Default is
'root') Finally Run:</p>
<p>ALTER USER 'username' IDENTIFIED WITH mysql_native_password BY
'password';</p>
</blockquote>
<p>And i say partially because every time i drop the database and start it up again, it gives me the same error and i have again to change the password in order to connect successfully.
Take in mind that i just put the same password and not a new one. It's not a wrong password error.</p>
"
"52640304","standard_init_linux.go:190: exec user process caused ""no such file or directory"" Docker with go basic web app","<docker><go><dockerfile>","65123256","standard_init_linux.go:211: exec user process caused ""no such file or directory"" from using net/http package","<docker><go>","<p>The very basic web app is created in Go</p>
<pre><code>package main

import(
   &quot;fmt&quot;
   &quot;net/http&quot;
   &quot;os&quot;
)

func hostHandler(w http.ResponseWriter, r *http.Request){
    name, err :=os.Hostname()

    if err != nil {
           panic(err)
        }

        fmt.Fprintf(w, &quot;&lt;h1&gt;HOSTNAME: %s&lt;/h1&gt;&lt;br&gt;&quot;,name)
        fmt.Fprintf(w, &quot;&lt;h1&gt;ENVIRONMENT VARS: &lt;/h1&gt;&lt;br&gt;&quot;)
        fmt.Fprintf(w, &quot;&lt;ul&gt;&quot;)

        for _, evar := range os.Environ(){
            fmt.Fprintf(w, &quot;&lt;li&gt;%s&lt;/li&gt;&quot;,evar)
        }
        fmt.Fprintf(w, &quot;&lt;/ul&gt;&quot;)

}

func rootHandler(w http.ResponseWriter, r *http.Request){

    fmt.Fprintf(w, &quot;&lt;h1&gt;Awesome site in Go!&lt;/h1&gt;&lt;br&gt;&quot;)
    fmt.Fprintf(w, &quot;&lt;a href='/host/'&gt;Host info&lt;/a&gt;&lt;br&gt;&quot;)

}

func main() {

        http.HandleFunc(&quot;/&quot;, rootHandler)
        http.HandleFunc(&quot;/host/&quot;, hostHandler)
        http.ListenAndServe(&quot;:8080&quot;, nil)


}
</code></pre>
<p>Docker File for it</p>
<pre><code>FROM scratch
WORKDIR /home/ubuntu/go
COPY webapp /
EXPOSE 8080
CMD [&quot;/webapp&quot;]
</code></pre>
<p>The image is built successfully</p>
<pre><code>ubuntu@ip-172-31-32-125:~/go/src/hello$ docker build -t &quot;webapp&quot; .
Sending build context to Docker daemon  6.152MB
Step 1/5 : FROM scratch
 ---&gt;
Step 2/5 : WORKDIR /home/ubuntu/go
 ---&gt; Using cache
 ---&gt; 8810a06c58c7
Step 3/5 : COPY webapp /
 ---&gt; Using cache
 ---&gt; d75222363d3a
Step 4/5 : EXPOSE 8080
 ---&gt; Using cache
 ---&gt; 45de0853de8e
Step 5/5 : CMD [&quot;/webapp&quot;]
 ---&gt; Using cache
 ---&gt; e9f9031f3632
Successfully built e9f9031f3632
Successfully tagged webapp:latest
</code></pre>
<p>But when i run the docker its show error.</p>
<pre><code>ubuntu@ip:~/go/src/hello$ docker run webapp
standard_init_linux.go:190: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>Please guide what is the issue, I am new to docker and go.</p>
<p>Environment-related information</p>
<pre><code>ubuntu@ip:~/go/src/hello$ ls
Dockerfile  webapp
ubuntu@ip:~/go/src/hello$ echo $GOPATH
/home/ubuntu/go
</code></pre>
<p>Code was compiled with <strong>go build webapp.go</strong> command</p>
","<p>Steps to reproduce the problem:</p>
<ol>
<li><code>main.go</code> is using <code>net/http</code>.</li>
</ol>
<pre><code>docker build -o . main.go   // This generates the main executable.
</code></pre>
<ol start=""2"">
<li>Now preparing one Dockerfile</li>
</ol>
<pre><code> FROM alpine:3.6
 ENTRYPOINT [&quot;/home/myhome/main&quot;]
</code></pre>
<ol start=""3"">
<li>Then, <code>docker build -t myimag3</code></li>
<li>Now when I run this image</li>
</ol>
<pre><code>docker run -it -v /home/myhome:/home/myhome myimage
</code></pre>
<p>it results in the following error:</p>
<pre><code>getting : standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;
</code></pre>
<p>I am facing this problem because of the main executable from <code>main.go</code>. If i remove all content and simply putting <code>fmt.Println(&quot;Hello world&quot;)</code> then it is working properly.
Only thing is when I try to use <code>net/http</code> packages at that time it is giving this error.
I am new to this go lang. Can anybody please provide some suggestions?</p>
<p>Code to <code>main.go</code></p>
<pre><code>https://tutorialedge.net/golang/go-docker-tutorial/
</code></pre>
"
"52946810","error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info","<docker>","65196724","Cannot find the file. ERROR during connect","<docker><dockerfile>","<p>System info:
Windows 10 pro 64 bit</p>

<p>C:\WINDOWS\system32>docker --version</p>

<p><strong>Docker version 18.06.1-ce, build e68fc7a</strong></p>

<p>C:\WINDOWS\system32>docker info</p>

<p><strong>error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>

<p>C:\WINDOWS\system32>docker pull hello-world</p>

<p>Using default tag: latest</p>

<p><strong>Warning: failed to get default registry endpoint from daemon (error during connect: Get <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/info</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.). Using system default: <a href=""https://index.docker.io/v1/"" rel=""noreferrer"">https://index.docker.io/v1/</a>
error during connect: Post <a href=""http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest"" rel=""noreferrer"">http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.38/images/create?fromImage=hello-world&amp;tag=latest</a>: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</strong></p>
","<p>C:\Users\maggi&gt;docker images
error during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/images/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.</p>
"
"55603421","Accessing Docker Volume content on MacOS","<macos><docker><docker-volume>","65201369","Accessing Volume data directly on Docker Desktop","<docker><docker-compose>","<p>When I do a <code>docker volume inspect &lt;dockerid&gt;</code> on a Mac, I can see the path to the data, this appears as a <code>/var/lib/docker/volumes/&lt;volume name&gt;</code></p>

<p>On a Mac, this link does not exist, because docker runs on inside a very tiny VM.</p>

<p>I can use <code>screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty</code> to get into the vm and then navigate to the folder to see the volumes.</p>

<p>So got all that, but my question is: <strong>How do I link what is in these volumes on my host machine?</strong></p>

<p>I have tried this: <code>docker run -it --volume hello:/hello2 --name access_volumes busybox:latest /bin/sh</code> Where hello is the name of a volume I have created.</p>

<p>I can link a folder on my host machine to the container, but I want to backup the content or edit the content of the Volume from my host machine.</p>

<p>How do I do that?</p>
","<p>Reading about <a href=""https://github.com/docker/getting-started/blob/master/docs/tutorial/persisting-our-data/index.md"" rel=""nofollow noreferrer"">persisting DB data</a> with Docker. It says:</p>
<blockquote>
<p>While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM.</p>
</blockquote>
<p>What does the part about &quot;you would need to first get inside of the VM&quot; actually mean in this context?</p>
<p>What I've done so far:</p>
<ul>
<li>Created a volume using <code>docker volume create todo-db</code></li>
<li>Run <code>docker volume inspect todo-db</code></li>
</ul>
<p>The <code>inspect</code> command output gives me this:</p>
<pre><code>[
    {
        &quot;CreatedAt&quot;: &quot;2020-12-08T14:43:23Z&quot;,
        &quot;Driver&quot;: &quot;local&quot;,
        &quot;Labels&quot;: {},
        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/todo-db/_data&quot;,
        &quot;Name&quot;: &quot;todo-db&quot;,
        &quot;Options&quot;: {},
        &quot;Scope&quot;: &quot;local&quot;
    }
]
</code></pre>
<p>The Mountpoint is specified as <code>/var/lib/docker/volumes/todo-db/_data</code>.</p>
<p>This does not appear to exist on either my local machine (where I'm running Docker) or when opening the CLI through Docker Desktop for my image.</p>
<p>It mentions root access so I've done <code>sudo su</code> on my local machine then tried</p>
<pre><code>cd /var/lib/docker/volumes/todo-db/_data
</code></pre>
<p>This errors with:</p>
<pre><code>cd: no such file or directory: /var/lib/docker/volumes/todo-db/_data
</code></pre>
<p>Because the documentation says &quot;you would need to first get inside of the VM&quot; I've tried opening the CLI using Docker Desktop for my image then trying</p>
<pre><code>cd /var/lib/docker/volumes/todo-db
</code></pre>
<p>This errors with:</p>
<pre><code>/bin/sh: cd: can't cd to /var/lib/docker/volumes/todo-db: No such file or directory
</code></pre>
<p>Where is that directory? And what does &quot;inside of the VM&quot; actually refer to here?</p>
<p>Running macOS 10.15.6 and Docker Desktop version 2.5.0.1 with Engine 19.03.13</p>
"
"59615266","Super Slow Docker Build","<php><docker><docker-compose><dockerfile>","65231110","Docker Takes Ages Before Starting Build","<performance><docker><docker-compose><docker-desktop><wsl-2>","<p>I think I'm going to go crazy.  I've searched all over and can't seem to find a working solution both here on Stack, GitHub and other far reaches of the interwebs.</p>

<p>On this particular project, running <code>docker-compose build</code> is taking <strong><em>FOREVER</em></strong>.  It didn't use to be this way, and on other projects that use Docker, it's not an issue at all.  And by forever... I'm talking around 10-15 minute build times when it used to only take around 2 minutes tops.  I had two separate coworkers DL the <em>same repo</em> (one on Ubuntu 18, and the other on macOS 14.x).  When they ran the <code>build</code> command, the entire process took ~2 minutes.  Both of these people had never built this project before, so they were starting from complete scratch.</p>

<p>I've uninstalled/reinstalled Docker, ran a complete <code>docker system prune -a</code>, connected via wifi, connected via Ethernet, tried a different wifi network, tweaked my compose file, tweaked my docker file -- nothing.  </p>

<p>My machine is a 2018 MacBook Pro with a quad-core 2.7GHz i7, running macOS 10.14.6 with 16gb of installed RAM with Docker Desktop 2.1.0.5.</p>

<p>I've allowed Docker Desktop to have up to 12gb or RAM.  During the build process, my machine cpu usage spikes on average from 110% up to 270% running the <code>com.docker.hyperkit</code> process.</p>

<p>To be clear, it's hanging on the ""Building php"" (or ""Building web"") status message(s) before anything really even starts. After that, the actual build process runs smoothly and quick.</p>

<p>Here is my <strong>docker-compose.yaml</strong> file:</p>

<pre><code>version: '3.1'

services:
  db:
    container_name: clientsname.db
    hostname: db
    image: mariadb:10.4.1-bionic
    volumes:
      - ./db-data:/var/lib/mysql:delegated
    ports:
      - 3307:3306
    environment:
      MYSQL_DATABASE: my_database
      MYSQL_USER: my_user
      MYSQL_PASSWORD: my_pass
      MYSQL_ROOT_PASSWORD: my_pass

  php:
    container_name: clientsname.php
    hostname: php
    build:
      dockerfile: php/php.dockerfile
      context: ./
    environment:
      XDEBUG_CONFIG: remote_host=${REMOTE_HOST}

    volumes:
      - ../web:/var/www/web
      - ../moodle:/var/www/moodle
      - ../moodledata:/var/www/moodledata
      - ./php/custom.ini:/usr/local/etc/php/conf.d/zzz-custom.ini
      - ./php/z-errors.ini:/usr/local/etc/php/conf.d/z-errors.ini:delegated
      - ./php/z-upload.ini:/usr/local/etc/php/conf.d/z-upload.ini:delegated
      - ./php/z-xdebug.ini:/usr/local/etc/php/conf.d/z-xdebug.ini:delegated
    depends_on:
      - db

  web:
    container_name: clientsname.web
    hostname: web
    build:
      dockerfile: nginx/nginx.dockerfile
      context: ./
    volumes:
      - ../web:/var/www/web
      - ../moodle:/var/www/moodle
      - ../moodledata:/var/www/moodledata
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs:/var/log/nginx
    ports:
      - 80:80
      - 443:443
    depends_on:
      - php
      - db

</code></pre>

<p>Here is the referenced <strong>php.dockerfile</strong> file:</p>

<pre><code>FROM php:7.2.26-fpm
LABEL maintainer=""My Clients Name""

# Environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV COMPOSER_ALLOW_SUPERUSER=1
ENV COMPOSER_NO_INTERACTION=1
ENV COMPOSER_HOME=/usr/local/share/composer

# Working Directory
WORKDIR /var/www/web
WORKDIR /var/www/moodle
WORKDIR /var/www/moodledata


RUN rm /etc/apt/preferences.d/no-debian-php &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends apt-utils \
        build-essential     \
        php-soap            \
        libzip-dev          \
        libmagickcore-dev   \
        libmagickwand-dev   \
        libmagic-dev        \
        libpng-dev          \
        libfreetype6-dev    \
        libjpeg62-turbo-dev \
        libmcrypt-dev       \
        libmemcached-dev    \
        zlib1g-dev          \
        nano                \
        sudo                \
        gnupg               \
        curl                \
        unzip &amp;&amp;            \
    docker-php-ext-install soap pdo_mysql mysqli &amp;&amp; \
    docker-php-ext-install -j$(nproc) gd iconv &amp;&amp; \
    docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ &amp;&amp; \
    pecl install zip-1.15.2 imagick memcached-3.0.4 xdebug &amp;&amp; \
    docker-php-ext-enable memcached imagick zip xdebug

# Install Composer, Node, Gulp, and SASS
RUN curl -s https://getcomposer.org/installer | php &amp;&amp; mv composer.phar /usr/local/bin/composer

RUN curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - &amp;&amp; apt-get install -y nodejs &amp;&amp; npm install npm@latest -g &amp;&amp; npm install --global gulp-cli &amp;&amp; npm config set unsafe-perm=true


# Export composer vendor path
RUN echo """" &gt;&gt; ~/.bashrc &amp;&amp; echo 'export PATH=""$HOME/.composer/vendor/bin:$PATH""' &gt;&gt; ~/.bashrc

</code></pre>

<p>And the referenced <strong>nginx.dockerfile</strong></p>

<pre><code>FROM nginx:stable-alpine

RUN apk add --update bash &amp;&amp; rm -rf /var/cache/apk/*

WORKDIR /var/www/web

</code></pre>

<p>It's driving me batty... what in the bloody hell could I be doing wrong?  If there is anything I've left out that you'd all like to know, please let me know and I'll update the post.</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Thanks to @BMitch and you all who have commented thus far.  I took my entire /docker build directory and moved it into a test folder, and then created empty /web, /moodle, and /moodledata directories before running the <code>build</code> command.  It started to compile immediately.</p>

<p>What's curious to me is that the other coworkers DL'd the same Git repo that I have and did not have any of the same issues.  Oooh... come to think of it... I bet I know what the issue is.</p>
","<p>Like many people, I was happy to hear about WSL 2, and about the improvements that it brings, including the ability to virtualize from it, meaning I could now use Docker directly from Linux. But when I build any image, the following happens.</p>
<h3>Symptoms</h3>
<ul>
<li><code>vmmem</code> grows to about 3.5GB (this is not a problem).</li>
<li>It says <code>building &lt;myservice&gt;</code>, as usual;</li>
<li>Nothing else happens for <strong>2-5 minutes</strong>, while <code>vmmem</code> consumes about 7-8% of my CPU.</li>
<li>Only after this the actual building starts, and output begins to appear. At this point the speed is OK.</li>
</ul>
<h3>Specs</h3>
<ul>
<li>Razer Blade Pro 17.</li>
<li>i7-9750H</li>
<li>32GB DDR4</li>
</ul>
<h3>.wslconfig</h3>
<pre><code>[wsl2] 
memory=10000MB
processors=4
swap=0
localhostForwarding=true
</code></pre>
<h3>Remarks</h3>
<p>It's probably related to <a href=""https://stackoverflow.com/questions/62154016/docker-on-wsl2-very-slow"">this question</a>, but I'm talking about build performance, not performance inside the container.</p>
"
"65090971","docker daemon unexpectedly exits","<docker>","65092848","Docker daemon stops on multiple systems at the same time","<docker><crash><daemon>","<p>Docker exited for no apparent reason and I'm trying to understand what happened.</p>
<p>Right now the docker daemon is a loaded service, inactive with an exit status 0 SUCCESS.</p>
<pre><code>$ systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
   Active: inactive (dead) since Tue 2020-12-01 06:25:16 UTC;
     Docs: https://docs.docker.com
 Main PID: 2218 (code=exited, status=0/SUCCESS)
</code></pre>
<p>Looking at the docker logs, this happened because it processed signal 'terminated':</p>
<pre><code>$ journalctl -u docker.service | tail -25
Nov 30 18:30:21 ip-10-38-4-210 dockerd[2218]: time=&quot;2020-11-30T18:30:21.728694550Z&quot; &lt;redacted irrelevant&gt;
Dec 01 06:25:05 ip-10-38-4-210 systemd[1]: Stopping Docker Application Container Engine...
Dec 01 06:25:05 ip-10-38-4-210 dockerd[2218]: time=&quot;2020-12-01T06:25:05.867748396Z&quot; level=info msg=&quot;Processing signal 'terminated'&quot;
Dec 01 06:25:16 ip-10-38-4-210 systemd[1]: Stopped Docker Application Container Engine.
</code></pre>
<p>No user was logged in on that host at that time, no user explicitly terminated the docker daemon.</p>
<ul>
<li>Why is this happening, what logs could provide a clue?</li>
<li>Can I configure dockerd to restart automatically upon exits?</li>
</ul>
<p>This is Docker version 18.09.7 (build 2d0083d) on Ubuntu 16.04.6 LTS on x86-64.</p>
<p>The relationship with systemd is being asked on Unix.stackexchange <a href=""https://unix.stackexchange.com/questions/622355/systemd-stopped-docker-daemon-for-no-obvious-reason"">here</a></p>
","<p>I have a situation where multiple docker daemons stopped on separate AWS instances in relatively close time frame (all in the same 45 minutes, two in the same 10 minutes).</p>
<p>There appears</p>
<p>Stopping Docker Application Container Engine...</p>
<p>in the logs without any previous event.</p>
<p>It is followed by</p>
<p>level=info msg=&quot;Processing signal 'terminated'&quot;
level=info msg=&quot;ignoring event&quot; module=libcontainerd namespace=moby topic=/tasks/delete type=&quot;*events.TaskDelete&quot;</p>
<p>and finally</p>
<p>Stopped Docker Application Container Engine.</p>
<p>after what Docker Daemon has to be started manually.</p>
<p>It happened before in one system but now it appeared in 3 systems simultaneously.</p>
<p>Application logs do not indicate anything suspicious.</p>
<p>Does anybody have similar experience? I am wondering at the moment what steps I should take to have more information during next such event.</p>
"
"3790454","How do I break a string in YAML over multiple lines?","<string><syntax><yaml><newline>","57483708","Docker-compose multi-line command","<docker><docker-compose><yaml>","<p>In YAML, I have a string that's very long. I want to keep this within the 80-column (or so) view of my editor, so I'd like to break the string. What's the syntax for this?</p>

<p>In other words, I have this:</p>

<pre><code>Key: 'this is my very very very very very very long string'
</code></pre>

<p>and I'd like to have this (or something to this effect):</p>

<pre><code>Key: 'this is my very very very ' +
     'long string'
</code></pre>

<p>I'd like to use quotes as above, so I don't need to escape anything within the string.</p>
","<p>I'm trying to figure out how to split a command to be on multiple lines in my docker-compose.yml file. The command is pretty long, and I would prefer to be able to see it without scrolling to the right in the file.</p>

<pre><code>services:
  mysql:
    ...
    command: mysqld --character-sets-dir='/usr/share/mysql/charsets' --character-set-client-handshake=OFF --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --init-connect='SET NAMES utf8mb4;'
</code></pre>

<p>Is there a way to split this into multiple lines that Docker will work with?</p>
"
"8405087","What is this date format? 2011-08-12T20:17:46.384Z","<java><date><time><format><simpledateformat>","56636872","What is the difference between date format “2019-06-17-04:00” and “2019-06-17Z”? exactly what time does “2019-06-17Z” point to?","<java><spring-boot><docker><openshift>","<p>I have the following date:  <code>2011-08-12T20:17:46.384Z</code>.  What format is this?  I'm trying to parse it with Java 1.4 via <code>DateFormat.getDateInstance().parse(dateStr)</code> and I'm getting</p>

<blockquote>
  <p>java.text.ParseException: Unparseable date: ""2011-08-12T20:17:46.384Z""</p>
</blockquote>

<p>I think I should be using <a href=""http://docs.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html"" rel=""noreferrer"">SimpleDateFormat</a> for parsing, but I have to know the format string first.  All I have for that so far is <code>yyyy-MM-dd</code>, because I don't know what the <code>T</code> means in this string--something time zone-related?  This date string is coming from the <code>lcmis:downloadedOn</code> tag shown on <a href=""http://www-10.lotus.com/ldd/appdevwiki.nsf/dx/Files_CMIS_download_history_media_type"" rel=""noreferrer"">Files CMIS download history media type</a>.</p>
","<p>I’m using <code>XMLGregorianCalendar</code> in my spring boot app to define a date range and using the same in the input while calling an REST service.  However, when I’m calling the service from my local, I see the date is being set as “2019-06-17-04:00” in the REST input XML. If I run the same app in Openshift container, the date is being set as “2019-06-17Z” in the request XML. Can you please let me know the reason for this? And what is the difference between these two date formats? </p>

<pre><code>XMLGregorianCalendar toDate = DatatypeFactory.newInstance().newXMLGregorianCalendar(new GregorianCalendar());
XMLGregorianCalendar fromDate = DatatypeFactory.newInstance().newXMLGregorianCalendar(&lt;some date&gt;);
</code></pre>
"
"13363553","Git error: ""Host Key Verification Failed"" when connecting to remote repository","<git><ssh><ssh-keys>","57089742","Composer in Docker not passing verification for private repo","<git><docker><ssl><ssh><composer-php>","<p>I am trying to connect to a remote Git repository that resides on my web server and clone it to my machine.</p>

<p>I am using the following format for my command:</p>

<pre><code>git clone ssh://username@domain.com/repository.git
</code></pre>

<p>This has worked fine for most of my team members.  Usually after running this command Git will prompt for the user's password, and then run the cloning.  However, when running on one of my machines I get the following error:</p>

<blockquote>
  <p>Host key verification failed. </p>
  
  <p>fatal: Could not read from remote
  repository.</p>
</blockquote>

<p>We are not using SSH keys to connect to this repository, so I'm not sure why Git is checking for one on this particular machine.</p>
","<p>I have a Docker container which is bringing up a project which has dependencies in a private repo. </p>

<p><code>Dockerfile</code> is copying my passwordless SSH keys (permissions are <code>600</code> &amp; <code>644</code>):</p>

<pre><code>COPY docker/config/id_rsa /root/.ssh/
COPY docker/config/id_rsa.pub /root/.ssh/
</code></pre>

<p>After copying, Composer dependencies are getting installed:</p>

<pre><code>RUN echo *** &gt;&gt; /etc/hosts \
  &amp;&amp; composer config -a -g *URL *USER *PASS \
  &amp;&amp; composer install --prefer-dist --no-progress
</code></pre>

<p>At the same time I have deleted <code>composer.lock</code> to make sure nothing is left from previous installs. </p>

<p>Repositories part from composer looks like this:</p>

<pre><code>""repositories"": [
    {
        ""type"": ""vcs"",
        ""url"": ""git@***:***/libs/***.git"",
        ""options"": {
            ""ssl"": {
                ""local_cert"": ""~/.ssh/id_rsa.pub""
            }
        }
    },
],
</code></pre>

<p>And during the creation of container I am getting an error saying:</p>

<pre><code>[RuntimeException]
Failed to execute git clone --mirror 'git@***:***/libs/***.git' '/root/.composer/cache/vcs/.../'
Cloning into bare repository '/root/.composer/cache/vcs/...'...
Host key verification failed.
fatal: Could not read from remote repository.
Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>Repository does indeed exist, SSH keys are valid, and strangest of all...if I omit the <code>composer install</code> command and enter the created container, change nothing and do it manually from terminal, it installs everything. </p>

<p><strong>EDIT:</strong></p>

<p>I have also tried manually in the <code>RUN</code> command writing the keys if by any case they weren't available during the container creation, but that didn't help.</p>

<p>I have also tried removing <code>""options""</code> section from Composer </p>
"
"16047306","How is Docker different from a virtual machine?","<docker><containers><virtual-machine><virtualization>","57579382","When should I use a Docker and when should I use a Virtual Machine?","<docker><virtual-machine><vmware><hypervisor><ovirt>","<p>I keep rereading <a href=""https://docs.docker.com/"" rel=""noreferrer"">the Docker documentation</a> to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy?</p>

<p>Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment?</p>
","<p>Is there any guidelines on when to use Dockers over VM's? (or vice versa)</p>

<p>It seems to me that services like NGINX, Apache, or Redis, should be a docker, but I am unsure if say an ElasticSearch docker should be used in a HPC environment. </p>

<p>Is a Docker always better then a VM? </p>
"
"16529658","Shell script not running, command not found","<bash><shell><unix><scripting>","57585782","'command not found' when trying to run bash script","<linux><bash><docker><unix>","<p>I am very, very new to UNIX programming (running on MacOSX Mountain Lion via Terminal). I've been learning the basics from a bioinformatics and molecular methods course (we've had two classes) where we will eventually be using perl and python for data management purposes.  Anyway, we have been tasked with writing a shell script to take data from a group of files and write it to a new file in a format that can be read by a specific program (Migrate-N). </p>

<p>I have gotten a number of functions to do exactly what I need independently when I type them into the command line, but when I put them all together in a script and try to run it I get an error. Here are the details (I apologize for the length):</p>

<pre><code>#! /bin/bash

grep -f Samples.NFCup.txt locus1.fasta &gt; locus1.NFCup.txt
grep -f Samples.NFCup.txt locus2.fasta &gt; locus2.NFCup.txt
grep -f Samples.NFCup.txt locus3.fasta &gt; locus3.NFCup.txt
grep -f Samples.NFCup.txt locus4.fasta &gt; locus4.NFCup.txt
grep -f Samples.NFCup.txt locus5.fasta &gt; locus5.NFCup.txt
grep -f Samples.Salmon.txt locus1.fasta &gt; locus1.Salmon.txt
grep -f Samples.Salmon.txt locus2.fasta &gt; locus2.Salmon.txt
grep -f Samples.Salmon.txt locus3.fasta &gt; locus3.Salmon.txt
grep -f Samples.Salmon.txt locus4.fasta &gt; locus4.Salmon.txt
grep -f Samples.Salmon.txt locus5.fasta &gt; locus5.Salmon.txt
grep -f Samples.Cascades.txt locus1.fasta &gt; locus1.Cascades.txt
grep -f Samples.Cascades.txt locus2.fasta &gt; locus2.Cascades.txt
grep -f Samples.Cascades.txt locus3.fasta &gt; locus3.Cascades.txt
grep -f Samples.Cascades.txt locus4.fasta &gt; locus4.Cascades.txt
grep -f Samples.Cascades.txt locus5.fasta &gt; locus5.Cascades.txt

echo 3 5 Salex_melanopsis &gt; Smelanopsis.mig
echo 656 708 847 1159 779 &gt;&gt; Smelanopsis.mig
echo 154 124 120 74 126 NFCup &gt;&gt; Smelanopsis.mig
cat locus1.NFCup.txt locus2.NFCup.txt locus3.NFCup.txt locus4.NFCup.txt locus5.NFCup.txt &gt;&gt; Smelanopsis.mig
echo 32 30 30 18 38 Salmon River &gt;&gt; Smelanopsis.mig
cat locus1.Salmon.txt locus2.Salmon.txt locus3.Salmon.txt locus4.Salmon.txt locus5.Salmon.txt &gt;&gt; Smelanopsis.mig
echo 56 52 24 29 48 Cascades &gt;&gt; Smelanopsis.mig
cat locus1.Cascades.txt locus2.Cascades.txt locus3.Cascades.txt locus4.Cascades.txt locus5.Cascades.txt &gt;&gt; Smelanopsis.mig
</code></pre>

<p>The series of greps are just pulling out DNA sequence data for each site for each locus into new text files. The Samples...txt files have the sample ID numbers for a site, the .fasta files have the sequence information organized by sample ID; the grepping works just fine in command line if I run it individually. </p>

<p>The second group of code creates the actual new file I need to end up with, that ends in .mig. The echo lines are data about counts (basepairs per locus, populations in the analysis, samples per site, etc.) that the program needs information on. The cat lines are to mash together the locus by site data created by all the grepping below the site-specific information dictated in the echo line. You no doubt get the picture.</p>

<p>For creating the shell script I've been starting in Excel so I can easily copy-paste/autofill cells, saving as tab-delimited text, then opening that text file in TextWrangler to remove the tabs before saving as a .sh file (Line breaks: Unix (LF) and Encoding: Unicode (UTF-8)) in the same directory as all the files used in the script. I've tried using <code>chmod +x FILENAME.sh</code> and <code>chmod u+x FILENAME.sh</code> to try to make sure it is executable, but to no avail. Even if I cut the script down to just a single grep line (with the #! /bin/bash first line) I can't get it to work.  The process only takes a moment when I type it directly into the command line as none of these files are larger than 160KB and some are significantly smaller. This is what I type in and what I get when I try to run the file (HW is the correct directory)</p>

<pre><code>localhost:HW Mirel$ MigrateNshell.sh
-bash: MigrateNshell.sh: command not found
</code></pre>

<p>I've been at this impass for two days now, so any input would be greatly appreciated! Thanks!!</p>
","<p>I am trying to run a bash script from a script called <strong>dev_ro</strong>, here is how it's being called. </p>

<pre><code>export SUBNET=""$(first_available_docker_network --lock-seconds 7200)""
</code></pre>

<p>I am calling dev_ro by ./dev_ro </p>

<p>I am confirm I have</p>

<pre><code>#!/bin/bash
</code></pre>

<p>at the top of both files. </p>

<p>Here are perms for both files</p>

<pre><code> $ ls -lh dev_ro 
-rwxrwxr-x 1 ME ME 423 Aug 21 15:57 dev_ro

$ ls -lh first_available_docker_network
-rwxrwxr-x 1 ME ME 2.2K Aug 21 15:55 first_available_docker_network
</code></pre>

<p>This is the output from running ./dev_ro </p>

<pre><code>++ first_available_docker_network --lock-seconds 7200
compose/everest-compose: line 25: first_available_docker_network: command not found
</code></pre>

<p>Additionally when I try to run the script:</p>

<pre><code>ME@SERVER:~/Rosetta/compose$ first_available_docker_network
first_available_docker_network: command not found
ME@SERVER:~/Rosetta/compose$ 
</code></pre>

<p>I have the same setup running on a different server and it's working. The code was pulled from Git, so it's the same codebase. </p>

<p>Any help is much appreciated. </p>

<pre><code>ME@OTHER_SERVER:~/Rosetta/compose$ first_available_docker_network
DEBUG:root:Docker subnets: [IPv4Network(... etc
ME@OTHER_SERVER:~/Rosetta/compose$ ^C
</code></pre>
"
"18093928","What does ""Could not find or load main class"" mean?","<java><class><main>","56782565","Docker to Run Small Java Class: Could Not Find Or Load Main Class (Even Though It Exists)","<java><docker>","<p>A common problem that new Java developers experience is that their programs fail to run with the error message:  <code>Could not find or load main class ...</code></p>

<p>What does this mean, what causes it, and how should you fix it?</p>
","<p>I realize that this question may already have been asked, but in my research I can't find an answer.  I'm probably making a simple mistake.</p>

<p>I would like to run a Java class locally with Docker, inside a container.  Below is my Dockerfile:</p>

<p><strong>Dockerfile</strong></p>

<pre><code>FROM maven:3.5.2-jdk-8

COPY src /src

RUN javac src/java/com/Main.java
CMD java src/java/com/Main
</code></pre>

<p>I then run these commands in order:</p>

<pre><code>docker build -t my_image_6_26_19:latest .
docker run -it my_image_6_26_19:latest
</code></pre>

<p>The <code>build</code> command runs fine, but the <code>run</code> command throws the following error:</p>

<pre><code>Error: Could not find or load main class src.java.com.Main
</code></pre>

<p>I have reviewed the following questions on SO, but no answers seem to work (or maybe I didn't catch the solution): </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/49922516/buildning-docker-image-from-dockerfile-with-maven-error-could-not-find-or-lo"">Buildning docker image from Dockerfile with maven - Error: &quot;Could not find or load main class&quot;</a></li>
<li><a href=""https://stackoverflow.com/questions/35291076/dockerfile-to-run-a-java-program"">DockerFile to run a java program</a></li>
<li><a href=""https://stackoverflow.com/questions/54144292/docker-error-could-not-find-or-load-main-class-main-main"">Docker Error: Could not find or load Main class Main.Main</a></li>
</ul>

<p>When I build the container, through some debugging (<code>RUN ls /src/java/com</code>) I can see that a file <code>Main.class</code> is being created.  I'm not sure why that file can't be found.  Additionally, I have tried changing the last line of my Dockerfile to <code>CMD java src/java/com/Main.class</code>, but no luck.</p>
"
"22111060","What is the difference between ""expose"" and ""publish"" in Docker?","<docker>","57670050","Why expose port is not used in Dockerfile?","<docker>","<p>I'm experimenting with Dockerfiles, and I think I understand most of the logic. However, I don't see the difference between ""exposing"" and ""publishing"" a port in this context.</p>

<p>All the tutorials I have seen first include the <code>EXPOSE</code> command in the Dockerfile:</p>

<pre><code>...
EXPOSE 8080
...
</code></pre>

<p>They then build an image from this Dockerfile:</p>

<pre><code>$ docker build -t an_image - &lt; Dockerfile
</code></pre>

<p>And then <em>publish</em> the same port as above when running the image:</p>

<pre><code>$ docker run -d -p 8080 an_image
</code></pre>

<p>or publish all ports using</p>

<pre><code>$ docker run -d -P an_image
</code></pre>

<p>What is the point of exposing a port in the Dockerfile, if it will be published anyway? Would there ever be a need to expose a port first, and <em>not</em> publish it later? Effectively, I would like to specify all the ports that I will use in the Dockerfile when creating the image, and then not bother with them again, running them simply with:</p>

<pre><code>$ docker run -d an_image
</code></pre>

<p>Is this possible?</p>
","<p>I'm new to Docker. I just learned it for 1 week. It really a perfect tool for deploying. I have some stuck with it too.  I've created 3 containers as you see below:</p>

<p>(1) 0.0.0.0:3000->3000/tcp</p>

<p>(2) 0.0.0.0:5000->3000/tcp</p>

<p>(3) 0.0.0.0:4000->5000/tcp</p>

<p>So, the container's port is the port that I defined in my ""index file"" ( const port = 3000). What I'm going to ask here is why the container's port is used instead ""EXPOSE"" port in Dockerfile?  </p>

<p>Thanks for any explanation.</p>
"
"23692470","Why can't I use Docker CMD multiple times to run multiple services?","<docker>","57946108","How can I run luigid and luigi task within docker?","<python><docker><luigi>","<p>I have built a base image from Dockerfile named centos+ssh. In centos+ssh's Dockerfile, I use CMD to run ssh service.</p>

<p>Then I want to build a image run other service named rabbitmq,the Dockerfile:</p>

<pre><code>FROM centos+ssh
EXPOSE 22
EXPOSE 4149
CMD /opt/mq/sbin/rabbitmq-server start
</code></pre>

<p>To start rabbitmq container，run：</p>

<pre><code>docker run -d -p 222:22 -p 4149:4149 rabbitmq
</code></pre>

<p>but ssh service doesn't work, it sense rabbitmq's Dockerfile CMD override centos's CMD.</p>

<ol>
<li>How does CMD work inside docker image?</li>
<li>If I want to run multiple service, how to? Using supervisor?</li>
</ol>
","<p>I am trying to dockerize my Luigi tasks and want to run Luigid service and tasks inside a docker however I am unable to start luigid within a docker due to which my Luigi tasks do not run since it keeps trying to connect to the central scheduler.   </p>

<p>I have created a docker file, Luigi config file and Luigid.sh file. </p>

<p>The docker file is as follows:</p>

<pre><code>FROM python:3
COPY . /
WORKDIR /
COPY luigi.cfg luigi.cfg
RUN pip install -r requirements.txt
#RUN chmod 644 ./luigi_app.py
ENTRYPOINT [""python""]
RUN mkdir /usr/local/luigi
EXPOSE 8082
CMD [""luigid""]
CMD [""./luigi_app.py""]
</code></pre>

<p><strong>EDIT:</strong> 
Here is another version of the docker file. </p>

<pre><code>ARG PYTHON_VERSION=3.7
FROM python:$PYTHON_VERSION-alpine3.10

ARG LUIGI_VERSION=2.8.9
ENV LUIGI_VERSION ${LUIGI_VERSION}
ENV PYTHON_VERSION ${PYTHON_VERSION}
ENV LUIGI_CONFIG_DIR /etc/luigi/
ENV LUIGI_CONFIG_PATH /etc/luigi/luigi.conf
ENV LUIGI_STATE_DIR /luigi/state

COPY . /pipeline
WORKDIR /pipeline

RUN apk add --no-cache --virtual .build-deps \
      build-base \
      gcc \
      musl-dev \
      libc-dev \
      libffi-dev \
      python3-dev \
      py-mysqldb \
     &amp;&amp; \
      python3 -m pip install \
      luigi==${LUIGI_VERSION} \
      sqlalchemy \
      mysql-connector-python \
      pandas \
      numpy \
      request \
      &amp;&amp; \
    apk --purge del .build-deps &amp;&amp; \
    mkdir -p ${LUIGI_CONFIG_DIR} &amp;&amp; \
    mkdir -p ${LUIGI_STATE_DIR}

COPY logging.cfg ${LUIGI_CONFIG_DIR}
COPY luigi.cfg ${LUIGI_CONFIG_DIR}
VOLUME [""${LUIGI_CONFIG_DIR}"", ""${LUIGI_STATE_DIR}""]

EXPOSE 8082/TCP

COPY luigid.sh /bin/run
RUN chmod +x /bin/run
#ENTRYPOINT [""/bin/run""]
CMD [""./luigi_app.py""]
</code></pre>

<p>The luigi.cfg file is </p>

<pre><code># 1 to 1 copy - this should be a base image
[core]
default-scheduler-host = luigid-service
default-scheduler-port = 8082
default-scheduler-url  = http://luigid-service:8082/
rpc-connect-timeout    = 5
rpc-retry-attempts     = 3
rpc-retry-wait         = 30

[scheduler]
record_task_history = True
state_path = /usr/local/luigi/luigi-state.pickle
# number of seconds (aka 6 hours) tasks are kept in the UI
remove_delay = 2160

[task_history]
db_connection = sqlite://///usr/local/luigi/luigi-task-hist.db

[retcode]
# The following return codes are the recommended exit codes for Luigi
# They are in increasing level of severity (for most applications)
already_running=0
missing_data=0
not_run=0
task_failed=30
scheduling_error=35
unhandled_exception=40

[execution_summary]
# display all task after a pipeline is run and print their status
summary-length=0
</code></pre>

<p><strong>EDIT</strong> Here is another version of the luigi.cfg</p>

<pre><code>core]
logging_conf_file: /etc/luigi/logging.cfg

[scheduler]
record_task_history: True
state-path: /luigi/state/luigi-state.pickle

[task_history]
db_connection: sqlite:////luigi/state/luigi-task-history.db
</code></pre>

<p>The luigid.sh file is </p>

<pre><code>#!/bin/sh
cat &lt;&lt; ""EOF""
 _____       __    __    _____      _____     _____
(_   _)      ) )  ( (   (_   _)    / ___ \   (_   _)
  | |       ( (    ) )    | |     / /   \_)    | |
  | |        ) )  ( (     | |    ( (  ____     | |
  | |   __  ( (    ) )    | |    ( ( (__  )    | |
__| |___) )  ) \__/ (    _| |__   \ \__/ /    _| |__
\________/   \______/   /_____(    \____/    /_____(
EOF
echo ""Luigi: $LUIGI_VERSION - Python: $(python --version)""

exec luigid
</code></pre>

<p>When I build up my docker image and run it, it gives error that it is unable to connect to the central scheduler which is basically luigid. Can someone please help me correct my files or method?</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","56821546","Can't connect to db from docker container","<mysql><docker>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have MySQL server installed on a server and dockerized project, The DB is not publicly accessible, only from inside the server. It's used by local non dockerized apps too. 
I want to connect to it from inside the docker with it remaining not publicly accessible, I tried 172.17.0.1 but I get connection refused.
the current bind_address is 127.0.0.1, What do you suggest the bind_address would be ??</p>
"
"24319662","From inside of a Docker container, how do I connect to the localhost of the machine?","<nginx><docker><reverse-proxy><docker-networking>","57546595","Dockerized springboot app connect to main MySQL","<mysql><spring-boot><docker><deployment><containers>","<p>So I have a Nginx running inside a docker container, I have a mysql running on localhost, I want to connect to the MySql from within my Nginx. The MySql is running on localhost and not exposing a port to the outside world, so its bound on localhost, not bound on the ip address of the machine.</p>

<p>Is there any way to connect to this MySql or any other program on localhost from within this docker container?</p>

<p>This question is different from ""How to get the IP address of the docker host from inside a docker container"" due to the fact that the IP address of the docker host could be the public IP or the private IP in the network which may or may not be reachable from within the docker container (I mean public IP if hosted at AWS or something). Even if you have the IP address of the docker host it does not mean you can connect to docker host from within the container given that IP address as your Docker network may be overlay, host, bridge, macvlan, none etc which restricts the reachability of that IP address.</p>
","<p>I have a "" dockerized "" springboot app which needs mysql to run.</p>

<p>I've mysql installed on my ubuntu and want to start a container of my app using docker and need to know what to specify in database url connection so that my app connect to the mysql installed on my ubuntu.</p>

<p>something like this <code>jdbc:mysql://locahost:3066/database_name</code> won't work since localhost refers to the container itself which doesn't have mysql instance.</p>

<p>another solution is to start a mysql docker container and specify its name in place of localhost... but i don't want to start a mysql docker container since i already have mysql installed (and would need volume config to keep data <strong>persisted</strong>)</p>

<p>Thanks in advance.</p>
"
"26067249","Reading quoted/escaped arguments correctly from a string","<bash><shell><sh>","57334104","Bash - Single Quotes Being Added to Spaces","<bash><docker><string-interpolation>","<p>I'm encountering an issue passing an argument to a command in a Bash script.</p>

<p>poc.sh:</p>

<pre><code>#!/bin/bash

ARGS='""hi there"" test'
./swap ${ARGS}
</code></pre>

<p>swap:</p>

<pre><code>#!/bin/sh
echo ""${2}"" ""${1}""
</code></pre>

<p>The current output is:</p>

<pre><code>there"" ""hi
</code></pre>

<p>Changing only poc.sh (as I believe swap does what I want it to correctly), how do I get poc.sh to pass ""hi there"" and test as two arguments, with ""hi there"" having no quotes around it?</p>
","<p>I have the following:
test.ini</p>

<pre><code>BUILD_ARGS='--build-arg user=""test user"" --build-arg pass=testPass'
</code></pre>

<p>test.sh</p>

<pre><code>#!/bin/bash
set -x
source test.ini
docker build -t test:test ${BUILD_ARGS} .
</code></pre>

<p>Output of test.sh</p>

<pre><code>+ source test.ini
++ BUILD_ARGS='--build-arg smb_user=""test user"" --build-arg smb_pass=testPass '
+ docker build -t test:test --build-arg 'user=""test' 'user""' --build-arg pass=testPass .
</code></pre>

<p>Why are extra single quotes being added in between ""test"" and ""user""? I would expect the command to be executed as:</p>

<pre><code>docker build -t test:test --build-arg user=""test user"" --build-arg pass=testPass .
</code></pre>
"
"27214757","Docker: Combine multiple images","<java><mysql><linux><docker>","56941164","Is it possible to build a docker container with two hub images (Python 3.7.3 and Ubuntu 18.0.4)?","<python><docker><docker-compose><dockerfile><ubuntu-18.04>","<p>Is it possible with Docker to combine two images into one?</p>

<p>Like this here:</p>

<pre><code>genericA --
            \
             ---&gt; specificAB
            /
genericB --
</code></pre>

<p>For example there's an image for Java and an image for MySQL. </p>

<p>I'd like to have an image with Java <strong>and</strong> MySQL.</p>
","<p>I currently have a docker-compose.yml file that builds two Dockerfiles. One docker file pulls Python and the other pulls Ubuntu . At the end, I have two containers named dataload_py and dataload_ubuntu. I mounted a file onto dataload_ubuntu that can only be ran on that container.</p>

<p>When I bash into the Python container using <code>docker exec -it dataload_py bash</code>, how can I execute the mounted file on the Ubuntu container? They are bridged through the same network.</p>

<p>My end goal is to be able to spin up a Docker container with <em>both</em> Python and Ubuntu . </p>
"
"28056522","Access host database from a docker container","<docker>","57102057","Connecting to mysql server from docker container","<java><mysql><spring-boot><docker><spring-data-jpa>","<p>If I have a mysql database running on some host machine, and that host is also running a docker container: How would I access the mysql database from within the docker container that is running on the host?.</p>

<p>For instance, is there a way to publish a hosts port to the container (the inverse of what docker run -p does)?</p>
","<p>I am a total noob when it come to docker. I am working in docker toolbox on windows 10 and I am trying to dockerize my Java Spring Boot application that connect to MySQL database on my local machine using spring-data-jpa but when trying to run my container I am getting <code>java.net.ConnectException: Connection refused (Connection refused)</code></p>

<pre><code>com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:835) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:455) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:240) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) [HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) [HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) [HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) [HikariCP-3.2.0.jar!/:na]
        at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:60) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcDatabaseMetaData(ImprovedExtractionContextImpl.java:67) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.getTables(InformationExtractorJdbcDatabaseMetaDataImpl.java:329) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.GroupedSchemaMigratorImpl.performTablesMigration(GroupedSchemaMigratorImpl.java:65) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.performMigration(AbstractSchemaMigrator.java:207) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:114) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:183) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:310) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939) [hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at com.docker.springboot.SpringBootDockerApplication.main(SpringBootDockerApplication.java:17) ~[classes!/:1.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_212]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_212]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_212]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_212]
        at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:47) ~[Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:86) ~[Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[Spring-Boot-Docker.jar:1.0]
Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_212]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_212]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_212]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_212]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.NativeSession.connect(NativeSession.java:152) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:955) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:825) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        ... 56 common frames omitted
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_212]
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_212]
        at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_212]
        at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        ... 59 common frames omitted

2019-07-18 19:46:03.053  WARN 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 08S01
2019-07-18 19:46:03.053 ERROR 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
2019-07-18 19:46:03.057 TRACE 1 --- [           main] o.h.type.spi.TypeConfiguration$Scope     : Handling #sessionFactoryClosed from [org.hibernate.internal.SessionFactoryImpl@26abb146] for TypeConfiguration
2019-07-18 19:46:03.058 DEBUG 1 --- [           main] o.h.type.spi.TypeConfiguration$Scope     : Un-scoping TypeConfiguration [org.hibernate.type.spi.TypeConfiguration$Scope@4b8729ff] from SessionFactory [org.hibernate.internal.SessionFactoryImpl@26abb146]
2019-07-18 19:46:03.061  WARN 1 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
2019-07-18 19:46:03.067  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2019-07-18 19:46:03.102  INFO 1 --- [           main] ConditionEvaluationReportLoggingListener :

Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2019-07-18 19:46:03.105 ERROR 1 --- [           main] o.s.boot.SpringApplication               : Application run failed

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar!/:2.1.6.RELEASE]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_212]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_212]
        at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:47) [Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:86) [Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) [Spring-Boot-Docker.jar:1.0]
        at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) [Spring-Boot-Docker.jar:1.0]
Caused by: javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
        at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:402) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) ~[spring-beans-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        ... 24 common frames omitted
Caused by: org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
        at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:115) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:42) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:113) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:99) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:69) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:60) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcDatabaseMetaData(ImprovedExtractionContextImpl.java:67) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.getTables(InformationExtractorJdbcDatabaseMetaDataImpl.java:329) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.getTablesInformation(DatabaseInformationImpl.java:120) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.GroupedSchemaMigratorImpl.performTablesMigration(GroupedSchemaMigratorImpl.java:65) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.performMigration(AbstractSchemaMigrator.java:207) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:114) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:183) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:310) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) ~[spring-orm-5.1.8.RELEASE.jar!/:5.1.8.RELEASE]
        ... 28 common frames omitted
Caused by: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:835) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:455) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:240) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:199) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) ~[HikariCP-3.2.0.jar!/:na]
        at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-3.2.0.jar!/:na]
        at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43) ~[hibernate-core-5.3.10.Final.jar!/:5.3.10.Final]
        ... 43 common frames omitted
Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_212]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_212]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_212]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_212]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:91) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.NativeSession.connect(NativeSession.java:152) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:955) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:825) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        ... 56 common frames omitted
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_212]
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_212]
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_212]
        at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_212]
        at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:65) ~[mysql-connector-java-8.0.16.jar!/:8.0.16]
        ... 59 common frames omitted
</code></pre>

<p>Docker run command:</p>

<pre><code>$ docker run -p 8080:8080 -p 3306:3306 spring-boot-docker
</code></pre>

<p>Dockerfile :</p>

<pre class=""lang-none prettyprint-override""><code>FROM openjdk:8
EXPOSE 8080
EXPOSE 3306
ADD target/Spring-Boot-Docker.jar Spring-Boot-Docker.jar
ENTRYPOINT [""java"", ""-jar"", ""/Spring-Boot-Docker.jar""]
</code></pre>

<p>application.properties file :</p>

<pre class=""lang-none prettyprint-override""><code>spring.datasource.url = jdbc:mysql://localhost:3306/microservices?useSSL=false
spring.datasource.username = root
spring.datasource.password = root

## Hibernate Properties
# The SQL dialect makes Hibernate generate better SQL for the chosen database
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5InnoDBDialect
</code></pre>

<p>Docker version:</p>

<pre class=""lang-none prettyprint-override""><code>$ docker version
Client:
Version:       18.03.0-ce
API version:   1.37
Go version:    go1.9.4
Git commit:    0520e24302
Built: Fri Mar 23 08:31:36 2018
OS/Arch:       windows/amd64
Experimental:  false
Orchestrator:  swarm

Server: Docker Engine - Community
Engine:
Version:      18.09.6
API version:  1.39 (minimum version 1.12)
Go version:   go1.10.8
Git commit:   481bc77
Built:        Sat May  4 02:41:08 2019
OS/Arch:      linux/amd64
Experimental: false
</code></pre>

<p>MySQL version: <code>8.0.16 (MySQL Community server-GPL)</code>.</p>
"
"28349392","How to push a docker image to a private repository","<docker><docker-registry>","57783925","no basic auth credentials for - `docker push image_name`","<docker><docker-image>","<p>I have a docker image tagged as <code>me/my-image</code>, and I have a private repo on the dockerhub named <code>me-private</code>.<br>
When I push my <code>me/my-image</code>, I end up always hitting the public repo.</p>

<p>What is the exact syntax to specifically push my image to my private repo? </p>
","<p>We have our own private registry for the docker images. I get <code>no basic auth credentials</code> after executing command <code>docker push  image_name</code>. Has it to do with access rights to push newly build image on the private registry?</p>
"
"30494050","How do I pass environment variables to Docker containers?","<docker><environment-variables><dockerfile>","57359238","How to define env vars in docker container to be used with my jar file","<linux><bash><shell><docker><environment-variables>","<p>I'm new to Docker, and it's unclear how to access an external database from a container. Is the best way to hard-code in the connection string?</p>

<pre><code># Dockerfile
ENV DATABASE_URL amazon:rds/connection?string
</code></pre>
","<p>I've a container which runs a java process via a jar file. (springboot application based)</p>

<p>My jar is using en vars from the container , thus my application.yml looks like this :</p>

<p><strong>application.yml :</strong></p>

<pre><code>spring:
        profiles:
            active: server

        datasource:
            url: ${DATASOURCE_URL}
            databaseName: 
            serverName: 
            username: ${DATASOURCE_USERNAME}
            password: ${DATASOURCE_PASSWORD}
            dataSourceClassName: com.mysql.jdbc.jdbc2.optional.MysqlDataSource
            registerMbeans: true
            maxPoolSize: ${DATASOURCE_MAXPOOLSIZE}
            cachePrepStmts: true
            prepStmtCacheSize: 250
            prepStmtCacheSqlLimit: 2048
            useServerPrepStmts: true
        data:
            couchbase:
                nodes: 
                    - ${COUCHBASE_NODE_1}
                    - ${COUCHBASE_NODE_2}
                bucket: ${COUCHBASE_BUCKET}
                password: ${COUCHBASE_PASSWORD}
                port: ${COUCHBASE_PORT}  
</code></pre>

<p>Where , <code>DATASOURCE_USERNAME</code> , <code>DATASOURCE_PASSWORD</code> ... are the env var of the container itself</p>

<p>My problem is where to define / declare thos variable , i ve tried to incldue it inside <code>.bachrc</code> , withinin a file , like this :</p>

<p><strong>.bachrc :</strong></p>

<pre><code># .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=

# User specific aliases and functions

########## CONFIGS FILES ############
source $HOME/envfile.list
</code></pre>

<p>and <strong>envfile.list</strong> look like this :</p>

<p><strong>envfile.list:</strong></p>

<pre><code>DATASOURCE_USERNAME=""aaa""
DATASOURCE_PASSWORD=""bbb""
...
</code></pre>

<p>My pb is that my java process cannot see those variables , 
NOTE : i want to set those variables explicitlyt , without <strong>docker run -e</strong></p>

<p><strong>suggestions ?</strong></p>
"
"33711818","Embed sqlite database to docker container?","<docker>","57177676","How to persist/keep sqlite database in docker container application?","<sqlite><docker>","<p>I'm new to <a href=""https://www.docker.com/"" rel=""noreferrer"">Docker</a>. Is it possible to embed a sqlite database in a docker container and have it updated every time my script in that container runs?</p>
","<p>I am using gophish framework, I have dockerized the gophish using the below docker file:</p>

<pre><code>FROM debian:jessie

RUN apt-get update &amp;&amp; \
apt-get install --no-install-recommends -y \
unzip \
ca-certificates \
wget &amp;&amp; \
apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

RUN apt-get update &amp;&amp; \ 
apt-get install -y sqlite3 libsqlite3-dev

# Set the Current Working Directory inside the container
WORKDIR /opt/gophish-v0.6.0-linux-64bit

# Copy everything from the current directory to the PWD(Present Working Directory) inside the container
COPY . .

RUN sed -i ""s|**.**.**.***|0.0.0.0|g"" config.json &amp;&amp; \
chmod +x gophish

EXPOSE 80 80 8089 8089

ENTRYPOINT [""./gophish""]
</code></pre>

<p>the app is running fine on the Azure virtual machine and on local, the problem is the SQLite database is in docker container itself so on every docker build the database become blank.
how can I keep the SQLite database?</p>

<p>New to docker.
any help?</p>
"
"35134713","Disable cache for specific RUN commands","<docker>","56997826","Best practice for triggering a docker build if requirements.txt hasn't changed","<python><docker><requirements.txt>","<p>I have a few <code>RUN</code> commands in my Dockerfile that I would like to run with <code>-no-cache</code> each time I build a Docker image.</p>

<p>I understand the <code>docker build --no-cache</code> will disable caching for the entire Dockerfile. </p>

<p>Is it possible to disable cache for a specific RUN command?</p>
","<p>I have a Python package I maintain called ""Juriscraper"". I always want the latest version of it in my docker image. As a result, in my requirements.txt I just have the following line:</p>

<p>juriscraper</p>

<p>It doesn't say the version number, it's not version locked, etc. That's how the requirements.txt file <em>should</em> be in this instance. </p>

<p>Buuuut, if I release a new version of Juriscraper, and then try to rebuild my docker image to get the latest version, I can't because the requirements.txt file hasn't changed, and so is cached. </p>

<p>The one solution I've found is to use <code>--no-cache</code> in the docker build, but that busts my entire cache and takes a while to rebuild (it's a complicated image). </p>

<p>Is there a better solution here? I guess I could also just put a comment into the requirements.txt file that I changed all the time, but that's not great either.</p>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","56731029","How to run a python cron application with docker?","<python><docker><cron>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>I am trying to run my two python files with cron in docker.
I have two files a.py and b.py, which are to be run with cron and specific time period between them.</p>

<p>How to approach this through docker?</p>
"
"37458287","How to run a cron job inside a docker container?","<docker><cron><containers><sh>","56725021","The crontab not running inside container","<ruby-on-rails><docker><cron>","<p>I am trying to run a cronjob inside a docker container that invokes a shell script.</p>

<p>Yesterday I have been searching all over the web and stack overflow, but I could not really find a solution that works.<br>
How can I do this?</p>

<p><strong>EDIT:</strong></p>

<p>I've created a <a href=""https://github.com/cheyer/docker-cron"" rel=""noreferrer"">(commented) github repository</a> with a working docker cron container that invokes a shell script at given interval.</p>
","<p>I'm trying to create a cron job inside the container to run a ruby class, but it's not running automatically.
When I'm trying to test it locally without docker and container, it works great. And also when running the following command <code>rails runner ""Delete.delete_old_requests""</code> in the terminal and it works.</p>

<p>I think the problem in the app path! which is called myapp. Thanks in advance</p>

<ol>
<li>I install the cron inside the container. <code>apt-get install cron</code></li>
<li>create a task in a ruby file called Delete.rb. and a method called delete_old_requests</li>
<li>create a cron task in crontab file as shown below:</li>
</ol>

<pre><code>* * * * *  /bin/bash -l -c 'cd /myapp &amp;&amp; RAILS_ENV=development bundle exec rails runner ""Delete.delete_old_requests""'
</code></pre>

<p>the code:
delete.rb</p>

<pre><code>class Delete &lt; ApplicationRecord
    def self.delete_old_requests
         users=User.all
         users.update_all(requests_num_per_day: 0)
    end
end
</code></pre>

<p>crontab</p>

<pre><code>* * * * *  /bin/bash -l -c '/myapp &amp;&amp; RAILS_ENV=development bundle exec rails runner ""Delete.delete_old_requests""'
</code></pre>
"
"37526509","How to install pdo driver in php docker image?","<php><mysql><pdo><docker>","57446636","The pdo_mysql extension is not detected on Wordpress Docker container","<php><wordpress><docker><pdo>","<p>I am using as a base the <a href=""https://hub.docker.com/_/php/"" rel=""noreferrer"">php docker container</a> with the tag:</p>



<pre class=""lang-none prettyprint-override""><code>php:5.6-apache
</code></pre>

<p>I linked it with a basic <code>mysql:5.6</code> image which I can reach at the host <code>mysql</code>. I created a DB, and filled a table with basic values.</p>

<p>Yet trying to access my app, I get:</p>

<pre class=""lang-none prettyprint-override""><code>Fatal error: Uncaught exception 'PDOException' with message
could not find driver' in /var/www/html/index.php:30 
Stack trace: #0 [internal function]: 
PDO-&gt;__construct('mysql:host=mysq...', 'root', 'root', Array) 
#1 [internal function]: Phalcon\Db\Adapter\Pdo-&gt;connect(Array)
#2 /var/www/html/index.php(30): Phalcon\Db\Adapter\Pdo-__construct(Array)
#3 [internal function]: {closure}()
#4 [internal function]: Phalcon\Di\Service-&gt;resolve(NULL, Object(Phalcon\Di\FactoryDefault))
#5 [internal function]: Phalcon\Di-&gt;get('db', NULL)
#6 [internal function]: Phalcon\Di-&gt;getShared('db')
#7 [internal function]: Phalcon\Mvc\Model\Manager-&gt;_getConnection(Object(Reviews), NULL)
#8 [internal function]: Phalcon\Mvc\Model\Manager-&gt;getReadConnection(Object(Reviews))
#9 [internal function]: Phalcon\Mvc\Model-&gt;getReadConnection()
#10 [internal function]: Phalcon\Mvc\Model\MetaData\Strategy\Introspection-&gt;getMetaData(Object(Reviews), Object(Phalcon\Di\FactoryDefault))
#11 [internal function]: Phalcon\Mvc\Model\MetaData-&gt;_initialize(Object(Rev in /var/www/html/index.php on line 30
</code></pre>

<p>Hence, I thought that the php container was lacking the <code>php-mysql</code> component I installed via:</p>

<pre class=""lang-none prettyprint-override""><code>apt-get install php5-mysql
</code></pre>

<p>I also added a mysql.ini at:</p>

<pre class=""lang-none prettyprint-override""><code>cat /usr/local/etc/php/conf.d/mysql.ini
; configuration for php MySQL module
; priority=20
extension=pdo_mysql.so
</code></pre>

<p>If I <code>echo phpinfo();die</code> it tells me that:</p>

<pre class=""lang-none prettyprint-override""><code>Additional .ini files parsed:
    /usr/local/etc/php/conf.d/mysql.ini,
    /usr/local/etc/php/conf.d/phalcon.ini
</code></pre>

<p>Yet still, the error persists.</p>

<p>Furthermore, when running:</p>

<pre class=""lang-none prettyprint-override""><code>php -i|grep PDO
</code></pre>

<p>I get:</p>

<pre class=""lang-none prettyprint-override""><code>PHP Warning:  PHP Startup: Unable to load dynamic library '/usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so' - /usr/local/lib/php/extensions/no-debug-non-zts-20131226/pdo_mysql.so: cannot open shared object file: No such file or directory in Unknown on line 0
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite
PDO Driver for SQLite 3.x =&gt; enabled
</code></pre>

<p>so it seems the mysql extension isn't even activated.</p>

<p>What am I doing wrong?</p>
","<p>I have installed several Wordpress websites by using Docker, specifically this <a href=""https://github.com/evertramos/docker-wordpress-letsencrypt"" rel=""nofollow noreferrer"">Docker Wordpress Let's Encrypt</a> repo by Evert Ramos.</p>

<p>While it makes it easy and fast to deploy as many Wordpress installations as you want, there're still some issues and lacks, such as the <code>sendmail()</code> function (then you have to install <a href=""https://wordpress.org/plugins/post-smtp/"" rel=""nofollow noreferrer"">an SMTP plugin</a> to work around the email sending).</p>

<p>The main issue I have found is that, after having installed this <a href=""https://wpvivid.com/"" rel=""nofollow noreferrer"">WPvivid plugin</a> for backing up Wordpress, I get the following error message:</p>

<blockquote>
  <p>The pdo_mysql extension is not detected. Please install the extension
  first.</p>
</blockquote>

<p>I have googled how to install the <code>pdo_mysql</code> extension for a Wordpress container or inside the Nginx container. However, I only have found answers about how to install it but for a PHP container.</p>

<p>The <a href=""https://github.com/evertramos"" rel=""nofollow noreferrer"">Evert Ramos</a>' repos do not use any PHP container, so I haven't found out how or where to install that <code>pdo_mysql</code> extension. </p>

<p>Here is my <code>docker-compose.yml</code> file of one of the Wordpress sites:</p>

<pre><code>    services:
        db_projects:
          container_name: ${CONTAINER_DB_NAME}
          image: mariadb:10.4
          restart: unless-stopped
          volumes:
            - ${DB_PATH}:/var/lib/mysql
          environment:
            MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
            MYSQL_DATABASE: ${MYSQL_DATABASE}
            MYSQL_USER: ${MYSQL_USER}
            MYSQL_PASSWORD: ${MYSQL_PASSWORD}

        wp_projects:
          depends_on:
            - db_projects
          container_name: ${CONTAINER_WP_NAME}
          image: wordpress:latest
          restart: unless-stopped
          volumes:
            - ${WP_CORE}:/var/www/html
            - ${WP_CONTENT}:/var/www/html/wp-content
            - ./conf.d/php.ini:/usr/local/etc/php/conf.d/php.ini
          environment:
            WORDPRESS_DB_HOST: ${CONTAINER_DB_NAME}:3306
            WORDPRESS_DB_NAME: ${MYSQL_DATABASE}
            WORDPRESS_DB_USER: ${MYSQL_USER}
            WORDPRESS_DB_PASSWORD: ${MYSQL_PASSWORD}
            WORDPRESS_TABLE_PREFIX: ${WORDPRESS_TABLE_PREFIX}
            VIRTUAL_HOST: ${DOMAINS}
            LETSENCRYPT_HOST: ${DOMAINS}
            LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL}
          logging:
            options:
          max-size: ${LOGGING_OPTIONS_MAX_SIZE:-200k}
    pma_projects:
      image: phpmyadmin/phpmyadmin
      restart: unless-stopped
      container_name: pma_projects
      links:
        - db_projects
      ports:
        - ${PMA_PORT}:80
      environment:
        #MYSQL_USERNAME: root
        #MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
        PMA_HOST: db_projects
        PMA_PORT: 3306
        #PMA_USER: ${MYSQL_USER}
        #PMA_PASSWORD: ${MYSQL_PASSWORD}
        PMA_ARBITRARY: 1
      depends_on:
        - wp_projects
        - db_projects

#   wpcli:
#     image: tatemz/wp-cli
#     volumes:
#       - ${WP_CORE}:/var/www/html
#       - ${WP_CONTENT}:/var/www/html/wp-content
#     depends_on:
#       - db
#     entrypoint: wp

networks:
    default:
       external:
         name: ${NETWORK}
</code></pre>

<p>Any ideas on how to install that extension or somebody who can shed some light to solve this?</p>

<p>(my useful links on my way to solve it: <a href=""https://docs.docker.com/samples/library/wordpress/"" rel=""nofollow noreferrer"">https://docs.docker.com/samples/library/wordpress/</a> &amp; <a href=""https://github.com/docker-library/wordpress/blob/c9f1ca12b6fa8181dee161dfc5ce1692eeaef1d1/php7.2/apache/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/docker-library/wordpress/blob/c9f1ca12b6fa8181dee161dfc5ce1692eeaef1d1/php7.2/apache/Dockerfile</a>
| <a href=""https://github.com/docker-library/wordpress/blob/c9f1ca12b6fa8181dee161dfc5ce1692eeaef1d1/php7.3/fpm/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/docker-library/wordpress/blob/c9f1ca12b6fa8181dee161dfc5ce1692eeaef1d1/php7.3/fpm/Dockerfile</a>)</p>

<p>Following Anh Tuan's answer, my dockerfile is as follows:</p>

<pre><code>FROM php:7.3-fpm

# install the PHP extensions we need (https://make.wordpress.org/hosting/handbook/handbook/server-environment/#php-extensions)
RUN set -ex; \
    \
    savedAptMark=""$(apt-mark showmanual)""; \
    \
    apt-get update; \
    apt-get install -y --no-install-recommends \
        libjpeg-dev \
        libmagickwand-dev \
        libpng-dev \
        libzip-dev \
    ; \
    \
    docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr; \
    docker-php-ext-install -j ""$(nproc)"" \
        bcmath \
        exif \
        gd \
        mysqli \
        opcache \
        zip \
                pdo_mysql \
    ; \
    pecl install imagick-3.4.4; \
    docker-php-ext-enable imagick; \
    \
# reset apt-mark's ""manual"" list so that ""purge --auto-remove"" will remove all build dependencies
    apt-mark auto '.*' &gt; /dev/null; \
    apt-mark manual $savedAptMark; \
    ldd ""$(php -r 'echo ini_get(""extension_dir"");')""/*.so \
        | awk '/=&gt;/ { print $3 }' \
        | sort -u \
        | xargs -r dpkg-query -S \
        | cut -d: -f1 \
        | sort -u \
        | xargs -rt apt-mark manual; \
    \
    apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \
    rm -rf /var/lib/apt/lists/*

# set recommended PHP.ini settings
# see https://secure.php.net/manual/en/opcache.installation.php
RUN { \
        echo 'opcache.memory_consumption=128'; \
        echo 'opcache.interned_strings_buffer=8'; \
        echo 'opcache.max_accelerated_files=4000'; \
        echo 'opcache.revalidate_freq=2'; \
        echo 'opcache.fast_shutdown=1'; \
    } &gt; /usr/local/etc/php/conf.d/opcache-recommended.ini
# https://wordpress.org/support/article/editing-wp-config-php/#configure-error-logging
RUN { \
# https://www.php.net/manual/en/errorfunc.constants.php
# https://github.com/docker-library/wordpress/issues/420#issuecomment-517839670
        echo 'error_reporting = E_ERROR | E_WARNING | E_PARSE | E_CORE_ERROR | E_CORE_WARNING | E_COMPILE_ERROR | E_COMPILE_WARNING | E_RECOVERABLE_ERROR'; \
        echo 'display_errors = Off'; \
        echo 'display_startup_errors = Off'; \
        echo 'log_errors = On'; \
        echo 'error_log = /dev/stderr'; \
        echo 'log_errors_max_len = 1024'; \
        echo 'ignore_repeated_errors = On'; \
        echo 'ignore_repeated_source = Off'; \
        echo 'html_errors = Off'; \
    } &gt; /usr/local/etc/php/conf.d/error-logging.ini

VOLUME /var/www/html

ENV WORDPRESS_VERSION 5.2.2
ENV WORDPRESS_SHA1 3605bcbe9ea48d714efa59b0eb2d251657e7d5b0

RUN set -ex; \
    curl -o wordpress.tar.gz -fSL ""https://wordpress.org/wordpress-${WORDPRESS_VERSION}.tar.gz""; \
    echo ""$WORDPRESS_SHA1 *wordpress.tar.gz"" | sha1sum -c -; \
# upstream tarballs include ./wordpress/ so this gives us /usr/src/wordpress
    tar -xzf wordpress.tar.gz -C /usr/src/; \
    rm wordpress.tar.gz; \
    chown -R www-data:www-data /usr/src/wordpress

COPY docker-entrypoint.sh /usr/local/bin/

ENTRYPOINT [""docker-entrypoint.sh""]
CMD [""php-fpm""]
</code></pre>

<p>And I only modified the <code>docker-compose.yml</code> file for creating the Wordpress image with the <code>mysql_pdo</code> extension:</p>

<pre><code>version: '3'

services:
    db_projects:
      container_name: ${CONTAINER_DB_NAME}
      image: mariadb:10.4
      restart: unless-stopped
      volumes:
        - ${DB_PATH}:/var/lib/mysql
      environment:
        MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
        MYSQL_DATABASE: ${MYSQL_DATABASE}
        MYSQL_USER: ${MYSQL_USER}
        MYSQL_PASSWORD: ${MYSQL_PASSWORD}

    wp_projects:
      depends_on:
        - db_projects
      container_name: ${CONTAINER_WP_NAME}
      #image: wordpress:latest
      build: ./wordpress
        #context: .
        #dockerfile: Custom-Wp-Dockerfile
      restart: unless-stopped
      volumes:
        - ${WP_CORE}:/var/www/html
        - ${WP_CONTENT}:/var/www/html/wp-content
        - ./conf.d/php.ini:/usr/local/etc/php/conf.d/php.ini
      environment:
        WORDPRESS_DB_HOST: ${CONTAINER_DB_NAME}:3306
        WORDPRESS_DB_NAME: ${MYSQL_DATABASE}
        WORDPRESS_DB_USER: ${MYSQL_USER}
        WORDPRESS_DB_PASSWORD: ${MYSQL_PASSWORD}
        WORDPRESS_TABLE_PREFIX: ${WORDPRESS_TABLE_PREFIX}
        VIRTUAL_HOST: ${DOMAINS}
        LETSENCRYPT_HOST: ${DOMAINS}
        LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL}
      logging:
        options:
          max-size: ${LOGGING_OPTIONS_MAX_SIZE:-200k}
    pma_projects:
      image: phpmyadmin/phpmyadmin
      restart: unless-stopped
      container_name: pma_projects
      links:
        - db_projects
      ports:
        - ${PMA_PORT}:80
      environment:
        #MYSQL_USERNAME: root
        #MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
        PMA_HOST: db_projects
        PMA_PORT: 3306
        #PMA_USER: ${MYSQL_USER}
        #PMA_PASSWORD: ${MYSQL_PASSWORD}
        PMA_ARBITRARY: 1
      depends_on:
        - wp_projects
        - db_projects

#   wpcli:
#     image: tatemz/wp-cli
#     volumes:
#       - ${WP_CORE}:/var/www/html
#       - ${WP_CONTENT}:/var/www/html/wp-content
#     depends_on:
#       - db
#     entrypoint: wp

networks:
    default:
       external:
         name: ${NETWORK}
</code></pre>

<p>After executing <code>docker-compose up -d</code> I get the following error:</p>

<blockquote>
  <p>[pathros@projects wp]$ sudo docker-compose up -d Building
  wp_projects Step 1/12 : FROM php:7.3-apache
  7.3-apache: Pulling from library/php 1ab2bdfe9778: Pulling fs layer 1448c64389e0: Pulling fs layer 4b8a4e62b444: Pulling fs layer
  9eb9d1e8e241: Waiting d20b2d19292c: Waiting 023060ea5930: Waiting
  a7fa99bc84ac: Waiting 084397ea0b0b: Waiting 27f2e3242e8a: Waiting
  c53d955b925a: Waiting 55a8a68dea39: Waiting b78786d44570: Waiting
  69dd7e866b60: Waiting 2907cf87b0bb: Waiting ERROR: Service
  'wp_projects' failed to build: error pulling image configuration: Get
  <a href=""https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/aa/aa4bdc74350b45a805ad9bdd39ec2cc38cd604a432e447497c3103ad0a8115d8/data?verify=1568225825-5P%2BOlgyxV6p%2FjAXyu%2BTLKCEU5RM%3D"" rel=""nofollow noreferrer"">https://production.cloudflare.docker.com/registry-v2/docker/registry/v2/blobs/sha256/aa/aa4bdc74350b45a805ad9bdd39ec2cc38cd604a432e447497c3103ad0a8115d8/data?verify=1568225825-5P%2BOlgyxV6p%2FjAXyu%2BTLKCEU5RM%3D</a>:
  EOF</p>
</blockquote>
"
"37584961","How to encrypt docker images or source code in docker images?","<encryption><docker>","56932386","Is there a way to keep content or codes inside docker container encrypted while running it?","<docker>","<p>Say I have a docker image, and I deployed it on some server. But I don't want other user to access this image. Is there a good way to encrypt the docker image ? </p>
","<p>Does docker provide a way to encrypt the codes running inside a container? I mean no one can look inside the image container easily? For example, I want to protect some sensitive data.</p>

<p>According to my search, there is no such a way. But still i want to ask here to see if any tricks to do this</p>
"
"37586811","Pass commands as input to another command (su, ssh, sh, etc)","<bash><shell><unix><ssh><sh>","57878487","How to run a sh script with CLI container using the docker exec command?","<bash><shell><docker>","<p>I have a script where I need to start a command, then pass some additional commands <em>as commands</em> to that command.  I tried</p>

<pre><code>su
echo I should be root now:
who am I
exit
echo done.
</code></pre>

<p>... but it doesn't work: The <code>su</code> succeeds, but then the command prompt is just staring at me.  If I type <code>exit</code> at the prompt, the <code>echo</code> and <code>who am i</code> etc start executing!  And the <code>echo done.</code> doesn't get executed at all.</p>

<p>Similarly, I need for this to work over <code>ssh</code>:</p>

<pre><code>ssh remotehost
# this should run under my account on remotehost
su
## this should run as root on remotehost
whoami
exit
## back
exit
# back
</code></pre>

<p>How do I solve this?</p>

<blockquote>
  <p>I am looking for answers which solve this in a general fashion, and which are not specific to <code>su</code> or <code>ssh</code> in particular.  The intent is for this question to become a <a href=""https://meta.stackoverflow.com/questions/291992/what-is-a-canonical-question-answer-and-what-is-their-purpose"">canonical</a> for this particular pattern.</p>
</blockquote>
","<p>I want to create s sh script but it stops when ""docker exec -it cli bash"" is executed and do  not go to the next line. How to run the other commands on root?</p>

<p>root@ee3abae377df:/opt/gopath/src/github.com/hyperledger/fabric/peer# 
Stops here and i am not able do execute the next command</p>

<pre><code>docker exec -it cli bash
export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp
export CORE_PEER_ADDRESS=peer0.org1.example.com:7051
export CORE_PEER_LOCALMSPID=""Org1MSP""
export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt
export CHANNEL_NAME=mychannel
</code></pre>
"
"37694987","Connecting to Postgresql in a docker container from outside","<postgresql><docker><remote-connection>","57987953","Connection string for postgresql that runned on docker","<c#><postgresql><docker><asp.net-core><connection-string>","<p>I have Postgresql on a server in a docker container. How can I connect to it from the outside, that is, from my local computer? What setting should I apply to allow that?</p>
","<p>I have an ASP.Net Core app on my local machine that uses postgresql. So I need to connect my postgresql database running in a docker container.
I connect to this db with pgAdmin, but when I set the connection string on my app and run the command <code>dotnet ef database update</code>, it shows the following error</p>

<pre><code>IPv4 address 0.0.0.0 and IPv6 address ::0 are unspecified addresses that cannot be used as a target address.
</code></pre>

<p>My connection string looks like this</p>

<pre><code>host=0.0.0.0;port=54320;database=mydatabase;user id=postgres;password=mypassword
</code></pre>
"
"38298645","How should I backup & restore docker named volumes","<backup><docker-compose>","57767299","SQLServer Docker: How do I backup & restore the data *volume*?","<sql-server><docker><docker-compose><docker-volume>","<p>I'm a little bit confused with the functionnality of named volumes in a docker compose file specifically when it comes to backup/restore my app. </p>

<p>I'm actually testing this dockercompose file : </p>

<pre><code>      version: '2'
      services:
          django:
              build: 
                  context: ""{{ build_dir }}/docker/django""
              depends_on:
                  - db
              environment:
                  [...]
              volumes:
                  - code:/data/code
                  - www:/var/www
                  - conf:/data/conf
              networks:
                  - front
                  - db
              expose:
                  - ""8080""
              entrypoint: ""/init""
          db:
              build:
                  context: ""{{ build_dir }}/docker/postgres"" 
              environment:
                  [...]
              volumes:
                  - data:/var/lib/postgresql/data
              networks:
                  - db

      volumes:
          data:
          www:
          code:
          conf:

      networks:
          front:
              external:
                  name: ""proxy_nw""
</code></pre>

<p>As the documentation said, I tried to use named volume instead of data only container. But how am I suppose to backup my data ? </p>

<p>With a data only container I would have done a <code>docker run --rm --volume-from DOC backup_container save</code> which is really easy. </p>

<p>Now I read <a href=""https://stackoverflow.com/questions/36011595/docker-named-volumes-vs-doc-data-only-containers"">in this topic</a> that I should use something like <code>docker run --rm --volume data --volume www --volume code --volume conf backup_container save</code>. This is not so simple because I have many applications with different types and names of volumes so it means that my command to save my data would have to be different for each application. It complicate automation process.</p>

<p><strong>Edit:</strong> Actually this syntaxe 
<code>docker run --volume data --volume www container_image my_command</code> is not correct. 
It needs the mountpoint inside the container, so it would be 
<code>docker run --volume data:/somewhere --volume www:/somewhereelse container_image my_command</code>. 
So it's even more complicated to use with a backup container. </p>

<p>So, what are the best practices in this case ? <del>Should I use just one named volume for all my containers ?</del></p>
","<p>I have a MS SQLServer 2017 Linux Docker container running with docker-compose. (Working on
a Windows host.)
The server is running, I added data, and this data is persistent across multiple <code>docker-compose up / down</code>
since the server uses a <strong>docker volume</strong>. The data disappears when I use <code>docker-compose down -v</code>. So <strong><em>this works as intended:</em></strong></p>

<pre><code>services:
    sql:
        image: mcr.microsoft.com/mssql/server:2017-GA-ubuntu
        volumes:
         - sqldata:/var/opt/mssql
        ...

volumes:
    sqldata:
        driver: local
        name: sqldata
</code></pre>

<p>Now I am trying to backup &amp; restore the database. I know the ""normal"" way, using the SQLServer directly. <strong><em>This works:</em></strong></p>

<pre><code># Restore a backup inside the container volume
docker exec -it sql mkdir /var/opt/mssql/backup
docker cp .\Test.bak sql:/var/opt/mssql/backup
sqlcmd -S 127.0.0.1,1433 -U sa -P Secr3tSA_Passw0rd -H 127.0.0.1,1433 -Q ""RESTORE DATABASE [Test] FROM DISK='/var/opt/mssql/backup/Test.bak' WITH REPLACE""

# Backup a database inside the container volume, then copy to local file
docker exec sql rm -rf /var/opt/mssql/backup/Test.bak
sqlcmd -S 127.0.0.1,1433 -U sa -P Secr3tSA_Passw0rd -H 127.0.0.1,1433 -Q ""BACKUP DATABASE [Test] TO DISK='/var/opt/mssql/backup/Test.bak'""
docker cp sql:/var/opt/mssql/backup/Test.bak .\Test.bak
</code></pre>

<p>Now I was thinking, maybe there is a better way than to put the SA password into a BAT file
and hand that out to my customers and service technicians. Simply grabbing a copy of the volume
should do the trick!</p>

<p>I found this:</p>

<pre><code># Make sure the SQLServer is not writing/blocking any files.
docker-compose stop sql

# Backup &amp; Restore the sqldata volume.
docker run --rm -v sqldata -v $pwd\backup:/backup ubuntu bash -c ""cd /whsqldata &amp;&amp; tar xvf /backup/backup.tar --strip 1""
docker run --rm -v sqldata -v $pwd\backup:/backup ubuntu bash -c ""cd /whsqldata &amp;&amp; tar cvf /backup/backup.tar .""

# Restart the SQLServer.
docker-compose start sql
</code></pre>

<p>This creates the expected backup.tar in my user directory... But it is suspiciously small! And after the
restore, the SQLServer cannot connect to the database. <strong>It looks like the backup.tar has no content.</strong>
But on closer inspection, so has my sqldata volume! It is empty!? When I start a bash that mounts that
same volume, I can see the directory but there is nothing in it:</p>

<pre><code>docker run --rm -v sqldata -it ubuntu
/ # ls sqldata/ -a
. ..
/ #
</code></pre>

<p>The SQLServer´s data persists. So it´s got to be saved <em>somewhere</em>, right? What am I missing?!</p>
"
"43442276","Docker : Can a container A call an executable located on an other container B?","<docker><docker-compose><pandoc><pdflatex>","57280580","Is it posible to send cli proccess comand from one container to another in docker?","<java><docker><nginx>","<p>I have two Docker images, one containing <a href=""http://pandoc.org/"" rel=""noreferrer""><code>pandoc</code></a> (an utility to convert documents in different formats to many formats), and an other containing <code>pdflatex</code> (from <a href=""https://www.tug.org/texlive/"" rel=""noreferrer""><code>texlive</code></a>, to convert <code>tex</code> files into <code>pdf</code>). My goal here is to convert documents from <code>md</code> to <code>pdf</code>.</p>
<p>I can run each image separately :</p>
<pre><code># call pandoc inside my-pandoc-image (md -&gt; tex)
docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md -o test.tex

# call pdflatex inside my-texlive-image (tex -&gt; pdf)
docker run --rm \
    -v $(pwd):/texlive \
    my-texlive-image \
    pdflatex test.tex # generates test.pdf
</code></pre>
<p>But, in fact, what I want is to call <code>pandoc</code> (from its container) directly to convert <code>md</code> into <code>pdf</code>, like this :</p>
<pre><code>docker run --rm \
    -v $(pwd):/pandoc \
    my-pandoc-image \
    pandoc -s test.md --latex-engine pdflatex -o test.pdf
</code></pre>
<p>This command does not work here, because <code>pandoc</code> inside the container tries to call <code>pdflatex</code> (that must be in <code>$PATH</code>) to generate the pdf, but <code>pdflatex</code> does not exist since it is not installed in the <code>my-pandoc-image</code>.</p>
<p>In my case, <code>pdflatex</code> is installed in the image <code>my-texlive-image</code>.</p>
<p>So, from this example, my question is : Can a container A call an executable located on an other container B ?</p>
<p>I am pretty sure this is possible, because if I install <code>pandoc</code> <strong>on my host</strong> (without <code>pdflatex</code>), I can run <code>pandoc -s test.md--latex-engine=pdflatex -o test.pdf</code> by simply aliasing the <code>pdflatex</code> command with :</p>
<pre><code>pdflatex() {
    docker run --rm \
        -v $(pwd):/texlive \
        my-texlive-image \
        pdflatex &quot;$@&quot;
}
</code></pre>
<p>Thus, when <code>pdflatex</code> is called by <code>pandoc</code>, a container starts and do the conversion.</p>
<p>But when using the 2 containers, how could I alias the <code>pdflatex</code> command to simulate its existence on the container having only <code>pandoc</code> ?</p>
<p>I took a look at <code>docker-compose</code>, since I have already used it to make 2 containers communicate (app communicating with a database). I even thought about <code>ssh</code>-ing from container A to container B to call the <code>pdflatex</code> command, but this is definitively <a href=""https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/"" rel=""noreferrer"">not the right solution</a>.</p>
<p>Finally, I also have built an image containing <code>pandoc</code> + <code>pdflatex</code> (it worked because the two executables were on the same image), but I really want to keep the 2 images separately, since they could be used independently by other images.</p>
<h3>Edit :</h3>
<p>A similar question is exposed <a href=""https://stackoverflow.com/questions/29907979/execute-command-in-linked-docker-container"">here</a>, as I understand the provided answer needs Docker to be installed on container A, and needs a docker socket binding (<code>/var/run/docker.sock</code>) between host and container A. I don't think this is best practice, it seems like a hack that can create <a href=""https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/"" rel=""noreferrer"">security issues</a>.</p>
","<p>I have one container running a java app and another container running a nginx server</p>

<p>The container running java app sometime needs to update the nginx.config file in the nginx container and send the reload command to it</p>

<p>I Know that i can share container's directory, but can i send cli command to another container?</p>

<p>Is it possible achieve this scenario using docker and runing the java app and nginx in differents containers?</p>
"
"44376852","How to start apache2 automatically in a ubuntu docker container?","<apache><ubuntu><docker><dockerfile>","57075686","Unable to start Docker example w/apache-php","<docker>","<p>I am trying to create a Dockerfile that will start apache automatically. Nothing has worked. But If I log into the container and run <code>service apache2 start</code> it works. Why can I not run that command from my Dockerfile?</p>

<pre><code>FROM ubuntu

# File Author / Maintainer
MAINTAINER rmuktader

# Update the repository sources list
RUN apt-get update

# Install and run apache
RUN apt-get install -y apache2 &amp;&amp; apt-get clean

#ENTRYPOINT [""/usr/sbin/apache2"", ""-k"", ""start""]


#ENV APACHE_RUN_USER www-data
#ENV APACHE_RUN_GROUP www-data
#ENV APACHE_LOG_DIR /var/log/apache2

EXPOSE 80
CMD service apache2 start
</code></pre>
","<p>I am following a <a href=""https://linuxconfig.org/how-to-customize-docker-images-with-dockerfiles"" rel=""nofollow noreferrer"">tutorial</a> about Docker. I run on Windows, so I already installed Docker for Windows and signed up for the Hub.</p>
<p>Currently I am in the process of creating my very first Apache instance on Docker.</p>
<p>Following the tutorial, I created the very first Dockerfile of a <code>debian</code> derivative and ran the container. My output was consistent with the tutorial (<code>&gt;</code> denotes Windows prompt)</p>
<pre><code>&gt; docker run -it --name mydebian_container  mydebian
root@ef9eb174874a:/# ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 02:43 pts/0    00:00:00 bash
root         9     1  0 02:43 pts/0    00:00:00 ps -ef
</code></pre>
<p>Then I followed the tutorial on running Apache from the container</p>
<p>Dockerfile</p>
<pre><code>FROM debian
RUN apt-get update &amp;&amp;\
    apt-get -y install procps libapache2-mod-php
CMD service apache2 start
</code></pre>
<p>Cool, yea? But when I run the container nothing happens, and the container itself exits successfully</p>
<pre><code>D:\IdeaDevOps\dockers\apache2&gt;docker run -it --name apache2  -v d:\IdeaDevOps\dockers\apache2:/var/www/html  debian_apache2
[....] Starting Apache httpd web server: apache2AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message
. ok


&gt;docker ps --all
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES
09d5b4b3fa49        debian_apache2      &quot;/bin/sh -c 'service…&quot;   24 seconds ago      Exited (0) 22 seconds ago                       apache2
8d1c4522aa30        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          elated_wing
ae769d388b36        ubuntu              &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          kind_murdock
5596841696f6        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          festive_booth
</code></pre>
<p>I really can't figure why it fails to keep a running Apache server. <code>AH00558</code> is a <strong>warning</strong>, it never blocks Apache from starting up.
I have a running instance of Apache 2.4 on my laptop listening on 80, so I stopped the Windows service relative to it.</p>
<p>I also tried to change the port bindings in Docker, or to not pass any port binding (<code>-p</code>) to Docker to see what happens.</p>
<p>As you can see, I am not running the container <code>-d</code> on purpose (unlike the linked tutorial) to see its output.</p>
<p>What is preventing the Apache-based container to start? I am not really interested in running a real Apache/php, but I just want to get acquainted with Docker and how to customize an image for your needs. TL;DR I am learning, do not have a real business need now.</p>
<h2>Update</h2>
<p>I tried to use a hammer of Thor approach. If I insisted in doing <code>docker start apache2</code> I can get the Apache server alive for a few seconds only, just the time to browse the root directory on my browser. The server ends eventually</p>
<pre><code>D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker start apache2
apache2

D:\IdeaDevOps\dockers\apache2&gt;docker exec apache2 bash

D:\IdeaDevOps\dockers\apache2&gt;docker exec apache2 bash

D:\IdeaDevOps\dockers\apache2&gt;docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
c669aacbed7a        debian_apache2      &quot;/bin/sh -c 'service…&quot;   20 minutes ago      Up 17 seconds       0.0.0.0:80-&gt;80/tcp   apache2

D:\IdeaDevOps\dockers\apache2&gt;docker ps --all
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS               NAMES
c669aacbed7a        debian_apache2      &quot;/bin/sh -c 'service…&quot;   20 minutes ago      Exited (1) 1 second ago                       apache2
8d1c4522aa30        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                        elated_wing
ae769d388b36        ubuntu              &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                        kind_murdock
5596841696f6        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                        festive_booth

D:\IdeaDevOps\dockers\apache2&gt;docker ps --all
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES
c669aacbed7a        debian_apache2      &quot;/bin/sh -c 'service…&quot;   21 minutes ago      Exited (1) 23 seconds ago                       apache2
8d1c4522aa30        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          elated_wing
ae769d388b36        ubuntu              &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          kind_murdock
5596841696f6        mydebian            &quot;bash&quot;                   2 hours ago         Exited (0) 2 hours ago                          festive_booth
</code></pre>
"
"44501130","Get path relative to executed flask app","<python><sqlite><flask>","56483253","Accessing a file inside a docker container using an absolute import","<docker><flask>","<p>In my flask app I recreate a sqlite database at every start.<br>
For this I use code as shown on the <a href=""http://flask.pocoo.org/docs/0.12/tutorial/dbinit/"" rel=""nofollow noreferrer"">official webpage</a></p>

<p>My project structure looks like this</p>

<pre><code>project_dir/
|-README.md
`-app/
  |-StubbyServer.py (contains the flask root)
  |-schema.sql
  `- (all the other files)
</code></pre>

<p>Now my <code>StubbyServer.py</code> contains: </p>

<pre><code>def get_db():
    db = getattr(Flask, '_database', None)
    if db is None:
        db = Flask._database = sqlite3.connect(DATABASE)
        with open('schema.sql', mode='r') as f:
            db.cursor().executescript(f.read())
        db.commit()
        db.row_factory = sqlite3.Row
    return db
</code></pre>

<p>If my working directory is <code>/path/project_dir/app</code> the command <code>python StubbyServer.py</code> works fine </p>

<p>If my working directory is <code>/path/project_dir</code> the command <code>python app/StubbyServer.py</code> fails with:</p>

<blockquote>
  <p>File ""app/StubbyServer.py"", line 43, in get_db<br>
      with open('schema.sql', mode='r') as f:<br>
  FileNotFoundError: [Errno 2] No such file or directory: 'schema.sql'</p>
</blockquote>

<p>I know why this happens but I don't know how I can work around this. 
I want my flask app to work fine independent from my current working dir, how can I achieve this?</p>
","<p>I'm trying to access a specific file within a folder inside my docker container and getting the error <code>No such file or directory: '/api/assets/dna/human.txt'</code></p>

<p>I'm using <code>with open('/api/assets/dna/human.txt'), 'r')</code> from the <code>dna.py</code>  file inside my models directory.</p>

<p>My folder structure is </p>

<pre><code>-api
 -models
  -dna.py
 -assets
  -dna
   -human.txt
-Dockerfile
</code></pre>

<p>However, if <code>human.txt</code> is in the root folder like so -</p>

<pre><code>-api
 -models
  -dna.py
 -assets
  -dna
-Dockerfile
-human.txt
</code></pre>

<p><code>with open('human.txt'), 'r') as f:</code> works just fine. </p>

<p>I feel silly for not being able to figure out why the absolute import is not able to find the <code>human.txt</code> file when its not located in the root folder, so thank you very much for pointing out what I'm doing wrong.</p>
"
"44603941","How to enable pdo_mysql in the php docker image","<php><mysql><docker>","57183109","How to enable php's mysql extensions in docker container?","<php><mysql><docker><docker-compose><dockerfile>","<p>I have a basic <code>Dockerfile</code> with the following in:</p>

<pre><code>FROM php:7.1-apache
RUN apt-get update &amp;&amp; docker-php-ext-install pdo_mysql
COPY . /var/www
EXPOSE 80
</code></pre>

<p>I have a docker-compose.yml file</p>

<pre><code>version: ""3""
services:
  app:
    build: .
    ports:
      - ""80:80""
    volumes:
      - .:/var/www
    depends_on:
      - mysql
  mysql:
    image: mysql:8
    ports:
      - ""3306:3306""
    environment:
      MYSQL_DATABASE: ""app""
      MYSQL_USER: ""app""
      MYSQL_PASSWORD: ""app""
      MYSQL_ROOT_PASSWORD: ""test""
</code></pre>

<p>I then ran <code>docker build -t app . &amp;&amp; docker-compose up</code> at the root of the project. Everything seems to build correctly, but when outputting <code>phpinfo</code> I don't see the mysql_pdo extension.</p>

<p><a href=""https://i.stack.imgur.com/PyaQI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PyaQI.png"" alt=""enter image description here""></a></p>

<p>Are there any steps I am missing?</p>
","<p>i have the following docker-compose.yml</p>

<pre><code>web:
  image: nginx:1.17.1-alpine
  ports:
    - ""80:80""
  volumes:
    - ./code:/code
    - ./site.conf:/etc/nginx/conf.d/site.conf
  links:
    - php

php:
  build: .
  volumes:
    - ./code:/code
  links:
    - mysql

mysql:
  image: yobasystems/alpine-mariadb:latest
  ports:
    - ""3306:3306""
  volumes:
    - ./mysql:/var/lib/mysql
environment: 
    - MYSQL_ROOT_PASSWORD=password
</code></pre>

<p>and following dockerfile </p>

<pre><code>FROM php:7.1.0-fpm-alpine

RUN docker-php-ext-install mysqli &amp;&amp; docker-php-ext-enable mysqli
</code></pre>

<p>In this setup , php's mysql extensions or docker mysql extensions never get installed.  i cannot access mysql from php container.  adminer.php complains saying ""None of the supported PHP extensions (MySQLi, MySQL, PDO_MySQL) are available.""</p>

<p>How do we fix this problem ?</p>
"
"47900844","Reached error page: about:neterror when trying to navigate to other tabs if there is a form submit under that tab","<selenium><firefox><selenium-webdriver><webdriver><geckodriver>","57598310","How do you set up Selenium Grid using the docker images in GitLab-CI?","<django><selenium><docker><gitlab><gitlab-ci>","<p>when I use Selenium to do automation testing, I hit an issue, here are all scenarios:</p>

<p>There are several tabs on top of the page, now that I want to click those tabs and fill up all forms under those tabs, but if I submit <code>formA</code> which under <code>tabA</code>, then I can not navigate to other tabs automatically. If I didn't submit the form data, the issue will not be happened. Here is the log:</p>

<pre><code>1513753361368 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361388 Marionette DEBUG Received DOM event ""beforeunload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""pagehide"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361391 Marionette DEBUG Received DOM event ""unload"" for ""https://192.168.1.20/link.cgi?1513753343333""
1513753361427 Marionette DEBUG Received DOM event ""DOMContentLoaded"" for ""about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82""
org.openqa.selenium.WebDriverException: Reached error page: about:neterror?e=connectionFailure&amp;u=https%3A//192.168.1.20/network.cgi&amp;c=UTF-8&amp;f=regular&amp;d=Firefox%20%E6%97%A0%E6%B3%95%E5%BB%BA%E7%AB%8B%E5%88%B0%20192.168.1.20%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E3%80%82
Build info: version: '3.8.1', revision: '6e95a6684b', time: '2017-12-01T18:33:54.468Z'
System info: host: 'PC-20161127KZEG', ip: '192.168.131.1', os.name: 'Windows 7', os.arch: 'amd64', os.version: '6.1', java.version: '1.8.0_111'
Driver info: org.openqa.selenium.firefox.FirefoxDriver
Capabilities {acceptInsecureCerts: true, browserName: firefox, browserVersion: 57.0.2, javascriptEnabled: true, moz:accessibilityChecks: false, moz:headless: false, moz:processID: 42248, moz:profile: C:\Users\Administrator\AppD..., moz:webdriverClick: false, pageLoadStrategy: normal, platform: XP, platformName: XP, platformVersion: 6.1, rotatable: false, timeouts: {implicit: 0, pageLoad: 300000, script: 30000}}
</code></pre>
","<p>Does anyone know how to configure the Selenium Docker images as services in GitLab-CI for running integration tests as part of a CI pipeline?</p>

<p>I'm testing a Django application, and I've set up the tests to connect to Selenium thus.</p>

<pre class=""lang-py prettyprint-override""><code>class SeleniumTestCase(LiveServerTestCase):
    host = os.environ.get('LIVE_TEST_CASE_HOST', os.environ('HOSTNAME'))

    @classmethod
    def setUpClass(cls):
        super().setUpClass()

        options = webdriver.FirefoxOptions()
        options.headless = True

        cls.browser = webdriver.Remote(
            'http://selenium:4444/wd/hub',
            webdriver.DesiredCapabilities.FIREFOX,
            options=options,
        )
</code></pre>

<p>I've configured the Selenium Docker images locally using <code>docker-compose</code> and it works great.</p>

docker-compose.yml

<pre><code>selenium:
  image: selenium/hub:3.141.59-titanium

firefox:
  image: selenium/node-firefox:3.141.59-titanium
  volumes:
    - /dev/shm:/dev/shm
  depends_on:
    - selenium
  environment:
    - HUB_HOST=selenium
    - HUB_PORT=4444
    - START_XVFB=false
</code></pre>

<p>But when I try to create a similar configuration for GitLab-CI using a Docker executor, I get a <code>WebDriverException</code>.</p>

.gitlab-ci.yml

<pre><code>test:
  stage: test
  image: $CMS_IMAGE_NAME
  variables:
    HUB_HOST: selenium
    HUB_PORT: 4444
    START_XVFB: 'false'
  services:
    - name: $DB_IMAGE_NAME
      alias: db
    - name: selenium/hub:3.141.59-titanium
      alias: selenium
    - name: selenium/node-firefox:3.141.59-titanium
  script:
    - export LIVE_TEST_CASE_HOST=$(ip addr show eth0 | grep inet | awk '{ print $2 }' | sed -En 's/^([^/]+)\/.*/\1/p'
    - cd /code
    - coverage run manage.py test
</code></pre>

<p>The <code>$LIVE_TEST_CASE_HOST</code> variable is just a way for me to make the IP address of the container in CI available for use as the host in the URLs used in my test cases. I've found that <code>$HOSTNAME</code> works locally in Docker, but not in CI for some reason. This configuration works if I drop <code>selenium/hub</code> and use <code>selenium/standalone-firefox</code>. But the above configuration causes the following exception.</p>

<pre><code>selenium.common.exceptions.WebDriverException: Message: Error forwarding the new session Empty pool of VM for setup Capabilities {acceptInsecureCerts: true, browserName: firefox, marionette: true, moz:firefoxOptions: {args: [-headless]}}
Stacktrace:
    at org.openqa.grid.web.servlet.handler.RequestHandler.process (RequestHandler.java:118)
    at org.openqa.grid.web.servlet.DriverServlet.process (DriverServlet.java:85)
    at org.openqa.grid.web.servlet.DriverServlet.doPost (DriverServlet.java:69)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:707)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)
    at org.seleniumhq.jetty9.servlet.ServletHolder.handle (ServletHolder.java:865)
    at org.seleniumhq.jetty9.servlet.ServletHandler.doHandle (ServletHandler.java:535)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.handle (ScopedHandler.java:146)
    at org.seleniumhq.jetty9.security.SecurityHandler.handle (SecurityHandler.java:548)
    at org.seleniumhq.jetty9.server.handler.HandlerWrapper.handle (HandlerWrapper.java:132)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.nextHandle (ScopedHandler.java:257)
    at org.seleniumhq.jetty9.server.session.SessionHandler.doHandle (SessionHandler.java:1595)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.nextHandle (ScopedHandler.java:255)
    at org.seleniumhq.jetty9.server.handler.ContextHandler.doHandle (ContextHandler.java:1340)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.nextScope (ScopedHandler.java:203)
    at org.seleniumhq.jetty9.servlet.ServletHandler.doScope (ServletHandler.java:473)
    at org.seleniumhq.jetty9.server.session.SessionHandler.doScope (SessionHandler.java:1564)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.nextScope (ScopedHandler.java:201)
    at org.seleniumhq.jetty9.server.handler.ContextHandler.doScope (ContextHandler.java:1242)
    at org.seleniumhq.jetty9.server.handler.ScopedHandler.handle (ScopedHandler.java:144)
    at org.seleniumhq.jetty9.server.handler.HandlerWrapper.handle (HandlerWrapper.java:132)
    at org.seleniumhq.jetty9.server.Server.handle (Server.java:503)
    at org.seleniumhq.jetty9.server.HttpChannel.handle (HttpChannel.java:364)
    at org.seleniumhq.jetty9.server.HttpConnection.onFillable (HttpConnection.java:260)
    at org.seleniumhq.jetty9.io.AbstractConnection$ReadCallback.succeeded (AbstractConnection.java:305)
    at org.seleniumhq.jetty9.io.FillInterest.fillable (FillInterest.java:103)
    at org.seleniumhq.jetty9.io.ChannelEndPoint$2.run (ChannelEndPoint.java:118)
    at org.seleniumhq.jetty9.util.thread.strategy.EatWhatYouKill.runTask (EatWhatYouKill.java:333)
    at org.seleniumhq.jetty9.util.thread.strategy.EatWhatYouKill.doProduce (EatWhatYouKill.java:310)
    at org.seleniumhq.jetty9.util.thread.strategy.EatWhatYouKill.tryProduce (EatWhatYouKill.java:168)
    at org.seleniumhq.jetty9.util.thread.strategy.EatWhatYouKill.run (EatWhatYouKill.java:126)
    at org.seleniumhq.jetty9.util.thread.ReservedThreadExecutor$ReservedThread.run (ReservedThreadExecutor.java:366)
    at org.seleniumhq.jetty9.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:765)
    at org.seleniumhq.jetty9.util.thread.QueuedThreadPool$2.run (QueuedThreadPool.java:683)
    at java.lang.Thread.run (Thread.java:748)
</code></pre>

<p>I can't figure out why this works locally, but not in GitLab-CI.</p>
"
"49189883","How to set redirect_uri protocol to HTTPS in Azure Web Apps","<asp.net-core><oauth-2.0><azure-web-app-service><azure-active-directory>","57425951","ASP.NET Core passing HTTP (not HTTPS) redirect URL in OAuth challenge when hosted in Linux container","<azure><docker><asp.net-core><oauth><asp.net-identity>","<p>I am facing the following problem. I have an ASP Net Core 2 web app that I want to deploy to Azure. The app authentication is integrated with the Azure Active Directory, so when I try to login the following requests happen:</p>

<pre><code>GET https://login.microsoftonline.com/ecf3f643-27e5-4aa7-9d56-fd350e1e9c37/oauth2/authorize?client_id=20a2bcb5-0433-4bb4-bba3-d7dc4c533e85&amp;redirect_uri=http://myapplication.mydomain.com/account/signin [...]  200 OK
POST http://myapplication.mydomain.com/account/signin 301 Redirect --&gt; https://myapplication.mydomain.com/account/signin
GET https://myapplication.mydomain.com/account/signin 500 Internal Server Error
</code></pre>

<p>The first GET is the normal Azure Active Directory login request. <strong>Notice the <code>redirect_uri</code> parameter has protocol http</strong>. </p>

<p>The second request is the redirection to the <code>redirect_uri</code>, a POST with some parameters. Since I have configured Azure to allow only HTTPS traffic, then IIS redirects to the same URL with HTTPS. That's the third request. Notice this third request is a GET request, since <a href=""https://softwareengineering.stackexchange.com/questions/99894/why-doesnt-http-have-post-redirect"">HTTP redirection is always a GET request</a> all the paremeters of the POST request are lost, and the authentication fails giving a HTTP 500 error in the backend.</p>

<p>I have tried to manually change the protocol in the <code>redirect_uri</code> parameter manually to HTTPS, and <strong>it works</strong> as expected. So, <strong>the only thing I need is to make ASP Net Core aware that the protocol is HTTPS</strong>. </p>

<p>How can that be done? I've searched tons of pages in the Internet without a clear answer. </p>

<p>Note: the <code>redirect_uri</code> is set by Kestrel. Since Azure App Service puts an IIS in front of my Kestrel and does the SSL termination there, Kestrel and my app do not know the protocol is HTTPS, and therefore use HTTP in the redirect uri.</p>

<p><strong>UPDATE 1</strong></p>

<p>Following the advice of <a href=""https://stackoverflow.com/users/6755924/bruce-chen"">@Bruce</a> I've tried the example <a href=""https://github.com/Azure-Samples/active-directory-dotnet-webapp-openidconnect-aspnetcore/blob/d55b22bb475f8cb101cb45ca5dcc27453069185c/README.md"" rel=""noreferrer"">here</a>, cloning the repository and configuring the application and the AD as stated there, and I am able to reproduce the error. </p>

<p>The redirect URI continues to be with <code>http</code> protocol. If I only add in the AD app configuration the <code>https</code> endpoint as reply URL, I get the error <code>The reply address 'http://testloginad.azurewebsites.net/signin-oidc' does not match the reply addresses configured for the application</code>. If I add the <code>http</code> protocol endpoint as reply URL, then I get an HTTP 500 error like the following:</p>

<pre><code>System.Exception: Correlation failed.
   at Microsoft.AspNetCore.Authentication.RemoteAuthenticationHandler`1.&lt;HandleRequestAsync&gt;d__12.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter`1.GetResult()
   at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.&lt;Invoke&gt;d__6.MoveNext()
</code></pre>

<p>I am still thinking <strong>the problem is related to Kestrel not knowing the connection is being done through HTTPS</strong>, but I do not know how to convey that information to it.</p>

<p><strong>UPDATE 2</strong></p>

<p>The configuration of the Azure web app I used:</p>

<ul>
<li>Wep App Type: Linux</li>
<li>Application Settings:

<ul>
<li>Stack: .NET Core 2.0</li>
<li>Startup file: dotnet ./WebApp-OpenIDConnect-DotNet.dll</li>
<li>WEBSITE_HTTPLOGGING_RETENTION_DAYS: 5</li>
<li>ASPNETCORE_ENVIRONMENT: Development</li>
<li>Always ON: On</li>
<li>ARR Affinity: On</li>
</ul></li>
<li>Custom domains:

<ul>
<li>HTTPS Only: On</li>
</ul></li>
<li>Diagnostic logs:

<ul>
<li>Docker Container Logging: File System</li>
<li>Quota (MB): 35</li>
<li>Retention Period (Days): 5</li>
</ul></li>
</ul>

<p>In the <code>web.config</code> file I modified the following line to read like this:</p>

<pre><code>&lt;aspNetCore processPath=""dotnet"" arguments=""./WebApp-OpenIDConnect-DotNet.dll"" stdoutLogEnabled=""false"" stdoutLogFile=""./stdout.log"" /&gt;
</code></pre>

<p>Basically I put forward slashes instead of back slashes to avoid problems with Linux paths.</p>

<p>Everything else is configured using default settings.</p>

<p><strong>UPDATE 3</strong>
As requested by <a href=""https://stackoverflow.com/users/2588374/tratcher"">@Tratcher</a>, I add here the headers of the server reponses (for the sake of brevity I include only the headers I consider relevant, if you want to see any other one, feel free to ask me to add it):</p>

<ul>
<li>First request (<code>GET https://login.microsoftonline.com/ecf...</code>): 

<ul>
<li>Server: <code>Microsoft-IIS/10.0</code></li>
<li>Set-Cookie: <code>ESTSAUTHPERSISTENT=AQAFCCEADDB…sts; path=/; secure; HttpOnly</code></li>
<li>Strict-Transport-Security: <code>max-age=31536000; includeSubDomains</code></li>
</ul></li>
<li>Second request (<code>POST http://testloginad.azurewebsites.net/signin-oidc</code>):

<ul>
<li>Location: <code>https://testloginad.azurewebsites.net/signin-oidc</code></li>
<li>Server: <code>Microsoft-IIS/10.0</code></li>
</ul></li>
<li>Third request (<code>GET https://testloginad.azurewebsites.net/signin-oidc</code>):

<ul>
<li>Server: <code>Kestrel</code></li>
</ul></li>
</ul>

<p>No <code>x-forwarded-proto</code> header appears in any of the requests.</p>

<p>Note that one the root of the problem may be in the redirect of the second request, that is redirecting the HTTP POST to an HTTPS GET. That redirect should not happen since the POST should have been requested through HTTPS in the first place, but that did not happen because of the wrong http protocol in the redirect_uri of the first request.</p>

<p><strong>UPDATE 4</strong></p>

<p>I have confirmed this issue only happens if the chosen service plan is a Linux one. The issue does not happen at all if the service plan is a Windows one (using exactly the same code and configuration from the example of UPDATE 1). This may be a workaround, but not a solution, to the problem. The Linux app service seem to be flawed.</p>
","<p>I have an implementation of Identity Server 4 which offers an option to sign in with Google. The app is registered with Google's developer console and has worked for some time when hosted on a Windows VM.</p>

<p>I've recently containerised this application and deployed it to a Linux container hosted as an Azure app service. I haven't changed any of the app code. The Azure app service is configured to serve HTTPS only, and I've verified the traffic is strictly secured with SSL certificates both between client browsers and Cloudflare (my DNS provider) and between Cloudflare and origin.</p>

<p>Here's what the Google OAuth button looks like. You can see it's a valid SSL connection:</p>

<p><a href=""https://i.stack.imgur.com/9YuLP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9YuLP.png"" alt=""enter image description here""></a></p>

<p>Here's the code which registers this OAuth provider:</p>

<pre><code>        services.AddAuthentication()
            .AddGoogle(""Google"", options =&gt;
            {
                options.SignInScheme = IdentityServerConstants.ExternalCookieAuthenticationScheme;

                options.ClientId = _externalAuthConfig.Google.ClientId;
                options.ClientSecret = _externalAuthConfig.Google.ClientSecret;
            });
</code></pre>

<p>I've confirmed the client and secret are valid. Here's the code executed when the button is clicked. You see this is all standard stuff.</p>

<pre><code>[HttpPost]
[AllowAnonymous]
[ValidateAntiForgeryToken]
[Route(""challenge"")]
public IActionResult ExternalLogin(
    [FromForm] string provider, 
    [FromQuery] string returnUrl = null)
{
    // Request a redirect to the external login provider.
    var redirectUrl = Url.Action(nameof(ExternalLoginCallback), new { returnUrl });
    var properties = _signInManager.ConfigureExternalAuthenticationProperties(provider, redirectUrl);
    return Challenge(properties, provider);
}
</code></pre>

<p>However, in this containerized world, when the browser is redirected to Google, this is what I see:</p>

<p><a href=""https://i.stack.imgur.com/fOlcM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fOlcM.png"" alt=""enter image description here""></a></p>

<p>Notice the <code>http://</code> in the redirect URL. Obviously the error is occurring because I only registered the HTTPS redirect URL. The same code running on a Windows VM correctly passes an HTTPS redirect URL in the querystring. I have no idea why this unsecure URL is being used in this containerised environment. The only difference as far as I can tell is the hosting infrastructure. </p>

<p>In case it's important, this new site uses the built-in Kestrel web server, whereas the old Windows version used IIS in front.</p>

<p>Anyone have any idea? I'm stumped!</p>
"
"51508150","standard_init_linux.go:190: exec user process caused ""no such file or directory"" - Docker","<docker><dockerfile><docker-for-windows>","57590432","standard_init_linux.go:211: exec user process caused ""no such file or directory""","<docker><docker-compose><dockerfile>","<p>When I am running my docker image on windows 10. I am getting this error:</p>

<pre><code>standard_init_linux.go:190: exec user process caused ""no such file or directory""
</code></pre>

<p>my docker file is:</p>

<pre><code>FROM openjdk:8

EXPOSE 8080

VOLUME /tmp

ADD appagent.tar.gz /opt/app-agent
ADD services.jar app.jar
ADD run.sh /run.sh

# Install compiler and perl stuff
RUN apt-get update
RUN apt-get install -y build-essential
RUN apt-get install -y gcc-multilib
RUN apt-get install -y perl

# Install Percona Toolkit
RUN apt-get install --yes percona-toolkit
RUN [""chmod"", ""+x"", ""/run.sh""]
ENTRYPOINT [""/run.sh""]
</code></pre>

<p>and the script is start with <strong>#!/bin/sh</strong></p>

<pre><code>#!/bin/sh
set -e

JAVA_OPTS=""-Dfile.encoding=UTF-8 -Djava.security.egd=file:/dev/urandom""

if [ ""${APPD_APP_NAME}"" != """" ]; then
JAVA_AGENT=""-javaagent:/opt/app-agent/javaagent.jar
fi

exec java ${JVM_OPTS} ${JAVA_OPTS} ${JAVA_AGENT} -jar /app.jar
</code></pre>

<p>Tried method1:
<strong>Tried changing #!/bin/sh to #!/bin/bash</strong> but getting same error.</p>

<p>Tried method2:
added dos2unix in docker file</p>

<pre><code>RUN apt-get install -y dos2unix
RUN dos2unix /run.sh
</code></pre>
","<p>I am building the docker image of my project using the Dockerfile provided with it but docker container is always remains in restarting state. Below is the container log which I see:-
standard_init_linux.go:211: exec user process caused ""no such file or directory""
Can some one prove me with the possible solution to it and also tell what's the root cause of this issue.</p>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","57232424","Configuring listener with ""localhost"" causes a failure to retrieve meta data about the broker","<docker><apache-kafka><docker-compose>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am trying to set up a single Kafka broker using a docker container and I'm using the image from <a href=""https://github.com/wurstmeister/kafka-docker"" rel=""nofollow noreferrer"">here</a>. The objective is to run the Kafka broker in docker, and run the Producer/Consumers from the Host.</p>

<p>While configuring the <code>KAFKA_LISTENERS</code> and <code>KAFKA_ADVERTISED_LISTENERS</code> properties, I realised that I am unable to retrieve broker meta data if the following configuration is in place</p>

<pre><code>KAFKA_BROKER_ID: 0
KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
# KAFKA_ADVERTISED_LISTENERS must be EQUAL or SUBSET of KAFKA_LISTENERS
KAFKA_LISTENERS: INTERNAL://kafka0:29092, EXTERNAL://localhost:9092
# INTERNAL://kafka0:29092 is specified because of the KAFKA_INTER_BROKER_LISTENER_NAME configuration.
KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka0:29092, EXTERNAL://localhost:9092
KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
KAFKA_MESSAGE_MAX_BYTES: '200000'
</code></pre>

<pre><code>&gt; kafkacat -b localhost:9092 -L
ERROR: Failed to acquire metadata: Local: Broker transport failure
</code></pre>

<p>But replacing <code>EXTERNAL://localhost:9092</code> with <code>EXTERNAL://kafka0:9092</code> or <code>EXTERNAL://:9092</code> (default interface) in the <code>KAFKA_LISTENERS</code> config <strong>fixes it.</strong></p>

<pre><code>&gt;kafkacat -b localhost:9092 -L
Metadata for all topics (from broker 0: localhost:9092/0):
 1 brokers:
  broker 0 at localhost:9092 (controller)
 0 topics:
</code></pre>

<p>This is highly puzzling. Nothing in the Kafka documentation for Kafka 2.2.x says anything about this behaviour. <a href=""http://kafka.apache.org/22/documentation.html#config"" rel=""nofollow noreferrer"">http://kafka.apache.org/22/documentation.html#config</a>. Hope someone can shed some light.</p>

<p>Full docker-compose file below</p>

<pre><code>version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - ""2181:2181""
  kafka0:
    image: wurstmeister/kafka:2.12-2.2.1
    ports:
      - ""9092:9092""
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      # KAFKA_ADVERTISED_LISTENERS must be EQUAL or SUBSET of KAFKA_LISTENERS
      KAFKA_LISTENERS: INTERNAL://kafka0:29092, EXTERNAL://localhost:9092
      # INTERNAL://kafka0:29092 is specified because of the KAFKA_INTER_BROKER_LISTENER_NAME configuration.
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka0:29092, EXTERNAL://localhost:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_MESSAGE_MAX_BYTES: '200000'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
</code></pre>

<p>Verbose output for error</p>

<pre><code>kafkacat -b localhost:9092 -L -d broker
%7|1564230563.245|BROKER|rdkafka#producer-1| [thrd:app]: localhost:9092/bootstrap: Added new broker with NodeId -1
%7|1564230563.245|BRKMAIN|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Enter main broker thread
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:app]: localhost:9092/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))
%7|1564230563.245|BRKMAIN|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Enter main broker thread
%7|1564230563.245|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.1.0 (0x10100ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, CC CXX PKGCONFIG OSXLD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x2)
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Received CONNECT op
%7|1564230563.245|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: broker in state TRY_CONNECT connecting
%7|1564230563.245|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230563.245|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230563.246|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connecting to ipv6#[::1]:9092 (plaintext) with socket 9
%7|1564230563.246|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected to ipv6#[::1]:9092
%7|1564230563.246|CONNECTED|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected (#1)
%7|1564230563.246|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion
%7|1564230563.246|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state CONNECT -&gt; APIVERSION_QUERY
%7|1564230563.246|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230563.246|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230563.247|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230563.247|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features -ApiVersion to
%7|1564230563.247|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -&gt; DOWN
%7|1564230563.247|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230563.247|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)
%7|1564230563.247|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230563.247|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230564.247|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: no cluster connection
%7|1564230564.248|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state DOWN -&gt; INIT
%7|1564230564.248|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230564.248|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:main]: localhost:9092/bootstrap: Selected for cluster connection: no cluster connection (broker has 1 connection attempt(s))
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Received CONNECT op
%7|1564230565.249|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: broker in state TRY_CONNECT connecting
%7|1564230565.249|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230565.249|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230565.250|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connecting to ipv6#[::1]:9092 (plaintext) with socket 9
%7|1564230565.250|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected to ipv6#[::1]:9092
%7|1564230565.250|CONNECTED|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected (#2)
%7|1564230565.250|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion
%7|1564230565.250|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state CONNECT -&gt; APIVERSION_QUERY
%7|1564230565.250|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230565.250|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230565.254|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230565.254|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features -ApiVersion to
%7|1564230565.254|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -&gt; DOWN
%7|1564230565.254|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230565.254|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)
%7|1564230565.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 45ms: application metadata request
%7|1564230565.254|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state DOWN -&gt; INIT
%7|1564230565.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 44ms: application metadata request
%7|1564230565.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 44ms: application metadata request
%7|1564230565.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 44ms: application metadata request
%7|1564230566.252|CONNECT|rdkafka#producer-1| [thrd:main]: localhost:9092/bootstrap: Selected for cluster connection: no cluster connection (broker has 2 connection attempt(s))
%7|1564230566.252|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Received CONNECT op
%7|1564230566.252|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT
%7|1564230566.252|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: broker in state TRY_CONNECT connecting
%7|1564230566.252|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT
%7|1564230566.252|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230566.252|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230566.253|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connecting to ipv6#[::1]:9092 (plaintext) with socket 9
%7|1564230566.253|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected to ipv6#[::1]:9092
%7|1564230566.253|CONNECTED|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected (#3)
%7|1564230566.253|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion
%7|1564230566.253|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state CONNECT -&gt; APIVERSION_QUERY
%7|1564230566.253|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230566.253|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request
%7|1564230566.254|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230566.254|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features -ApiVersion to
%7|1564230566.254|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -&gt; DOWN
%7|1564230566.254|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)
%7|1564230566.254|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)
%7|1564230566.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230566.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230567.254|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: no cluster connection
%7|1564230567.254|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state DOWN -&gt; INIT
%7|1564230567.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230567.254|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230568.248|CONNECT|rdkafka#producer-1| [thrd:app]: localhost:9092/bootstrap: Selected for cluster connection: application metadata request (broker has 3 connection attempt(s))
%7|1564230568.248|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230568.248|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Received CONNECT op
%7|1564230568.248|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT
%7|1564230568.248|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: broker in state TRY_CONNECT connecting
%7|1564230568.248|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT
% ERROR: Failed to acquire metadata: Local: Broker transport failure
</code></pre>

<p>Verbose output for expected output</p>

<pre><code>kafkacat -b localhost:9092 -L -d broker
%7|1564230960.441|BROKER|rdkafka#producer-1| [thrd:app]: localhost:9092/bootstrap: Added new broker with NodeId -1
%7|1564230960.441|BRKMAIN|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Enter main broker thread
%7|1564230960.441|BRKMAIN|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Enter main broker thread
%7|1564230960.441|CONNECT|rdkafka#producer-1| [thrd:app]: localhost:9092/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))
%7|1564230960.441|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.1.0 (0x10100ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, CC CXX PKGCONFIG OSXLD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x2)
%7|1564230960.441|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230960.441|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230960.441|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Received CONNECT op
%7|1564230960.441|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT
%7|1564230960.442|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: broker in state TRY_CONNECT connecting
%7|1564230960.442|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT
%7|1564230960.442|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230960.442|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 49ms: application metadata request
%7|1564230960.443|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connecting to ipv6#[::1]:9092 (plaintext) with socket 9
%7|1564230960.443|CONNECT|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected to ipv6#[::1]:9092
%7|1564230960.443|CONNECTED|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connected (#1)
%7|1564230960.443|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion
%7|1564230960.443|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state CONNECT -&gt; APIVERSION_QUERY
%7|1564230960.443|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230960.443|CONNECT|rdkafka#producer-1| [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 47ms: application metadata request
%7|1564230960.448|FEATURE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Updated enabled protocol features to MsgVer1,ApiVersion,BrokerBalancedConsumer,ThrottleTime,Sasl,SaslHandshake,BrokerGroupCoordinator,LZ4,OffsetTime,MsgVer2,IdempotentProducer,ZSTD
%7|1564230960.448|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -&gt; UP
%7|1564230960.451|CLUSTERID|rdkafka#producer-1| [thrd:main]: localhost:9092/bootstrap: ClusterId update """" -&gt; ""d3yPPxe8T6adHMiwXDT4Vg""
%7|1564230960.451|CONTROLLERID|rdkafka#producer-1| [thrd:main]: localhost:9092/bootstrap: ControllerId update -1 -&gt; 0
%7|1564230960.451|UPDATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: NodeId changed from -1 to 0
%7|1564230960.451|UPDATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Name changed from localhost:9092/bootstrap to localhost:9092/0
%7|1564230960.451|LEADER|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Mapped 0 partition(s) to broker
%7|1564230960.451|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Broker changed state UP -&gt; UPDATE
%7|1564230960.451|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Broker changed state UPDATE -&gt; UP
Metadata for all topics (from broker 0: localhost:9092/0):
 1 brokers:
  broker 0 at localhost:9092 (controller)
 0 topics:
%7|1564230960.453|DESTROY|rdkafka#producer-1| [thrd:app]: Terminating instance (destroy flags none (0x0))
%7|1564230960.453|DESTROY|rdkafka#producer-1| [thrd:main]: Destroy internal
%7|1564230960.453|DESTROY|rdkafka#producer-1| [thrd:main]: Removing all topics
%7|1564230960.453|DESTROY|rdkafka#producer-1| [thrd:main]: Sending TERMINATE to localhost:9092/0
%7|1564230960.453|TERM|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Received TERMINATE op in state UP: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs
%7|1564230960.453|TERM|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Received TERMINATE op in state INIT: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs
%7|1564230960.453|BROKERFAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: failed: err: Local: Broker handle destroyed: (errno: Undefined error: 0)
%7|1564230960.453|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: failed: err: Local: Broker handle destroyed: (errno: Resource temporarily unavailable)
%7|1564230960.453|FAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Client is terminating (after 11ms in state INIT)
%7|1564230960.453|STATE|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Broker changed state INIT -&gt; DOWN
%7|1564230960.453|TERMINATE|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Handle is terminating in state DOWN: 1 refcnts (0x7fb393810b28), 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs: failed 0 request(s) in retry+outbuf
%7|1564230960.453|BROKERFAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: failed: err: Local: Broker handle destroyed: (errno: Undefined error: 0)
%7|1564230960.453|FAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Client is terminating (after 1ms in state UP)
%7|1564230960.453|STATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Broker changed state UP -&gt; DOWN
%7|1564230960.453|TERMINATE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: Handle is terminating in state DOWN: 1 refcnts (0x7fb393811728), 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs: failed 0 request(s) in retry+outbuf
%7|1564230960.453|BROKERFAIL|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/0: failed: err: Local: Broker handle destroyed: (errno: Resource temporarily unavailable)
</code></pre>
"
"51630260","Connect to Kafka running in Docker","<java><docker><apache-kafka><confluent-platform>","57912853","Unable to connect from Java to kafka running inside docker container","<java><docker><apache-kafka>","<p>I setup a single node Kafka Docker container on my local machine like it is described in <a href=""https://docs.confluent.io/current/installation/docker/docs/installation/recipes/single-node-client.html"" rel=""noreferrer"">the Confluent documentation</a> (steps 2-3).</p>

<p>In addition, I also exposed Zookeeper's port 2181 and Kafka's port 9092 so that I'll be able to connect to them from a client running on local machine:</p>

<pre><code>$ docker run -d \
    -p 2181:2181 \
    --net=confluent \
    --name=zookeeper \
    -e ZOOKEEPER_CLIENT_PORT=2181 \
    confluentinc/cp-zookeeper:4.1.0

$ docker run -d \
    --net=confluent \
    --name=kafka \
    -p 9092:9092 \
    -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:4.1.0
</code></pre>

<p><strong>Problem:</strong> When I try to connect to Kafka from the host machine, the connection fails because it <code>can't resolve address: kafka:9092</code>.</p>

<p>Here is my Java code:</p>

<pre><code>Properties props = new Properties();
props.put(""bootstrap.servers"", ""localhost:9092"");
props.put(""client.id"", ""KafkaExampleProducer"");
props.put(""key.serializer"", LongSerializer.class.getName());
props.put(""value.serializer"", StringSerializer.class.getName());
KafkaProducer&lt;Long, String&gt; producer = new KafkaProducer&lt;&gt;(props);
ProducerRecord&lt;Long, String&gt; record = new ProducerRecord&lt;&gt;(""foo"", 1L, ""Test 1"");
producer.send(record).get();
producer.flush();
</code></pre>

<p>The exception:</p>

<pre><code>java.io.IOException: Can't resolve address: kafka:9092
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.common.network.Selector.connect(Selector.java:214) ~[kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:266) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238) [kafka-clients-2.0.0.jar:na]
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:176) [kafka-clients-2.0.0.jar:na]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
Caused by: java.nio.channels.UnresolvedAddressException: null
    at sun.nio.ch.Net.checkAddress(Net.java:101) ~[na:1.8.0_144]
    at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622) ~[na:1.8.0_144]
    at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233) ~[kafka-clients-2.0.0.jar:na]
    ... 7 common frames omitted
</code></pre>

<p><strong>Question:</strong> How to connect to Kafka running in Docker? My code is running from host machine, not Docker. </p>

<p>Note: I know that I could theoretically play around with DNS setup and <code>/etc/hosts</code> but it is a workaround - it shouldn't be like that.</p>

<p>There is also similar question <a href=""https://stackoverflow.com/q/32302444/2065796"">here</a>, however it is based on <code>ches/kafka</code> image. I use <code>confluentinc</code> based image which is not the same.</p>
","<p>I am very new to kafka and docker, and I want to connect from Java to Kafka running inside the docker container, but I get an error, and it seems to be a lack of networking knowledge from my side. </p>

<p>I have defined an env variable: <code>KAFKA_HOSTNAME=kafka</code></p>

<p>Here is my <code>docker-compose.yml</code> file:</p>

<pre><code>version: '1.1'

networks:
  sb:
    driver: bridge

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    hostname: zookeeper
    networks:
     - sb
    ports:
      - ""2181:2181""
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    hostname: ${KAFKA_HOSTNAME:-kafka}
    depends_on:
      - zookeeper
    networks:
     - sb
    ports:
      - ""9092:9092""
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_HOST_NAME: ${KAFKA_HOSTNAME:-kafka}
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_HOSTNAME:-kafka}:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
</code></pre>

<p>When I run the <code>docker-compose up</code>, I can see that the zookeeper and the Kafka broker are started successfully:</p>

<pre><code>CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                                        NAMES
60460c26ef86        confluentinc/cp-kafka:latest       ""/etc/confluent/dock…""   19 minutes ago      Up 19 minutes       0.0.0.0:9092-&gt;9092/tcp                       kafka
0d1fd4000a83        confluentinc/cp-zookeeper:latest   ""/etc/confluent/dock…""   19 minutes ago      Up 19 minutes       2888/tcp, 0.0.0.0:2181-&gt;2181/tcp, 3888/tcp   zookeeper
</code></pre>

<p>Here is my simple java program I try to run:</p>

<pre><code>private static final String bootstrapServers = ""192.168.0.102:9092"";

//create producer properties
Properties properties = new Properties();
properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

//create producer
KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties);

//producer record
ProducerRecord&lt;String, String&gt; record = new ProducerRecord(""first_topic"", ""hello from java"");

//send data
producer.send(record);


producer.flush();
producer.close();
</code></pre>

<p>In the docker logs I can see that it is creating the <code>first-topic</code> inside the kafka container, but in the Java program I get the following error:</p>

<p><code>[kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error connecting to node kafka:9092 (id: 1 rack: null)
java.net.UnknownHostException: kafka</code></p>

<p>I have a few questions:
1. What am I missing?
2. How should I correctly set the <code>KAFKA_ADVERTISED_HOST_NAME</code> property in docker-compose file? Is it correct to hardcode the IP?
3. How should I correctly set the <code>KAFKA_ADVERTISED_LISTENERS:</code> property?</p>

<p>Thank you very much!</p>
"
"52968361","Different process are running as PID 1 when running CMD/ENTRYPOINT in shell form when the base images is centos vs ubuntu:trusty","<docker><dockerfile>","57355640","Docker container runs CMD command three different ways, why?","<python><bash><docker><dockerfile><docker-build>","<p>Build and run an image using the below dockerfile. </p>

<p>Dockerfile1</p>

<pre><code>FROM ubuntu:trusty
ENTRYPOINT ping localhost
</code></pre>

<p>Now run the below command to see the processes running in the container.</p>

<pre><code>docker exec -it &lt;container&gt; ps -ef
</code></pre>

<p><strong>PID 1 process is running /bin/sh -c ping localhost</strong></p>

<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 11:35 ?        00:00:00 /bin/sh -c ping localhost
root         8     1  0 11:35 ?        00:00:00 ping localhost
root         9     0  0 11:35 pts/0    00:00:00 ps -ef
</code></pre>

<p>Now change <strong>ONLY</strong> the base image to centos:latest.</p>

<p>Modified Dockerfile</p>

<pre><code>FROM centos:latest
ENTRYPOINT ping localhost
</code></pre>

<p>Build and run an image using the modified dockerfile.
Run the 'docker exec -it  ps -ef' command again.</p>

<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 11:32 ?        00:00:00 ping localhost
root         8     0  0 11:33 pts/0    00:00:00 ps -ef
</code></pre>

<p><strong>But now PID 1 process is running 'ping localhost'</strong> </p>

<p>This happen even when ENTRYPOINT is replaced with CMD.</p>

<p>I thought when using the shell form /bin/sh is the process with PID as 1 (both when ENTRYPOINT/CMN being used).</p>

<p>Any ideas why I am seeing a different behaviour <strong>just</strong> by changing the base image?</p>
","<p>I have set up my Dockerfile, it looks like this:</p>

<pre><code>FROM python:3.6
ARG label
ARG seeds
ARG dataset_name=${label}_terms

RUN mkdir /prodigy
WORKDIR /prodigy
COPY ./prodigy-1.8.1-cp35.cp36.cp37-cp35m.cp36m.cp37m-linux_x86_64.whl /prodigy

RUN pip install prodigy-1.8.1-cp35.cp36.cp37-cp35m.cp36m.cp37m-linux_x86_64.whl
RUN pip install -U spacy
RUN python -m spacy download en_core_web_lg

EXPOSE 8080

RUN mkdir /work
ENV PRODIGY_HOME /work
WORKDIR /work
COPY ./prodigy.json /work

RUN prodigy dataset ${dataset_name}

ENV LABEL=${label}
ENV SEEDS=${seeds}

CMD prodigy terms.teach ${LABEL}_terms en_core_web_lg --seed ""$SEEDS""
</code></pre>

<p>It works, but not as expected. It should run CMD command just once. Instead it does it 3 different ways (ps aux output):</p>

<pre><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4280   692 ?        Ss   08:47   0:00 /bin/sh -c prodigy terms.teach ${LABEL}_terms en_core_web_lg --seed ""$SEEDS""
root         8  0.0  0.0   4280   740 ?        S    08:47   0:00 /bin/sh /usr/local/bin/prodigy terms.teach TRANSFER_terms en_core_web_lg --seed transfer, relocation, relegation
root         9 46.1 13.7 2329976 1687016 ?     Sl   08:47  15:13 python -m prodigy terms.teach TRANSFER_terms en_core_web_lg --seed transfer, relocation, relegation

</code></pre>

<p>I wonder what is this a standard behavior? How can i make my Dockerfile clean?</p>
"
"53733312","Where is JRE 11?","<java><java-11>","57740608","How to install only JRE from Java 12 into docker image?","<java><docker><openjdk-12>","<h1>UPDATE:</h1>
<p><sup><em>(to be <strong>more clear</strong>)</em></sup></p>
<p>You can find <a href=""https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html"" rel=""noreferrer"">JRE 8</a>, <a href=""https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase9-3934878.html"" rel=""noreferrer"">JRE 9</a> and <a href=""https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase10-4425482.html"" rel=""noreferrer"">JRE 10</a> on Oracle's official website (click on each). <em>But where is <strong>JRE 11</strong>?!</em></p>
<p>Also, JDK 11 doesn't include a JRE. I was expecting JRE to be installed with JDK.</p>
<p><em><strong>Do final users of our apps need to install JDK?</strong></em></p>
<hr />
<h1>ORIGINAL version of the question:</h1>
<p>I downloaded and installed Oracle JDK 11 from its <a href=""https://www.oracle.com/technetwork/java/javase/downloads/jdk11-downloads-5066655.html"" rel=""noreferrer"">official site</a>. I installed both  <code>..._linux-x64_bin.rpm</code> and <code>..._windows-x64_bin.exe</code> (first on a Linux machine and second on a Windows machine). But I saw an unexpected thing! Where is JRE?</p>
<p>This is a snapshot of installation path on CentOS 7. As you can see there is no <code>jre</code> folder:</p>
<pre><code># ls /usr/java/jdk-11.0.1/
bin  conf  include  jmods  legal  lib  README.html  release
</code></pre>
<p>Same snapshot about Oracle JDK 8 (See <code>jre</code> folder specially):</p>
<pre><code># ls /usr/java/jdk1.8.0_191-amd64/
bin             lib          src.zip
COPYRIGHT       LICENSE      THIRDPARTYLICENSEREADME-JAVAFX.txt
include         man          THIRDPARTYLICENSEREADME.txt
javafx-src.zip  README.html
jre             release
</code></pre>
<hr />
<p>Same snapshots on Windows machine:</p>
<pre><code>&gt; dir /b &quot;C:\Program Files\Java\jdk-11.0.1&quot; 
bin                                           
conf                                          
COPYRIGHT                                     
include                                       
jmods                                         
legal                                         
lib                                           
README.html                                   
release                                                                                     
           
&gt; dir /b &quot;C:\Program Files\Java\jdk1.8.0_181&quot;  
bin                                           
COPYRIGHT                                     
include                                       
javafx-src.zip                                
jre                                           
lib                                           
LICENSE                                       
README.html                                   
release                                       
src.zip                                       
THIRDPARTYLICENSEREADME-JAVAFX.txt            
THIRDPARTYLICENSEREADME.txt 
</code></pre>
<hr />
<p>On Windows machine, there are also two another differences between JDK 8 and JDK 11.</p>
<ol>
<li><p>A standalone <code>JRE</code> alongside <code>JDK</code> as you can see:</p>
<pre><code>&gt; dir /b &quot;C:\Program Files\Java&quot;            
jdk-11.0.1   
jdk1.8.0_181 
jre1.8.0_181 
</code></pre>
</li>
<li><p>In path <code>C:\Program Files (x86)\Common Files\Oracle\Java</code>:</p>
<pre><code>&gt; dir &quot;C:\Program Files (x86)\Common Files\Oracle\Java&quot;                                                                   
...                                                                                                                   
...                14 java.settings.cfg                                                                  
...    &lt;JUNCTION&gt;     javapath [C:\Program Files (x86)\Common Files\Oracle\Java\javapath_target_3015921] 
...    &lt;DIR&gt;          javapath_target_3015921 
...
</code></pre>
<p>As you see <code>javapath</code> (that is in <code>PATH</code> environment variable) points to <code>javapath_target_3015921</code>. This folder contains 3 executables of JDK 8 (that aren't <em>link</em>s!):</p>
<pre><code>&gt; dir /b &quot;C:\Program Files (x86)\Common Files\Oracle\Java\javapath&quot; 
java.exe                         
javaw.exe                        
javaws.exe 
</code></pre>
</li>
</ol>
<hr />
<p>Finally, I searched the web to find a standalone JRE and found out it doesn't exist!</p>
<p><em><strong>Do final users of our programs need to install JDK?</strong></em></p>
","<p>Everyone!</p>

<p>I'm trying to minimize my docker images which has to run java apps.
With Java 8 and previous versions, I had an option to install only JRE into the image. But since Java 9 was released, I've lost such option. Seems, they don't provide separate distribution for JRE anymore.</p>

<p>Probably, someone of you had some experience which might be helpful.</p>

<p>Cheers &amp; have a nice day!</p>
"
"53769821","jekyll site in docker serving locally","<docker><jekyll>","57931381","Connect to Jekyll serve inside docker","<docker><jekyll>","<p>I'm trying to work with a jekyll site locally using vim and github, all within a docker container on my Windows 10 machine. I want to work in the container like it is a linux virtual machine with my Downloads directory as a volume and jekyll served on port 55.</p>

<p>My docker initialization is</p>

<pre><code>docker container run -t -d -p 55:4000 -v ${PWD}:""/home/Downloads/"" [container ID]
</code></pre>

<p>To start the jekyll site I run the following within the docker container</p>

<pre><code>jekyll new my-awesome-site
cd my-awesome-site
bundle exec jekyll serve
...
Server address: http://127.0.0.1:4000/
Server running... press ctrl-c to stop.
</code></pre>

<p>There is nothing showing up locally at <code>127.0.0.1:55</code> 
<a href=""https://i.stack.imgur.com/acOLN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/acOLN.png"" alt=""Chrome Error""></a></p>

<p><strong>What am I doing wrong?</strong></p>

<p>Here is my Dockerfile</p>

<pre><code>FROM: ubuntu:latest

RUN apt-get update &amp;&amp; apt-get -y upgrade
RUN apt-get install -y git &amp;&amp; apt-get install -y software-properties-common
RUN apt-get install -y python-pip python-dev ruby-full build-essential
RUN pip install --upgrade pip
RUN apt-get install -y vim

# build ruby configuration
RUN mkdir gems \
    &amp;&amp; echo '# Install Ruby Gems to /gems' &gt;&gt; /gems/.bashrc \
    &amp;&amp; echo 'export GEM_HOME=/gems' &gt;&gt; /gems/.bashrc \
    &amp;&amp; echo 'export PATH=/gems/bin:$PATH' &gt;&gt; /gems/.bashrc \
    &amp;&amp; /bin/bash -c ""source /gems/.bashrc""

# install jekyll
RUN gem install jekyll bundler

EXPOSE 4000

WORKDIR /home/work
</code></pre>

<p><code>docker ps</code></p>

<pre><code>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                  NAMES
e25f6359ce49        572                 ""/bin/bash""         28 minutes ago      Up 28 minutes       0.0.0.0:55-&gt;4000/tcp   loving_gagarin
</code></pre>
","<p>I have Jekyll in docker. I'm running this command:</p>

<pre><code>$ docker run --rm -ti -v $(pwd):/tmp/www -p 127.0.0.1:8080:4000 jcubic.pl
Configuration file: /tmp/www/_config.yml
            Source: /tmp/www
       Destination: /tmp/www/_site
 Incremental build: disabled. Enable with --incremental
      Generating... 
                    done in 13.443 seconds.
 Auto-regeneration: enabled for '/tmp/www'
    Server address: http://127.0.0.1:4000/
  Server running... press ctrl-c to stop.
</code></pre>

<p>But I can't access port 8080 on my local machine. Port is working because I can use:</p>

<pre><code>$ docker run --rm -ti -v $(pwd):/tmp/www -p 127.0.0.1:8080:4000 jcubic.pl bash
bash-5.0# nc -lp localhost 4000
nc: bad local port 'localhost'
bash-5.0# nc -lp 4000
xxxx

$ nc localhost 8080
xxxx
</code></pre>

<p>If I run netcat and access 8080 from browser I got <code>ERR_SOCKET_NOT_CONNECTED</code> but when I run jekyll I've got <code>ERR_CONNECTION_RESET</code>.</p>

<p>If I run:</p>

<pre><code>python -m SimpleHTTPServer 4000
</code></pre>

<p>I've got my website but it's broken because site url is wrong. How can I access Jekyll server inside docker?</p>
"
"53863887","Java heap profiling crashes with SIGABRT","<java><jvm><profiling><heap>","57954172","Why is `valgrind` reporting memory leak for `java -version`?","<java><docker><ubuntu><jvm><valgrind>","<p>I'm trying to profile native memory allocated by C-written methods and plugged to <code>JVM</code> through <code>JNI</code>. I installed </p>

<pre><code>$ valgrind --version
valgrind-3.13.0
</code></pre>

<p>And tried to run JVM with the following options:</p>

<pre><code>valgrind --tool=massif --massif-out-file=/tmp/massif-j.out java 
    -XX:+UnlockDiagnosticVMOptions //...
</code></pre>

<p>The thing is it crashes with core dump created</p>

<pre><code>  0x00000000080e4196: fxrstor64 (%rsp)
  0x00000000080e419b: add     $0x200,%rsp
  0x00000000080e41a2: mov     (%rsp),%r15
  0x00000000080e41a6: mov     0x8(%rsp),%r14
  0x00000000080e41ab: mov     0x10(%rsp),%r13
  0x00000000080e41b0: mov     0x18(%rsp),%r12
  0x00000000080e41b5: mov     0x20(%rsp),%r11
  0x00000000080e41ba: mov     0x28(%rsp),%r10
  0x00000000080e41bf: mov     0x30(%rsp),%r9
  0x00000000080e41c4: mov     0x38(%rsp),%r8
  0x00000000080e41c9: mov     0x40(%rsp),%rdi
  0x00000000080e41ce: mov     0x48(%rsp),%rsi
  0x00000000080e41d3: mov     0x50(%rsp),%rbp
  0x00000000080e41d8: mov     0x60(%rsp),%rbx
  0x00000000080e41dd: mov     0x68(%rsp),%rdx
  0x00000000080e41e2: mov     0x70(%rsp),%rcx
  0x00000000080e41e7: mov     0x78(%rsp),%rax
  0x00000000080e41ec: add     $0x80,%rsp
  0x00000000080e41f3: add     $0x8,%rsp
  0x00000000080e41f7: Fatal error: Disassembling failed with error code: 15#
# A fatal error has been detected by the Java Runtime Environment:
#
#  Internal Error (sharedRuntime.cpp:834), pid=12441, tid=0x0000000021385700
#  fatal error: exception happened outside interpreter, nmethods and vtable stubs at pc 0x00000000080e4147
#
# JRE version: Java(TM) SE Runtime Environment (8.0_181-b13) (build 1.8.0_181-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Core dump written. Default location: /var/log/prj/core or core.12441
#
# An error report file with more information is saved as:
# /var/log/prj/hs_err_pid12441.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
==12441==
==12441== Process terminating with default action of signal 6 (SIGABRT): dumping core
==12441==    at 0x54AAE97: raise (raise.c:51)
==12441==    by 0x54AC800: abort (abort.c:79)
==12441==    by 0x658B3C4: ??? (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x672F5B2: ??? (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x615EE98: ??? (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x662A099: ??? (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x6591A49: JVM_handle_linux_signal (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x6587652: ??? (in /usr/lib/jvm/java-oracle-8-amd64/jdk/jre/lib/amd64/server/libjvm.so)
==12441==    by 0x4E4588F: ??? (in /lib/x86_64-linux-gnu/libpthread-2.27.so)
==12441==    by 0x80E4146: ???
==12441==    by 0x107: ???
==12441==    by 0x84CBC43: ???
==12441==    by 0x10001BD37: ???
==12441==    by 0xFDC7103F: ???
==12441==    by 0xA3FFFFFFFF: ???
==12441==    by 0xFF9275A7: ???
==12441==
</code></pre>

<p>Can anyone give an idea of what could go wrong? Is that because of <code>Fatal error: Disassembling failed with error code: 15#</code>?</p>
","<p>I am on OS X using Docker for <code>ubuntu:16.04</code>.</p>

<pre><code>FROM ubuntu:16.04

RUN apt-get update
RUN apt-get upgrade -y
RUN apt-get install g++ valgrind -y
RUN apt install default-jdk -y
</code></pre>

<p>When I do this:</p>

<pre><code>docker build -t a .
docker run -ti a bash
</code></pre>

<ol>
<li>Why do I get illegal write of size 4?</li>
<li>Why do I get leaks?</li>
</ol>

<p>It is said that <code>valgrind</code> is not reliable on OS X, however I trust Docker to be a reliable wrapper.  I'd be shocked if this is not happening in real Ubuntu 16.04 machines.</p>
"
"53969865","Could not connect to postgres server in a docker from a dockerized app","<django><postgresql><docker>","56594476","Creating a dockerfile with python and postgresql","<python><postgresql><docker><docker-compose><dockerfile>","<p>I would like to run a dockerized Django app with a dockerized postgres.</p>

<p>I run the dockerized Django app by using: </p>

<pre><code>docker run --rm --env-file /path/to/variables -d -p 8000:8000 django_app:test
</code></pre>

<p>I run a dockerized postgres by using:</p>

<pre><code>docker run --rm -d --env-file /path/to/secrets/variables -p 5432:5432 \
    -v ""$PWD/my-postgres.conf"":/etc/postgresql/postgresql.conf \
    --mount src=/path/to/db/data,dst=/var/lib/postgresql/data,type=bind \
    postgres:alpine -c 'config_file=/etc/postgresql/postgresql.conf'
</code></pre>

<p>my postgres config is the default config that is suggested in the <a href=""https://hub.docker.com/_/postgres/"" rel=""nofollow noreferrer"">postgres docker container documentation</a>. It is essentially a config file that contains <code>listen_addresses = '*'</code></p>

<p>I use the <strong>same</strong> environment variables for both containers:</p>

<pre><code>DJANGO_SETTINGS_MODULE=settings.module
PROJECT_KEY=xxyyzzabcdefg
DB_ENGINE=django.db.backends.postgresql
POSTGRES_DB=db_name
POSTGRES_USER=db_user
POSTGRES_PASSWORD=verydifficultpassword
POSTGRES_HOST=localhost # I've also tried to use 0.0.0.0
POSTGRES_PORT=5432
</code></pre>

<p>My Django settings module for the database is:</p>

<pre><code> DATABASES = {
    'default': {
        'ENGINE': os.environ.get('DB_ENGINE'),
        'NAME': os.environ.get('POSTGRES_DB'),
        'USER': os.environ.get('POSTGRES_USER'),
        'PASSWORD': os.environ.get('POSTGRES_PASSWORD'),
        'HOST': os.environ.get('POSTGRES_HOST'),
        'PORT': os.environ.get('POSTGRES_PORT')
        }
  }
</code></pre>

<p>However, I keep on getting:</p>

<pre><code>django.db.utils.OperationalError: could not connect to server: Connection refused
    Is the server running on host ""0.0.0.0"" and accepting
    TCP/IP connections on port 5432?
</code></pre>

<p>The Dockerfiles for my django app looks like:</p>

<pre><code>FROM python:alpine3.7
COPY --from=installer /app /app
# required for postgres
COPY --from=installer /usr/lib /usr/lib
COPY --from=installer /usr/local/bin /usr/local/bin
COPY --from=installer /usr/local/lib /usr/local/lib
ARG SETTINGS_MODULE
WORKDIR /app
ENTRYPOINT python manage.py migrate &amp;&amp;\
           python manage.py test &amp;&amp;\
           python manage.py create_default_groups &amp;&amp;\
           python manage.py set_screen_permissions &amp;&amp;\
           python manage.py create_test_users &amp;&amp;\
           python manage.py init_core &amp;&amp;\
           python manage.py runserver 0.0.0.0:8000
</code></pre>

<p>Another interesting fact is that if I run the app locally <code>python manage.py runserver</code> and have the postgres container running, the app seems to be working.</p>

<p>Can anyone help me try to figure out why am I getting a connection refused? Thanks in advance!</p>
","<p>In my project, I have a flask application which connects to a postgresql DB, and I need to package it into a docker. I created the Dockerfile like this:</p>

<pre><code>FROM python:3.6

ENV PYTHONUNBUFFERED 1

ADD . /todo
WORKDIR /todo
RUN pip install -r requirements.txt
</code></pre>

<p>and the docker-compose.yml file like this:</p>

<pre><code>web:
  build: .
  command: python run.py
  ports:
    - ""5000:5000""
  volumes:
    - .:/todo
  links:
    - db

db:
  image: postgres
  environment:
    POSTGRES_USER: todo
    POSTGRES_PASSWORD: todo321
    POSTGRES_DB: todo
  ports:
      - 5432:5432
  restart: always
</code></pre>

<p>I have the configuration file, specifying the DB connection for the flask app.</p>

<pre><code>SQLALCHEMY_DATABASE_URI = ""postgresql://todo:todo321@db/todo""
</code></pre>

<p>then ran the <code>docker-compose build</code> without any error. But when try to run the container with the table creation with the migration script:</p>

<pre><code>docker-compose run web python migrate.py db migrate
</code></pre>

<p>It gives me erorrs:</p>

<pre><code>sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
    Is the server running on host ""localhost"" (127.0.0.1) and accepting
    TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
    Is the server running on host ""localhost"" (::1) and accepting
    TCP/IP connections on port 5432?
</code></pre>

<p>dependencies:</p>

<pre><code>flask == 1.0.3
flask-restful == 0.3.7
flask-sqlalchemy == 2.4.0
marshmallow == 2.19.2
flask-marshmallow == 0.10.1
flask-script == 2.0.6
flask-migrate == 2.5.2
psycopg2 == 2.8.2
passlib == 1.7.1
flask-jwt-extended == 3.18.2
marshmallow-sqlalchemy
</code></pre>

<p>What is the solution for this?</p>
"
"54504502","Which Visual Studio component contain the MSVC Hostx files?","<windows><visual-studio><visual-studio-2017><docker-for-windows><visual-studio-2017-build-tools>","56686688","How to install editbin.exe without installing an entire Visual Studio?","<visual-studio><docker><msbuild><dockerfile>","<p>My <code>.NET</code> solution contains an <code>editbin</code> command in <code>PostBuild</code> events.</p>

<p>We try to create a <em>Docker Container</em> for compiling our solution. To do that, we installed the <code>VS17</code> with the <a href=""https://docs.microsoft.com/en-us/visualstudio/install/build-tools-container?view=vs-2017#step-5-create-and-build-the-dockerfile"" rel=""nofollow noreferrer"">installer</a>.</p>

<p>We can't find which <a href=""https://docs.microsoft.com/en-us/visualstudio/install/workload-and-component-ids?view=vs-2017"" rel=""nofollow noreferrer"">component</a> we should select to get the <code>editbin</code>.</p>

<p>In my machine, the <code>editbin</code> exist in <code>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\bin\Host{x86/x64}\{x86/x64/arm64}</code> in 6 places- all the compinations of the placeholders.</p>
","<p>I am trying to move my current build server, which is based on Visual Studio, to a containerized environment. As such, I am trying to use the <code>mcr.microsoft.com/dotnet/framework/sdk:4.7.2-windowsservercore-ltsc2019</code> image available on docker hub.</p>

<p>One step required during the build is to remove the <code>NXCOMPAT</code> flag from the resulting executable (<a href=""http://jasper-net.blogspot.com/2011/05/nxcompat-and-c-compiler.html"" rel=""nofollow noreferrer"">more details available here for the why</a>). To do so, I need to use <code>editbin.exe</code> which ships with Visual Studio. The current image version does not include it. So I would like to add it myself as I already have to customize the image a bit to fit my needs.</p>

<p>Which package can I install to get <code>editbin.exe</code> working in my docker image?</p>

<p>I don't want to install a full Visual Studio as it will defeat the purpose of using the dotnet sdk image. I am not even sure I can as the container is not running in an interactive mode.</p>

<p>I originally reported my issue to the <code>microsoft/dotnet-framework-docker</code> <a href=""https://github.com/microsoft/dotnet-framework-docker/issues/300"" rel=""nofollow noreferrer"">repo here</a>.</p>
"
"55153944","PhpStorm + Xdebug: Connection established, no Debug window popup in PhpStorm","<vagrant><phpstorm><xdebug>","57309940","PhpStorm didn't display any information of Xdebug","<docker><phpstorm><xdebug>","<p>I'm struggling to get PhpStorm and Xdebug to play nicely on a specific Vagrant VM.</p>

<p><em>How can I effectively track down and resolve the issue in connecting PhpStorm and Xdebug for this VM?</em></p>

<p>While I can see that PhpStorm and Xdebug are communicating to a degree, the connection from Xdebug does not launch PhpStorm's debug window, and it's not possible to use PhpStorm's debugging capabilities.</p>

<p>The combination of tools in question is:</p>

<ul>
<li>Vagrant VM with Ubuntu 16-04</li>
<li>PHP 7.1 with php-xdebug 2.7</li>
<li>PhpStorm 2018.2.1</li>
<li>Firefox Xdebug Helper extension</li>
<li>Drupal 8.6.10</li>
<li>Breakpoint is inserted in <code>index.php</code> at line 16.</li>
</ul>

<p>I'm able to get this Vagrant VM to connect to Atom's <code>php-debug</code> Xdebug integration. I'm also able to get this PhpStorm to connect to other Vagrant instance's Xdebug!</p>

<p>Xdebug config from the server in question:</p>

<pre><code># xdebug.idekey = ""PHPSTORM""
# xdebug.remote_connect_back = on
xdebug.remote_autostart = off
# xdebug.remote_connect_back = on
xdebug.remote_enable = on
xdebug.remote_handler=dbgp
xdebug.remote_host = 10.0.2.2 # IP of the host system
xdebug.remote_log=""/tmp/xdebug.log""
xdebug.remote_mode=req
xdebug.remote_port=9000
</code></pre>

<p>I see output in the logfile <code>/tmp/xdebug.log</code> when using this configuration.</p>

<ul>
<li>With Xdebug Helper to ""Disable"", the page loads in the browser. No log entries are recorded to <code>/tmp/xdebug.log</code>.</li>
<li>With PhpStorm set to ""Stop listening for PHP Debug connections"", the page loads in the browser. No log entries are recorded to <code>/tmp/xdebug.log</code>.</li>
<li>With Xdebug Helper set to ""Debug"" and PhpStorm set to ""Start listening for PHP debug connections"", the page is paused as though the breakpoint is recognised. However, PhpStorm does not bring up a debug window, so it's not possible to debug. The following is logged to <code>/tmp/xdebug.log</code>:</li>
</ul>

<pre><code>[8264] Log opened at 2019-03-14 01:39:02
[8264] I: Connecting to configured address/port: 10.0.2.2:9000.
[8264] I: Connected to client. :-)
[8264] -&gt; &lt;init xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" fileuri=""file:///var/www/html/drupal/web/index.php"" language=""PHP"" xdebug:language_version=""7.1.26-1+ubuntu16.04.1+deb.sury.org+1"" protocol_version=""1.0"" appid=""8264"" idekey=""PHPSTORM""&gt;&lt;engine version=""2.7.0""&gt;&lt;![CDATA[Xdebug]]&gt;&lt;/engine&gt;&lt;author&gt;&lt;![CDATA[Derick Rethans]]&gt;&lt;/author&gt;&lt;url&gt;&lt;![CDATA[https://xdebug.org]]&gt;&lt;/url&gt;&lt;copyright&gt;&lt;![CDATA[Copyright (c) 2002-2019 by Derick Rethans]]&gt;&lt;/copyright&gt;&lt;/init&gt;
[8264]
[8264] &lt;- feature_set -i 1 -n show_hidden -v 1
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""feature_set"" transaction_id=""1"" feature=""show_hidden"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- feature_set -i 2 -n max_depth -v 1
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""feature_set"" transaction_id=""2"" feature=""max_depth"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- feature_set -i 3 -n max_children -v 100
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""feature_set"" transaction_id=""3"" feature=""max_children"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- feature_set -i 4 -n extended_properties -v 1
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""feature_set"" transaction_id=""4"" feature=""extended_properties"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- feature_set -i 5 -n notify_ok -v 1
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""feature_set"" transaction_id=""5"" feature=""notify_ok"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- stdout -i 6 -c 1
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""stdout"" transaction_id=""6"" success=""1""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- status -i 7
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""status"" transaction_id=""7"" status=""starting"" reason=""ok""&gt;&lt;/response&gt;
[8264]
[8264] &lt;- step_into -i 8
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""step_into"" transaction_id=""8"" status=""break"" reason=""ok""&gt;&lt;xdebug:message filename=""file:///var/www/html/drupal/web/index.php"" lineno=""14""&gt;&lt;/xdebug:message&gt;&lt;/response&gt;
[8264]
[8264] &lt;- eval -i 9 -- aXNzZXQoJF9TRVJWRVJbJ1BIUF9JREVfQ09ORklHJ10p
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""eval"" transaction_id=""9""&gt;&lt;property type=""bool""&gt;&lt;![CDATA[0]]&gt;&lt;/property&gt;&lt;/response&gt;
[8264]
[8264] &lt;- eval -i 10 -- aXNzZXQoJF9TRVJWRVJbJ1NFUlZFUl9OQU1FJ10p
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""eval"" transaction_id=""10""&gt;&lt;property type=""bool""&gt;&lt;![CDATA[1]]&gt;&lt;/property&gt;&lt;/response&gt;
[8264]
[8264] &lt;- eval -i 11 -- KHN0cmluZykoJF9TRVJWRVJbJ1NFUlZFUl9OQU1FJ10p
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""eval"" transaction_id=""11""&gt;&lt;property type=""string"" size=""9"" encoding=""base64""&gt;&lt;![CDATA[bG9jYWxob3N0]]&gt;&lt;/property&gt;&lt;/response&gt;
[8264]
[8264] &lt;- eval -i 12 -- KHN0cmluZykoJF9TRVJWRVJbJ1NFUlZFUl9QT1JUJ10p
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""eval"" transaction_id=""12""&gt;&lt;property type=""string"" size=""4"" encoding=""base64""&gt;&lt;![CDATA[ODAwMA==]]&gt;&lt;/property&gt;&lt;/response&gt;
[8264]
[8264] &lt;- eval -i 13 -- KHN0cmluZykoJF9TRVJWRVJbJ1JFUVVFU1RfVVJJJ10p
[8264] -&gt; &lt;response xmlns=""urn:debugger_protocol_v1"" xmlns:xdebug=""https://xdebug.org/dbgp/xdebug"" command=""eval"" transaction_id=""13""&gt;&lt;property type=""string"" size=""1"" encoding=""base64""&gt;&lt;![CDATA[Lw==]]&gt;&lt;/property&gt;&lt;/response&gt;
[8264]
</code></pre>

<p>If I then click ""Stop listening for PHP debug connections"", I immediately see the following logged to <code>/tmp/xdebug.log</code>:</p>

<pre><code>[8264] &lt;- run -i 14
</code></pre>

<p>Then that's quickly followed by <a href=""https://gist.github.com/xurizaemon/b7f2392f5e7118a2d222f4461eefc8d4"" rel=""nofollow noreferrer"">this logged output</a> (too large for this question), and the page and resources load as per before.</p>

<p><strong>How can I effectively track down and resolve the issue in connecting PhpStorm and Xdebug for this VM?</strong></p>

<p>PhpStorm > Languages > PHP:</p>

<p><a href=""https://i.stack.imgur.com/dDlKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dDlKi.png"" alt=""PhpStorm &gt; Languages &gt; PHP""></a></p>

<p>PhpStorm > Languages > PHP > Debug:
<a href=""https://i.stack.imgur.com/lmhbG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lmhbG.png"" alt=""PhpStorm &gt; Languages &gt; PHP &gt; Debug""></a></p>
","<p>I am new at Xdebug and trying to configure Xdebug with PhpStorm. The problem is, i can't get any information in phpstorm debug console even i hit a breakpoint. You can see the below picture phpstorm console when i hit a breakpoint </p>

<p><a href=""https://i.stack.imgur.com/ryeVv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ryeVv.png"" alt=""When i run debug phpstorm didn&#39;t display anything""></a></p>

<p><strong>Here is my configurations</strong></p>

<p>1.Xdebug.ini Configurations</p>

<pre><code>zend_extension=xdebug.so
xdebug.remote_autostart=0
xdebug.remote_enable=1
xdebug.remote_port=9000
xdebug.remote_connect_back=1
xdebug.remote_host=192.168.0.161
</code></pre>

<p>2.Xdebug Verification
<a href=""https://i.stack.imgur.com/oYwaq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oYwaq.png"" alt=""enter image description here""></a></p>

<p>3.1.PHPStorm Setting
<a href=""https://i.stack.imgur.com/XxMOn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XxMOn.png"" alt=""enter image description here""></a></p>

<p>3.2
<a href=""https://i.stack.imgur.com/ei1ft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ei1ft.png"" alt=""enter image description here""></a></p>

<p>3.3
<a href=""https://i.stack.imgur.com/AqISR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AqISR.png"" alt=""enter image description here""></a></p>
"
"55339396","Kubernetes - How to read logs that are written to files in pods instead of stdout/stderr?","<kubernetes>","56781829","How to make Kubernetes manage my pods log files?","<docker><kubernetes>","<p>I have a pod in a state of <code>CrashLoopBackOff</code>, the logs I'm seeing from <code>kubectl logs &lt;pod-name&gt; -p</code> present only a partial picutre. Other logs are found in other files (e.g. <code>/var/log/something/something.log</code>).</p>

<p>Since this pod is crashed, I can't <code>kubectl exec</code> into a shell there and look at the files.</p>

<p>How can I look at the log files produced by a container that is no longer running?</p>

<p>To be more specific, I'm looking for log files file under <code>$HOME/logs/es.log</code> (in the container that failed)</p>
","<p>Currently I have a Kubernetes pod that is running a cron job. The cron job writes to a log file at <code>/var/log/cronlog</code>. However, when I type <code>kubectl log my-pod-name</code>, the contents of <code>/var/log/cronlog</code> is not included.</p>

<p>How do I make it so that the contents of <code>/var/log/cronlog</code> is included with the output of <code>kubectl log my-pod-name</code>?</p>
"
"56333540","React: Module not found: Can't resolve 'react-html-parser'","<reactjs><docker><docker-compose>","56977774","could not find a module after add new package in docker","<node.js><docker>","<p>I am trying to install <code>react-html-parser</code> in my docker container 'client':</p>

<pre><code>docker-compose.yml
client/
      Dockerfile-dev
      node_modules/
      src/   
         components/
                   Seeds.jsx
</code></pre>

<p>and try and import it here:</p>

<p><strong>Seeds.jsx</strong></p>

<pre><code>import React, { Component } from 'react';
import ReactHtmlParser, { processNodes, convertNodeToElement, htmlparser2 }
from 'react-html-parser';
import axios from 'axios';
</code></pre>

<p><code>'axios'</code> and <code>'react'</code> are instaled allright, but not <code>'react-html-parser'</code>. Console logs me the error:</p>

<pre><code>index.js:1437 ./src/components/Seeds.jsx
Module not found: Can't resolve 'react-html-parser' in '/usr/src/app/src/components'
</code></pre>

<p><strong>package.json</strong></p>

<pre><code>{
  ""name"": ""client"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""dependencies"": {
    ""axios"": ""^0.18.0"",
    ""react"": ""^16.8.2"",
    ""react-dom"": ""^16.8.2"",
    ""react-router-dom"": ""^4.3.1"",
    ""react-scripts"": ""2.1.5"",
    ""react-html-parser"":""^2.0.2"", # &lt;---------NEW
    ""spotify-web-api-js"": ""^0.22.1""
  },
  ""scripts"": {
    ""start"": ""react-scripts start"",
    ""build"": ""react-scripts build"",
    ""test"": ""react-scripts test"",
    ""eject"": ""react-scripts eject""
  },
  ""eslintConfig"": {
    ""extends"": ""react-app""
  },
  ""browserslist"": [
    ""&gt;0.2%"",
    ""not dead"",
    ""not ie &lt;= 11"",
    ""not op_mini all""
  ],
  ""devDependencies"": {
    ""enzyme"": ""^3.8.0"",
    ""enzyme-adapter-react-16"": ""^1.7.1""
  }
}
</code></pre>

<p><strong>Dockerfile-dev</strong></p>

<pre><code># base image
FROM node:11.6.0-alpine

# set working directory
WORKDIR /usr/src/app

# add `/usr/src/app/node_modules/.bin` to $PATH
ENV PATH /usr/src/app/node_modules/.bin:$PATH

# install and cache app dependencies
COPY package.json /usr/src/app/package.json

RUN npm install --silent
RUN npm install react-scripts@2.1.2 -g --silent

# start app
CMD [""npm"", ""start""]
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>  client:
    build:
      context: ./services/client
      dockerfile: Dockerfile-dev
    volumes:
      - './services/client:/usr/src/app'
      - '/usr/src/app/node_modules'
    ports:
      - 3000:3000
    environment:
      - NODE_ENV=development
      - REACT_APP_WEB_SERVICE_URL=${REACT_APP_WEB_SERVICE_URL}
    depends_on:
      - web
</code></pre>

<p>I can'f find the module inside my <code>node_modules</code> folder...</p>

<p>what am I missing here?</p>

<hr>

<p><strong>EDIT</strong>: npm install without --silent:</p>

<pre><code>npm WARN deprecated fsevents@1.2.4: Way too old
npm WARN deprecated core-js@2.6.4: core-js@&lt;2.6.8 is no longer maintained. Please, upgrade to core-js@3 or at least to actual version of core-js@2.
npm WARN deprecated joi@11.4.0: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor.
npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit &lt;https://github.com/lukeed/kleur/releases/tag/v3.0.0\&gt; for migration path(s).
npm WARN deprecated flatten@1.0.2: I wrote this module a very long time ago; you should use something else.
npm WARN deprecated hoek@4.2.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated topo@2.0.2: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()

&gt; core-js@2.6.9 postinstall /usr/src/app/node_modules/core-js
&gt; node scripts/postinstall || echo ""ignore""

Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!

The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
&gt; https://opencollective.com/core-js 
&gt; https://www.patreon.com/zloirock 

Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)


&gt; core-js-pure@3.1.3 postinstall /usr/src/app/node_modules/core-js-pure
&gt; node scripts/postinstall || echo ""ignore""

Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!

The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
&gt; https://opencollective.com/core-js 
&gt; https://www.patreon.com/zloirock 

Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)

npm notice created a lockfile as package-lock.json. You should commit this file.
npm WARN ts-pnp@1.1.2 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/chokidar/node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
</code></pre>
","<p>Here is my docker file:</p>

<pre><code>FROM node:10.15.1-alpine
WORKDIR ""/test""
COPY ./package.json ./
RUN npm install
COPY . .
</code></pre>

<p>and here is my dockerCompose:</p>

<pre><code>version: ""3""
services:
  api:
    build: "".""
    command: npm run dev
    volumes:
      - . :/test/
      - /test/node_modules
</code></pre>

<p>When I add a new package and run docker-compose build, npm install called correctly but when I restart my container it gives me that package doesn't install. I'll fix it by docker-compose down -v but I couldn't understand why should I remove a volume after install a new package.</p>
"
"56333540","React: Module not found: Can't resolve 'react-html-parser'","<reactjs><docker><docker-compose>","57234592","Failed to compile. Module not found: Error: Can't resolve 'package-name' in '/path/to/file/using/package' vue/vue-loader/cache-loader","<docker><webpack><vuejs2><node-modules><yarnpkg>","<p>I am trying to install <code>react-html-parser</code> in my docker container 'client':</p>

<pre><code>docker-compose.yml
client/
      Dockerfile-dev
      node_modules/
      src/   
         components/
                   Seeds.jsx
</code></pre>

<p>and try and import it here:</p>

<p><strong>Seeds.jsx</strong></p>

<pre><code>import React, { Component } from 'react';
import ReactHtmlParser, { processNodes, convertNodeToElement, htmlparser2 }
from 'react-html-parser';
import axios from 'axios';
</code></pre>

<p><code>'axios'</code> and <code>'react'</code> are instaled allright, but not <code>'react-html-parser'</code>. Console logs me the error:</p>

<pre><code>index.js:1437 ./src/components/Seeds.jsx
Module not found: Can't resolve 'react-html-parser' in '/usr/src/app/src/components'
</code></pre>

<p><strong>package.json</strong></p>

<pre><code>{
  ""name"": ""client"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""dependencies"": {
    ""axios"": ""^0.18.0"",
    ""react"": ""^16.8.2"",
    ""react-dom"": ""^16.8.2"",
    ""react-router-dom"": ""^4.3.1"",
    ""react-scripts"": ""2.1.5"",
    ""react-html-parser"":""^2.0.2"", # &lt;---------NEW
    ""spotify-web-api-js"": ""^0.22.1""
  },
  ""scripts"": {
    ""start"": ""react-scripts start"",
    ""build"": ""react-scripts build"",
    ""test"": ""react-scripts test"",
    ""eject"": ""react-scripts eject""
  },
  ""eslintConfig"": {
    ""extends"": ""react-app""
  },
  ""browserslist"": [
    ""&gt;0.2%"",
    ""not dead"",
    ""not ie &lt;= 11"",
    ""not op_mini all""
  ],
  ""devDependencies"": {
    ""enzyme"": ""^3.8.0"",
    ""enzyme-adapter-react-16"": ""^1.7.1""
  }
}
</code></pre>

<p><strong>Dockerfile-dev</strong></p>

<pre><code># base image
FROM node:11.6.0-alpine

# set working directory
WORKDIR /usr/src/app

# add `/usr/src/app/node_modules/.bin` to $PATH
ENV PATH /usr/src/app/node_modules/.bin:$PATH

# install and cache app dependencies
COPY package.json /usr/src/app/package.json

RUN npm install --silent
RUN npm install react-scripts@2.1.2 -g --silent

# start app
CMD [""npm"", ""start""]
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>  client:
    build:
      context: ./services/client
      dockerfile: Dockerfile-dev
    volumes:
      - './services/client:/usr/src/app'
      - '/usr/src/app/node_modules'
    ports:
      - 3000:3000
    environment:
      - NODE_ENV=development
      - REACT_APP_WEB_SERVICE_URL=${REACT_APP_WEB_SERVICE_URL}
    depends_on:
      - web
</code></pre>

<p>I can'f find the module inside my <code>node_modules</code> folder...</p>

<p>what am I missing here?</p>

<hr>

<p><strong>EDIT</strong>: npm install without --silent:</p>

<pre><code>npm WARN deprecated fsevents@1.2.4: Way too old
npm WARN deprecated core-js@2.6.4: core-js@&lt;2.6.8 is no longer maintained. Please, upgrade to core-js@3 or at least to actual version of core-js@2.
npm WARN deprecated joi@11.4.0: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor.
npm WARN deprecated kleur@2.0.2: Please upgrade to kleur@3 or migrate to 'ansi-colors' if you prefer the old syntax. Visit &lt;https://github.com/lukeed/kleur/releases/tag/v3.0.0\&gt; for migration path(s).
npm WARN deprecated flatten@1.0.2: I wrote this module a very long time ago; you should use something else.
npm WARN deprecated hoek@4.2.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated topo@2.0.2: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()

&gt; core-js@2.6.9 postinstall /usr/src/app/node_modules/core-js
&gt; node scripts/postinstall || echo ""ignore""

Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!

The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
&gt; https://opencollective.com/core-js 
&gt; https://www.patreon.com/zloirock 

Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)


&gt; core-js-pure@3.1.3 postinstall /usr/src/app/node_modules/core-js-pure
&gt; node scripts/postinstall || echo ""ignore""

Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!

The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
&gt; https://opencollective.com/core-js 
&gt; https://www.patreon.com/zloirock 

Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)

npm notice created a lockfile as package-lock.json. You should commit this file.
npm WARN ts-pnp@1.1.2 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/chokidar/node_modules/fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
</code></pre>
","<p>I'm having an odd issue when trying to install a new node package with <code>yarn add</code> (built with webpack, running on a docker container). When I install the package and import it to a component I get the following error:</p>

<pre><code>Failed to compile.

./src/components/tests/OrderForm.vue?vue&amp;type=script&amp;lang=js&amp; (./node_modules/cache-loader/dist/cjs.js??ref--12-0!./node_modules/babel-loader/lib!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options!./src/components/tests/OrderForm.vue?vue&amp;type=script&amp;lang=js&amp;)
Module not found: Error: Can't resolve 'hello-bar' in '/opt/front-end/src/components/tests'
</code></pre>

<p>I've tried searching online for awhile but the closest thing I found was here (In particular the last two comments in that issue, where they have the same error output): <a href=""https://github.com/vuejs/vue-cli/issues/1419"" rel=""nofollow noreferrer"">https://github.com/vuejs/vue-cli/issues/1419</a></p>

<p>The odd thing is that if I also uninstall a pre-existing package, Vue still loads all the content for that package as if I had not uninstalled it at all! I don't know if this is a setting with maybe <code>vue-loader</code> or <code>cache-loader</code> that I need to change?</p>

<p>My webpack config has:</p>

<pre><code>module.exports = {
    entry: [
        '@babel/polyfill',
        './src/main.js',
    ],
    optimization: {
        splitChunks: {
            chunks: 'all',
        },
    },
    devtool: process.env.NODE_ENV === 'production' ? '#source-map' : '#eval-source-map',
};
</code></pre>

<p>Update: </p>

<p>This is my <code>docker-compose.yml</code> file:</p>

<pre><code>version: '3.1'
services:
  front-end:
    build: ../etalon-front-end
    env_file: .frontend.env
    volumes:
      - ../etalon-front-end:/opt/front-end
      - /opt/front-end/node_modules
    ports:
      - ""8080:8080""
  mysql:
    build: ./mariadb
    env_file: .mysql.env
    volumes:
      - ./mariadb/backup.sql:/docker-entrypoint-initdb.d/dump.sql
      - /var/lib/mysql
    ports:
      - ""3306:3306""
  api:
    build: ../etalon-node
    env_file: .backend.env
    volumes:
      - ../etalon-node:/opt/node
      - /opt/node/node_modules
    ports:
      - ""3000:3000""
    depends_on:
      - mysql
</code></pre>

<p>And I run it with <code>docker-compose up</code></p>
"
"56555787","can't open jupyter on google cloud platform with gpu and docker","<docker><google-cloud-platform><jupyter-notebook>","56815940","How to set docker port after Published ports are discarded?","<docker><tensorflow><nvidia-docker>","<p>I wanted to use following command to open jupyter:</p>

<pre><code>docker run --runtime=nvidia --name tensorflow1 -it -p 8888:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter
</code></pre>

<p>I can't open it with browser. The systerm looks good because it says:</p>

<pre><code>To access the notebook, open this file in a browser:
    file:///root/.local/share/jupyter/runtime/nbserver-8-open.html
Or copy and paste one of these URLs:
    http://(568ebbf84a86 or 127.0.0.1):8888/?token=17fc57d57c89f56c460748f464b488c59f8ddccf5793e7
</code></pre>

<p>But when I open it with external ip address, I can't connect and system says:</p>

<pre><code>[W 06:15:52.336 NotebookApp] 404 GET http://110.249.212.46/testget?q=23333&amp;port=8888 (110.249.212.46) 38.11ms referer=None
</code></pre>

<p>I have built external ip address and built firewall.</p>

<p>There is no problem if I use the following command:</p>

<pre><code>docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu 
</code></pre>

<p>and following test has pasted:</p>

<pre><code>python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
</code></pre>

<p>how to solve this problem?</p>
","<p>I want to use the following method to install tensorflow:</p>

<pre><code>docker run --runtime=nvidia --name tensorflow3 -it -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter
</code></pre>

<p>but I get:</p>

<blockquote>
  <p>WARNING: Published ports are discarded when using host network mode</p>
</blockquote>

<p>How to set port?</p>
"
"56555787","can't open jupyter on google cloud platform with gpu and docker","<docker><google-cloud-platform><jupyter-notebook>","56822649","can I deploy docker on google cloud VM instance without host network","<docker><tensorflow><google-cloud-platform>","<p>I wanted to use following command to open jupyter:</p>

<pre><code>docker run --runtime=nvidia --name tensorflow1 -it -p 8888:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter
</code></pre>

<p>I can't open it with browser. The systerm looks good because it says:</p>

<pre><code>To access the notebook, open this file in a browser:
    file:///root/.local/share/jupyter/runtime/nbserver-8-open.html
Or copy and paste one of these URLs:
    http://(568ebbf84a86 or 127.0.0.1):8888/?token=17fc57d57c89f56c460748f464b488c59f8ddccf5793e7
</code></pre>

<p>But when I open it with external ip address, I can't connect and system says:</p>

<pre><code>[W 06:15:52.336 NotebookApp] 404 GET http://110.249.212.46/testget?q=23333&amp;port=8888 (110.249.212.46) 38.11ms referer=None
</code></pre>

<p>I have built external ip address and built firewall.</p>

<p>There is no problem if I use the following command:</p>

<pre><code>docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu 
</code></pre>

<p>and following test has pasted:</p>

<pre><code>python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
</code></pre>

<p>how to solve this problem?</p>
","<p>I want to use following method to open jupyter with brower:</p>

<pre><code>docker run  --runtime=nvidia --name tensorflow1 -it  -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3-jupyter
</code></pre>

<p>but it did't work.</p>

<p>while I can open website with following method:</p>

<pre><code>docker run --rm -d --network host --name my_nginx nginx
</code></pre>

<p>my question is can I use docker without host network and how?</p>
"
"56555787","can't open jupyter on google cloud platform with gpu and docker","<docker><google-cloud-platform><jupyter-notebook>","56823408","how to use docker bridge network on google cloud platform?","<docker><google-cloud-platform>","<p>I wanted to use following command to open jupyter:</p>

<pre><code>docker run --runtime=nvidia --name tensorflow1 -it -p 8888:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter
</code></pre>

<p>I can't open it with browser. The systerm looks good because it says:</p>

<pre><code>To access the notebook, open this file in a browser:
    file:///root/.local/share/jupyter/runtime/nbserver-8-open.html
Or copy and paste one of these URLs:
    http://(568ebbf84a86 or 127.0.0.1):8888/?token=17fc57d57c89f56c460748f464b488c59f8ddccf5793e7
</code></pre>

<p>But when I open it with external ip address, I can't connect and system says:</p>

<pre><code>[W 06:15:52.336 NotebookApp] 404 GET http://110.249.212.46/testget?q=23333&amp;port=8888 (110.249.212.46) 38.11ms referer=None
</code></pre>

<p>I have built external ip address and built firewall.</p>

<p>There is no problem if I use the following command:</p>

<pre><code>docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu 
</code></pre>

<p>and following test has pasted:</p>

<pre><code>python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
</code></pre>

<p>how to solve this problem?</p>
","<p>I can't use the following ways to open website with hostip:8000:</p>

<pre><code>docker network create -d bridge mybridge 

docker run -d --net mybridge --name db redis 
docker run -d --net mybridge -e DB=db -p 8000:5000 --name web chrch/web
</code></pre>

<p>I have build gpu VM instance without docker images and install docker by myself</p>

<p>how to solve this problem?</p>
"
"56590443","Install modules with perlbrew and cpm - perlbrew switch does not change @INC during docker build","<perl><docker><perlbrew><cpanm>","57657526","Perlbrew switch fails on docker build","<perl><docker><perlbrew>","<p>I'd like to use fast <a href=""https://github.com/skaji/cpm"" rel=""nofollow noreferrer"">cpm</a> module installer instead of cpanm in my projects.</p>

<p>Also I install target perl version using <a href=""https://perlbrew.pl/"" rel=""nofollow noreferrer"">perlbrew</a>.</p>

<p>According <a href=""https://metacpan.org/pod/App::cpm::Tutorial#First-step"" rel=""nofollow noreferrer"">documentation</a> of cpm <code>-g</code> option will install modules into current @INC</p>

<p>How to force perlbrew change @INC in Dockerfile ?</p>

<p>Below is part of my Dockerfile</p>

<pre><code>RUN perl -le 'print for @INC' &amp;&amp; \
    perlbrew switch perl-5.31.0 &amp;&amp; \
    perl -le 'print for @INC' &amp;&amp; \
    cpm install -gv CGI &amp;&amp; \
    perlbrew list-modules
</code></pre>

<p>When I build Dockerfile output of <code>perl -le 'print for @INC'</code> is same both times:</p>

<pre><code>/etc/perl
/usr/local/lib/x86_64-linux-gnu/perl/5.22.1
/usr/local/share/perl/5.22.1
/usr/lib/x86_64-linux-gnu/perl5/5.22
/usr/share/perl5
/usr/lib/x86_64-linux-gnu/perl/5.22
/usr/share/perl/5.22
/usr/local/lib/site_perl
/usr/lib/x86_64-linux-gnu/perl-base
</code></pre>

<p>But if I do same manually result is ok:</p>

<pre><code>$ docker run -it pavelsr/xxxhub
root@1a34ea34a3fb:/# perl -le 'print for @INC'
/etc/perl
/usr/local/lib/x86_64-linux-gnu/perl/5.22.1
/usr/local/share/perl/5.22.1
/usr/lib/x86_64-linux-gnu/perl5/5.22
/usr/share/perl5
/usr/lib/x86_64-linux-gnu/perl/5.22
/usr/share/perl/5.22
/usr/local/lib/site_perl
/usr/lib/x86_64-linux-gnu/perl-base
.
root@1a34ea34a3fb:/# perlbrew switch perl-5.31.0

A sub-shell is launched with perl-5.31.0 as the activated perl. Run 'exit' to finish it.

root@1a34ea34a3fb:/# perl -le 'print for @INC'
/usr/local/perlbrew/perls/perl-5.31.0/lib/site_perl/5.31.0/x86_64-linux
/usr/local/perlbrew/perls/perl-5.31.0/lib/site_perl/5.31.0
/usr/local/perlbrew/perls/perl-5.31.0/lib/5.31.0/x86_64-linux
/usr/local/perlbrew/perls/perl-5.31.0/lib/5.31.0
root@1a34ea34a3fb:/# cpm install -g CGI
DONE install HTML-Tagset-3.20
DONE install HTML-Parser-3.72
DONE install CGI-4.44
3 distributions installed.
root@1a34ea34a3fb:/# perlbrew list-modules
CGI
HTML::Parser
HTML::Tagset
Perl
</code></pre>
","<p>I want to perlbrew on docker building phase. I successfully install a perl but can't <code>perlbrew switch</code> to it so <code>cpanm</code> installs the module into wrong place.</p>

<p>The Dockerfile is as follows.</p>

<pre><code>FROM centos:centos7
SHELL [""/bin/bash"", ""-c""]
RUN yum -y update
RUN yum -y groupinstall ""Development Tools""
RUN yum -y install iproute cpan gcc perl-ExtUtils-Manifest
RUN curl -L https://install.perlbrew.pl | SHELL=/bin/bash bash
RUN echo ""source /root/perl5/perlbrew/etc/bashrc"" &gt;&gt; /root/.bashrc
RUN \source ~/perl5/perlbrew/etc/bashrc
RUN /root/perl5/perlbrew/bin/perlbrew install --notest --noman perl-5.30.0
RUN /root/perl5/perlbrew/bin/perlbrew install-cpanm
RUN /root/perl5/perlbrew/bin/perlbrew switch perl-5.30.0
RUN /root/perl5/perlbrew/bin/cpanm File::Find::Rule

CMD /sbin/init
</code></pre>

<p>It fails on <code>perlbrew switch</code> with errors.</p>

<pre><code>Step 6/13 : RUN curl -L https://install.perlbrew.pl | SHELL=/bin/bash bash
 ---&gt; Running in 2d003bbacf0e
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   170  100   170    0     0    189      0 --:--:-- --:--:-- --:--:--   189
100  1548  100  1548    0     0   1261      0  0:00:01  0:00:01 --:--:--     0

## Download the latest perlbrew

## Installing perlbrew
Using Perl &lt;/usr/bin/perl&gt;
perlbrew is installed: ~/perl5/perlbrew/bin/perlbrew

perlbrew root (~/perl5/perlbrew) is initialized.

Append the following piece of code to the end of your ~/.bash_profile and start a
new shell, perlbrew should be up and fully functional from there:

    source ~/perl5/perlbrew/etc/bashrc

Simply run `perlbrew` for usage details.

Happy brewing!

## Installing patchperl

## Done.
Removing intermediate container 2d003bbacf0e
 ---&gt; cf3fe0430858
Step 7/13 : RUN echo ""source /root/perl5/perlbrew/etc/bashrc"" &gt;&gt; /root/.bashrc
 ---&gt; Running in 835a04a14f9f
Removing intermediate container 835a04a14f9f
 ---&gt; d06c29928706
Step 8/13 : RUN \source ~/perl5/perlbrew/etc/bashrc
 ---&gt; Running in e655fe87bc0c
Removing intermediate container e655fe87bc0c
 ---&gt; fbc2aa1dcbb7
Step 9/13 : RUN /root/perl5/perlbrew/bin/perlbrew install --notest --noman perl-5.30.0
 ---&gt; Running in 1bb4851a6e5d
Fetching perl 5.30.0 as /root/perl5/perlbrew/dists/perl-5.30.0.tar.gz
Download http://www.cpan.org/src/5.0/perl-5.30.0.tar.gz to /root/perl5/perlbrew/dists/perl-5.30.0.tar.gz
Installing /root/perl5/perlbrew/build/perl-5.30.0/perl-5.30.0 into ~/perl5/perlbrew/perls/perl-5.30.0

This could take a while. You can run the following command on another shell to track the status:

  tail -f ~/perl5/perlbrew/build.perl-5.30.0.log

perl-5.30.0 is successfully installed.
Removing intermediate container 1bb4851a6e5d
 ---&gt; 5c2f139f28c4
Step 10/13 : RUN /root/perl5/perlbrew/bin/perlbrew install-cpanm
 ---&gt; Running in c8650ea79674

cpanm is installed to

    /root/perl5/perlbrew/bin/cpanm

Removing intermediate container c8650ea79674
 ---&gt; c3e92218cf8f
Step 11/13 : RUN /root/perl5/perlbrew/bin/perlbrew switch perl-5.30.0
 ---&gt; Running in 551369fc6a61
Use of uninitialized value $shell in pattern match (m//) at /root/perl5/perlbrew/bin/perlbrew line 2445.
Use of uninitialized value $shell in concatenation (.) or string at /root/perl5/perlbrew/bin/perlbrew line 2485.

A sub-shell is launched with perl-5.30.0 as the activated perl. Run 'exit' to finish it.

HOSTNAME=551369fc6a61
SHELL=
PATH=/root/perl5/perlbrew/bin:/root/perl5/perlbrew/perls/perl-5.30.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
_=/usr/bin/env
PWD=/
HOME=/root
SHLVL=2
PERLBREW_PERL=perl-5.30.0
PERLBREW_VERSION=0.86
MANPATH=/root/perl5/perlbrew/perls/perl-5.30.0/man:
PERL_LOCAL_LIB_ROOT=
PERL5LIB=
PERLBREW_MANPATH=/root/perl5/perlbrew/perls/perl-5.30.0/man
PERLBREW_LIB=
PERLBREW_SKIP_INIT=1
PERLBREW_PATH=/root/perl5/perlbrew/bin:/root/perl5/perlbrew/perls/perl-5.30.0/bin
PERLBREW_ROOT=/root/perl5/perlbrew
Removing intermediate container 551369fc6a61
 ---&gt; b76db56bd82f
Step 12/13 : RUN /root/perl5/perlbrew/bin/cpanm File::Find::Rule
 ---&gt; Running in 5574895c6069
--&gt; Working on File::Find::Rule
Fetching http://www.cpan.org/authors/id/R/RC/RCLAMP/File-Find-Rule-0.34.tar.gz ... OK
Configuring File-Find-Rule-0.34 ... OK
==&gt; Found dependencies: Test::More, Text::Glob, Number::Compare
--&gt; Working on Test::More
Fetching http://www.cpan.org/authors/id/E/EX/EXODIST/Test-Simple-1.302167.tar.gz ... OK
Configuring Test-Simple-1.302167 ... OK
Building and testing Test-Simple-1.302167 ... OK
Successfully installed Test-Simple-1.302167
--&gt; Working on Text::Glob
Fetching http://www.cpan.org/authors/id/R/RC/RCLAMP/Text-Glob-0.11.tar.gz ... OK
Configuring Text-Glob-0.11 ... OK
Building and testing Text-Glob-0.11 ... OK
Successfully installed Text-Glob-0.11
--&gt; Working on Number::Compare
Fetching http://www.cpan.org/authors/id/R/RC/RCLAMP/Number-Compare-0.03.tar.gz ... OK
Configuring Number-Compare-0.03 ... OK
Building and testing Number-Compare-0.03 ... OK
Successfully installed Number-Compare-0.03
Building and testing File-Find-Rule-0.34 ... OK
Successfully installed File-Find-Rule-0.34
4 distributions installed
Removing intermediate container 5574895c6069
 ---&gt; b7cbd8213dcc
Step 13/13 : CMD /sbin/init
 ---&gt; Running in 3180df4e4deb
Removing intermediate container 3180df4e4deb
 ---&gt; b6ebc0ca82a5
Successfully built b6ebc0ca82a5
</code></pre>

<p>I also tried to switch/use perl-5.28.2 on centos7/ubuntu and results the same.
It fails only on building stage and I can <code>perlbrew switch</code> successfully by attaching to the container after building.</p>

<p>The direct cause of this seems to be the lack of <code>shell</code> environment variable in the sub shell forked in perlbrew swich/use.</p>

<pre><code>Use of uninitialized value $shell in pattern match (m//) at /root/perl5/perlbrew/bin/perlbrew line 2445.
</code></pre>

<p>How can I solve the problem?</p>

<p>My environment is</p>

<pre><code>App::Perlbrew 0.86
Docker version 19.03.1, build 74b1e89
</code></pre>
"
"56880821","Docker exec on all running containers","<docker>","57372761","How do i run a single docker exec command against multiple containers","<docker><docker-exec>","<p>I am running several docker containers running on my server, and need to exec a git pull for a repository that is on all of them.</p>

<p>I have tried using this:</p>

<pre><code>docker exec $(docker ps -q) bash -c ""cd /var/www/html &amp;&amp; git pull""
</code></pre>

<p>but it errors out with this:</p>

<pre><code>OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused ""exec: \""606a1083d0be\"": executable file not found in $PATH"": unknown
</code></pre>

<p>It worked at one point, but then suddenly stopped working for no apparent reason (I didn't change any docker configuration)</p>

<p>Note: the output of <code>docker ps -q</code> is only container ids:</p>

<pre><code>511c76a25dcc
995bd453c467
</code></pre>
","<p>I am currently running a mac mini hosting at anytime 2 to 6 docker containers built off the same agent. The container function as build agents but every now and then builds fail and do not clean up correctly. This leads to files being left over that take up space. </p>

<p>I want to setup and cron job, that runs nightly, a docker exec command against every container and deletes my build directory.</p>

<p>i've tried the simple: 
<code>docker exec -it  `docker ps -q` ls /root/build-dir</code></p>

<p>but this fails with:
<code>OCI runtime exec failed: exec failed: container_linux.go:344: starting container process caused ""exec: \""ee89958ce4bc\"": executable file not found in $PATH"": unknown</code></p>

<p>However this works:
<code>docker exec -it ee89958ce4bc ls /root/build-dir</code></p>

<p>Is there a way to use docker exec against multiple container without writing a complicated script that will loop through container ID's from docker ps?</p>

<p>NOTE: I do not want to change the docker container and add the cron job there. I want this to run on the host machine</p>
"
"56965489","Having trouble communicating between docker-compose services","<docker><docker-compose><docker-networking>","56986176","Container internal communication","<reactjs><spring-boot><docker><docker-compose>","<p>I have the following <code>docker-compose</code> file:</p>

<pre><code>version: ""3""

services:
  scraper-api:
    build: ./ATPScraper
    volumes:
      - ./ATPScraper:/usr/src/app
    ports:
      - ""5000:80""
  test-app:
    build: ./test-app
    volumes:
      - ""./test-app:/app""
      - ""/app/node_modules""
    ports:
      - ""3001:3000""
    environment:
      - NODE_ENV=development
    depends_on:
      - scraper-api
</code></pre>

<p>Which build the following <code>Dockerfile</code>'s:</p>

<p><code>scraper-api</code> (a python <code>flask</code> application):</p>

<pre><code>FROM python:3.7.3-alpine

WORKDIR /usr/src/app

COPY requirements.txt ./

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [""python"", ""./app.py""]
</code></pre>

<p><code>test-app</code> (a test <code>react</code> application for the api):</p>

<pre><code># base image
FROM node:12.2.0-alpine

# set working directory
WORKDIR /app

# add `/app/node_modules/.bin` to $PATH
ENV PATH /app/node_modules/.bin:/app/src/node_modules/.bin:$PATH

# install and cache app dependencies
COPY package.json /app/package.json
RUN npm install --silent
RUN npm install react-scripts@3.0.1 -g --silent
RUN npm install axios -g

# start app
CMD [""npm"", ""start""]
</code></pre>

<p>Admittedly, I'm a newbie when it comes to Docker networking, but I am trying to get the <code>react</code> app to communicate with the <code>scraper-api</code>. For example, the <code>scraper-api</code> has the following endpoint: <code>/api/top_10</code>. I have tried various permutations of the following url:
<code>http://scraper-api:80/api/test_api</code>. None of them have been working for me.</p>

<p>I've been scavenging the internet and I can't really find a solution.</p>
","<p>I'm trying to run a full-stack application using the docker-compose file. However, back-end(SpringBoot) and front-end(ReactJs) containers can not communicate with each other using the service name.</p>

<p>I know that containers can communicate in user-defined bridge networks.   my back-end connects to the database using db:5432 address. 
  However, when I try to make a get request from frontend to the back-end:
example: <a href=""http://back-end:5055/getLogs?page=0"" rel=""nofollow noreferrer"">http://back-end:5055/getLogs?page=0</a></p>

<p>It is not working -> net::ERR_NAME_NOT_RESOLVED</p>

<p>So, Frontend couldn't make any requests to the backend.</p>

<p>If I expose ports from back-end and use <a href=""http://localhost:5055/getLogs?page=0"" rel=""nofollow noreferrer"">http://localhost:5055/getLogs?page=0</a> everything works fine.
 However, I want to expose only front-end port to the user and the other stuff (back-end/DB) I want it to be isolated.</p>

<p>How to set up communication between these two???</p>

<p>docker-compose file </p>

<pre><code>services:
  db:
    ... env vars
    networks:
      - my-net
  back-end:
    image: back/preproduction:0.0.1
    networks:
      - my-net
  front-end:
    image: front/preproduction:0.0.1
    ports:
      - ""80:3055""
    networks:
      - my-net

... volumes

networks:
  my-net:
    driver: bridge 
</code></pre>
"
"56992186","How to access CSV file (located in pc hdd) from a docker container with python pandas?","<python><pandas><docker><docker-image><seal>","56968776","How to add a CSV file in PySEAL docker container","<python><docker><dockerfile><docker-image><seal>","<p>I want to implement a Machine Learning algorithm which can operate on homomorphic data using <a href=""https://github.com/Lab41/PySEAL"" rel=""nofollow noreferrer"">PySEAL</a> library. PySEAL library is released as a docker container with an 'examples.py' file which shows some homomorphic encryption example. I want to edit the 'examples.py' file to implement the ML algorithm. I trying to import a CSV file in this way -</p>

<pre><code>dataset = pd.read_csv ('Dataset.csv')
</code></pre>

<p>I have imported pandas library successfully. I have tried many approaches to import the CSV file but failed. How can I import it?</p>

<p>I am new to Docker. Detailed procedure will be really helpful.</p>
","<p>I am doing a Machine Learning project using <a href=""https://github.com/Lab41/PySEAL"" rel=""nofollow noreferrer"">PySEAL</a> library. I have been able to run the examples included in it but now I want to edit the /SEALPythonExamples/examples.py file so that it can import data from a csv file using pandas library. I have been able to import pandas library. But I can't figure out how to add my CSV dataset file in the container. I have searched online but could not get a direct solution to this.</p>
"
"57349761","Why am I getting a line Feed Error when loading data via Powershell into clickhouse in Docker on Windows?","<r><powershell><docker><clickhouse>","57349021","How to access Docker Container with Clickhouse in Windows for loading data?","<powershell><docker><clickhouse>","<p>I am attempting to load data into clickhouse in a docker container built in windows docker desktop. I have my mock data prepared in R, written as a csv and my table created in clickhouse (i'm ommiting the connections):</p>

<pre><code>library(dplyr)
library(data.table)
library(clickhouse)
setwd(""C:/Users/xxxx/Documents/testing_load"")
my_df = data.table(datetime = as.character(c(""2018-01-01 11:21:00"", ""2019-01-01 11:45:00"")))
c(2018, 2019) %&gt;%
  lapply(function(y) {
    print(y)
    fwrite(my_df[substr(datetime,1,4) == y],
              paste(""test_"",y,"".csv""),
              row.names = F,
              col.names = F
           )
  })


dbSendQuery(con,
            paste(
              ""CREATE TABLE test(
              datetime DateTime

              ) ENGINE = Log;""
            )
)
</code></pre>

<p>The data I am trying to load is huge, so this is just a sample to show the setup and why I am getting an error on the first line. I want to load the data using clickhouse client in powershell accessing the docker container as such:</p>

<pre><code>#loop through files and load
$files = Get-ChildItem ""C:\Users\xxxx\Documents\testing_load""

foreach ($f in $files){
    $outfile = $f.FullName | Write-Host
    Import-Csv –Delimiter "","" $f.FullName | Write-Host
    Get-Date | Write-Host    
    ""Start loading"" + $f.FullName | Write-Host
    docker run -it --rm --link chanalytics:clickhouse-server yandex/clickhouse-client --host clickhouse-server clickhouse-client --query=""INSERT INTO test FORMAT CSV""
     Get-Date | Write-Host 
    ""End loading"" + $f.FullName | Write-Host
}
</code></pre>

<p>I added reading the data as well in case but I'm getting an error from clickhouse regarding the datetime time. I have tried switching to positxc in R and it makes no difference. I can't tell but feel that this is a very simple thing I am not understanding. Below is the error I get:</p>

<p><a href=""https://i.stack.imgur.com/BjlA3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BjlA3.png"" alt=""enter image description here""></a></p>

<p>Opening excel I can see that the datetime is fine in date format. Any suggestions on what might be going on?</p>
","<p>I am interested in loading data into a docker container with clickhouse currently running in windows docker desktop. I have built my tables and prepped the data in R in .csv format but now want to load the data using the system instead via R or Python. In Ubuntu I have done this before without docker using code from this blog to load into my clickhouse database:</p>

<p><a href=""https://tech.marksblogg.com/billion-nyc-taxi-clickhouse.html"" rel=""nofollow noreferrer"">https://tech.marksblogg.com/billion-nyc-taxi-clickhouse.html</a></p>

<pre><code> time (for filename in /home/mark/trips/trips_x*.csv.gz; do
            gunzip -c $filename | \
                python trans.py | \
                clickhouse-client \
                    --query=""INSERT INTO trips FORMAT CSV""
        done)
</code></pre>

<p>How would I go about implementing a similar procedure with windows bash or powershell while accessing the docker container? Now I am in windows so I have clickhouse dockerized and running there.</p>

<p>EDIT</p>

<p>my files are in csv format so here is my first attempt:</p>

<pre><code>#loop through files
$files = Get-ChildItem ""C:\Users\xxxxx\Documents\testing_load""

foreach ($f in $files){
    $outfile = $f.FullName | Write-Host `
    docker run -it --rm --link chanalytics:clickhouse-server yandex/clickhouse-client --host clickhouse-server `
    INSERT INTO trips FORMAT CSV

}
</code></pre>

<p>trying to invoke the insert command on each csv file in the directory </p>
"
"57683975","docker running but can't access on the server","<node.js><docker><jenkins><docker-compose>","57692740","docker run on container but not run on server","<node.js><docker><ubuntu><jenkins><docker-compose>","<p>I have stages for Jenkins jobs to test and deploy my nodejs with docker, I run docker on port 3000 but when I tried to browser my <code>ip-server:3000</code>, it doesn't work and nothing 
my dockers are running </p>

<p>here is my Jenkinsfile </p>

<pre><code>node {
  try {
    stage('Checkout') {
      checkout scm
    }
    stage('Environment') {
      sh 'git --version'
      echo ""Branch: ${env.BRANCH_NAME}""
      sh 'docker -v'
      sh 'printenv'
    }
    stage('Build Docker test'){
     sh 'docker build -t crud-test -f Dockerfile.test --no-cache .'
    }
    stage('Docker test'){
      sh 'docker run --rm crud-test'
    }
    stage('Clean Docker test'){
      sh 'docker rmi crud-test'
    }
    stage('Deploy'){
      if(env.BRANCH_NAME == 'master'){
        sh 'docker build -t crud --no-cache .'
        sh 'docker run -d -p 3000:3000 -e DB_USERNAME=myusername -e DB_PASSWORD=12345678 -e DB_NAME=employee crud'
      }
    }
  }
  catch (err) {
    throw err
  }
}
</code></pre>

<p>Dockerfile: </p>

<pre><code># Extending image
FROM node:carbon

RUN apt-get update
RUN apt-get upgrade -y
RUN apt-get -y install autoconf automake libtool nasm make pkg-config git apt-utils

# Create app directory
RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

# Versions
RUN npm -v
RUN node -v

COPY ./server/ /usr/src/app

RUN npm install

# Port to listener
EXPOSE 3000

# Environment variables
ENV PORT 4000
ENV DB_USERNAME myusername
ENV DB_PASSWORD 12345678
ENV DB_NAME employee

# Main command
CMD [ ""npm"", ""run"", ""dev"" ]
</code></pre>

<p>I run Jenkins via docker-compose on my ubuntu server,
is that something I missing or wrong ?? 
because my goal here using Jenkins to test my server and after the test successfully it gonna deploy my Nodejs on my ubuntu server 
so after building finish on Jenkins, I could browser my server API on the browser to make sure it works on <code>ip-sever:3000</code> </p>

<p>but on the above pipeline, is that correct that job do every time I push to master, my server API will be updated without click <code>build now</code> on Jenkins? if not, how to configure it?</p>

<p>I am also don't know how to hide my ENV, so I show up on my stage Jenkinsfile because on multibranches pipeline does not have any option for env parameters</p>
","<p>i create Jenkinsfile, Dockerfile, Dockerfile.test to CI and CD my server API on GitHub, i build it on Jenkins and the build was successfully, and my docker run on the container as well,
on Jenkinsfile stages, i create for test and deploy on server API,
and using docker for the container</p>
<p>i also run Jenkins on docker also,
using docker-compose</p>
<p>here is my Dockerfile on my ubuntu server</p>
<pre><code>FROM jenkins/jenkins:lts

USER root
</code></pre>
<p>and here is my docker-compose on ubuntu server</p>
<pre><code>version: '3'

services:

  jenkins:
    build: .
    container_name: jenkins
    privileged: true
    restart: always
    ports:
      - 8080:8080
    volumes:
      - ./jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker

  registry:
    image: registry
    container_name: registry
    restart: always
    ports:
      - 5000:5000
</code></pre>
<p>what i did above , i follow <a href=""https://hackernoon.com/continuous-delivery-of-react-app-with-jenkins-and-docker-8a1ae1511b86"" rel=""nofollow noreferrer"">this intruction</a>
then i tried ro run it and login on my jenkins server,</p>
<p>my jenkinsfile something like this</p>
<pre><code>  try {
    stage('Checkout') {
      checkout scm
    }
    stage('Environment') {
      sh 'git --version'
      echo &quot;Branch: ${env.BRANCH_NAME}&quot;
      sh 'docker -v'
      sh 'printenv'
    }
    stage('Build Docker test'){
     sh 'docker build -t employee-test -f Dockerfile.test --no-cache .'
    }
    stage('Docker test'){
      sh 'docker run --rm employee-test'
    }
    stage('Clean Docker test'){
      sh 'docker rmi employee-test'
    }
    stage('Deploy'){
      if(env.BRANCH_NAME == 'master'){
        sh 'docker build -t employee --no-cache .'
        sh 'docker run -d -p 4000:4000 -e DB_USERNAME=admin -e DB_PASSWORD=adminxxx -e DB_NAME=employee employee'
      }
    }
  }
  catch (err) {
    throw err
  }
}
</code></pre>
<p>and my Dockerfile for those jobs</p>
<pre><code>FROM node:carbon

RUN apt-get update
RUN apt-get upgrade -y
RUN apt-get -y install autoconf automake libtool nasm make pkg-config git apt-utils

RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

RUN npm -v
RUN node -v

COPY ./server/ /usr/src/app

RUN npm install

EXPOSE 4000

ENV PORT 4000
ENV DB_USERNAME admin
ENV DB_PASSWORD adminxxx
ENV DB_NAME employee

CMD [ &quot;npm&quot;, &quot;run&quot;, &quot;dev&quot; ]
</code></pre>
<p>the jenkins job build it successfully and on the last stage my Jenkins, u can see that i run it on my docker container on my ubuntu server, after that finish, i tried to call server API on postman for <code>http://ip-server:4000</code> , but it was nothing response, and i did set up the firewall tcp on my ubuntu serrver though</p>
<p>how can i solve this? so after Jenkins job finish, what i want i could call that server API on my postman to test it</p>
"